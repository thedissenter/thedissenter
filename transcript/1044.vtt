WEBVTT

1
00:00:00.209 --> 00:00:02.970
Hello, everyone. Welcome to a new episode of the

2
00:00:02.970 --> 00:00:05.960
Dissenter. I'm your host, as always, Ricardo Lops, and

3
00:00:05.960 --> 00:00:08.000
today I'm, I have a return guest with me,

4
00:00:08.119 --> 00:00:11.789
Doctor David Pinsoff. And we, in our first interview,

5
00:00:11.840 --> 00:00:14.920
we talked about the alliance theory of political belief

6
00:00:14.920 --> 00:00:17.629
systems, the meaning of life and morality. I'm leaving

7
00:00:17.629 --> 00:00:19.950
a link to it in the description down below.

8
00:00:20.299 --> 00:00:23.780
And today, we're talking about a couple of these

9
00:00:23.780 --> 00:00:28.770
substack uh posts, incentives are everything and opinions are

10
00:00:29.020 --> 00:00:32.220
BS and so we're going to talk about incentives

11
00:00:32.220 --> 00:00:36.380
and opinions from an evolutionary perspective. So, David, welcome

12
00:00:36.380 --> 00:00:38.000
to the show. It's a huge pleasure to have

13
00:00:38.000 --> 00:00:38.139
you

14
00:00:38.139 --> 00:00:40.979
back. Thanks, Ricardo. I'm glad to be back, uh,

15
00:00:41.099 --> 00:00:43.060
as always, a huge fan of the podcast and

16
00:00:43.060 --> 00:00:44.259
really appreciate what you're doing.

17
00:00:44.919 --> 00:00:49.189
Thank you. So let's start with the, with incentives

18
00:00:49.189 --> 00:00:52.630
are everything then and there you start by claiming

19
00:00:52.630 --> 00:00:57.830
that behavior is determined by incentives. So what do

20
00:00:57.830 --> 00:01:01.869
you mean by that and what are incentives basically?

21
00:01:02.389 --> 00:01:05.480
Yeah, so the conventional way in which we understand

22
00:01:05.480 --> 00:01:09.169
incentives is in terms of money. So economists talk

23
00:01:09.169 --> 00:01:13.569
about incentives, uh, you know, uh, working for a

24
00:01:13.569 --> 00:01:16.089
particular raise or getting a bonus or companies can

25
00:01:16.089 --> 00:01:20.089
offer incentives like discounts. Um, THAT'S one of the

26
00:01:20.089 --> 00:01:22.970
most common ways of thinking about incentives. Occasionally it's

27
00:01:22.970 --> 00:01:25.919
been broadened a bit. You sometimes hear people talk

28
00:01:25.919 --> 00:01:30.209
about social incentives, um, you sometimes hear an analogy

29
00:01:30.209 --> 00:01:35.339
to economics, uh, in the phrase prestige economy. Um,

30
00:01:35.550 --> 00:01:41.180
AND I advocate an extreme broadening of the concept

31
00:01:41.629 --> 00:01:45.290
beyond what has been conventionally used. So I wanna

32
00:01:45.290 --> 00:01:47.949
talk about incentives in a way that's broader than

33
00:01:47.949 --> 00:01:52.440
money, broader than particular kinds of social rewards, praise

34
00:01:52.440 --> 00:01:56.089
and punishment. I want to refer to incentives as

35
00:01:56.089 --> 00:02:00.930
literally anything that we want. Um, AND since I'm

36
00:02:00.930 --> 00:02:04.250
an evolutionary psychologist, I think our basic motivations have

37
00:02:04.250 --> 00:02:07.769
been powerfully shaped by natural selection. So if you're

38
00:02:07.769 --> 00:02:11.199
going to be an incentive determinist like me, um,

39
00:02:11.369 --> 00:02:12.979
then I think you need to be an evolutionary

40
00:02:12.979 --> 00:02:16.449
psychologist like me, because I think evolution is really

41
00:02:16.449 --> 00:02:20.050
the only way to understand our basic motivations. I

42
00:02:20.050 --> 00:02:22.690
don't see an alternative theoretical perspective for understanding them.

43
00:02:23.490 --> 00:02:27.169
Um, SO, yeah, basically it's, it's a common sense

44
00:02:27.169 --> 00:02:29.449
view that what we do is determined by what

45
00:02:29.449 --> 00:02:31.520
we want to do, and it's just saying that,

46
00:02:31.529 --> 00:02:33.770
uh, everything we do is determined by what we

47
00:02:33.770 --> 00:02:35.009
want to do, and what we want to do

48
00:02:35.009 --> 00:02:36.970
is ultimately determined by natural selection.

49
00:02:38.410 --> 00:02:41.110
And so here when it comes to the bit

50
00:02:41.110 --> 00:02:44.190
about what we want to do and it being

51
00:02:44.190 --> 00:02:49.750
determined, uh, are you claiming then that there's no

52
00:02:49.750 --> 00:02:52.929
space for free will here? Or is there?

53
00:02:53.550 --> 00:02:55.509
It depends, it depends on on your definition of

54
00:02:55.509 --> 00:02:58.210
free will. So if you think of free will

55
00:02:58.210 --> 00:03:00.850
as being able to do things uh that we're

56
00:03:00.850 --> 00:03:02.929
not motivated to do, or that we have no

57
00:03:02.929 --> 00:03:05.270
incentive to do, then yes, there's no free will.

58
00:03:05.729 --> 00:03:07.009
Um, BUT if you, but I think there are

59
00:03:07.009 --> 00:03:09.449
ways of thinking of of free will that are

60
00:03:09.449 --> 00:03:12.009
consistent with incentive determinism. You could just think of

61
00:03:12.009 --> 00:03:14.770
free will as the ability to respond to your

62
00:03:14.770 --> 00:03:17.880
incentives, um, and that makes a lot of sense

63
00:03:17.880 --> 00:03:21.770
of a lot of our punitive, uh, institutions and

64
00:03:21.770 --> 00:03:26.800
intuitions. Um, IF I am incapable of responding to

65
00:03:26.800 --> 00:03:29.000
my incentives, then it really makes no sense to

66
00:03:29.000 --> 00:03:31.080
punish me, because the whole point of punishing me

67
00:03:31.080 --> 00:03:34.520
is to incentivize me to behave differently. On the

68
00:03:34.520 --> 00:03:36.949
other hand, if I am responsive to my incentives,

69
00:03:37.240 --> 00:03:39.240
uh, and when I made my decision, I was

70
00:03:39.240 --> 00:03:42.070
aware of the potential, uh, costs of doing so

71
00:03:42.070 --> 00:03:43.839
and the potential punishments, then it makes a lot

72
00:03:43.839 --> 00:03:46.229
of sense to punish me because you are, uh,

73
00:03:46.240 --> 00:03:48.830
uh, incentivizing me to act differently in the future,

74
00:03:48.940 --> 00:03:51.350
and you're incentivizing other people who are like me,

75
00:03:51.679 --> 00:03:54.750
uh, to act differently. In the future. So, I

76
00:03:54.750 --> 00:03:58.100
think uh our notion of free will corresponds pretty

77
00:03:58.100 --> 00:04:02.139
well to an ability to respond to incentives, um,

78
00:04:02.149 --> 00:04:05.029
and I think a lot of our intuitions and

79
00:04:05.029 --> 00:04:08.470
practices surrounding free will can be successfully reframed in

80
00:04:08.470 --> 00:04:10.589
those terms. There are some other intuitions that might

81
00:04:10.589 --> 00:04:13.929
not translate. Very well, like the this idea that

82
00:04:13.929 --> 00:04:15.809
we're, you know, we exist outside of our incentives,

83
00:04:15.850 --> 00:04:18.410
and we can defy our incentives and pull ourselves

84
00:04:18.410 --> 00:04:20.839
up by our own bootstraps or or whatever. I

85
00:04:20.839 --> 00:04:23.850
think that notion of free will is uh dumb

86
00:04:23.850 --> 00:04:25.450
for lack of a better word, and hard to

87
00:04:25.450 --> 00:04:27.619
reconcile with the view that I'm putting forward.

88
00:04:28.709 --> 00:04:31.040
But then just for people to understand a little

89
00:04:31.040 --> 00:04:33.529
bit better what we're talking about here, would you

90
00:04:33.529 --> 00:04:37.230
like to illustrate with a few examples? Give us

91
00:04:37.230 --> 00:04:39.950
perhaps 2 or 3 examples of incentives.

92
00:04:41.209 --> 00:04:43.109
Sure, I mean, there are so many, um, if,

93
00:04:43.119 --> 00:04:45.559
if there, if, if the concept is as broad

94
00:04:45.559 --> 00:04:46.739
as I want to make it, then there are,

95
00:04:47.049 --> 00:04:48.329
you know, the list is endless, but, you know,

96
00:04:48.489 --> 00:04:53.690
food, sex, uh, status, uh, comfort, health, safety, um.

97
00:04:54.100 --> 00:04:55.320
Yeah, you name it.

98
00:04:55.850 --> 00:05:00.010
So, and uh you mentioned evolutionary psychology at a

99
00:05:00.010 --> 00:05:02.730
certain point there. So where would you say our

100
00:05:02.730 --> 00:05:05.739
incentives stem from? Where do they come from?

101
00:05:06.089 --> 00:05:07.450
Uh, SO yeah, as I said before, I think

102
00:05:07.450 --> 00:05:09.730
they come from natural selection. I don't see an

103
00:05:09.730 --> 00:05:13.209
alternative theory for, for where they come from, which

104
00:05:13.209 --> 00:05:16.130
means that, uh, our basic motivations, uh, and I'm

105
00:05:16.130 --> 00:05:18.730
not talking about our instrumental motivations, uh, it's actually,

106
00:05:18.769 --> 00:05:20.079
it might be helpful to to back up and

107
00:05:20.089 --> 00:05:22.750
and talk about the distinction. So, uh, the things

108
00:05:23.000 --> 00:05:25.720
that we, uh, basically want are things we want

109
00:05:25.720 --> 00:05:28.119
sort of in and of themselves intrinsically, uh, for

110
00:05:28.119 --> 00:05:30.480
their own sake, and, uh, you could think about

111
00:05:30.480 --> 00:05:34.829
another set of desires as instrumental desires. Um, uh,

112
00:05:34.920 --> 00:05:38.359
WE want, uh, say money because it can buy

113
00:05:38.359 --> 00:05:40.950
us. Food. So the desire for food would be

114
00:05:40.950 --> 00:05:44.029
basic, it would be from evolution, and the desire

115
00:05:44.029 --> 00:05:46.470
for money would be instrumental. We only want money

116
00:05:46.470 --> 00:05:49.070
because it can buy us things like food. If

117
00:05:49.070 --> 00:05:50.709
the if the money can no longer buy us

118
00:05:50.709 --> 00:05:53.380
things like food, then we stop wanting money, whereas

119
00:05:53.380 --> 00:05:56.929
um we're never gonna stop wanting food. Uh, BECAUSE

120
00:05:56.929 --> 00:05:59.489
that's just built into us. Unless, of course, it

121
00:05:59.489 --> 00:06:02.170
conflicts with some other desire, like, for status, maybe

122
00:06:02.170 --> 00:06:05.609
I, I would fast to display my devotion to

123
00:06:05.609 --> 00:06:08.929
a particular religious community, uh, in, in, in that

124
00:06:08.929 --> 00:06:10.959
sense, you know, incentives can conflict with one another,

125
00:06:11.170 --> 00:06:13.010
but you're never going to make one incentive go

126
00:06:13.010 --> 00:06:14.809
away. You're never going to make your hunger, for

127
00:06:14.809 --> 00:06:15.929
example, go away.

128
00:06:16.809 --> 00:06:20.649
Yeah. So where you come from in terms of

129
00:06:20.649 --> 00:06:26.649
your psychological framework, even if cross culturally, apparently, we

130
00:06:26.649 --> 00:06:30.410
can find a huge array of different kinds of

131
00:06:30.410 --> 00:06:34.609
incentives, would you say that those are simply different

132
00:06:34.609 --> 00:06:40.929
cultural manifestations of the same set of underlying evolved

133
00:06:40.929 --> 00:06:42.769
um motivations?

134
00:06:43.470 --> 00:06:45.679
Yes, I would say so. I think a large

135
00:06:45.679 --> 00:06:48.170
part of cultural variation comes from what I would

136
00:06:48.170 --> 00:06:51.410
call status games, um, and I argue in a

137
00:06:51.410 --> 00:06:53.570
post called Status is Weird, that status games are

138
00:06:53.570 --> 00:06:57.450
very dynamic, uh, and constantly shifting, um, because of

139
00:06:57.450 --> 00:07:00.929
the paradoxical nature of status, that is, overtly seeking

140
00:07:00.929 --> 00:07:04.040
status often lowers your status, and for that reason,

141
00:07:04.130 --> 00:07:06.130
becoming aware that we're playing a status game can

142
00:07:06.130 --> 00:07:08.529
often make that status game collapse, or at least

143
00:07:08.529 --> 00:07:12.200
significantly invert or rearrange the social hierarchy, where those

144
00:07:12.200 --> 00:07:15.154
at the Top are the iciest, uh, most vainglorious,

145
00:07:15.234 --> 00:07:18.524
narcissistic, selfish status seekers, and those at the bottom

146
00:07:18.755 --> 00:07:22.304
are more uh humble and authentic. So, uh, uh,

147
00:07:22.315 --> 00:07:26.315
recognizing a status game can often invert, uh, the

148
00:07:26.315 --> 00:07:29.274
payoff structure of the game. Um, SO because of

149
00:07:29.274 --> 00:07:32.274
that dynamism, that status games are constantly collapsing and

150
00:07:32.274 --> 00:07:34.845
re-emerging in antithetical forms, you get lots of cultural

151
00:07:34.845 --> 00:07:38.035
variation in what gets you status and what lowers

152
00:07:38.035 --> 00:07:41.350
your status. Um, SO a big part of cultural

153
00:07:41.350 --> 00:07:44.390
variation in our incentive structures are just different ways

154
00:07:44.390 --> 00:07:47.040
that we can get status, different strategies we pursue

155
00:07:47.040 --> 00:07:48.720
for getting status. What gets you status in one

156
00:07:48.720 --> 00:07:50.799
place is gonna be different from what gets you

157
00:07:50.799 --> 00:07:53.480
status in a different place, but the basic incentive

158
00:07:53.480 --> 00:07:54.679
of status is the same.

159
00:07:56.410 --> 00:08:01.059
Right. Uh, AND are there occasions where conflicts or

160
00:08:01.059 --> 00:08:05.179
conflict, not conflict, sorry, where incentives might conflict with

161
00:08:05.179 --> 00:08:07.980
one another and if so, how do we deal

162
00:08:07.980 --> 00:08:10.260
with conflicting incentives?

163
00:08:10.869 --> 00:08:13.200
Yeah, absolutely. So if incentives are just things we

164
00:08:13.200 --> 00:08:15.510
want, then obviously, you know, things that we want

165
00:08:15.510 --> 00:08:18.040
can conflict with each other. We experience this almost

166
00:08:18.040 --> 00:08:21.570
every day. Um, EXERCISING, for example, uh we might

167
00:08:21.959 --> 00:08:25.279
uh want the long term health benefits of exercise,

168
00:08:25.320 --> 00:08:28.380
but the short term discomfor. Uh, MIGHT be unpleasant,

169
00:08:28.579 --> 00:08:30.059
but if we care more about the long term

170
00:08:30.059 --> 00:08:32.900
health benefits, then, uh, that's going to outweigh the

171
00:08:32.900 --> 00:08:36.619
short term, uh, discomfort of doing it. Um, LIKEWISE,

172
00:08:36.630 --> 00:08:39.820
we could be internally conflicted about doing unethical things

173
00:08:39.820 --> 00:08:43.010
like lying or or cheating on our spouses, um,

174
00:08:43.099 --> 00:08:44.340
and in that case, there will be a tug

175
00:08:44.340 --> 00:08:47.739
of war between the uh the incentive to behave

176
00:08:47.739 --> 00:08:51.440
honestly or ethically and the self-interested in incentive to

177
00:08:51.440 --> 00:08:55.200
cheat. Uh, OR, or steal or lie, or whatever.

178
00:08:55.419 --> 00:08:57.169
Um, AND I think we resolve those sorts of

179
00:08:57.169 --> 00:09:00.599
conflicts in a pretty straightforward way. I think we,

180
00:09:00.650 --> 00:09:04.210
we think about the different outcomes that will result

181
00:09:04.210 --> 00:09:07.210
if we take either choice, uh, we play out

182
00:09:07.210 --> 00:09:10.049
the scenarios in our head. So ultimately, uh, the

183
00:09:10.049 --> 00:09:11.960
way we resolve these kinds of trade-offs, I think,

184
00:09:11.969 --> 00:09:15.409
is that the stronger incentive wins. It's pretty straightforward.

185
00:09:15.489 --> 00:09:17.599
The one, the the the the more powerful motivation

186
00:09:17.599 --> 00:09:19.609
is going to be the one that moves you

187
00:09:19.609 --> 00:09:20.409
more effectively.

188
00:09:21.960 --> 00:09:26.710
Uh, OK, but what makes an incentive more powerful

189
00:09:26.710 --> 00:09:28.760
than another? I mean, does it have to do

190
00:09:28.760 --> 00:09:31.489
with the kinds of incentives we might be considering?

191
00:09:31.520 --> 00:09:33.960
Does it have anything to do with the context?

192
00:09:34.090 --> 00:09:35.719
I don't know if you want to illustrate that

193
00:09:35.719 --> 00:09:37.210
with, with an example.

194
00:09:38.320 --> 00:09:39.960
Sure, I mean, a, a big part of it

195
00:09:39.960 --> 00:09:43.109
is probably just random genetic variation. So we all

196
00:09:43.109 --> 00:09:46.880
have these basic set of evolved motives for food,

197
00:09:47.000 --> 00:09:50.799
sex, status, whatever, um, and just by random roll

198
00:09:50.799 --> 00:09:53.599
of the genetic dice, you know, our, uh, weightings

199
00:09:53.599 --> 00:09:55.840
of these different motivations will be a bit stronger

200
00:09:55.840 --> 00:09:57.760
in some of us than in others. Some of

201
00:09:57.760 --> 00:10:00.239
us will be more curious or horny or hungry

202
00:10:00.239 --> 00:10:03.000
than others. Some of us will be more narcissistic

203
00:10:03.000 --> 00:10:07.359
and inglorious than others, just randomly. Um, SO for

204
00:10:07.359 --> 00:10:10.200
that reason, which incentives loom loom larger for any

205
00:10:10.200 --> 00:10:13.119
particular individual is going to depend on their, uh,

206
00:10:13.130 --> 00:10:17.549
particular, uh, genetic constellation of traits. Um, AND then

207
00:10:17.549 --> 00:10:20.229
there's also, you know, environmental and and cultural differences

208
00:10:20.229 --> 00:10:23.390
about the strength of different incentives. Um, THERE could

209
00:10:23.390 --> 00:10:26.510
be stronger status rewards and punishments in one culture

210
00:10:26.510 --> 00:10:29.429
than in another, uh, in one culture, uh, speaking

211
00:10:29.429 --> 00:10:31.789
out against the dominant narrative could get you burned

212
00:10:31.789 --> 00:10:33.869
at the stake, and another it could merely get

213
00:10:33.869 --> 00:10:36.630
you canceled, and another it could, uh, merely get

214
00:10:36.630 --> 00:10:40.039
you frowned at. Um, THOSE kinds of cultural differences

215
00:10:40.039 --> 00:10:42.669
are going to determine the strength of the incentive.

216
00:10:42.880 --> 00:10:45.320
In that case, there could be ecological differences in

217
00:10:45.320 --> 00:10:48.359
some cultures, um, pathogens might be more prevalent and

218
00:10:48.359 --> 00:10:51.119
dangerous, so the incentives of behaving in a hygienic

219
00:10:51.119 --> 00:10:53.200
way are going to be stronger in those cultures.

220
00:10:53.520 --> 00:10:56.400
Um, POORER cultures are going to have different incentive

221
00:10:56.400 --> 00:11:00.880
structures, uh, surrounding, uh, institutions of uh reciprocity and

222
00:11:00.880 --> 00:11:04.239
risk pooling that reduce the risk of, uh, starvation

223
00:11:04.239 --> 00:11:08.229
or deprivation, um. Yeah, stuff like that. That makes

224
00:11:08.229 --> 00:11:08.539
sense.

225
00:11:08.609 --> 00:11:11.669
Mhm. Yes, yes, it makes perfect sense. And one

226
00:11:11.669 --> 00:11:14.950
very interesting example of an incentive that I guess

227
00:11:14.950 --> 00:11:18.909
we have in all human societies, of course, it

228
00:11:18.909 --> 00:11:22.549
has different, it manifests in different ways, but I

229
00:11:22.549 --> 00:11:25.390
imagine it should be present in all human societies

230
00:11:25.390 --> 00:11:28.599
is that we feel the Need or we feel

231
00:11:28.599 --> 00:11:32.760
they want to look virtuous to other people. So,

232
00:11:32.929 --> 00:11:35.849
uh, could you explain that? First of all, what

233
00:11:35.849 --> 00:11:40.849
does virtuosity mean, particularly from an evolutionary perspective and

234
00:11:40.849 --> 00:11:45.289
then why do we want to look virtuous, virtuous

235
00:11:45.289 --> 00:11:46.090
to other people?

236
00:11:47.020 --> 00:11:52.059
Sure, um, I think of virtue, uh, as roughly

237
00:11:52.059 --> 00:11:55.630
desirability as a social partner. I think our concept

238
00:11:55.630 --> 00:11:59.380
of virtue probably refers more to the cooperative aspects

239
00:11:59.380 --> 00:12:02.500
of social relationships. So if I'm going to be

240
00:12:02.500 --> 00:12:05.619
fair with you in dividing resources, I'm going to

241
00:12:05.619 --> 00:12:07.179
be honest with you and not lie to you,

242
00:12:07.219 --> 00:12:10.320
or cheat you or manipulate you. I'm going to

243
00:12:10.320 --> 00:12:12.979
be kind to you and and generous to you,

244
00:12:13.270 --> 00:12:17.099
uh, and share resources with you. Those are those

245
00:12:17.099 --> 00:12:19.299
are generally the types of things that we talk

246
00:12:19.299 --> 00:12:21.969
about when we talk about virtue. So I think

247
00:12:21.969 --> 00:12:25.830
of virtue as as your your value as a

248
00:12:25.830 --> 00:12:29.789
cooperative partner. Um, AND because we live in a

249
00:12:29.789 --> 00:12:33.030
social marketplace where we get to choose between potential

250
00:12:33.030 --> 00:12:36.590
cooperative partners and invest in some cooperative relationships over

251
00:12:36.590 --> 00:12:41.150
other cooperative relationships, it behooves us to signal our

252
00:12:41.150 --> 00:12:43.469
value as a cooperative partner, so that people will

253
00:12:43.469 --> 00:12:46.765
want to with us and not partner with our

254
00:12:46.765 --> 00:12:50.284
rivals. Um, SO we are constantly competing to display

255
00:12:50.284 --> 00:12:53.125
our virtue, um, not just in an absolute sense,

256
00:12:53.164 --> 00:12:54.835
but in a relative sense, that is, we want

257
00:12:54.835 --> 00:12:57.405
to look more virtuous than our competitors in the

258
00:12:57.405 --> 00:12:59.815
social marketplace, so there is a competition for moral

259
00:12:59.815 --> 00:13:02.965
superiority, uh, and we find tons of clever and

260
00:13:02.965 --> 00:13:06.255
creative ways to signal our virtue to one another,

261
00:13:06.315 --> 00:13:08.684
and in turn to see through those signals as

262
00:13:08.684 --> 00:13:09.724
bullshit occasionally.

263
00:13:11.140 --> 00:13:15.820
Would this constitute an example of a stereo game

264
00:13:15.820 --> 00:13:16.169
then?

265
00:13:17.010 --> 00:13:19.369
Yeah, I think virtue, virtue games are a kind

266
00:13:19.369 --> 00:13:22.010
of status game. Um, uh, Will Storr has a

267
00:13:22.010 --> 00:13:23.770
wonderful book where he talks about status games that

268
00:13:23.770 --> 00:13:26.919
he carves it up into three different categories, uh,

269
00:13:27.809 --> 00:13:30.969
success games, uh, virtue games, and dominance games. Uh,

270
00:13:31.010 --> 00:13:32.409
I think that's a pretty good way of, of

271
00:13:32.409 --> 00:13:33.929
carving it up. I might add a few more

272
00:13:33.929 --> 00:13:37.210
like surrounding, um, say epistemic status games, I think

273
00:13:37.210 --> 00:13:39.849
is another one, but yeah, I think that's a

274
00:13:39.849 --> 00:13:40.849
pretty good way to carve it up.

275
00:13:41.950 --> 00:13:45.609
Epistemic status games, what does that mean exactly? Would

276
00:13:45.609 --> 00:13:48.619
that be something that you find among, for example,

277
00:13:48.750 --> 00:13:50.780
intellectuals and academics?

278
00:13:51.549 --> 00:13:55.070
Yeah, I think so. So, um, we trust certain

279
00:13:55.070 --> 00:13:57.510
sources of information more than others. We deem them

280
00:13:57.510 --> 00:14:02.520
more credible, more reliable, more trustworthy. Um, AND there

281
00:14:02.520 --> 00:14:05.479
are huge, uh, benefits to being seen as a

282
00:14:05.479 --> 00:14:08.239
credible and reliable source of information. There are benefits

283
00:14:08.239 --> 00:14:11.309
to be seen to being seen as reasonable and

284
00:14:11.440 --> 00:14:15.280
rational, um, and so a lot of the status

285
00:14:15.280 --> 00:14:18.640
games that intellectuals play with one another are competitions

286
00:14:18.640 --> 00:14:23.119
to be more reasonable, um, more thoughtful, um, more

287
00:14:23.119 --> 00:14:28.320
epistemically credible and reliable, more meticulous. Um, THOSE, those

288
00:14:28.320 --> 00:14:31.950
types of competitions. So there's, there's actually surprising scarcity

289
00:14:32.200 --> 00:14:34.880
of, uh, academic literature on epistemic status games. I,

290
00:14:34.919 --> 00:14:36.679
I feel like there should be tons more on

291
00:14:36.679 --> 00:14:39.609
this topic, um, but I think it's because it's

292
00:14:39.609 --> 00:14:41.559
the water that intellectuals swim in that they have

293
00:14:41.559 --> 00:14:42.640
a hard time seeing it.

294
00:14:44.419 --> 00:14:47.010
So, uh, one thing that you mentioned in your,

295
00:14:47.099 --> 00:14:50.099
in your, not, not your paper, your post is

296
00:14:50.099 --> 00:14:54.219
that when trying to look virtues at the same

297
00:14:54.219 --> 00:14:57.890
time, we have to seem humble. So, how do

298
00:14:57.890 --> 00:15:00.820
we go about doing that? What kinds of strategies

299
00:15:00.820 --> 00:15:01.580
do we employ?

300
00:15:02.770 --> 00:15:04.390
Yeah, I mean, it's, it's pretty hard to do,

301
00:15:04.520 --> 00:15:07.119
uh, as you can imagine, to look virtuous without

302
00:15:07.119 --> 00:15:09.950
looking like you were trying to look virtuous. Uh

303
00:15:09.950 --> 00:15:12.640
IT'S a pretty difficult hoop to jump through, um,

304
00:15:12.760 --> 00:15:14.880
but we find all sorts of clever and creative

305
00:15:14.880 --> 00:15:18.270
ways of jumping through it. Um, THERE'S a hilarious

306
00:15:18.270 --> 00:15:21.919
episode of Curb Enthusiasm where Ted Danson uh donates

307
00:15:21.919 --> 00:15:24.070
anonymously, I think it was to an art museum,

308
00:15:24.440 --> 00:15:26.440
uh, and then he proceeds to tell everyone that

309
00:15:26.440 --> 00:15:29.659
he was the person who donated anonymously. Uh, THEREBY

310
00:15:29.659 --> 00:15:33.369
obviating his anonymous donation, and yet he still got

311
00:15:33.739 --> 00:15:36.140
tons of status, and he got extra status for

312
00:15:36.140 --> 00:15:39.539
donating anonymously. And by that, by that logic, he,

313
00:15:39.640 --> 00:15:43.859
uh, upstaged his rival, Larry David, who donated conspicuously

314
00:15:43.859 --> 00:15:45.619
by putting his name on the wing. And so

315
00:15:45.619 --> 00:15:48.169
Ted Danson ended up gaining more status than Larry,

316
00:15:48.340 --> 00:15:50.580
because he donated anonymously and then told everyone that

317
00:15:50.580 --> 00:15:54.010
he donated anonymously. Um, SO that's one type of

318
00:15:54.010 --> 00:15:56.489
maneuver we might make is we might, uh, do

319
00:15:56.489 --> 00:15:59.719
good deeds anonymously, or at least discreetly or inconspicuously,

320
00:15:59.789 --> 00:16:01.960
and hope that some people pick up on that,

321
00:16:02.030 --> 00:16:03.539
and the people who do pick up on that

322
00:16:03.539 --> 00:16:05.729
will give us extra status for having done it

323
00:16:05.729 --> 00:16:09.140
inconspicuously, or anonymously or discreetly.

324
00:16:11.099 --> 00:16:15.200
And do incentives tie in any way to morality?

325
00:16:15.299 --> 00:16:19.119
What does it mean to be good or bad?

326
00:16:20.179 --> 00:16:23.380
Yeah, no, I think uh morality is just another

327
00:16:23.380 --> 00:16:27.299
kind of incentive structure. I think morality can be

328
00:16:27.299 --> 00:16:30.549
pretty cleanly reduced to the virtue game, the competition

329
00:16:30.549 --> 00:16:33.440
to look virtuous and fair and trustworthy. Um, IT

330
00:16:33.440 --> 00:16:35.429
might not exhaust all of morality, but I think

331
00:16:35.429 --> 00:16:37.549
it's a pretty good chunk of it is just

332
00:16:37.549 --> 00:16:41.630
competing to look like a virtuous person, uh, and

333
00:16:41.630 --> 00:16:44.630
obviously we, we compete in, in, in ways that

334
00:16:44.630 --> 00:16:47.109
try to show that we don't care about competing

335
00:16:47.309 --> 00:16:50.280
for virtue, uh, as, as I just discussed. Um,

336
00:16:50.450 --> 00:16:52.419
BUT yeah, I think, I think of morality as,

337
00:16:52.429 --> 00:16:54.849
as largely a kind of virtue game.

338
00:16:56.419 --> 00:17:00.340
And do we have any conscious access to all

339
00:17:00.340 --> 00:17:03.739
of this information? I mean, do we understand or

340
00:17:03.739 --> 00:17:07.339
do we have conscious access to the fact that

341
00:17:07.339 --> 00:17:11.459
we are behaving this way with this kind of

342
00:17:11.459 --> 00:17:14.410
motives behind our behavior or not?

343
00:17:15.170 --> 00:17:18.130
Absolutely not. Yeah, no, um, in fact, not only

344
00:17:18.130 --> 00:17:20.489
are we uh unaware of it, I think it

345
00:17:20.489 --> 00:17:23.969
is often, uh, detrimental to us to become aware

346
00:17:23.969 --> 00:17:27.089
of it. Um, SO, as I mentioned earlier, becoming

347
00:17:27.089 --> 00:17:28.650
aware of the fact that we're playing a status

348
00:17:28.650 --> 00:17:32.329
game can sometimes make that status game collapse and

349
00:17:32.329 --> 00:17:37.810
invert, uh, the existing hierarchy. Um, SO being aware

350
00:17:37.810 --> 00:17:42.319
of our status seeking, um, Can often uh hinder

351
00:17:42.319 --> 00:17:45.000
our desire for status. Uh, IT can lower our

352
00:17:45.000 --> 00:17:46.939
status insofar as we are seen as a status

353
00:17:46.939 --> 00:17:49.640
seeker. And so, in order to deceive other people

354
00:17:49.640 --> 00:17:51.760
that we don't really care about status, we often

355
00:17:51.760 --> 00:17:54.520
deceive ourselves in order to more effectively deceive others,

356
00:17:54.680 --> 00:17:56.520
or in our attempt to deceive others, we often

357
00:17:56.520 --> 00:17:59.800
inadvertently deceive ourselves as well. So I think um

358
00:17:59.800 --> 00:18:02.199
a lot of these things go on in the

359
00:18:02.199 --> 00:18:06.800
dark, covertly. Um, SO, uh, we convince ourselves through

360
00:18:06.800 --> 00:18:08.959
sacred narratives that what we're really pursuing is some

361
00:18:08.959 --> 00:18:13.420
sacred value. Um, LIKE knowledge or beauty or authenticity,

362
00:18:13.489 --> 00:18:17.000
or equality or self-actualization, or something that sounds lovely,

363
00:18:17.469 --> 00:18:19.630
um, and we convince ourselves that that's the thing

364
00:18:19.630 --> 00:18:22.469
we're pursuing, not status. And, and, and then we

365
00:18:22.469 --> 00:18:24.869
compete to show each other that we care more

366
00:18:24.869 --> 00:18:28.219
about the sacred value than than any other selfish

367
00:18:28.219 --> 00:18:31.030
pursuit, and the competition is itself a kind of

368
00:18:31.030 --> 00:18:33.069
status competition, but nobody knows that it's a status

369
00:18:33.069 --> 00:18:35.349
competition. Um, AND if they did know, then it

370
00:18:35.349 --> 00:18:38.719
would, it would collapse, uh, and, um. It would

371
00:18:38.719 --> 00:18:41.170
be most damaging to those who are winning in

372
00:18:41.170 --> 00:18:43.280
the status game, or who have achieved high status

373
00:18:43.280 --> 00:18:45.880
already, and it might be most beneficial to those

374
00:18:45.880 --> 00:18:48.680
who are uh lower status uh in the game

375
00:18:48.680 --> 00:18:50.400
and feel like they're not getting the status they're

376
00:18:50.400 --> 00:18:52.880
entitled to, and that can give rise to strategic

377
00:18:52.880 --> 00:18:55.880
cynicism whereby people try to make a status game

378
00:18:55.880 --> 00:18:58.709
collapse in order to increase their relative standing, uh,

379
00:18:58.719 --> 00:19:01.510
and put others down. Uh, THIS all gets very

380
00:19:01.510 --> 00:19:04.040
complicated as you might imagine, um, but that's just

381
00:19:04.040 --> 00:19:05.400
a taste of of the way I'm thinking about

382
00:19:05.400 --> 00:19:05.560
it.

383
00:19:06.510 --> 00:19:10.469
But even if we don't have conscious access to

384
00:19:10.469 --> 00:19:14.229
all of these underlying motives and what goes around

385
00:19:14.229 --> 00:19:17.439
in our psychology to produce the kinds of behaviors

386
00:19:17.439 --> 00:19:20.880
that we exhibit, I guess that we still need

387
00:19:21.199 --> 00:19:25.790
explanations for our behavior. I guess that even primarily

388
00:19:25.790 --> 00:19:30.280
because we also need to sometimes explain our own

389
00:19:30.280 --> 00:19:33.650
behavior and our motives to other people. As other

390
00:19:33.650 --> 00:19:37.449
people need explanations, but also I guess that even

391
00:19:37.449 --> 00:19:41.369
psychologically speaking, for our, for our own, for our

392
00:19:41.369 --> 00:19:45.050
own psychological well-being, we also need to sort of

393
00:19:45.050 --> 00:19:49.170
integrate things within a personal narrative of some kind.

394
00:19:49.250 --> 00:19:51.290
I, I mean, do you agree with that? And

395
00:19:51.489 --> 00:19:55.329
if so, uh, I mean, those kinds of narratives

396
00:19:55.329 --> 00:19:59.530
that we might create, what functions do they serve?

397
00:20:00.359 --> 00:20:02.430
Yeah, so one function, as I mentioned, is, is

398
00:20:02.430 --> 00:20:05.630
just keeping our status games stable, keeping them from

399
00:20:05.630 --> 00:20:09.630
collapsing or inverting, um, and that is often more

400
00:20:09.630 --> 00:20:12.510
in the interests of higher status people than lower

401
00:20:12.510 --> 00:20:15.709
status or embittered people, um, which is why you

402
00:20:15.709 --> 00:20:18.910
might expect lower status or embittered people to favor

403
00:20:18.910 --> 00:20:22.589
more cynical and nihilistic ideas that challenge the prevailing

404
00:20:22.589 --> 00:20:26.410
sacred narratives. Um, SORRY, uh, uh, uh, could you

405
00:20:26.410 --> 00:20:28.219
repeat the question because I feel like I got

406
00:20:28.709 --> 00:20:29.349
sidetracked.

407
00:20:30.030 --> 00:20:32.670
Yeah, no, no problem. So I was asking you

408
00:20:32.670 --> 00:20:35.750
basically if the kinds of narratives that we feel

409
00:20:35.750 --> 00:20:38.630
the need to elaborate to tell other people about

410
00:20:38.630 --> 00:20:41.160
our own behavior and the motives behind. And it

411
00:20:41.160 --> 00:20:44.319
tend also to tell ourselves to, to, I don't

412
00:20:44.319 --> 00:20:49.119
know, psychological wellbeing for for psychological well-being or something

413
00:20:49.119 --> 00:20:53.520
like that. What kinds of functions do those narratives

414
00:20:53.520 --> 00:20:53.880
serve?

415
00:20:54.160 --> 00:20:55.439
Got it. What are the functions? Yeah, so yeah,

416
00:20:55.520 --> 00:20:57.719
one of them is stabilizing our status games, um.

417
00:20:58.609 --> 00:21:03.890
Another is just um gaining status itself. So um

418
00:21:03.890 --> 00:21:07.680
if I do something unethical or unflattering, uh it

419
00:21:07.680 --> 00:21:10.530
is in my interest to try to paint what

420
00:21:10.530 --> 00:21:14.260
I did in the most flattering possible terms. Um,

421
00:21:14.310 --> 00:21:16.430
AND in my efforts to convince other people that

422
00:21:16.430 --> 00:21:18.739
what I did wasn't so bad, I might convince

423
00:21:18.739 --> 00:21:22.469
myself as a tactic for more effectively convincing others

424
00:21:22.469 --> 00:21:24.469
in the same way that an attorney might practice

425
00:21:24.469 --> 00:21:26.900
their opening statement in front of a mirror, uh,

426
00:21:26.949 --> 00:21:29.640
before they enter the courtroom. We often practice our

427
00:21:29.640 --> 00:21:32.219
own opening statements in our in in our minds,

428
00:21:32.229 --> 00:21:34.145
in the privacy. Of our own heads, that's what

429
00:21:34.145 --> 00:21:36.625
a lot of our fantasies and ruminations and obsessions

430
00:21:36.625 --> 00:21:38.344
are about, just playing out what we're gonna say

431
00:21:38.344 --> 00:21:42.025
in our heads. Um, AND uh we in doing

432
00:21:42.025 --> 00:21:44.505
that often convince ourselves of all sorts of things,

433
00:21:44.545 --> 00:21:47.625
then helps us to more effectively convince others if

434
00:21:47.625 --> 00:21:49.905
and when we were ever put on the stand

435
00:21:49.905 --> 00:21:53.025
or put on trial, so to speak. So that's

436
00:21:53.025 --> 00:21:55.625
another function of them. Um, THERE are probably other

437
00:21:55.625 --> 00:21:57.944
functions, I think coordination may be one of them.

438
00:21:58.260 --> 00:22:00.640
Um, ALL of us, uh, getting on the same

439
00:22:00.640 --> 00:22:04.359
page, um, status games themselves depend on coordination. We

440
00:22:04.359 --> 00:22:06.560
all need to agree on what is high status

441
00:22:06.560 --> 00:22:09.359
or what is low status, um, and you could

442
00:22:09.359 --> 00:22:13.040
imagine that being pretty damaging if I'm, uh, discoordinated

443
00:22:13.040 --> 00:22:14.839
with other people about that. If I end up

444
00:22:14.839 --> 00:22:19.119
praising and admiring a social pariah, or if I

445
00:22:19.119 --> 00:22:22.949
end up dissing and insulting a widely regarded, uh,

446
00:22:22.959 --> 00:22:25.020
person. Um, THAT is going to be very bad

447
00:22:25.020 --> 00:22:26.969
for me. So we all need to coordinate on

448
00:22:26.979 --> 00:22:29.540
what the rules of our status game, uh, are.

449
00:22:30.380 --> 00:22:33.400
Um, SO status games are largely a coordination problem,

450
00:22:33.489 --> 00:22:37.560
and I think lots of um myths and narratives

451
00:22:37.560 --> 00:22:42.349
are also, uh, uh, coordination strategies. Um, OFTEN, uh,

452
00:22:42.359 --> 00:22:44.280
it is more important for us to coordinate with

453
00:22:44.280 --> 00:22:46.160
one another and get on the same page with

454
00:22:46.160 --> 00:22:48.959
one another, uh, than it is to coordinate with

455
00:22:48.959 --> 00:22:50.229
reality itself.

456
00:22:51.550 --> 00:22:54.369
But even if we don't have a conscious access

457
00:22:54.369 --> 00:22:58.880
to our own underlying incentives, when we think about,

458
00:22:58.949 --> 00:23:01.969
for example, people we like versus people we don't

459
00:23:01.969 --> 00:23:06.689
like or uh another example would be political parties

460
00:23:06.689 --> 00:23:09.275
that we like. Like versus political parties that are

461
00:23:09.275 --> 00:23:12.665
opposed to them, for example, we tend to frame

462
00:23:12.665 --> 00:23:15.964
things in terms of incentives, right? So, and how

463
00:23:15.964 --> 00:23:18.944
do we go about doing that when it comes

464
00:23:18.944 --> 00:23:22.994
to what we like versus what we don't like.

465
00:23:23.589 --> 00:23:26.099
Yeah, so when, when we talk about likable people

466
00:23:26.099 --> 00:23:28.729
or unlikable people or heroes and villains, I think

467
00:23:28.979 --> 00:23:31.780
uh a lot of this incentive talk goes out

468
00:23:31.780 --> 00:23:33.920
the window, uh, cause if you think about it,

469
00:23:33.939 --> 00:23:38.290
there's a pretty basic conflict between incentive-based thinking and

470
00:23:38.339 --> 00:23:42.260
uh Manichean good versus evil thinking, uh, which is

471
00:23:42.260 --> 00:23:45.979
that, uh, it Increases your sympathy for the bad

472
00:23:45.979 --> 00:23:48.780
guys and lowers your praise and esteem for the

473
00:23:48.780 --> 00:23:50.739
good guys. If the bad guys are just bad

474
00:23:50.739 --> 00:23:52.380
because they were incentivized to be bad, well, then

475
00:23:52.380 --> 00:23:55.660
it doesn't, you know, make much sense to uh

476
00:23:55.660 --> 00:23:57.060
hate them and demonize them, or at least as

477
00:23:57.060 --> 00:24:00.500
much. Um, AND if the good guys are just

478
00:24:00.500 --> 00:24:02.510
good because they're incentivized to be good because it's

479
00:24:02.510 --> 00:24:05.459
in their self-interest to be good, well then, you

480
00:24:05.459 --> 00:24:07.339
know, you kind of lose some of your uh

481
00:24:07.339 --> 00:24:12.140
admiration for them. Um, AND because, uh, coalitions, uh,

482
00:24:12.150 --> 00:24:15.510
compete with each other by coordinating more effectively and

483
00:24:15.510 --> 00:24:18.989
thinking of themselves as, uh, superior to rival groups

484
00:24:18.989 --> 00:24:21.229
in order to bolster commitment to the group and

485
00:24:21.229 --> 00:24:24.150
prevent defection to other groups, it is often in

486
00:24:24.150 --> 00:24:28.109
coalition's interests to uh spread these kinds of self

487
00:24:28.109 --> 00:24:33.329
glorifying other demonizing narratives, um. To help them compete

488
00:24:33.329 --> 00:24:35.569
with other groups, and I think in creating those

489
00:24:35.569 --> 00:24:39.770
narratives, uh, incentives, uh, go out the window and

490
00:24:39.770 --> 00:24:42.449
in in some cases they're even t taboo to

491
00:24:42.449 --> 00:24:43.050
acknowledge.

492
00:24:44.339 --> 00:24:46.910
And you mentioned the term likable at a certain

493
00:24:46.910 --> 00:24:50.109
point there, so what makes a person likable?

494
00:24:50.430 --> 00:24:52.260
I could go on all day here. I mean,

495
00:24:52.349 --> 00:24:55.469
we are one of the judgiest, probably the judgedgiest

496
00:24:55.469 --> 00:24:58.829
species on the planet. Uh, THE number of traits

497
00:24:58.829 --> 00:25:00.939
and dimensions on which we judge each other is,

498
00:25:01.150 --> 00:25:04.939
uh, enormous and bewildering, um, but yeah, I mean,

499
00:25:05.060 --> 00:25:06.709
I could, I could go on all day, honesty,

500
00:25:06.869 --> 00:25:13.420
kindness, fairness, generosity. Uh, BRAVERY, loyalty, uh, cleverness, sense

501
00:25:13.420 --> 00:25:16.660
of humor, uh, self-awareness, uh, I, I could go

502
00:25:16.660 --> 00:25:17.140
on all day.

503
00:25:18.489 --> 00:25:21.150
But I mean, what makes a person likable at

504
00:25:21.150 --> 00:25:23.900
the end of the day, isn't it basically doing

505
00:25:24.229 --> 00:25:27.459
the right things or saying and doing the right

506
00:25:27.459 --> 00:25:29.109
things, isn't that it, basically.

507
00:25:30.030 --> 00:25:32.510
Yeah, so, uh, if you think in terms of

508
00:25:32.510 --> 00:25:35.869
the social marketplace perspective where we're uh picking and

509
00:25:35.869 --> 00:25:39.030
choosing different social partnerships to invest in, and we're

510
00:25:39.030 --> 00:25:41.109
evaluating people based on their value as a social

511
00:25:41.109 --> 00:25:45.449
partner. Then, uh, yeah, one of the main sources

512
00:25:45.449 --> 00:25:48.209
of information we attend to when evaluating social partners

513
00:25:48.209 --> 00:25:51.660
is language, is what people are saying. Um, THAT'S

514
00:25:51.660 --> 00:25:53.859
a pretty good heuristic a lot of the times

515
00:25:53.859 --> 00:25:55.880
for picking up on a lot of these cues.

516
00:25:56.050 --> 00:25:58.189
What are people talking about, uh, who are they

517
00:25:58.189 --> 00:26:02.000
displaying loyalty to, who are they displaying hostility to,

518
00:26:02.329 --> 00:26:04.489
um, what kinds of traits can be inferred based

519
00:26:04.489 --> 00:26:06.849
on the words that they use, uh, and so

520
00:26:06.849 --> 00:26:11.550
one of the, the biggest Um, sources of our

521
00:26:11.550 --> 00:26:14.550
social decision making, uh, and our social evaluations just

522
00:26:14.550 --> 00:26:16.430
come from what people say, whether they're saying the

523
00:26:16.430 --> 00:26:18.589
right things or the wrong things. And, and, you

524
00:26:18.589 --> 00:26:20.069
know, if they're saying the right things, we like

525
00:26:20.069 --> 00:26:21.310
them, if they're saying the wrong things, we don't

526
00:26:21.310 --> 00:26:23.630
like them. And that incentivizes us to say the

527
00:26:23.630 --> 00:26:25.790
right things to get people to like us, and

528
00:26:25.790 --> 00:26:28.060
to avoid saying the wrong things, uh, to avoid

529
00:26:28.060 --> 00:26:29.349
eliciting others dislike.

530
00:26:30.380 --> 00:26:33.930
And how do we determine what are the right

531
00:26:33.930 --> 00:26:35.630
things to say and do?

532
00:26:36.449 --> 00:26:42.229
Um, WELL, um, whatever, uh, wins us likability points,

533
00:26:42.420 --> 00:26:45.650
whatever gets people to like us. Um, SO, uh,

534
00:26:45.660 --> 00:26:48.780
we can, we often, um, use our own judgments

535
00:26:48.780 --> 00:26:52.219
of others to simulate how others will judge us.

536
00:26:52.459 --> 00:26:56.619
So, uh, you know, things that we will judge

537
00:26:56.619 --> 00:27:00.449
positively, uh, in other people. Um, WE will engage

538
00:27:00.449 --> 00:27:03.630
in those same behaviors ourselves. So if I, uh,

539
00:27:03.640 --> 00:27:08.119
like people who, um, say progressive things or who

540
00:27:08.119 --> 00:27:11.109
say things that are, um, supportive of progressive ideology,

541
00:27:11.560 --> 00:27:13.839
um, chances are other people are going to like

542
00:27:13.839 --> 00:27:17.280
me for saying those same progressive things, so I'm

543
00:27:17.280 --> 00:27:19.550
going to be motivated to say those things, uh,

544
00:27:19.560 --> 00:27:21.959
to, uh, get other people to like me more

545
00:27:21.959 --> 00:27:24.880
and to uh win the various status games that

546
00:27:24.880 --> 00:27:26.119
I'm playing with my peers.

547
00:27:27.989 --> 00:27:31.560
But that happens if you are included in the

548
00:27:31.560 --> 00:27:35.959
circle of mostly uh mostly progressive people, right? Because

549
00:27:35.959 --> 00:27:39.319
if it's mostly conservative, I would imagine that you're

550
00:27:39.319 --> 00:27:44.160
saying progressive things wouldn't garner you much uh status

551
00:27:44.160 --> 00:27:45.390
or likability.

552
00:27:45.599 --> 00:27:47.699
Right. Yes, absolutely. So it's gonna depend on your

553
00:27:47.699 --> 00:27:51.479
particular social environment and what particular types of words

554
00:27:51.479 --> 00:27:55.239
and deeds are rewarded in your social environment. Yeah.

555
00:27:55.989 --> 00:27:59.380
So, moving on to the other post, I would

556
00:27:59.380 --> 00:28:01.939
like to ask you about today where you talk

557
00:28:01.939 --> 00:28:05.430
about opinions and the title is opinions are bullshit.

558
00:28:05.540 --> 00:28:08.739
So, what is an opinion anyway? Because I, I

559
00:28:08.739 --> 00:28:12.339
was thinking until I read your article or, or

560
00:28:12.339 --> 00:28:15.540
your post that it would, it was just a

561
00:28:15.540 --> 00:28:18.369
preference. When we talk about opinions, we were talking,

562
00:28:18.660 --> 00:28:22.329
we would be talking about preferences, but you argue

563
00:28:22.329 --> 00:28:24.859
in a different way. So could you tell us

564
00:28:24.859 --> 00:28:25.300
about that?

565
00:28:25.859 --> 00:28:28.760
Yeah, so I think they're not just preferences. I

566
00:28:28.760 --> 00:28:31.800
think, you know, they're partly preferences. I think that's

567
00:28:31.800 --> 00:28:33.920
that's part of what's going on with our opinions,

568
00:28:34.119 --> 00:28:36.359
but it's not the whole story. um, AND that's

569
00:28:36.359 --> 00:28:38.359
just because we already know how to talk about

570
00:28:38.359 --> 00:28:41.709
our preferences. We have a whole language for talking

571
00:28:41.709 --> 00:28:43.640
about our preferences. We say, you know, I like

572
00:28:43.640 --> 00:28:45.439
this, I don't like this. I don't care for

573
00:28:45.439 --> 00:28:50.199
this. Um, SO if if opinions were just preferences,

574
00:28:50.599 --> 00:28:54.310
Then we would not need uh the language of

575
00:28:54.310 --> 00:28:56.359
opinions, right? We would just talk about what we

576
00:28:56.359 --> 00:28:57.359
like and what we don't like, and that would

577
00:28:57.359 --> 00:28:59.880
be it. Um, AND if you bring the language

578
00:28:59.880 --> 00:29:02.560
of opinions into our preferences, it sounds weird. So

579
00:29:02.560 --> 00:29:04.800
if I say something like, it is my strongly

580
00:29:04.800 --> 00:29:08.160
held opinion that I like cilantro, that sounds really

581
00:29:08.160 --> 00:29:10.400
weird, doesn't it? Wouldn't you just say, I like

582
00:29:10.400 --> 00:29:12.199
cilantro? Like, why, why do you need to say

583
00:29:12.199 --> 00:29:14.959
it's your opinion? Like, what additional information does that

584
00:29:14.959 --> 00:29:17.479
bring into it? Right? So opinions have to be

585
00:29:17.479 --> 00:29:22.560
something more than just preferences. And so, uh, what

586
00:29:22.560 --> 00:29:27.189
I think opinions are, are preferences, uh, combined with

587
00:29:27.439 --> 00:29:30.280
a set of positive judgments about the type of

588
00:29:30.280 --> 00:29:32.920
people who hold those preferences, and a set of

589
00:29:32.920 --> 00:29:36.040
negative judgments about the people who don't hold those

590
00:29:36.040 --> 00:29:40.109
preferences. So, me saying I like cilantro is not

591
00:29:40.109 --> 00:29:43.410
an opinion. But me saying I like cilantro, and

592
00:29:43.410 --> 00:29:45.750
the people who like cilantro, uh have a more

593
00:29:45.750 --> 00:29:48.550
sophisticated palate and are more discerning and are, you

594
00:29:48.550 --> 00:29:51.069
know, smarter and more authentic and have better taste

595
00:29:51.069 --> 00:29:54.130
in food, and the people who Don't like cilantro,

596
00:29:54.380 --> 00:29:58.540
or Philistines, uh, they're barbarous, uh, and, uh, they,

597
00:29:58.660 --> 00:30:00.780
their taste in food should not be respected. Uh,

598
00:30:00.900 --> 00:30:03.260
THEN all of a sudden those additional judgments I'm

599
00:30:03.260 --> 00:30:06.660
making, those bring my preference for cilantro into the

600
00:30:06.660 --> 00:30:10.739
realm of opinion. So opinions are preferences combined with

601
00:30:10.739 --> 00:30:13.010
a set of social judgments about the people who

602
00:30:13.260 --> 00:30:16.140
hold those preferences or lack those preferences.

603
00:30:17.229 --> 00:30:21.079
But because different people have different opinions, I mean,

604
00:30:21.239 --> 00:30:24.359
does it have anything to do with some of

605
00:30:24.359 --> 00:30:30.920
our psychological predispositions, like, for example, different personality traits

606
00:30:30.920 --> 00:30:32.319
and things like that.

607
00:30:33.030 --> 00:30:35.699
Yeah, absolutely, um, you know, our, our preferences are

608
00:30:35.709 --> 00:30:38.819
are certainly shaped by um our personality traits by,

609
00:30:38.910 --> 00:30:43.709
you know, random genetic, genetically heritable dispositions. Um, THEY'RE

610
00:30:43.709 --> 00:30:46.229
also shaped by a variety of cultural and environmental

611
00:30:46.229 --> 00:30:49.589
sources, um, often what we prefer will be socially

612
00:30:49.589 --> 00:30:53.439
incentivized, uh, in our social environment. Um, IF I

613
00:30:53.439 --> 00:30:56.439
get status for being a wine snob, I'm going

614
00:30:56.439 --> 00:30:58.920
to be more likely to develop a taste for

615
00:30:58.920 --> 00:31:02.459
wine, and in particular the fancier kinds of wine.

616
00:31:02.839 --> 00:31:06.390
Um, SO yeah, uh, preferences are multiply determined, uh,

617
00:31:06.400 --> 00:31:09.760
from, from various, uh, both biological and cultural and

618
00:31:09.760 --> 00:31:10.680
environmental sources.

619
00:31:11.739 --> 00:31:16.300
But particularly when we express an opinion that deviates

620
00:31:16.300 --> 00:31:20.979
from the majority opinion, if we express certain minority

621
00:31:20.979 --> 00:31:24.939
tastes, for example, do you think that has anything

622
00:31:24.939 --> 00:31:28.140
to do with the actual quality of the things

623
00:31:28.140 --> 00:31:30.729
they refer to or is it something else?

624
00:31:32.030 --> 00:31:34.900
Um, THAT could be part of what's going on,

625
00:31:35.069 --> 00:31:36.750
but I think people are going to be very

626
00:31:36.750 --> 00:31:41.349
biased, uh, to think that about their opinions, because

627
00:31:41.349 --> 00:31:45.020
the, the point of expressing an opinion, I think,

628
00:31:45.390 --> 00:31:49.780
is to shape social norms in our favor. Um,

629
00:31:49.790 --> 00:31:51.150
SO at this point it might be helpful to

630
00:31:51.150 --> 00:31:54.550
backtrack, uh, and, and think about what what norms

631
00:31:54.550 --> 00:31:58.780
are. Um, I think norms are just widely shared

632
00:31:59.030 --> 00:32:02.800
opinions. Um, YOU could think of opinions as pieces

633
00:32:02.800 --> 00:32:05.839
or fragments of norms, and when everyone shares the

634
00:32:05.839 --> 00:32:08.520
same opinion, you get a full-fledged norm. Um, SO

635
00:32:08.520 --> 00:32:11.099
if opinions are just the judgments we make about

636
00:32:11.099 --> 00:32:13.439
other people based on their preferences, then when we

637
00:32:13.439 --> 00:32:14.959
all share the same opinion, then we're all gonna

638
00:32:14.959 --> 00:32:17.239
make the same judgments about each other based on

639
00:32:17.239 --> 00:32:19.319
what we do. Um, SO if we all share

640
00:32:19.319 --> 00:32:22.729
the opinion that McDonald's is great, Uh, that McDonald's

641
00:32:22.729 --> 00:32:26.239
eaters are authentic and honest, uh, and, and blunt,

642
00:32:26.369 --> 00:32:27.489
and they tell it like it is, and they

643
00:32:27.489 --> 00:32:30.180
don't virtue signal. Well, that's going to create a

644
00:32:30.180 --> 00:32:33.329
social norm among our group to, uh, like and

645
00:32:33.329 --> 00:32:36.010
eat at McDonald's. If I like and eat at

646
00:32:36.010 --> 00:32:38.209
McDonald's, I'm going to gain status points, uh, in

647
00:32:38.209 --> 00:32:40.770
our group. I'm gonna be judged as uh honest

648
00:32:40.770 --> 00:32:44.589
and blunt and authentic. And uh anti virtue signaling,

649
00:32:45.040 --> 00:32:49.119
uh, and if I uh rant about McDonald's as

650
00:32:49.119 --> 00:32:51.359
an evil corporation, uh, then I'm going to lose

651
00:32:51.359 --> 00:32:56.319
status points in our group. Um, SO those, uh,

652
00:32:56.329 --> 00:32:59.800
victories and losses and status are basically what we

653
00:32:59.800 --> 00:33:02.079
talk about when we talk about norms. A norm

654
00:33:02.079 --> 00:33:03.959
is just what we gain status for doing and

655
00:33:03.959 --> 00:33:07.069
what we lose status for not doing. Um, I,

656
00:33:07.109 --> 00:33:10.959
I hope I said that correctly, um, and Yeah,

657
00:33:11.119 --> 00:33:17.020
so backtracking, um, so if opinions are widely, so

658
00:33:17.020 --> 00:33:19.719
sorry, if norms are widely shared opinions, what we're

659
00:33:19.719 --> 00:33:21.800
doing when we express our opinion is we're trying

660
00:33:21.800 --> 00:33:25.589
to shape uh social norms in our favor, then

661
00:33:25.589 --> 00:33:28.680
that means that when we express our opinions, we're

662
00:33:28.680 --> 00:33:30.400
gonna be biased in all sorts of ways, cause

663
00:33:30.400 --> 00:33:32.880
we're gonna want to shape social norms in ways

664
00:33:32.880 --> 00:33:35.089
that advance our self-interest and that it and that

665
00:33:35.089 --> 00:33:38.680
boost our status, and that lower our rivals status.

666
00:33:39.130 --> 00:33:41.930
Um, IN order to win the opinion game and

667
00:33:41.930 --> 00:33:44.310
change social norms, we're going to have to make

668
00:33:44.310 --> 00:33:47.089
our opinions look more objective than they really are.

669
00:33:47.209 --> 00:33:49.229
So we're gonna have to argue that our opinions

670
00:33:49.229 --> 00:33:53.250
are actually rooted in some objective feature of reality.

671
00:33:53.569 --> 00:33:55.739
Um, SO if I like a particular kind of

672
00:33:55.739 --> 00:33:58.489
music, I'm going to argue that it's objectively good,

673
00:33:58.770 --> 00:34:01.140
and uh if you can't see that, then you

674
00:34:01.140 --> 00:34:04.489
must be dumb or unsophisticated or have bad taste

675
00:34:04.780 --> 00:34:07.770
in music, or maybe you're too biased and conformist,

676
00:34:08.060 --> 00:34:10.379
uh, maybe, you know, you're not authentic enough like

677
00:34:10.379 --> 00:34:13.770
me. Um, IN order to express my opinion about

678
00:34:13.938 --> 00:34:16.060
that particular type of music, I'm going to have

679
00:34:16.060 --> 00:34:19.620
to, uh, diss either implicitly or explicitly the people

680
00:34:19.620 --> 00:34:22.728
who don't like that music. And if I'm successful,

681
00:34:22.978 --> 00:34:26.699
then I can make, uh, uh, liking that music

682
00:34:26.699 --> 00:34:29.260
a new kind of social norm, and that benefits

683
00:34:29.260 --> 00:34:31.750
me because I'm the person who discovered that band,

684
00:34:31.899 --> 00:34:33.699
I'm the person who liked that music, and that's

685
00:34:33.699 --> 00:34:36.679
going to boost my status. And all this is

686
00:34:36.679 --> 00:34:39.070
going to occur independently of whether my arguments are

687
00:34:39.070 --> 00:34:41.719
correct. So, I could be totally wrong about about

688
00:34:41.719 --> 00:34:43.478
the music. It could be that you don't need

689
00:34:43.478 --> 00:34:45.340
to be smart or sophisticated at all to to

690
00:34:45.399 --> 00:34:47.399
to see that particular thing, or maybe that particular

691
00:34:47.399 --> 00:34:50.520
thing isn't actually that impressive or skillful, or maybe

692
00:34:50.520 --> 00:34:53.120
the music isn't actually that good, and I'm the

693
00:34:53.120 --> 00:34:56.909
one who's biased by conformity or status seeking or

694
00:34:56.918 --> 00:35:00.280
or whatever. These kinds of considerations are kind of

695
00:35:00.280 --> 00:35:03.000
uh ignored when we express our opinions, because the

696
00:35:03.000 --> 00:35:06.060
goal is not to accurate. DESCRIBE what we're talking

697
00:35:06.060 --> 00:35:08.679
about or accurately describe reality, the goal is to

698
00:35:08.679 --> 00:35:12.120
win the opinion game and transform our preferences into

699
00:35:12.120 --> 00:35:14.679
social norms. So yeah, it could be the case

700
00:35:14.679 --> 00:35:17.000
that if you like indie music or indie movies,

701
00:35:17.080 --> 00:35:18.919
you might be correct about some feature of those

702
00:35:18.919 --> 00:35:22.330
movies, but I think it's pretty unlikely, given our

703
00:35:22.330 --> 00:35:26.080
biases to uh win the opinion game. Our motivation

704
00:35:26.080 --> 00:35:28.000
is to win the opinion game, not to accurately

705
00:35:28.000 --> 00:35:28.840
describe reality.

706
00:35:29.750 --> 00:35:33.439
Mhm. But isn't it, at least to some extent,

707
00:35:33.820 --> 00:35:37.689
aren't we also looking for, uh, uh, I mean,

708
00:35:38.010 --> 00:35:43.540
particularly in certain social circles to distinguish ourselves from

709
00:35:43.540 --> 00:35:46.659
other people and then, I mean, I, I, I'm

710
00:35:46.659 --> 00:35:49.739
not sure if in those particular cases, the objective

711
00:35:49.739 --> 00:35:52.385
or the goal would be to, to turn. Our

712
00:35:52.385 --> 00:35:56.024
opinions into social norms, but to turn our opinions

713
00:35:56.024 --> 00:36:01.024
into something that people recognize as being superior to

714
00:36:01.024 --> 00:36:04.385
what is even normative or common, right? But that,

715
00:36:04.425 --> 00:36:07.425
that happens a lot uh and this, that is

716
00:36:07.425 --> 00:36:10.985
one of the ways by which we acquire status

717
00:36:10.985 --> 00:36:13.614
we, we play that game well, right?

718
00:36:14.129 --> 00:36:16.340
Yeah, so what you're I think what you're talking

719
00:36:16.340 --> 00:36:18.139
about is ultimately the same as the norm. So

720
00:36:18.139 --> 00:36:20.419
if we all think that a certain preference is

721
00:36:20.419 --> 00:36:23.540
superior, say Radiohead is superior to Taylor Swift, and

722
00:36:23.540 --> 00:36:25.739
that is identical to saying that liking Radiohead is

723
00:36:25.739 --> 00:36:27.739
a social norm. I now have a social incentive

724
00:36:27.739 --> 00:36:29.620
to say I like Radiohead and to say I

725
00:36:29.620 --> 00:36:31.979
don't like Taylor Swift. If I say I like

726
00:36:31.979 --> 00:36:34.459
Taylor Swift and hate Radiohead, that's going to lower

727
00:36:34.459 --> 00:36:37.810
my status, uh, that's going to cost me, uh,

728
00:36:37.820 --> 00:36:41.449
social points. And so those winning and winnings and

729
00:36:41.449 --> 00:36:44.360
losings of social points, those are what norms are.

730
00:36:44.719 --> 00:36:46.600
Uh, IF we all think something is superior, then

731
00:36:46.600 --> 00:36:48.070
that creates a norm for us to say we

732
00:36:48.070 --> 00:36:50.550
like that thing. Um, SO yeah, I, I view

733
00:36:50.550 --> 00:36:52.179
them as, as, uh, identical.

734
00:36:52.629 --> 00:36:55.989
Mhm. Uh, BUT social norms, we can have different

735
00:36:55.989 --> 00:36:59.600
social norms for different social groups or social circles,

736
00:36:59.699 --> 00:37:02.070
right? Because I mean, if I am a Swifty,

737
00:37:02.080 --> 00:37:04.750
of course, for me and for people who are

738
00:37:04.750 --> 00:37:07.830
part of my group, Taylor Swift will be the

739
00:37:07.830 --> 00:37:11.340
best artist ever. But if we, if I prefer,

740
00:37:11.510 --> 00:37:14.429
if I'm part of another social group, maybe it's.

741
00:37:14.544 --> 00:37:16.594
Do you have and so on and so forth,

742
00:37:16.695 --> 00:37:19.534
right? So, uh, also because we, we see that

743
00:37:19.534 --> 00:37:22.264
in our society, for example, it is very common

744
00:37:22.655 --> 00:37:27.264
for intellectuals or or people who identify as intellectual

745
00:37:27.264 --> 00:37:30.534
to have particular tastes when it comes to, for

746
00:37:30.534 --> 00:37:34.965
example, art that are very different from tastes of

747
00:37:35.215 --> 00:37:36.465
uh common people.

748
00:37:37.389 --> 00:37:39.479
Yeah, absolutely. So it's gonna be powerfully shaped by

749
00:37:39.479 --> 00:37:42.760
the nature of our social networks. But any social

750
00:37:42.760 --> 00:37:46.239
network is not going to be 100% homogeneous. So

751
00:37:46.239 --> 00:37:47.879
yeah, if I'm a swifty, I'm gonna hang out

752
00:37:47.879 --> 00:37:50.280
with other swifties probably, but it's not that everyone

753
00:37:50.280 --> 00:37:51.719
I know in my social network is going to

754
00:37:51.719 --> 00:37:53.879
be a swifty, uh, and it would still benefit

755
00:37:53.879 --> 00:37:56.719
me to convince those other people in my social

756
00:37:56.719 --> 00:37:59.280
network that I'm superior for being a swifty, right?

757
00:37:59.399 --> 00:38:01.959
Cause then I get my status raised and other

758
00:38:01.959 --> 00:38:04.320
Swifties get their status raised and my group gets

759
00:38:04.320 --> 00:38:06.959
its status raised. Uh, AND so that's still in

760
00:38:06.959 --> 00:38:09.239
my interest, so I might try to talk the

761
00:38:09.239 --> 00:38:11.550
non-Swifties and my social network into being a swifty,

762
00:38:11.560 --> 00:38:14.159
or at least giving some status to Swifties and,

763
00:38:14.199 --> 00:38:17.060
and Taylor Swift's music. Uh, SAME, same thing might

764
00:38:17.060 --> 00:38:20.290
go for the particular tastes and preferences of intellectuals.

765
00:38:20.610 --> 00:38:22.379
Um, YEAH, intellectuals are gonna hang out with each

766
00:38:22.379 --> 00:38:23.939
other, but they might hang out with some people

767
00:38:23.939 --> 00:38:26.610
who are not, uh, in that circle or not,

768
00:38:26.860 --> 00:38:29.969
uh, intellectuals per se, and they might try to,

769
00:38:30.020 --> 00:38:33.209
uh, wax poetic about the, uh, the value of,

770
00:38:33.219 --> 00:38:35.580
of knowledge and and the beauty of their theories

771
00:38:35.580 --> 00:38:38.969
and and how they're, um. What they're doing is

772
00:38:38.969 --> 00:38:41.449
making the world a better place and how ideas

773
00:38:41.449 --> 00:38:44.760
uh uh matter and and determine the course of

774
00:38:44.760 --> 00:38:46.129
history, and yada yada yada.

775
00:38:47.639 --> 00:38:52.540
But uh how do norms, or how do opinions

776
00:38:52.540 --> 00:38:55.830
become norms? How are norms established basically?

777
00:38:56.649 --> 00:39:00.199
Yeah, so usually it takes a high status, or

778
00:39:00.199 --> 00:39:04.919
charismatic, or a politically powerful person to get on

779
00:39:04.919 --> 00:39:08.360
their soapbox, express their opinions, and then cause other

780
00:39:08.360 --> 00:39:12.000
people to shift their opinions toward the high status

781
00:39:12.000 --> 00:39:14.959
person. So if everybody loves me, uh, everyone is

782
00:39:14.959 --> 00:39:17.790
going to be biased to copy what I do,

783
00:39:18.120 --> 00:39:20.399
uh, in order to endear themselves to me. They're

784
00:39:20.399 --> 00:39:22.489
gonna be biased to agree with whatever I say,

785
00:39:22.840 --> 00:39:25.479
uh, in order to get closer to me. So,

786
00:39:25.510 --> 00:39:29.149
uh, in that way, norms bend toward the interests

787
00:39:29.149 --> 00:39:31.790
of high status people. If I'm high status, my

788
00:39:31.790 --> 00:39:34.340
opinions are going to be uh more highly respected,

789
00:39:34.469 --> 00:39:36.189
people are going to listen to my opinions, and

790
00:39:36.189 --> 00:39:37.830
they're going to be more likely to share my

791
00:39:37.830 --> 00:39:41.520
opinions, merely because I'm high status, right? Um, ANOTHER

792
00:39:41.520 --> 00:39:44.939
aspect of it is, uh, political power or uh

793
00:39:44.939 --> 00:39:47.629
network power. The more people I have who will

794
00:39:47.629 --> 00:39:50.000
listen to me, the more likely. My opinions are

795
00:39:50.000 --> 00:39:51.879
going to win out in the opinion game. If

796
00:39:51.879 --> 00:39:54.320
I get to shout my opinions on a megaphone

797
00:39:54.320 --> 00:39:56.560
to thousands of people, or if I have a

798
00:39:56.560 --> 00:39:58.840
popular sub stack that thousands of people read, or

799
00:39:58.840 --> 00:40:00.739
if I, you know, write a column in The

800
00:40:00.739 --> 00:40:02.679
New York Times that, you know, most of America

801
00:40:02.679 --> 00:40:04.520
needs, I'm, you know, I'm going to have a

802
00:40:04.520 --> 00:40:06.760
pretty big effect on the prevailing opinions on the

803
00:40:06.760 --> 00:40:10.600
prevailing norms in my culture. So, a lot of

804
00:40:10.600 --> 00:40:13.560
it just comes down to um norms bending toward

805
00:40:13.560 --> 00:40:15.949
the interests of the high status and the powerful.

806
00:40:16.969 --> 00:40:18.899
But to be a bit more hopeful, part of

807
00:40:18.899 --> 00:40:22.889
it could come from um the genuinely better opinions

808
00:40:22.889 --> 00:40:25.770
rising to the top, and the competition to win

809
00:40:25.770 --> 00:40:28.139
the opinion game. So if I have genuinely better

810
00:40:28.139 --> 00:40:29.739
arguments that the food I like or the music

811
00:40:29.739 --> 00:40:31.629
I listen to, or the the movies I prefer

812
00:40:31.820 --> 00:40:34.979
are actually better in some way. Maybe they convey

813
00:40:35.379 --> 00:40:38.179
uh the skill of the artist uh uh more

814
00:40:38.179 --> 00:40:43.050
effectively. Um, MAYBE they, um, create, you know, better

815
00:40:43.050 --> 00:40:46.209
social incentives, maybe they uplift us and, and inspire

816
00:40:46.209 --> 00:40:47.370
us to be better people in some way. If

817
00:40:47.370 --> 00:40:49.889
I'm right about that, um, and I have good

818
00:40:49.889 --> 00:40:53.139
arguments about that, well, then my opinion might naturally

819
00:40:53.139 --> 00:40:54.770
rise to the top as people see it as

820
00:40:54.770 --> 00:40:58.530
just more persuasive, um. So, uh, you know, if,

821
00:40:58.540 --> 00:41:00.989
if, if a particular food is unhealthy, and it

822
00:41:00.989 --> 00:41:03.669
really is unhealthy, well then it's, it's probably my

823
00:41:03.669 --> 00:41:05.219
opinion that the food is bad is going to

824
00:41:05.219 --> 00:41:07.449
be more likely to win the opinion game, in

825
00:41:07.449 --> 00:41:10.149
so far as the evidence uh of that food's

826
00:41:10.149 --> 00:41:14.139
ill health effects are undeniable. Um, SO there is

827
00:41:14.139 --> 00:41:16.530
some room for, for good ideas to win out,

828
00:41:16.770 --> 00:41:19.219
um, but it's going to be heavily biased by

829
00:41:19.219 --> 00:41:21.419
the interests of high status and powerful people, as

830
00:41:21.419 --> 00:41:22.100
most things are.

831
00:41:23.510 --> 00:41:26.510
That's very interesting. So would you say that this

832
00:41:26.510 --> 00:41:31.090
sort of opinion game also plays out in science,

833
00:41:31.149 --> 00:41:33.310
like science at the end of the day is

834
00:41:33.310 --> 00:41:37.830
basically people with different, in this particular case, we're

835
00:41:37.830 --> 00:41:41.070
going to call them opinions instead of hypotheses, for

836
00:41:41.070 --> 00:41:46.100
example, but people would, would, with different hypotheses and

837
00:41:46.300 --> 00:41:48.709
the way we arrive. THOSE hypothesis. I mean, in

838
00:41:48.709 --> 00:41:50.550
this case, it could be a lot of different

839
00:41:50.550 --> 00:41:54.510
things. We could just have particular psychological predispositions to

840
00:41:54.510 --> 00:41:58.030
prefer one explanation to the other or perhaps we

841
00:41:58.030 --> 00:42:01.189
come from a different framework and we want to

842
00:42:01.189 --> 00:42:04.510
acquire, acquire status through that framework and so we

843
00:42:04.510 --> 00:42:06.870
want that framework to be the correct one. I

844
00:42:06.870 --> 00:42:10.550
mean, so we also see this playing out in

845
00:42:10.550 --> 00:42:13.989
science. As an institution, right? It's not really, or,

846
00:42:14.030 --> 00:42:15.870
or perhaps at the end of the day, if

847
00:42:15.870 --> 00:42:20.179
we are honest with ourselves, it's not really about

848
00:42:20.179 --> 00:42:24.909
people really wanting to figure out the truth, but

849
00:42:24.909 --> 00:42:28.459
perhaps, uh, it's that kind of game playing out.

850
00:42:28.590 --> 00:42:32.899
And then, and then later, it's when, yeah, eventually

851
00:42:32.899 --> 00:42:34.919
we figure out the truth, right.

852
00:42:35.310 --> 00:42:38.250
Yeah, no, that's a really interesting point. I hadn't

853
00:42:38.250 --> 00:42:41.739
thought a lot about whether the opinion game applies

854
00:42:41.739 --> 00:42:45.570
to science, but I think it may well, uh,

855
00:42:45.580 --> 00:42:48.979
apply. Um, SO I do agree that science is

856
00:42:48.979 --> 00:42:52.419
a status game, uh, and it's a particular kind

857
00:42:52.419 --> 00:42:55.979
of status game, uh, that truth emerges from over

858
00:42:55.979 --> 00:42:59.060
time, and that's what makes it. So wonderful and

859
00:42:59.060 --> 00:43:00.780
beneficial to us. But at the end of the

860
00:43:00.780 --> 00:43:03.020
game, it still is a status game. Scientists gain

861
00:43:03.020 --> 00:43:05.939
status for making new discoveries and advancing new theories

862
00:43:05.939 --> 00:43:09.459
that end up uh being judged as correct. Um,

863
00:43:09.500 --> 00:43:12.540
AND so you could think of scientific hypotheses and

864
00:43:12.540 --> 00:43:17.399
theories as opinions. Uh, INSOFAR as the scientist, uh,

865
00:43:17.409 --> 00:43:19.889
makes negative judgments about the people who agree with

866
00:43:20.090 --> 00:43:22.649
that theory, and, uh, sorry, positive judgments about the

867
00:43:22.649 --> 00:43:24.929
people who agree with that theory and negative judgments

868
00:43:24.929 --> 00:43:27.560
about the people who disagree with that theory, um,

869
00:43:27.929 --> 00:43:31.780
then, uh, You could call that theory an opinion.

870
00:43:32.389 --> 00:43:34.469
Um, AND it, it sort of makes sense that,

871
00:43:34.560 --> 00:43:37.429
you know, if, if I think my theory is

872
00:43:37.429 --> 00:43:40.929
objectively correct and insightful, And you don't see that,

873
00:43:41.010 --> 00:43:43.129
well, why don't you see that? Maybe you're not

874
00:43:43.129 --> 00:43:45.489
as smart as me. Maybe you're not as insightful

875
00:43:45.489 --> 00:43:48.090
or or or sophisticated as me. Maybe you're biased

876
00:43:48.090 --> 00:43:50.610
in some way, maybe you're biased to uh fit

877
00:43:50.610 --> 00:43:54.169
in with uh your subculture. Maybe you're biased by

878
00:43:54.169 --> 00:43:57.370
your politics, maybe you're biased because you uh studied

879
00:43:57.370 --> 00:44:00.010
under a particular person and you want to uh

880
00:44:00.129 --> 00:44:03.050
gain status uh with that person, independent of whether

881
00:44:03.050 --> 00:44:05.659
their ideas are correct. Um, THERE could be all

882
00:44:05.659 --> 00:44:07.939
sorts of negative inferences I might make about you

883
00:44:07.939 --> 00:44:11.100
for rejecting my theory. So in that case, insofar

884
00:44:11.100 --> 00:44:13.620
as I'm making those judgments about you, And I'm

885
00:44:13.620 --> 00:44:16.310
making positive judgments about myself for being smart and

886
00:44:16.310 --> 00:44:19.469
insightful enough to uh devise that theory, then you

887
00:44:19.469 --> 00:44:22.469
could call that theory an opinion. And insofar as

888
00:44:22.469 --> 00:44:25.590
that opinion wins out, uh, in the opinion game

889
00:44:25.590 --> 00:44:27.800
and people recognize that it is correct, and it

890
00:44:27.800 --> 00:44:30.110
is insightful, uh, well, people will start to gain

891
00:44:30.110 --> 00:44:33.699
status for, uh, adhering to the theory, uh, for,

892
00:44:33.750 --> 00:44:36.469
uh, believing in it and endorsing it, um, and

893
00:44:36.469 --> 00:44:38.429
people will start to lose status for rejecting it

894
00:44:38.429 --> 00:44:41.350
or questioning it or being skeptical of it, um.

895
00:44:41.750 --> 00:44:44.280
And uh so you could, you could maybe argue

896
00:44:44.280 --> 00:44:47.800
that that um Science is a kind of opinion

897
00:44:47.800 --> 00:44:50.199
game, um, and it is a special kind of

898
00:44:50.199 --> 00:44:53.370
opinion game with an incentive structure that uh uh

899
00:44:53.370 --> 00:44:56.719
particularly allows for the truest ideas to win, as

900
00:44:56.719 --> 00:44:59.000
opposed to the highest status ideas. Now, of course,

901
00:44:59.040 --> 00:45:01.760
there are some biases along those lines, even in

902
00:45:01.760 --> 00:45:04.560
science. So there are fads in science, there are

903
00:45:04.560 --> 00:45:07.639
high status or prestigious uh theories or ways of

904
00:45:07.639 --> 00:45:10.840
looking at things, um, and so these social and

905
00:45:10.840 --> 00:45:14.800
political elements can distort the search for truth, um.

906
00:45:15.639 --> 00:45:19.090
The prevailing theories will often um bend toward the

907
00:45:19.090 --> 00:45:22.800
interests of the intelligentsia and the politically powerful, uh,

908
00:45:22.810 --> 00:45:24.760
and the high status. And so that is a

909
00:45:24.800 --> 00:45:26.570
a huge bias that we try to overcome with

910
00:45:26.570 --> 00:45:29.159
science, but the hope is that that bias will

911
00:45:29.159 --> 00:45:31.689
be overcome and the truest ideas will ultimately win

912
00:45:31.689 --> 00:45:32.060
out.

913
00:45:33.199 --> 00:45:35.399
Yeah, but that came to my mind. I mean,

914
00:45:35.489 --> 00:45:38.729
and, and it's very interesting if it is true

915
00:45:38.729 --> 00:45:42.489
that science works that way because, I mean, if

916
00:45:42.489 --> 00:45:45.520
we, if people really are like this and we

917
00:45:45.520 --> 00:45:48.209
have these kinds, these sort of motives and we

918
00:45:48.209 --> 00:45:51.209
play status games or opinion games in this in

919
00:45:51.209 --> 00:45:54.919
this specific case. Then if we are to be

920
00:45:54.919 --> 00:45:58.679
realistic, then people work like that and they are

921
00:45:58.679 --> 00:46:02.919
not really trying to be or they aren't even

922
00:46:02.919 --> 00:46:06.520
objective in terms of their search their supposed search

923
00:46:06.520 --> 00:46:09.810
for truth. They aren't impartial, they are very Very

924
00:46:09.810 --> 00:46:14.629
much, uh, biased, they are very much partial, but

925
00:46:14.949 --> 00:46:19.110
that's the way the the game works and that's

926
00:46:19.110 --> 00:46:21.870
what eventually leads to proof. So at the end

927
00:46:21.870 --> 00:46:24.780
of the day, if that's the way things work,

928
00:46:24.870 --> 00:46:26.469
it's a good thing, right.

929
00:46:26.879 --> 00:46:29.790
Yeah, I agree, um, you know, obviously I agree.

930
00:46:29.830 --> 00:46:31.709
I'm, I'm, I'm a social scientist, so I, I

931
00:46:31.709 --> 00:46:34.870
believe in it, in the institution, at least to

932
00:46:34.870 --> 00:46:37.540
some extent. Obviously there are many problems in it,

933
00:46:37.550 --> 00:46:39.510
and there are many ways in which the search

934
00:46:39.510 --> 00:46:42.949
for truth, uh, can be damaged and corrupted and

935
00:46:42.949 --> 00:46:45.510
subverted, uh, and you know, I could go on

936
00:46:45.510 --> 00:46:46.949
all day about the ways in which it is.

937
00:46:47.580 --> 00:46:49.020
But at the end of the day, there's still

938
00:46:49.020 --> 00:46:51.219
some sliver of hope that the best ideas will

939
00:46:51.219 --> 00:46:54.860
prevail. Um, I think, um, the mechanism that leads

940
00:46:54.860 --> 00:46:57.290
to true and useful ideas is probably more powerful

941
00:46:57.620 --> 00:46:59.699
in the natural and physical sciences than it is

942
00:46:59.699 --> 00:47:02.620
in the social sciences, sadly, just because um there

943
00:47:02.620 --> 00:47:05.689
are stronger biases at play in the social sciences,

944
00:47:05.699 --> 00:47:08.100
because it's a science of us, and we have

945
00:47:08.100 --> 00:47:11.699
uh lots of strong opinions about us. Um, YOU

946
00:47:11.699 --> 00:47:14.610
know, uh, the great Robert Trivers once said that,

947
00:47:14.659 --> 00:47:17.699
um, the greater the social content of a discipline,

948
00:47:17.739 --> 00:47:20.899
the slower its progress. Uh, I think that is

949
00:47:20.899 --> 00:47:23.110
a deep insight, I think that is correct, um,

950
00:47:23.139 --> 00:47:25.479
and I think that is why, you know, other

951
00:47:25.479 --> 00:47:27.500
sciences have gotten us to the moon, and social

952
00:47:27.500 --> 00:47:31.209
sciences have gotten us to a replication crisis, um.

953
00:47:32.159 --> 00:47:33.600
Which isn't to say that other fields haven't had

954
00:47:33.600 --> 00:47:36.040
a, you know, replication crises as well, but I

955
00:47:36.040 --> 00:47:38.800
think um I, I do agree with Trivers that

956
00:47:38.800 --> 00:47:41.479
there are unique problems uh uh in in the

957
00:47:41.479 --> 00:47:44.429
social sciences and in any discipline that um touches

958
00:47:44.429 --> 00:47:48.159
on human affairs. I think once those sciences do

959
00:47:48.159 --> 00:47:53.590
touch on socially uh contentious topics, um, the search

960
00:47:53.590 --> 00:47:56.280
for truth can often be corrupted in various ways.

961
00:47:57.429 --> 00:47:59.909
Well, I mean, but maybe this would be a

962
00:47:59.909 --> 00:48:04.840
great idea for one of your upcoming substack posts.

963
00:48:05.149 --> 00:48:09.070
Maybe you could write something like uh figuring out

964
00:48:09.070 --> 00:48:11.639
the truth or wanting to figure out the truth

965
00:48:11.639 --> 00:48:15.540
is bullshit because that's not, that's not what. People

966
00:48:15.540 --> 00:48:18.439
really want. I mean, that that ends up happening,

967
00:48:18.580 --> 00:48:21.860
but it's not the primary uh motive. Right.

968
00:48:22.020 --> 00:48:24.260
Yes, exactly. Scientists like to tell themselves that it's

969
00:48:24.260 --> 00:48:26.100
their primary motive, but I don't think it is.

970
00:48:26.209 --> 00:48:28.939
I think it is a byproduct of the institution

971
00:48:28.939 --> 00:48:31.179
when it's working well, but it's not the goal

972
00:48:31.179 --> 00:48:34.270
of any individual scientist. Uh, AND that might be

973
00:48:34.270 --> 00:48:35.860
a, a depressing thing to hear, but I think

974
00:48:35.860 --> 00:48:38.100
it's ultimately a correct description of human psychology.

975
00:48:39.370 --> 00:48:41.689
Yup. So let me just ask you one last

976
00:48:41.689 --> 00:48:45.330
question because uh earlier when we were talking about

977
00:48:45.330 --> 00:48:49.129
incentives that and narratives specifically at a certain point

978
00:48:49.129 --> 00:48:53.250
I mentioned that we need narratives also because we

979
00:48:53.250 --> 00:48:56.550
need explanations to give to other people. So in

980
00:48:56.550 --> 00:48:59.760
the particular case of opinions, do you think that

981
00:49:00.050 --> 00:49:04.050
when we present arguments to other people as to

982
00:49:04.050 --> 00:49:07.850
why our preferences are better. Again, do you think

983
00:49:07.850 --> 00:49:10.800
that, again, we are really trying to figure out

984
00:49:10.800 --> 00:49:13.669
the truth or that we are trying to convince

985
00:49:13.959 --> 00:49:17.040
other people that we are right to acquire status.

986
00:49:17.399 --> 00:49:19.080
I think that's the latter. We're trying to convince

987
00:49:19.080 --> 00:49:21.840
other people that we're right, um, but there are

988
00:49:21.840 --> 00:49:24.040
some cases where the only way to convince other

989
00:49:24.040 --> 00:49:26.399
people that we're right is to say true and

990
00:49:26.399 --> 00:49:29.870
convincing and compelling. Things that's gonna depend on the

991
00:49:29.870 --> 00:49:33.040
nature of of your social environment. Um, AND it's

992
00:49:33.040 --> 00:49:34.870
gonna depend on, you know, what other people find

993
00:49:34.870 --> 00:49:38.110
persuasive, whether they find good arguments persuasive and and

994
00:49:38.110 --> 00:49:41.629
strong evidence persuasive, or whether they find uh um

995
00:49:41.629 --> 00:49:46.110
status persuasive, independent of the the the power of

996
00:49:46.110 --> 00:49:49.750
the arguments. Um, SO yeah, it's gonna depend. But

997
00:49:49.750 --> 00:49:51.979
yeah, I, I, I, I don't wanna say that,

998
00:49:52.149 --> 00:49:56.149
you know, um, truth is, uh, plays zero role

999
00:49:56.149 --> 00:49:57.020
in our opinions.

1000
00:49:59.419 --> 00:50:04.060
Uh, YEAH, and I, and I'm in. Oh, no,

1001
00:50:04.219 --> 00:50:06.739
no, no, sorry, sorry, you were uh you were

1002
00:50:06.739 --> 00:50:09.699
just cut off for a few seconds there, you

1003
00:50:09.699 --> 00:50:11.860
were saying that you don't want to say that

1004
00:50:11.860 --> 00:50:15.389
truth doesn't matter at all or something along those

1005
00:50:15.389 --> 00:50:15.939
lines.

1006
00:50:16.020 --> 00:50:18.179
Yeah, just, you know, if, if the goal is

1007
00:50:18.179 --> 00:50:21.659
to convince someone, um, then you're going to have

1008
00:50:21.659 --> 00:50:24.350
to say things that the other person will find

1009
00:50:24.620 --> 00:50:29.209
convincing. Often you can convince people by uh saying

1010
00:50:29.419 --> 00:50:32.850
untrue things. Um, OFTEN you can convince them by

1011
00:50:32.850 --> 00:50:34.860
not caring about the truth and just appealing to

1012
00:50:34.860 --> 00:50:38.250
your superior status or your superior wisdom, or, uh,

1013
00:50:38.260 --> 00:50:40.939
by intimidating them, uh, into thinking that if they

1014
00:50:40.939 --> 00:50:42.790
don't agree with you, then bad things will happen

1015
00:50:42.790 --> 00:50:46.129
to them. Those are alternative means of persuading someone,

1016
00:50:46.540 --> 00:50:47.939
uh, but one of the means by which we

1017
00:50:47.939 --> 00:50:51.310
persuade someone is by just making good arguments and

1018
00:50:51.320 --> 00:50:53.620
and and trying to appeal to what is out

1019
00:50:53.620 --> 00:50:55.820
there that they can see. So yeah, I don't

1020
00:50:55.820 --> 00:50:59.330
wanna say that, you know, 0%. Uh, OF our

1021
00:50:59.330 --> 00:51:02.899
opinions or arguments are, are related or connected to

1022
00:51:02.899 --> 00:51:05.100
the truth. Um, BUT I do want to say

1023
00:51:05.100 --> 00:51:06.780
that the percentage is lower than we might like

1024
00:51:06.780 --> 00:51:07.179
to think.

1025
00:51:08.530 --> 00:51:12.060
So when it comes to science, specifically, I guess

1026
00:51:12.060 --> 00:51:14.330
that at the end of the day, what we

1027
00:51:14.330 --> 00:51:18.770
really need there are good incentives, right? Because if

1028
00:51:18.770 --> 00:51:22.209
people are have proper incentives and if we have

1029
00:51:22.209 --> 00:51:26.580
proper norms set in place that, uh, I mean,

1030
00:51:26.689 --> 00:51:30.570
that tend to lead us to truth, then uh

1031
00:51:30.570 --> 00:51:32.350
science works better, right?

1032
00:51:32.860 --> 00:51:36.260
Yeah, absolutely. So you could think about the incentives

1033
00:51:36.260 --> 00:51:38.290
that led to the replication crisis as being a

1034
00:51:38.290 --> 00:51:42.100
bad set of incentives. Um, SO you get lots

1035
00:51:42.100 --> 00:51:47.100
of prestige in science currently, and, uh, ideally, hopefully

1036
00:51:47.100 --> 00:51:50.459
less so, but certainly before the replication crisis, you

1037
00:51:50.459 --> 00:51:53.139
got lots of status for making a big flashy,

1038
00:51:53.260 --> 00:51:57.850
counterintuitive discovery, and you got no status at all

1039
00:51:57.850 --> 00:52:01.939
for trying to replicate other people's work. Um, AND

1040
00:52:01.939 --> 00:52:05.350
so that created an incentive for, uh, pumping out

1041
00:52:05.350 --> 00:52:08.139
lots of low quality studies that gave flashy and

1042
00:52:08.139 --> 00:52:12.100
counterintuitive results, uh, and no work at all trying

1043
00:52:12.100 --> 00:52:16.159
to replicate those results. Um, SO that would be

1044
00:52:16.159 --> 00:52:19.149
an example of a perverse incentive structure, uh, where

1045
00:52:19.149 --> 00:52:22.120
in that case you gain status for, um, obscuring

1046
00:52:22.120 --> 00:52:25.899
the truth, rather than finding it. And currently, you

1047
00:52:25.899 --> 00:52:27.580
know, there are efforts to try to correct those

1048
00:52:27.580 --> 00:52:31.530
incentives. So there are uh uh multiple projects being

1049
00:52:31.979 --> 00:52:34.979
undertaken to try to replicate others' work, um, their

1050
00:52:34.979 --> 00:52:38.639
institutions uh for uh trying to reward replication work

1051
00:52:38.639 --> 00:52:41.379
with with greater status. Journals are starting to uh

1052
00:52:41.379 --> 00:52:44.850
publish replication work um more frequently than they used

1053
00:52:44.850 --> 00:52:48.959
to. Um, THERE are incentives, uh, for not pea

1054
00:52:48.959 --> 00:52:51.770
hacking, so for pre-registering your hypotheses so that you

1055
00:52:51.770 --> 00:52:54.649
can't, you know, twiddle with the statistics until you

1056
00:52:54.649 --> 00:52:56.090
get the result you want, you have to say

1057
00:52:56.090 --> 00:52:58.290
what statistics you're gonna run in advance and then

1058
00:52:58.290 --> 00:53:01.850
run those exact statistics. Um, SO that's another, uh,

1059
00:53:02.050 --> 00:53:07.879
positive development. Um, Sample sizes have gotten larger, uh,

1060
00:53:08.149 --> 00:53:12.310
which is good, um. We've, I think we're correctly

1061
00:53:12.310 --> 00:53:14.909
focusing more on effect sizes rather than P values.

1062
00:53:15.010 --> 00:53:16.510
This is more in the weeds here, but I

1063
00:53:16.510 --> 00:53:18.469
think lots of positive developments have taken place in

1064
00:53:18.469 --> 00:53:21.169
the wake of the replication crisis to try to

1065
00:53:21.169 --> 00:53:23.949
correct these incentives so that the truth will emerge

1066
00:53:24.189 --> 00:53:26.469
more effectively. There's still plenty of work to be

1067
00:53:26.469 --> 00:53:28.620
done, there's still plenty of problems, but I think

1068
00:53:28.620 --> 00:53:29.709
it's headed in the right direction.

1069
00:53:30.669 --> 00:53:32.760
And so, and, and this will be my last

1070
00:53:32.760 --> 00:53:36.120
question. Do you think that then if people, scientists,

1071
00:53:36.250 --> 00:53:41.909
philosophers of science were aware of these more evolutionary

1072
00:53:41.909 --> 00:53:47.399
take on incentives and opinions that perhaps they would

1073
00:53:47.399 --> 00:53:51.560
or they would develop better and new ways of

1074
00:53:51.560 --> 00:53:53.110
doing science?

1075
00:53:53.830 --> 00:53:56.270
Well, it is my opinion that uh that is

1076
00:53:56.270 --> 00:53:58.750
correct, and it is, of course, my self-serving opinion,

1077
00:53:58.830 --> 00:54:01.030
as most opinions are, that that is correct because

1078
00:54:01.030 --> 00:54:04.070
I'm an evolutionary psychologist, and so, uh, touting the

1079
00:54:04.070 --> 00:54:07.429
virtues of evolutionary psychology is in my status interests.

1080
00:54:08.260 --> 00:54:11.270
Uh, BUT yes, despite the fact that this opinion

1081
00:54:11.270 --> 00:54:13.830
is self-interested, I do think it is correct. I

1082
00:54:13.830 --> 00:54:18.179
think, um, The best or perhaps the only way

1083
00:54:18.179 --> 00:54:21.149
to understand human nature, the human condition, is by

1084
00:54:21.149 --> 00:54:23.860
viewing it correctly, uh, as the product of natural

1085
00:54:23.860 --> 00:54:28.030
selection. Um, I think that is the source of

1086
00:54:28.300 --> 00:54:31.500
humanity, that is where we come from. Um, I

1087
00:54:31.500 --> 00:54:34.699
find it, uh, baffling that people think that when

1088
00:54:34.699 --> 00:54:37.139
trying to study the structure and function of an

1089
00:54:37.139 --> 00:54:40.169
evolved organ, it is OK to not think about

1090
00:54:40.580 --> 00:54:43.830
the process that made that organ. Um, I, I,

1091
00:54:43.969 --> 00:54:45.610
I continue to be baffled by it. I think

1092
00:54:45.610 --> 00:54:48.169
it is one of the great distortionary pressures in

1093
00:54:48.169 --> 00:54:51.570
science that evolutionary psychology is not common sense at

1094
00:54:51.570 --> 00:54:55.050
this point, that viewing the human animal in evolutionary

1095
00:54:55.050 --> 00:54:57.969
terms, whether cultural evolution or biological evolution, whatever, I,

1096
00:54:58.000 --> 00:55:00.050
I don't have a dog in that fight. Any

1097
00:55:00.050 --> 00:55:02.280
kind of evolution applied to the human animal is

1098
00:55:02.280 --> 00:55:05.250
going to deepen and enrich her understanding of the

1099
00:55:05.250 --> 00:55:07.090
human animal, and the fact that this perspective has

1100
00:55:07.090 --> 00:55:11.129
not caught on and become fully mainstream, uh, is

1101
00:55:11.129 --> 00:55:15.350
deeply saddening. Um, NEVERTHELESS, there is some hope, there

1102
00:55:15.350 --> 00:55:18.270
has been some progress made, uh, evolutionary perspectives have

1103
00:55:18.270 --> 00:55:21.790
gotten considerably more mainstream than they have been, uh,

1104
00:55:21.830 --> 00:55:24.340
in the past few decades. Uh, IT has gotten,

1105
00:55:24.669 --> 00:55:28.969
uh, more respectable, uh, more prestigious evolutionary psychology papers

1106
00:55:28.969 --> 00:55:31.149
are getting published in the top journals more and

1107
00:55:31.149 --> 00:55:34.659
more. Uh, PEOPLE are more frequently, uh, viewing it

1108
00:55:34.659 --> 00:55:38.439
as a legitimate and powerful perspective to take. There

1109
00:55:38.439 --> 00:55:40.919
is progress. It's slower than I would hope it

1110
00:55:40.919 --> 00:55:43.199
would be, but there there is progress being made.

1111
00:55:44.739 --> 00:55:47.830
So where can people find you on the internet?

1112
00:55:48.729 --> 00:55:51.409
Well, you can find me at Twitter at David

1113
00:55:51.409 --> 00:55:54.530
Pinsoff, uh, you can find my blog, Everything is

1114
00:55:54.530 --> 00:55:58.830
bullshit at Everything is bullshit.blog. Uh, YOU can feel

1115
00:55:58.830 --> 00:56:02.350
free to, uh, direct message me, uh, happy to

1116
00:56:02.750 --> 00:56:04.510
answer any questions and, and chat.

1117
00:56:05.750 --> 00:56:08.370
Great. So thank you so much for doing this

1118
00:56:08.370 --> 00:56:11.129
again. It's also very fun to talk with you.

1119
00:56:11.689 --> 00:56:12.889
Yeah, my pleasure, Ricardo.

1120
00:56:14.100 --> 00:56:16.620
Hi guys, thank you for watching this interview until

1121
00:56:16.620 --> 00:56:18.770
the end. If you liked it, please share it,

1122
00:56:18.939 --> 00:56:21.729
leave a like and hit the subscription button. The

1123
00:56:21.729 --> 00:56:23.929
show is brought to you by Nights Learning and

1124
00:56:23.929 --> 00:56:28.010
Development done differently, check their website at Nights.com and

1125
00:56:28.010 --> 00:56:31.729
also please consider supporting the show on Patreon or

1126
00:56:31.729 --> 00:56:34.209
PayPal. I would also like to give a huge

1127
00:56:34.209 --> 00:56:37.639
thank you to my main patrons and PayPal supporters

1128
00:56:37.639 --> 00:56:41.540
Pergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyches Olaf,

1129
00:56:41.649 --> 00:56:44.899
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

1130
00:56:45.030 --> 00:56:48.320
Erika Lenny, John Connors, Philip Fors Connolly. Then the

1131
00:56:48.320 --> 00:56:52.120
Mari Robert Windegaruyasi Zup Mark Nes called in Holbrookfield

1132
00:56:52.120 --> 00:56:56.879
governor Michael Stormir, Samuel Andre, Francis Forti Agnseroro and

1133
00:56:56.879 --> 00:57:01.639
Hal Herzognun Macha Joan Labray and Samuel Corriere, Heinz,

1134
00:57:01.699 --> 00:57:05.360
Mark Smith, Jore, Tom Hummel, Sardus France David Sloan

1135
00:57:05.360 --> 00:57:10.149
Wilson, asila dearauurumen Roach Diego London Correa. Yannick Punter

1136
00:57:10.149 --> 00:57:15.280
Darusmani Charlotte blinikol Barbara Adamhn Pavlostaevsky nale back medicine,

1137
00:57:15.350 --> 00:57:19.939
Gary Galman Sam of Zaledrianeioltonin John Barboza, Julian Price,

1138
00:57:20.229 --> 00:57:24.669
Edward Hall Edin Bronner, Douglas Fre Franca Bartolotti Gabrielon

1139
00:57:24.669 --> 00:57:29.149
Scorteseus Slelitsky, Scott Zachary Fish Tim Duffyani Smith John

1140
00:57:29.149 --> 00:57:34.080
Wieman. Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai

1141
00:57:34.080 --> 00:57:39.260
Giorgio Theophanous, Chris Williamson, Peter Wozin, David Williams, Diosta,

1142
00:57:39.340 --> 00:57:43.580
Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez, Coralli

1143
00:57:43.580 --> 00:57:47.939
Chevalier, bungalow atheists, Larry D. Lee Junior, old Erringbo.

1144
00:57:48.699 --> 00:57:52.739
Sterry Michael Bailey, then Sperber, Robert Grassy, Zigoren, Jeff

1145
00:57:52.739 --> 00:57:57.300
McMann, Jake Zu, Barnabas radix, Mark Campbell, Thomas Dovner,

1146
00:57:57.419 --> 00:58:01.820
Luke Neeson, Chris Storry, Kimberly Johnson, Benjamin Gilbert, Jessica

1147
00:58:01.820 --> 00:58:07.489
Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George Eoriatis,

1148
00:58:07.530 --> 00:58:13.320
Valentin Steinman, Perkrolis, Kate van Goller, Alexander Aubert, Liam

1149
00:58:13.610 --> 00:58:19.780
Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner, Ursulauddinov,

1150
00:58:19.929 --> 00:58:24.570
Gregory Hastings, David Pinsoff Sean Nelson, Mike Levine, and

1151
00:58:24.570 --> 00:58:27.750
Jos Net. A special thanks to my producers. These

1152
00:58:27.750 --> 00:58:31.199
are Webb, Jim, Frank Lucas Steffini, Tom Venneden, Bernard

1153
00:58:31.199 --> 00:58:35.510
Curtis Dixon, Benick Muller, Thomas Trumbull, Catherine and Patrick

1154
00:58:35.510 --> 00:58:38.840
Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick Golden,

1155
00:58:39.110 --> 00:58:42.350
and to my executive producers Matthew Levender, Sai Quadrian,

1156
00:58:42.469 --> 00:58:45.100
Bogdan Kanivets, and Rosie. Thank you for all.

