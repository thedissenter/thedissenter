WEBVTT

1
00:00:00.370 --> 00:00:02.819
Hello, everyone. Welcome to a new episode of The

2
00:00:02.819 --> 00:00:05.650
Dissenter. I'm your host, as always, Ricardo Lops, and

3
00:00:05.650 --> 00:00:08.649
today I'm here with a return guest, Doctor Alberto

4
00:00:08.649 --> 00:00:12.210
Aervi. He's associate professor of sociology at the University

5
00:00:12.210 --> 00:00:16.340
of Trento in Italy. And he's the author of

6
00:00:16.340 --> 00:00:19.940
a new book, uh, Technopanico. I mean, in English,

7
00:00:19.979 --> 00:00:23.360
it would be something like Technopanic Digital Media Between

8
00:00:23.780 --> 00:00:27.270
Reasonable caution and unjustified fears, and we're going to

9
00:00:27.540 --> 00:00:30.700
talk about the book today. So Doctor Aerbi, welcome

10
00:00:30.700 --> 00:00:32.619
back to the show. It's always a pleasure to

11
00:00:32.619 --> 00:00:32.909
everyone.

12
00:00:33.380 --> 00:00:35.959
Thank you. Big pleasure for me, Jay Ricardo.

13
00:00:37.209 --> 00:00:40.650
So let me start with this question. What do

14
00:00:40.650 --> 00:00:44.930
you mean by techno panic by using, I'm using

15
00:00:44.930 --> 00:00:46.840
the word in English here, of course. I mean,

16
00:00:46.970 --> 00:00:49.450
how would you characterize a techno panic?

17
00:00:49.930 --> 00:00:53.209
Yes, techno panic is a, is a word that

18
00:00:53.209 --> 00:00:56.689
has been used a bit in some uh literature

19
00:00:56.689 --> 00:01:01.869
recently to talk about some. Excessive uh fear or

20
00:01:01.869 --> 00:01:09.470
alarmistic narrative about uh technologies and um In the

21
00:01:09.470 --> 00:01:13.069
case of the book is uh specific about uh

22
00:01:13.069 --> 00:01:19.830
communication technologies and in particular about contemporary uh communication

23
00:01:19.830 --> 00:01:24.889
technologies through digital media, social media, smartphone. We know

24
00:01:24.889 --> 00:01:28.500
that there has been a big tension about the,

25
00:01:28.580 --> 00:01:32.470
the, the effect of, uh, social media smartphone on

26
00:01:32.470 --> 00:01:37.029
society and on us. And for some people, including

27
00:01:37.029 --> 00:01:40.269
myself, uh, uh, sometime there has been a kind

28
00:01:40.269 --> 00:01:47.470
of uh Overly negative and alarmistic narrative about the

29
00:01:47.470 --> 00:01:53.080
effect of digital technologies while the image that Came

30
00:01:53.080 --> 00:01:56.879
out from researches at least uh more than once

31
00:01:57.480 --> 00:02:00.199
and so the the term techno panics is exactly

32
00:02:00.199 --> 00:02:04.269
about this, about the fact that uh um. The,

33
00:02:04.360 --> 00:02:10.279
the, this overly alarmistic and negative uh narrative about

34
00:02:10.279 --> 00:02:14.039
uh digital technologies. Then of course, uh there is

35
00:02:14.039 --> 00:02:18.720
a history behind this, uh, this term. For example,

36
00:02:18.839 --> 00:02:20.960
uh, and I talk about this in the, in

37
00:02:20.960 --> 00:02:24.520
the first chapter of the book in, um, sociology,

38
00:02:24.600 --> 00:02:28.740
there is this idea of, uh, moral panics which

39
00:02:29.119 --> 00:02:35.029
are indeed. The, the, uh, uh, the society reacting

40
00:02:35.029 --> 00:02:37.509
for a series of reasons and then I explore

41
00:02:37.509 --> 00:02:39.630
and we can talk about it, uh, a little

42
00:02:39.630 --> 00:02:41.940
bit more in the detail if you want, uh,

43
00:02:42.149 --> 00:02:45.330
but again, the idea is, is negative and, and

44
00:02:45.330 --> 00:02:49.830
not necessarily always, uh, empirically grounded reaction to new

45
00:02:49.830 --> 00:02:50.800
technologies.

46
00:02:51.539 --> 00:02:54.720
Uh, SO in our previous conversation, we talked about

47
00:02:54.720 --> 00:02:58.589
uh cultural evolution. I mean, uh, does this book

48
00:02:58.589 --> 00:03:01.309
and the way you, you, you approach this sort

49
00:03:01.309 --> 00:03:05.009
of panics connect in any way to your work

50
00:03:05.179 --> 00:03:06.669
in cultural evolution?

51
00:03:08.509 --> 00:03:10.949
Yes, I would say, I would say so. I,

52
00:03:10.990 --> 00:03:13.500
I, it connects, I think in, in two ways.

53
00:03:13.789 --> 00:03:18.729
One is I would say more like uh um.

54
00:03:20.039 --> 00:03:22.850
Personal way in the sense uh so I, I

55
00:03:22.850 --> 00:03:26.529
start uh to work about the uh the effect

56
00:03:26.529 --> 00:03:30.270
of digital media using this cultural evolution perspective you're

57
00:03:30.270 --> 00:03:35.669
talking about around 10 years ago or even more

58
00:03:35.669 --> 00:03:40.910
now and um. Just after I started to work

59
00:03:40.910 --> 00:03:44.429
on this, so in, let's say in 2016 there

60
00:03:44.429 --> 00:03:48.050
was the first, uh, Trump election, then there was,

61
00:03:48.149 --> 00:03:52.229
uh, Brexit, then you had all the increase of

62
00:03:52.229 --> 00:03:55.949
populist parties in Europe and everywhere, and in this

63
00:03:55.949 --> 00:03:59.750
time there's also started to be a big backslash

64
00:03:59.750 --> 00:04:03.210
against the social media. We started, you know, the,

65
00:04:03.270 --> 00:04:05.279
the, the, the. The idea of fake news and

66
00:04:05.279 --> 00:04:10.619
misinformation started basically to be very popular in 2016

67
00:04:10.619 --> 00:04:14.559
with Trump election. There was Cambridge Analytica. So while

68
00:04:14.559 --> 00:04:18.399
I was working for my research interest uh related

69
00:04:18.399 --> 00:04:21.540
to cultural revolution to this topic. I could see

70
00:04:21.540 --> 00:04:25.070
that was emerging also all these interests and uh

71
00:04:25.459 --> 00:04:28.739
as I was saying before my impression was from

72
00:04:28.739 --> 00:04:33.040
the beginning that this was not really reflecting what

73
00:04:33.380 --> 00:04:37.899
we would know about, about uh uh about the

74
00:04:37.899 --> 00:04:42.269
effort of social media so. I started to be

75
00:04:42.269 --> 00:04:45.489
interested not only in the, in my, in my,

76
00:04:45.549 --> 00:04:49.029
let's say basic research about, about digital media and

77
00:04:49.029 --> 00:04:51.429
cultural evolution but also I, I started to try

78
00:04:51.429 --> 00:04:55.510
to understand the reason behind this narrative, why this

79
00:04:55.510 --> 00:04:58.149
narrative was there, what were the effect of this

80
00:04:58.149 --> 00:05:00.470
narrative. So there is this link that is, you

81
00:05:00.470 --> 00:05:03.940
know, more personal. Uh, BUT also I think there

82
00:05:03.940 --> 00:05:09.019
is a more, uh, theoretical and, uh, possibly more

83
00:05:09.019 --> 00:05:12.899
interesting, uh, uh, link uh because I think that

84
00:05:12.899 --> 00:05:16.100
if we have, uh, uh, an evolutionary perspective, a

85
00:05:16.100 --> 00:05:22.679
cultural evolutionary perspective, uh. We, we have also a,

86
00:05:22.720 --> 00:05:27.329
a view about how propaganda works, about social influence

87
00:05:27.329 --> 00:05:30.010
work, and I also talk about that, especially in

88
00:05:30.010 --> 00:05:33.170
the. In the, in the second chapter of the

89
00:05:33.170 --> 00:05:35.320
book, and this view has been of course influenced

90
00:05:35.320 --> 00:05:37.799
by other people that you interview like uh like

91
00:05:37.799 --> 00:05:41.640
Hugo Mercier or, or other epistemic vigilance, and the

92
00:05:41.640 --> 00:05:45.100
basic idea is that we are not uh overly

93
00:05:45.100 --> 00:05:48.640
uh gullible, but we tend to be pretty stubborn

94
00:05:48.640 --> 00:05:53.920
that propaganda doesn't work in this automatic way that

95
00:05:53.920 --> 00:05:57.000
sometimes is kind of implicitly assumed when people talk

96
00:05:57.000 --> 00:05:59.839
about the, the, the danger of, uh, of social

97
00:05:59.839 --> 00:06:02.309
media and so on and so. This is a

98
00:06:02.309 --> 00:06:06.470
more um theoretical reason why I think that the

99
00:06:06.470 --> 00:06:10.709
background in evolutionary er social science is important when

100
00:06:10.709 --> 00:06:12.660
we look about, uh, when we look to these,

101
00:06:12.940 --> 00:06:14.440
uh these phenomena. Mhm.

102
00:06:15.470 --> 00:06:17.589
Yes, we're going to talk a little bit later

103
00:06:17.589 --> 00:06:21.109
about how inf influenceable are people really, I mean,

104
00:06:21.230 --> 00:06:24.760
how gullible they are really. But uh before that,

105
00:06:24.869 --> 00:06:27.470
let's just talk a little bit more about moral

106
00:06:27.470 --> 00:06:31.190
panic. So, which do you, would you say are

107
00:06:31.190 --> 00:06:35.820
the aspects of our psychology that explain the development

108
00:06:35.820 --> 00:06:37.410
of moral panics?

109
00:06:38.450 --> 00:06:41.089
Yes, there are, there are several. I, I, I,

110
00:06:41.170 --> 00:06:44.220
I think that the, the, the clearest one are,

111
00:06:44.230 --> 00:06:48.380
uh, uh, a general negative bias. So this is

112
00:06:48.380 --> 00:06:52.470
one of the few results in psychology that are

113
00:06:52.470 --> 00:06:54.929
quite robust, uh, that we can say now, OK,

114
00:06:55.010 --> 00:06:56.950
this, this exists, of course, there is a lot

115
00:06:56.950 --> 00:07:01.540
of variations and contextual factor, but We kind of

116
00:07:01.540 --> 00:07:05.540
know that in general, uh there is a tendency

117
00:07:05.540 --> 00:07:09.059
for everybody again with individual difference and everything, but

118
00:07:09.059 --> 00:07:14.540
there is a tendency to find negative information, more

119
00:07:14.540 --> 00:07:20.899
um attention catching, more memorable, more, uh, cognitively attractive

120
00:07:20.899 --> 00:07:25.019
than uh positive information. This again is really nothing

121
00:07:25.019 --> 00:07:28.635
new we know that. The negative news are more

122
00:07:28.635 --> 00:07:32.244
successful than positive news, but that is, is, I

123
00:07:32.244 --> 00:07:34.644
think part of the, the background of this. So

124
00:07:34.644 --> 00:07:39.154
it's kind of easy to, to, uh, say, oh,

125
00:07:39.304 --> 00:07:42.804
that the smartphone is destroying a generation, the social

126
00:07:42.804 --> 00:07:45.804
media are destroying democracy. This is something that, uh,

127
00:07:46.054 --> 00:07:51.385
uh, traveled very well, especially on social media paradoxically,

128
00:07:51.804 --> 00:07:54.579
and, uh, so this is one. There is a

129
00:07:54.579 --> 00:07:57.540
second aspect that is also I think important which

130
00:07:57.540 --> 00:08:03.269
is uh uh some sort of um We, we

131
00:08:03.269 --> 00:08:06.829
think that the past was better. This is er

132
00:08:06.829 --> 00:08:10.149
also pretty, pretty obvious, but it, it, it can

133
00:08:10.149 --> 00:08:13.109
also be tested uh with the experiment. This is

134
00:08:13.109 --> 00:08:15.230
interesting. I, I mentioned some of the experiments that

135
00:08:15.230 --> 00:08:19.140
try to test this effect. But also a, a,

136
00:08:19.380 --> 00:08:22.420
a feature of these, of these techno panics if

137
00:08:22.420 --> 00:08:25.540
you want that we found, uh, also today with

138
00:08:25.540 --> 00:08:29.100
social media smartphone is like the past was better

139
00:08:29.339 --> 00:08:32.619
like the, the, the idea, the idea of post-truth

140
00:08:32.619 --> 00:08:37.308
era which was very, very common related exactly to

141
00:08:37.308 --> 00:08:39.590
social media in the last in the last, uh,

142
00:08:39.609 --> 00:08:46.010
10 years or so. It's, it presupposes explicitly that,

143
00:08:46.169 --> 00:08:48.530
that, that, that there has been a, a era

144
00:08:48.530 --> 00:08:50.640
of truth. If we are in a post-era there

145
00:08:50.640 --> 00:08:52.650
should, there should have been something before in which

146
00:08:52.650 --> 00:08:56.450
things were better and this again, it's uh, uh,

147
00:08:56.460 --> 00:08:58.809
uh, an idea of a kind of mythical past

148
00:08:58.809 --> 00:09:01.690
which is pretty difficult to defend if we think

149
00:09:01.690 --> 00:09:05.530
about it traditionally, uh, Jonathan Haid, which is one

150
00:09:05.530 --> 00:09:09.900
of the. Most vocal, uh, character in the, in,

151
00:09:10.070 --> 00:09:13.349
in showing that, the, the possible uh danger and

152
00:09:13.349 --> 00:09:17.429
risk of uh smartphone and social media usage, especially

153
00:09:17.429 --> 00:09:23.039
for, for, uh, teenagers and young people. Is, is

154
00:09:23.039 --> 00:09:26.919
also, is also often, you know. Talking about the

155
00:09:26.919 --> 00:09:31.190
importance of uh free games like the, like having

156
00:09:31.190 --> 00:09:33.559
the kids going in the, in the, in the,

157
00:09:33.809 --> 00:09:36.080
in the, in the countryside in the past, which

158
00:09:36.080 --> 00:09:38.039
again, it's, it's in a way, it's kind of

159
00:09:38.039 --> 00:09:41.320
reasonable. The, the, the, the, the interesting point here

160
00:09:41.320 --> 00:09:44.039
is that a, a feature of this techno panics

161
00:09:44.039 --> 00:09:47.000
is to have a, a, a reference to an

162
00:09:47.000 --> 00:09:49.479
era in the past in which things were, were

163
00:09:49.479 --> 00:09:52.950
better. And then of course uh uh so the

164
00:09:52.950 --> 00:09:56.250
these are kind of psychological background of this of

165
00:09:56.250 --> 00:09:59.929
this idea but then you have also a series

166
00:09:59.929 --> 00:10:04.059
of features of this uh uh moral panic or,

167
00:10:04.090 --> 00:10:07.130
or techno panics. So I, I used in the

168
00:10:07.130 --> 00:10:09.489
book the as I was saying before, the concept

169
00:10:09.489 --> 00:10:12.950
of, of moral panics that is a moral sociological

170
00:10:13.210 --> 00:10:17.559
concept. Um, THIS is from a, a sociologist's called,

171
00:10:17.640 --> 00:10:20.919
uh, Stanley Cohen of a few, a few decades

172
00:10:20.919 --> 00:10:25.280
ago, and, um, so he was studying some the,

173
00:10:25.359 --> 00:10:28.880
the, the, the reaction of media and, uh, and

174
00:10:28.880 --> 00:10:31.869
of the, the, the general population about some uh.

175
00:10:32.130 --> 00:10:35.530
Uh YOUTH disorder that were happening in the UK

176
00:10:35.530 --> 00:10:39.090
in the early 60s and um he, he was

177
00:10:39.090 --> 00:10:41.570
the first one, let's say, well, he, he was

178
00:10:41.570 --> 00:10:44.849
lucky to invent the term moral panic, but also

179
00:10:45.090 --> 00:10:47.330
was the first one to really focus on the

180
00:10:47.330 --> 00:10:52.539
fact that uh these disorders were not really. Major,

181
00:10:52.690 --> 00:10:56.460
uh, didn't create major problems, but still the reaction

182
00:10:56.460 --> 00:10:59.739
of the, uh, of the, uh, of the media

183
00:10:59.739 --> 00:11:03.330
and of the population was clearly, clearly, uh, bigger

184
00:11:03.330 --> 00:11:06.979
than what was the problem and he started to

185
00:11:07.520 --> 00:11:10.359
try to understand what were the reason related to

186
00:11:10.359 --> 00:11:13.479
this. So for example, the way to, to easily

187
00:11:13.479 --> 00:11:18.770
find some, uh, scapegoat about some more complex societal

188
00:11:18.770 --> 00:11:24.880
problem. Uh, FINDING some generalized water maybe local problem

189
00:11:24.880 --> 00:11:28.940
too big, uh, too big, um, to like all

190
00:11:28.940 --> 00:11:31.820
the world problems. So maybe social media usage is

191
00:11:31.820 --> 00:11:35.260
problematic for someone, but we say social media is

192
00:11:35.260 --> 00:11:38.619
destroying a generation. So this you, you make bigger

193
00:11:38.619 --> 00:11:40.460
the, the problem. So there are a series of

194
00:11:40.460 --> 00:11:47.030
features that are then, uh, characterize Hispanic. For the

195
00:11:47.030 --> 00:11:49.020
psychological reason and I was saying you have this

196
00:11:49.020 --> 00:11:52.669
negativity, you have this uh nostalgia for the past

197
00:11:52.669 --> 00:11:55.070
and they kind of go together to create a

198
00:11:55.070 --> 00:11:57.590
very uh attractive story.

199
00:11:59.020 --> 00:12:02.739
Yeah, I mean, it's very interesting because there are

200
00:12:02.739 --> 00:12:05.940
two aspects here that are not new at all.

201
00:12:06.020 --> 00:12:09.780
I mean, first of all, people claiming that the

202
00:12:09.780 --> 00:12:15.539
younger generation or generations have some sort of problem

203
00:12:15.539 --> 00:12:19.429
that they are decadent or they are degenerates or

204
00:12:19.482 --> 00:12:22.002
As the new practices they have or the new

205
00:12:22.002 --> 00:12:25.843
technologies they use, they, uh, their moral character is

206
00:12:25.843 --> 00:12:29.242
weaker or they, uh, and, uh, also moral panic

207
00:12:29.242 --> 00:12:32.523
surrounding new technology like, I mean, I've heard about

208
00:12:32.523 --> 00:12:38.283
moral panics, uh, regarding writing romance novels, the radio,

209
00:12:38.362 --> 00:12:41.466
the TV, video games, so. I mean this, this

210
00:12:41.466 --> 00:12:44.466
seems to be just part of a very old

211
00:12:44.466 --> 00:12:47.065
historic trend among people, right?

212
00:12:47.226 --> 00:12:52.265
Yes, absolutely. So, uh, in, in, again I, I

213
00:12:52.265 --> 00:12:56.145
go through some of these, of these moral panics

214
00:12:56.145 --> 00:12:59.866
related to, uh, communication technology because this is the

215
00:12:59.866 --> 00:13:02.895
topic and as you say, we have like from,

216
00:13:02.995 --> 00:13:08.260
from the writing itself, uh, to the radio. Uh,

217
00:13:08.469 --> 00:13:10.440
TV video game, so there is, is a kind

218
00:13:10.440 --> 00:13:14.440
of a recurrent pattern that we, we, we can,

219
00:13:14.479 --> 00:13:19.469
uh, we can see, um. There are, again, there

220
00:13:19.469 --> 00:13:23.239
are some uh good reason in a way I

221
00:13:23.239 --> 00:13:25.080
would say good in a sense of we, we

222
00:13:25.080 --> 00:13:28.799
can understand why this happens. So I, I talked

223
00:13:28.799 --> 00:13:32.909
about the psychological reason but it's also uh a

224
00:13:32.909 --> 00:13:38.200
fact that these uh new technologies, new communication technologies

225
00:13:38.200 --> 00:13:41.750
have a strong impacts on our society. So it's,

226
00:13:41.789 --> 00:13:44.799
it's normal in a way that we can, we

227
00:13:44.799 --> 00:13:48.299
can be uh worried about that. I, I will

228
00:13:48.299 --> 00:13:50.739
give you just the example, for example, the printing

229
00:13:50.739 --> 00:13:54.059
press, that's, you know, seems to be a technology

230
00:13:54.059 --> 00:13:57.159
that we would say, OK, it's books are not

231
00:13:57.159 --> 00:14:00.059
that dangerous, but, uh, but of course they are.

232
00:14:00.340 --> 00:14:03.539
Books are very dangerous and when the printing press

233
00:14:03.539 --> 00:14:07.940
appeared, it, it, it's reasonable that people were worried

234
00:14:07.940 --> 00:14:10.469
and is, is nice because I did that. I

235
00:14:10.469 --> 00:14:12.510
mean, it's not my, my main topic, but I

236
00:14:12.510 --> 00:14:14.469
did a bit of research about the, you know,

237
00:14:14.549 --> 00:14:17.549
the reaction on the printing press and when you

238
00:14:17.549 --> 00:14:19.690
read these things seems to read the like uh

239
00:14:19.690 --> 00:14:21.909
it, it could have been written 10 years ago.

240
00:14:22.030 --> 00:14:25.909
People are worried about the spread of misinformation, who

241
00:14:25.909 --> 00:14:28.789
is uh checking if what is written in these

242
00:14:28.789 --> 00:14:33.070
books is true or not. And uh and it's

243
00:14:33.070 --> 00:14:35.520
quite interesting because it's true that the, the printing

244
00:14:35.520 --> 00:14:40.919
press was a disruptive technologies that could potentially have

245
00:14:40.919 --> 00:14:45.380
negative effect. What I think that we need to

246
00:14:45.380 --> 00:14:47.789
learn from history in this case is that the,

247
00:14:47.799 --> 00:14:52.179
the introduction of new technologies, any kind of technologies

248
00:14:52.179 --> 00:14:55.500
is in general, I, I, I say in the

249
00:14:55.500 --> 00:15:00.489
book a process of adaptation, co-adaptation. So in generally,

250
00:15:00.500 --> 00:15:04.015
people that, that tend to feel. Very strongly this

251
00:15:04.015 --> 00:15:08.155
new technology thing that the technology arrived, it changes

252
00:15:08.934 --> 00:15:12.005
everything. We are kind of society's kind of passive

253
00:15:12.005 --> 00:15:16.375
and is molded by the, the new technology, but

254
00:15:16.375 --> 00:15:20.534
this is not usually what happens, uh, for example,

255
00:15:20.575 --> 00:15:23.895
in the case of the printing press, there are,

256
00:15:23.974 --> 00:15:28.174
uh, innovations that follow the printing press to manage,

257
00:15:28.215 --> 00:15:31.400
for example, the fact that there are. Many new

258
00:15:31.400 --> 00:15:35.679
books, uh, people invented the, the index at the

259
00:15:35.679 --> 00:15:38.280
end of the book, so it, it's, it's a

260
00:15:38.280 --> 00:15:40.599
way when you have a lot of information you

261
00:15:40.599 --> 00:15:42.679
need to find a way to organize the information.

262
00:15:42.799 --> 00:15:47.000
So this is an adaptation to the increasing information.

263
00:15:47.320 --> 00:15:49.479
It's that, I mean, again is a bit is

264
00:15:49.479 --> 00:15:52.239
a bit, uh, uh, niche, but there are some

265
00:15:52.239 --> 00:15:58.799
interesting stuff about how, uh, annotating books changed from.

266
00:15:59.219 --> 00:16:02.059
Uh, BEFORE and after the invention of the printing

267
00:16:02.059 --> 00:16:05.669
press, so, or even before with manuscript we annotated

268
00:16:05.669 --> 00:16:10.460
by, by readers or monks or whatever, but after

269
00:16:10.460 --> 00:16:12.979
when, when, when you start to have a big

270
00:16:12.979 --> 00:16:16.179
circulation of books, the, the way you annotate books,

271
00:16:16.219 --> 00:16:19.070
it changed because it became a way to, uh,

272
00:16:19.080 --> 00:16:22.539
summarize and synthesize information. So that there is, it's

273
00:16:22.539 --> 00:16:25.239
always a process and, and I think that uh

274
00:16:26.159 --> 00:16:29.520
Even now something like this uh is happening and

275
00:16:29.520 --> 00:16:32.239
it will happen that's, that's, uh, and then I

276
00:16:32.239 --> 00:16:34.359
think this is so the the let let's say

277
00:16:34.359 --> 00:16:37.080
that the election from the past technologies is not

278
00:16:37.080 --> 00:16:39.280
too much to say it is it already happened.

279
00:16:39.400 --> 00:16:42.400
So I mean it could be different now but

280
00:16:42.400 --> 00:16:44.739
I think that the important lesson is that uh

281
00:16:44.960 --> 00:16:49.119
we need to remember how thing uh change and

282
00:16:49.119 --> 00:16:53.479
that is not just a a a deterministic um.

283
00:16:54.090 --> 00:16:57.169
Introduction of the new technologies, but it's a process

284
00:16:57.169 --> 00:17:00.690
of adaptation between society and technology.

285
00:17:01.549 --> 00:17:03.599
So I would like to get into the topic

286
00:17:03.599 --> 00:17:08.750
or the specific panic surrounding online misinformation, but just

287
00:17:08.750 --> 00:17:11.469
before that, let me ask you more broadly. I

288
00:17:11.469 --> 00:17:14.589
mean, people have this very common idea, or at

289
00:17:14.589 --> 00:17:18.390
least people who deal with or talk about online

290
00:17:18.390 --> 00:17:22.608
misinformation have this very an idea that people are

291
00:17:22.608 --> 00:17:26.529
extremely gullible and whatever kind of information they are

292
00:17:26.529 --> 00:17:30.989
exposed to, whatever they read, they watch, they listen

293
00:17:30.989 --> 00:17:33.989
to, they will fall for it. But I mean,

294
00:17:34.168 --> 00:17:38.048
how influenable or how gullible are people

295
00:17:38.048 --> 00:17:43.900
really. Yes, that's a million dollar question that they

296
00:17:43.900 --> 00:17:48.060
say. The, the, the, the short answer is, I

297
00:17:48.060 --> 00:17:52.959
think less than what we tend to think. Uh,

298
00:17:53.010 --> 00:17:55.719
I, I, I try to give a bit longer

299
00:17:55.719 --> 00:17:59.079
answer. Well, first, one interesting thing that uh, that

300
00:17:59.079 --> 00:18:02.359
is related to this is that, uh, uh, which

301
00:18:02.359 --> 00:18:04.760
is kind of curious. We, we tend to think

302
00:18:04.760 --> 00:18:09.479
that other people are gullible, not much ourselves. So

303
00:18:09.479 --> 00:18:12.060
we actually did a, a study about this with,

304
00:18:12.160 --> 00:18:16.020
uh, with Sasha Chai. So we, um, we wanted

305
00:18:16.020 --> 00:18:21.459
to. Understand what kind of beliefs were linked to

306
00:18:21.459 --> 00:18:25.160
this idea that misinformation is very dangerous and it's

307
00:18:25.160 --> 00:18:29.640
very widespread so we had our participant to answer

308
00:18:29.640 --> 00:18:32.400
to this question about misinformation and also about a

309
00:18:32.400 --> 00:18:36.599
series of uh uh other other question about you

310
00:18:36.599 --> 00:18:39.500
know if they were. Worried about the status of

311
00:18:39.500 --> 00:18:42.739
the world if they were thinking, sorry, that they

312
00:18:42.739 --> 00:18:46.739
were not really good at detecting misinformation, if they

313
00:18:46.739 --> 00:18:49.140
were thinking that others were not really good a

314
00:18:49.140 --> 00:18:53.439
series of questions. And uh what we found was

315
00:18:53.439 --> 00:18:58.599
that the, the, the most consistent correlation because it's

316
00:18:58.599 --> 00:19:01.439
a, is a correlational study but the most consistent

317
00:19:01.439 --> 00:19:05.839
belief that was correlated with the thinking that misinformation

318
00:19:05.839 --> 00:19:09.719
was a a a big problem was exactly the

319
00:19:09.719 --> 00:19:14.170
idea that uh. Other people were gullible, basically, it's

320
00:19:14.170 --> 00:19:17.260
called in, in psychology, it's called sometimes first person

321
00:19:17.260 --> 00:19:21.729
effort is that the, the other people have, are

322
00:19:21.729 --> 00:19:24.810
really socially influenceable but we are not. So that's

323
00:19:24.810 --> 00:19:29.520
a kind of another uh pretty consistent pattern like

324
00:19:29.520 --> 00:19:31.530
I was saying about the negativity bias. So it's

325
00:19:31.530 --> 00:19:34.209
something that seems to be pretty robust in different

326
00:19:34.209 --> 00:19:38.670
study. And I think say something about the fact

327
00:19:38.670 --> 00:19:43.949
that uh our uh intuitive ideas about uh social

328
00:19:43.949 --> 00:19:47.550
influence are not super good so we tend to

329
00:19:47.550 --> 00:19:51.380
think that other are we not too much and

330
00:19:51.380 --> 00:19:54.469
then going more on the, on the, you know,

331
00:19:54.550 --> 00:19:56.750
on the science about that as I was saying

332
00:19:56.750 --> 00:20:01.880
before. Uh, I'm coming from, from cultural evolution. Uh,

333
00:20:02.010 --> 00:20:06.349
IN classic cultural evolution there is this idea of,

334
00:20:06.449 --> 00:20:10.530
um, what are called social learning strategies or transmission

335
00:20:10.530 --> 00:20:13.410
biases. So the idea is that we do not

336
00:20:13.410 --> 00:20:17.000
copy at random, but we have some. Kind of,

337
00:20:17.060 --> 00:20:22.380
uh, general heuristic like a copy, the majority, let's

338
00:20:22.380 --> 00:20:26.900
say copy from prestigious people copy certain content and

339
00:20:26.900 --> 00:20:28.900
not added. So this is a kind of a

340
00:20:28.900 --> 00:20:31.459
first level so we don't just, we are not

341
00:20:31.459 --> 00:20:35.630
just a random influence them. Uh, BUT I think

342
00:20:35.630 --> 00:20:39.469
that one can, uh, go a bit further than

343
00:20:39.469 --> 00:20:42.469
that. So, mm, in, in, in the previous book

344
00:20:42.469 --> 00:20:44.630
in Cultural Revolution in the digital age, I was

345
00:20:44.630 --> 00:20:49.569
actually trying to understand whether these, uh, these, uh,

346
00:20:49.589 --> 00:20:55.069
heuristic, these, uh, these strategies were possibly, uh, deleterious

347
00:20:55.069 --> 00:20:57.469
in online dynamics because, you know, if we copy

348
00:20:57.469 --> 00:21:01.469
prestigious people we could just follow some influencer or

349
00:21:01.469 --> 00:21:05.550
some politician. But I was saying that this. Didn't

350
00:21:05.550 --> 00:21:08.150
seem to be the case and uh and in

351
00:21:08.150 --> 00:21:10.390
this past year I tried to go on in

352
00:21:10.390 --> 00:21:12.930
this in this direction and for example we found

353
00:21:12.930 --> 00:21:18.859
that in general um. Not only people do not

354
00:21:18.859 --> 00:21:22.420
use much these uh strategies, but also in the,

355
00:21:22.500 --> 00:21:26.359
in the Cultural Revolution literature, we found that that

356
00:21:26.819 --> 00:21:30.619
people do not copy other even when they should.

357
00:21:31.410 --> 00:21:36.069
So we did a uh like a Sort of,

358
00:21:36.349 --> 00:21:41.589
sort of reviews of uh uh experiment of, in

359
00:21:41.589 --> 00:21:44.569
which people could use social learning or individual learning.

360
00:21:45.130 --> 00:21:48.449
And um what, what we found is that uh

361
00:21:49.069 --> 00:21:52.510
People are just, at least in this experiment made

362
00:21:52.510 --> 00:21:55.930
by research and in cultural evolution that we tasting

363
00:21:55.930 --> 00:22:00.469
the, the optimal social learning usage, but participants were

364
00:22:00.469 --> 00:22:04.349
not doing it, so people in experiments tended to

365
00:22:04.349 --> 00:22:08.020
be uh stubborn instead of being too gullible. And

366
00:22:08.020 --> 00:22:10.949
then when you start to look uh uh in,

367
00:22:11.030 --> 00:22:13.439
in various literature, you find that this is a

368
00:22:13.439 --> 00:22:16.430
bit of a pattern. So there are studies about

369
00:22:16.430 --> 00:22:20.349
the fact that advertisements do not work that much.

370
00:22:20.430 --> 00:22:22.949
So if you put together the effect of advertisement,

371
00:22:23.579 --> 00:22:26.469
some work very well and usually people remember the

372
00:22:26.469 --> 00:22:29.660
effective advertisement, some do not, and so the effect

373
00:22:29.660 --> 00:22:33.430
tend to be zero. Studies about in, uh, in

374
00:22:33.430 --> 00:22:38.079
sociology, about the fact that people tend to. Basically

375
00:22:38.079 --> 00:22:41.640
not change too much their idea during their life

376
00:22:41.640 --> 00:22:45.880
course they they say about the stable disposition model

377
00:22:45.880 --> 00:22:48.770
so that people do not change too much and

378
00:22:48.959 --> 00:22:52.359
so can this of provide a different, a different

379
00:22:52.359 --> 00:22:56.930
picture of uh social influence and then again uh

380
00:22:57.119 --> 00:22:59.839
in in in this in this my, this my,

381
00:22:59.859 --> 00:23:04.010
my, my. My, my research, I arrived to this

382
00:23:04.010 --> 00:23:05.849
idea, as I was saying before I was mentioned

383
00:23:05.849 --> 00:23:09.510
in the work of, uh, Hugo Mercier or, or

384
00:23:09.510 --> 00:23:12.670
the people in Paris about epistemic vigilance, which is,

385
00:23:13.250 --> 00:23:18.000
uh, um, again is an evolutionary approach to, uh,

386
00:23:18.010 --> 00:23:20.810
social influence but is a bit more, let's say

387
00:23:21.170 --> 00:23:24.630
strict than what would be standard cultural evolution. Now

388
00:23:24.630 --> 00:23:26.650
I, I don't enter in the detail of this

389
00:23:26.650 --> 00:23:29.569
approach. I'm sure that. You already talk with them

390
00:23:29.569 --> 00:23:30.229
and so you would,

391
00:23:31.130 --> 00:23:33.839
yeah, I've, I've have, I've had Hugo Murcia and

392
00:23:33.839 --> 00:23:35.089
Dan Sperber on the show,

393
00:23:35.270 --> 00:23:39.239
yeah, exactly, exactly, and, and I think to me

394
00:23:39.239 --> 00:23:41.609
at the moment this seemed to be the, the,

395
00:23:41.689 --> 00:23:45.290
the theoretical framework that at least in my ideas

396
00:23:45.290 --> 00:23:48.770
seem to describe to explain better what we see

397
00:23:48.770 --> 00:23:53.280
in reality. And uh and um which of course

398
00:23:53.280 --> 00:23:56.199
doesn't mean that we do not change our mind,

399
00:23:56.280 --> 00:23:58.439
but it means that the way we change our

400
00:23:58.439 --> 00:24:02.199
mind is not by seeing a few fake news

401
00:24:02.199 --> 00:24:05.920
or uh because of political propaganda, but we need

402
00:24:05.920 --> 00:24:10.589
to take into account much. More, uh, many factors

403
00:24:10.589 --> 00:24:13.250
and, uh, and so on and so let's say

404
00:24:13.560 --> 00:24:16.630
going, going to, to, to, to, to, to the,

405
00:24:16.709 --> 00:24:19.829
the, the topic, the, the techno panic topic with

406
00:24:19.829 --> 00:24:22.270
this background let's say, OK, we are not that

407
00:24:22.270 --> 00:24:26.550
gullible changing our mind is not that easy, um.

408
00:24:26.930 --> 00:24:29.839
It, it, it, it gives you some, some, some,

409
00:24:29.849 --> 00:24:33.209
uh, perspective to look at all the aspects that,

410
00:24:33.250 --> 00:24:38.329
you know, from, uh, microtargeting to misinformation to the

411
00:24:38.329 --> 00:24:40.770
effect on, on mental health they say OK, but

412
00:24:40.770 --> 00:24:44.520
maybe they're not so direct and so, uh, so,

413
00:24:44.689 --> 00:24:49.449
so, so clear as they are sometimes presented in

414
00:24:49.449 --> 00:24:52.449
the, in the, in the narrative. Mhm.

415
00:24:53.599 --> 00:24:56.030
So, but let me ask you then, what do

416
00:24:56.030 --> 00:25:00.439
we know about the availability of online disinformation? Because

417
00:25:00.439 --> 00:25:03.520
again, people who write on this topic, uh, I

418
00:25:03.520 --> 00:25:07.040
mean, not everyone, of course, but many people, popular

419
00:25:07.040 --> 00:25:10.349
writers who write on this topic, uh, tend to

420
00:25:10.349 --> 00:25:13.239
present it as a moral panic in the sense

421
00:25:13.239 --> 00:25:19.599
that, oh, online disinformation is everywhere and it influences

422
00:25:19.599 --> 00:25:22.459
a lot of people and it spreads. It's very

423
00:25:22.459 --> 00:25:25.030
easily. I mean, what do we know about that?

424
00:25:25.099 --> 00:25:27.900
Is it really the case that it is, there's

425
00:25:27.900 --> 00:25:31.199
lots of disinformation on the internet and that it

426
00:25:31.739 --> 00:25:35.599
spreads very easily? Like, for example, even just during

427
00:25:35.599 --> 00:25:39.619
the, the last pandemic, the COVID-19 pandemic, at a

428
00:25:39.619 --> 00:25:43.939
certain point, some people were panicking because apparently they

429
00:25:43.939 --> 00:25:48.579
thought that many people suddenly were anti-vaxxers. I mean,

430
00:25:48.660 --> 00:25:50.239
is that really the case or not?

431
00:25:51.439 --> 00:25:55.310
Yeah, yeah, yeah, that's, that's, that's uh excellent. Uh,

432
00:25:55.530 --> 00:25:59.119
NO, it's not. So it's uh, it's, and again,

433
00:25:59.209 --> 00:26:01.729
it, it, it, it's strange because it, it was

434
00:26:01.729 --> 00:26:04.569
uh kind of surprising for me when I started

435
00:26:04.569 --> 00:26:06.290
to study these things as I was saying before,

436
00:26:06.449 --> 00:26:11.079
say 2016 when everybody was talking about, uh, uh,

437
00:26:11.089 --> 00:26:13.969
you know, social media is full of misinformation, the

438
00:26:13.969 --> 00:26:18.020
internet is full of misinformation. The few studies that

439
00:26:18.020 --> 00:26:20.579
started to appear, they were saying, OK, that's, that's

440
00:26:20.579 --> 00:26:23.930
not the case. I mean, the, the, the, the

441
00:26:24.140 --> 00:26:27.459
majority, of course, the, the point is that obviously

442
00:26:27.459 --> 00:26:30.660
if you go online you can find all the

443
00:26:30.660 --> 00:26:34.180
misinformation that you want. I can just now Google

444
00:26:34.180 --> 00:26:38.300
for, uh, Bill Gates, uh, whatever was doing with

445
00:26:38.300 --> 00:26:40.500
the vaccine or whatever, yeah, yeah, the,

446
00:26:40.660 --> 00:26:42.729
the chips on the vaccine chips,

447
00:26:42.739 --> 00:26:47.579
yeah. I, I, I would find thousands or millions

448
00:26:47.579 --> 00:26:50.739
of pages and various versions. So, so obviously it's

449
00:26:50.739 --> 00:26:53.979
there. The point is that we need to, if

450
00:26:53.979 --> 00:26:57.339
we want to assess the quantity of misinformation, we

451
00:26:57.339 --> 00:26:59.699
need to assess it in the context of the

452
00:26:59.699 --> 00:27:03.140
total quantity of information. So this would be the,

453
00:27:03.219 --> 00:27:06.099
the numerator of a fraction in which the denominator

454
00:27:06.099 --> 00:27:08.780
is all the information that we can find online.

455
00:27:09.380 --> 00:27:12.060
When we look at this, at it in this

456
00:27:12.060 --> 00:27:17.670
way, then it's always like uh. Few percentage points.

457
00:27:17.750 --> 00:27:20.349
Of course, it depends. There are many ways to

458
00:27:20.349 --> 00:27:24.469
measure it. It's, it's an open problem and, and,

459
00:27:24.479 --> 00:27:28.869
uh, um, still the, the results, I think now

460
00:27:28.869 --> 00:27:32.689
give a, a, a story that I think everybody

461
00:27:32.689 --> 00:27:35.890
would agree that if we look at the quantity

462
00:27:36.160 --> 00:27:40.030
of uh misinformation is always very very limited also

463
00:27:40.030 --> 00:27:44.310
because the the majority of people uh tend to

464
00:27:44.310 --> 00:27:48.589
look at the mainstream uh mainstream uh things that

465
00:27:48.589 --> 00:27:52.869
usually do not have a big incentive to to

466
00:27:52.869 --> 00:27:59.770
uh to share misinformation uh. It's also generally um

467
00:27:59.910 --> 00:28:02.910
like the people that are a lot of online

468
00:28:02.910 --> 00:28:06.109
they will also consume and share more misinformation but

469
00:28:06.109 --> 00:28:08.829
they would consume and share more of everything so

470
00:28:08.829 --> 00:28:13.109
that's that's again a problem of uh uh of

471
00:28:13.109 --> 00:28:16.199
um. You know, you, you have to put this

472
00:28:16.199 --> 00:28:19.359
in relation between the total activity or the total

473
00:28:19.359 --> 00:28:22.939
quantity of information. So this is about, about the

474
00:28:22.939 --> 00:28:29.119
quantity. Um, WE don't have any strong uh support

475
00:28:29.119 --> 00:28:33.040
of the idea that uh misinformation is more effective

476
00:28:33.040 --> 00:28:37.810
to spread than uh true information. In fact, it's

477
00:28:37.810 --> 00:28:43.130
um. Intuitive to think that uh uh true information

478
00:28:43.130 --> 00:28:46.569
is more likely to, to spread than false information

479
00:28:46.569 --> 00:28:50.050
and in this case the intuition is probably correct

480
00:28:50.489 --> 00:28:53.209
um so there's, there's nothing really. I, I mean

481
00:28:53.209 --> 00:28:56.280
I discussed a a a famous paper that was,

482
00:28:56.650 --> 00:29:01.050
that was uh uh proposing this idea but I

483
00:29:01.050 --> 00:29:03.589
think it's not uh uh we have strong support

484
00:29:03.589 --> 00:29:07.829
for that. The, the main point though is, um,

485
00:29:08.180 --> 00:29:10.500
the idea that one could say, OK, even if

486
00:29:10.500 --> 00:29:14.540
there is not much misinformation could have some strong

487
00:29:14.540 --> 00:29:19.810
effect on people. It could, but the, the, the

488
00:29:19.810 --> 00:29:23.709
point is that, uh, nobody really knows, and, uh,

489
00:29:23.729 --> 00:29:28.050
so the people that claim this are just assuming

490
00:29:28.050 --> 00:29:31.969
that this is the case. Uh, IT'S obviously very

491
00:29:31.969 --> 00:29:34.729
difficult to measure the, the efforts. So what what

492
00:29:34.729 --> 00:29:37.530
I'm saying here is just that, uh, uh, we

493
00:29:37.530 --> 00:29:41.849
don't know and then. If we have a a

494
00:29:41.849 --> 00:29:44.930
perspective like the one I have so about an

495
00:29:44.930 --> 00:29:49.489
evolutionary perspective, the safe assumption is to say that

496
00:29:49.489 --> 00:29:52.969
probably there is not much effort. What is, what

497
00:29:52.969 --> 00:29:56.920
is worrying is that still now people tend to

498
00:29:56.920 --> 00:30:03.989
conflate. Consumption of misinformation and change of uh uh

499
00:30:03.989 --> 00:30:08.069
beliefs of ideas so people are exposed to misinformation

500
00:30:08.310 --> 00:30:11.630
so you see you you don't uh you are

501
00:30:11.630 --> 00:30:17.729
uh anti-vaccination because you saw some misinformation to me

502
00:30:17.790 --> 00:30:21.579
it's usually the, the, the, the causal chain doesn't

503
00:30:21.579 --> 00:30:25.270
go in this direction so you are, uh, opposite

504
00:30:25.270 --> 00:30:30.209
to the vaccine for complicated, uh. Um, REASON that

505
00:30:30.209 --> 00:30:33.880
can be your, your, uh, related to your, uh,

506
00:30:33.979 --> 00:30:37.380
economical and social background, to your belief, uh, and

507
00:30:37.380 --> 00:30:41.540
your trust in, uh, institution and in politics, so

508
00:30:41.540 --> 00:30:45.369
you're also skeptical about vaccines. So you see some,

509
00:30:45.780 --> 00:30:48.459
uh, our friend, uh, Bill Gates and the, and

510
00:30:48.459 --> 00:30:51.959
the chip, and you just, uh. Engage or share

511
00:30:51.959 --> 00:30:55.439
with this misinformation not because you change your belief,

512
00:30:55.479 --> 00:30:57.800
not to have other change belief because it would

513
00:30:57.800 --> 00:31:01.719
not work, but you just signaling, I look at

514
00:31:01.719 --> 00:31:05.640
this. I, I'm against the government so I think

515
00:31:05.640 --> 00:31:09.000
that's, that would be a more plausible explanation of

516
00:31:09.000 --> 00:31:12.310
why some people would do this. And, uh, and,

517
00:31:12.319 --> 00:31:15.060
uh, so, so I, I, I really think that,

518
00:31:15.079 --> 00:31:17.270
uh, that, then we can talk more about the,

519
00:31:17.599 --> 00:31:21.119
the, the, the more deeper ethical problem about the

520
00:31:21.119 --> 00:31:24.160
fight against the misinformation, but for, for staying on

521
00:31:24.160 --> 00:31:27.380
the, on the empirical uh part. I think there

522
00:31:27.380 --> 00:31:30.099
has been a lot of uh not uh not

523
00:31:30.099 --> 00:31:35.140
uh really robust um fear about misinformation and about

524
00:31:35.140 --> 00:31:38.180
the effect, about the quantity, about the, the, the

525
00:31:38.180 --> 00:31:41.560
changing people's mind that is really not uh not

526
00:31:41.780 --> 00:31:44.020
very robust from the empirical point of view.

527
00:31:45.000 --> 00:31:48.329
But do you think there are any aspects of

528
00:31:48.329 --> 00:31:52.290
disinformation we should worry about, and do you think

529
00:31:52.290 --> 00:31:57.369
that we should do something to fight it, and

530
00:31:57.369 --> 00:31:58.530
if so, how?

531
00:32:00.010 --> 00:32:03.650
OK, that's, that's a complicated question. I, I, I,

532
00:32:03.760 --> 00:32:09.010
I, of course, some aspect of misinformation are, are

533
00:32:09.010 --> 00:32:12.569
not, uh, so, so like, like, let's, let's use

534
00:32:12.569 --> 00:32:16.369
again the example of the vaccine misinformation. So this,

535
00:32:16.410 --> 00:32:19.010
this is, uh, this is, uh, uh, we, we

536
00:32:19.010 --> 00:32:22.130
don't want it, so. The point is that as

537
00:32:22.130 --> 00:32:24.569
I would put the, the thing, what, what we

538
00:32:24.569 --> 00:32:27.369
don't want is the people to be against the

539
00:32:27.369 --> 00:32:29.930
vaccine. This, we agree that we don't want, we

540
00:32:29.930 --> 00:32:33.569
want that people uh would uh again I'm let,

541
00:32:33.650 --> 00:32:37.760
let, let, let's. Keep aside all the debates about

542
00:32:37.760 --> 00:32:40.160
the vaccine. Just, just say, OK, vaccinate is good.

543
00:32:40.439 --> 00:32:43.239
We agree on that. Uh, WE do want that

544
00:32:43.239 --> 00:32:47.290
people, uh, do it. Um, AND we, we have

545
00:32:47.290 --> 00:32:50.329
to fight for this, but I don't think we

546
00:32:50.329 --> 00:32:52.609
have to, to fight for this. Uh, uh, WE

547
00:32:52.609 --> 00:32:56.469
cannot just fight against the, the misinformation on the

548
00:32:56.469 --> 00:33:02.489
vaccine. We have to, uh, make these people not

549
00:33:02.489 --> 00:33:06.290
receptive to this misinformation, and this misinformation will disappear.

550
00:33:06.609 --> 00:33:08.609
So I think that what we have to fight

551
00:33:08.609 --> 00:33:13.680
for is more the. Uh, THE demand for misinformation

552
00:33:13.680 --> 00:33:17.239
and the production because, you know, if the, we

553
00:33:17.239 --> 00:33:19.719
can do all the fight of misinformation that we

554
00:33:19.719 --> 00:33:22.239
want to try to avoid this, but if the

555
00:33:22.239 --> 00:33:25.890
demand will be there. Somehow the demand will be

556
00:33:25.890 --> 00:33:30.329
satisfied in, in, in somewhere in social media or

557
00:33:30.329 --> 00:33:33.089
in the internet so we, we, we used to

558
00:33:33.089 --> 00:33:36.569
say that misinformation is more a symptom than a

559
00:33:36.569 --> 00:33:39.920
cause of something so. Yes, we want to fight

560
00:33:40.339 --> 00:33:44.500
something that appears as misinformation, but the, the, the

561
00:33:44.500 --> 00:33:47.640
way to fight it is not to fight misinformation,

562
00:33:47.660 --> 00:33:49.699
but is to fight the reason why people would

563
00:33:49.699 --> 00:33:54.819
be sensitive or willing to accept this misinformation. So

564
00:33:54.819 --> 00:33:57.579
I think that's, that's, and, and, and I think

565
00:33:57.579 --> 00:34:00.280
really this is a, a, a key point because.

566
00:34:00.979 --> 00:34:03.540
You know, think, uh, let, let, let's leave on

567
00:34:03.540 --> 00:34:06.579
the side and vaccine case, but like people focused

568
00:34:06.579 --> 00:34:09.978
for, for, uh, in the first Trump election to

569
00:34:09.978 --> 00:34:13.458
say, OK, we have to. Trump was elected by

570
00:34:13.458 --> 00:34:15.458
with the fake news that they were called at

571
00:34:15.458 --> 00:34:19.219
the moment, so we have to fight, uh, misinformation.

572
00:34:20.438 --> 00:34:24.290
And uh we're still there so maybe maybe we

573
00:34:24.290 --> 00:34:27.149
had to find something else uh and and uh

574
00:34:27.649 --> 00:34:29.770
and so yeah I, I, I think we we

575
00:34:29.770 --> 00:34:32.168
need to be careful to, to this is an

576
00:34:32.168 --> 00:34:35.409
easy way to, to, you know, we, you have

577
00:34:35.409 --> 00:34:38.330
something that you don't like and you identify and

578
00:34:38.330 --> 00:34:42.469
this is another big feature of techno panics so

579
00:34:42.530 --> 00:34:45.250
we don't like Trump, let's say, and so we

580
00:34:45.250 --> 00:34:49.428
say OK, it's easy to identify. The problem in

581
00:34:49.428 --> 00:34:54.629
the misinformation is May be easy to uh fight

582
00:34:54.629 --> 00:34:58.110
it, but if we are not identifying the real

583
00:34:58.110 --> 00:35:01.790
problem, the social, the cultural, the economic cause, we

584
00:35:01.790 --> 00:35:04.149
are a bit of uh wasting our time.

585
00:35:05.770 --> 00:35:10.040
So, uh, a topic that is somewhat related to

586
00:35:10.040 --> 00:35:13.050
misinformation but is not exactly the same, at least

587
00:35:13.050 --> 00:35:16.929
psychologically. Uh, LET'S talk a little bit about conspiracy

588
00:35:16.929 --> 00:35:21.810
theories. So, um, how, how do conspiracy theories come

589
00:35:21.810 --> 00:35:26.250
about? I mean, what aspects of our psychology explain

590
00:35:26.250 --> 00:35:31.729
them and are conspiracy theories necessarily irrational?

591
00:35:33.340 --> 00:35:38.350
Yes, that's, that's another interesting point, so. There has

592
00:35:38.350 --> 00:35:40.810
been also in this case, I think a sort

593
00:35:40.810 --> 00:35:47.530
of Panics about the relationship between social media and

594
00:35:47.530 --> 00:35:51.010
conspiracy theories, this idea that, uh, uh, the world

595
00:35:51.010 --> 00:35:54.850
is full of, uh, conspiracies now, uh, this also,

596
00:35:54.929 --> 00:35:56.969
I think, is the same feature of what you

597
00:35:56.969 --> 00:36:02.040
already said about, uh, uh, misinformation. So not, uh,

598
00:36:02.050 --> 00:36:05.679
strong, uh, empirical uh support, at least, uh, uh,

599
00:36:05.689 --> 00:36:12.300
uh, dubious empirical support. Uh, UNCLEAR causal link between

600
00:36:12.300 --> 00:36:17.040
believing in a conspiracy theory and changing behaviors and

601
00:36:17.040 --> 00:36:21.909
ideas. Uh, CONSPIRACY theories do exist. They existed for,

602
00:36:21.919 --> 00:36:26.360
uh, centuries or millennia, as you were saying, conspiracy

603
00:36:26.360 --> 00:36:31.270
theories, they exist and they are. Up to a

604
00:36:31.270 --> 00:36:36.989
point relatively common because they do um have some,

605
00:36:37.189 --> 00:36:41.679
some psychological function. For example, they have uh um

606
00:36:41.870 --> 00:36:45.550
epistemic uh functions so they make us, you know,

607
00:36:45.629 --> 00:36:48.270
it's, it's the world is a very complicated place

608
00:36:48.270 --> 00:36:53.189
and they are uh usually simple way to, uh,

609
00:36:53.469 --> 00:36:56.750
understand the, uh, events that are complicated. Let's, let's

610
00:36:56.750 --> 00:36:59.530
use again the, the, the COVID example. It's very.

611
00:37:00.310 --> 00:37:02.949
Difficult to, to know what exactly happened, but if

612
00:37:02.949 --> 00:37:07.179
you say, you know, some, some, uh, whatever, uh,

613
00:37:07.189 --> 00:37:09.750
produce it on purpose and throw it around is

614
00:37:09.750 --> 00:37:13.149
explained, uh, give you like an epistemic value they

615
00:37:13.149 --> 00:37:19.189
have social value so, um, beside knowing and understanding,

616
00:37:19.310 --> 00:37:22.870
give us an explanation that, uh, uh, make the,

617
00:37:22.949 --> 00:37:28.479
the, the. The complexity of, of uh, of events

618
00:37:28.479 --> 00:37:30.199
in a way that you can understand and say

619
00:37:30.199 --> 00:37:33.080
OK that's that's why this happened so, so there

620
00:37:33.080 --> 00:37:37.149
are a lot of reasons. Um, SO it's not

621
00:37:37.149 --> 00:37:41.149
surprising that they are around. Uh, THE question is,

622
00:37:41.229 --> 00:37:45.909
is there any special relationship within social media or

623
00:37:45.909 --> 00:37:48.550
there has been an increase in social with in

624
00:37:48.550 --> 00:37:52.229
the past year because of social media of beliefs

625
00:37:52.229 --> 00:37:56.510
in, uh, conspiracy theories. And again, this is not,

626
00:37:56.520 --> 00:38:01.159
uh, clear. So again, the, the, the, the, the

627
00:38:01.159 --> 00:38:04.840
post-true era, it's, it's a bit, uh, problematic. Conspiracy

628
00:38:04.840 --> 00:38:09.360
theories were there before. It's even more difficult here

629
00:38:09.360 --> 00:38:13.840
than measuring misinformation like calculating the proportion of beliefs

630
00:38:13.840 --> 00:38:18.790
in conspiracy theories in 60 years ago. It's, it's

631
00:38:18.790 --> 00:38:22.320
very complicated, but some people tried, uh, tried to

632
00:38:22.320 --> 00:38:26.129
do the, the Joe Sinski and the group. And

633
00:38:26.129 --> 00:38:27.929
when you look at this again it doesn't seem

634
00:38:27.929 --> 00:38:30.689
that there is a clear pattern in which they,

635
00:38:30.719 --> 00:38:36.250
they increase of course social media are, uh, conspiracy

636
00:38:36.250 --> 00:38:39.320
theories spreads on social media because everything spread on

637
00:38:39.320 --> 00:38:42.370
social media so you will not be, uh, surprised

638
00:38:42.370 --> 00:38:46.370
about that. In the book I discuss the fact

639
00:38:46.370 --> 00:38:49.409
that maybe this is a pretty speculative. I found

640
00:38:49.409 --> 00:38:54.169
it interesting though that uh. One aspect of conspiracy

641
00:38:54.169 --> 00:38:58.360
theories is that uh um people that believe the

642
00:38:58.360 --> 00:39:04.209
conspiracy theory can somehow, somehow participate in the construction

643
00:39:04.209 --> 00:39:06.770
of the theory. He, a, a good example is

644
00:39:06.770 --> 00:39:09.889
uh QA on the, the, the story. I don't

645
00:39:09.889 --> 00:39:13.689
know if you remember the, the, um, I mean

646
00:39:13.689 --> 00:39:16.370
now is a bit quieter but there was this

647
00:39:16.370 --> 00:39:21.149
queue that was. Putting drops online that had to

648
00:39:21.149 --> 00:39:25.090
be interpreted by the, the, the, and, and I

649
00:39:25.090 --> 00:39:27.810
think this is an interesting case because uh in

650
00:39:27.810 --> 00:39:29.530
this case there is a link with social media

651
00:39:29.530 --> 00:39:32.310
you could do this only on social media uh

652
00:39:32.310 --> 00:39:34.370
50 years ago would have been a problem for

653
00:39:34.370 --> 00:39:38.090
you to have people to collaborate in this creation

654
00:39:38.090 --> 00:39:40.320
of the conspiracy because how would you do it

655
00:39:40.320 --> 00:39:42.949
you can do it in your. Uh, YOU know,

656
00:39:43.169 --> 00:39:45.909
cafe or something, but would have not been very

657
00:39:45.909 --> 00:39:48.989
effective. So in a way I think social media

658
00:39:48.989 --> 00:39:54.459
makes this collaborative construction of conspiracy theories maybe, uh,

659
00:39:54.469 --> 00:39:58.310
easier. So maybe they can change some features of

660
00:39:58.310 --> 00:40:01.830
the conspiracy theories, but there has been a, a,

661
00:40:01.909 --> 00:40:07.620
a. There has been an increase because of social

662
00:40:07.620 --> 00:40:10.610
media is, is not, uh, is not clear again

663
00:40:10.610 --> 00:40:13.229
they are there and I think if I can

664
00:40:13.229 --> 00:40:15.290
add the, the, the, the logic to me is

665
00:40:15.290 --> 00:40:18.050
the same of misinformation so we, we see them

666
00:40:18.610 --> 00:40:22.389
and, uh, you know. Now we can again look

667
00:40:22.389 --> 00:40:25.229
for a weird conspiracy theory online and we see

668
00:40:25.229 --> 00:40:29.070
them 50 years ago we had to to go

669
00:40:29.070 --> 00:40:33.389
and check with weird publications or with the with

670
00:40:33.389 --> 00:40:37.379
people. Well now I, I call it availability so

671
00:40:37.379 --> 00:40:39.750
everything is there, everything can be measured and counted

672
00:40:39.750 --> 00:40:42.830
so it appears that oh the world is very

673
00:40:42.830 --> 00:40:47.489
weird, but maybe it was weird or weirder even

674
00:40:47.870 --> 00:40:50.060
before we just didn't know because we were in

675
00:40:50.060 --> 00:40:53.050
our. Uh, WE, we didn't have the access to

676
00:40:53.050 --> 00:40:54.840
this information. Mhm.

677
00:40:56.110 --> 00:40:59.229
So, let me ask you about another very common

678
00:40:59.229 --> 00:41:04.030
claim that people make, that online and particularly on

679
00:41:04.030 --> 00:41:09.209
social media, we live in echo chambers and there's

680
00:41:09.209 --> 00:41:12.229
filter filter bubbles, and so basically we're, what people

681
00:41:12.229 --> 00:41:16.709
are saying is that The sort of information diet

682
00:41:16.709 --> 00:41:21.750
we get online is very biased and tailored to

683
00:41:21.750 --> 00:41:24.989
our own beliefs, our own political beliefs and other

684
00:41:24.989 --> 00:41:27.750
kinds of beliefs, and so we don't really get

685
00:41:27.750 --> 00:41:32.110
exposed to uh contrary views. We don't really get

686
00:41:32.110 --> 00:41:35.989
exposed to information that contradicts or that would tell

687
00:41:35.989 --> 00:41:39.610
us something different than what we already believe in.

688
00:41:39.709 --> 00:41:41.209
I mean, is that really the case?

689
00:41:42.219 --> 00:41:47.429
Yeah, this has been another, another big, uh big

690
00:41:47.429 --> 00:41:50.570
fear related to social media game that has been,

691
00:41:50.709 --> 00:41:54.669
you know, social media start to be widespread and

692
00:41:54.669 --> 00:41:57.229
you have Trump and you have populist Party and

693
00:41:57.229 --> 00:41:59.909
you have Brexit, and so as in the case

694
00:41:59.909 --> 00:42:05.770
of uh misinformation and conspiracy theories, it It was

695
00:42:05.770 --> 00:42:09.250
natural to, to, to, to, to link the two

696
00:42:09.250 --> 00:42:14.129
things, social media and these increasing people uh believing

697
00:42:14.129 --> 00:42:16.729
things that we didn't like we in this case,

698
00:42:16.810 --> 00:42:20.409
you know, I'm an academic liberal professor of sociology,

699
00:42:20.489 --> 00:42:23.169
so I'm like, oh wow. But, uh, even in

700
00:42:23.169 --> 00:42:26.250
this case I think it's, it's, uh, and, and

701
00:42:26.250 --> 00:42:29.149
I, I would say that now in this. Uh,

702
00:42:29.310 --> 00:42:31.050
IT, it, it, it kind of the fashion of

703
00:42:31.050 --> 00:42:34.169
the echo chambers on the internet can, kind of

704
00:42:34.229 --> 00:42:38.429
went down, I think, and again, even in this

705
00:42:38.429 --> 00:42:41.669
case, the, the, the basic idea is plausible, so

706
00:42:41.669 --> 00:42:44.500
we know that the algorithm would give us, uh,

707
00:42:44.830 --> 00:42:48.709
uh, on average similar things to what we already

708
00:42:48.709 --> 00:42:52.550
like uh we can create, we can choose to

709
00:42:52.550 --> 00:42:57.260
follow someone and not follow some other. Availability also

710
00:42:57.260 --> 00:42:59.959
this is is very good because if you look

711
00:43:00.520 --> 00:43:03.449
at the, the transmission of information in a social

712
00:43:03.449 --> 00:43:06.510
network you would obviously find clusters that tend to

713
00:43:06.629 --> 00:43:10.729
communicate more within than with the external and so

714
00:43:10.729 --> 00:43:14.669
this was another idea that was very uh popular

715
00:43:14.669 --> 00:43:19.409
and I think uh again not very, very well

716
00:43:19.409 --> 00:43:22.870
grounded and indeed the the more studies that happened

717
00:43:22.870 --> 00:43:30.280
they showed that uh um. The the the. The

718
00:43:30.280 --> 00:43:32.939
the echo chamber in nature of social media is

719
00:43:32.939 --> 00:43:37.439
less echo chamber than what people would would think

720
00:43:37.439 --> 00:43:42.459
that uh. That when you compare especially the situation

721
00:43:42.639 --> 00:43:47.280
in social media and not in social media in,

722
00:43:47.360 --> 00:43:51.719
in say offline life, uh, echo chambers and polarization

723
00:43:51.719 --> 00:43:56.520
are also present so um you know, traditional media

724
00:43:56.520 --> 00:44:00.919
are as much if not more polarized than than

725
00:44:00.919 --> 00:44:07.889
the social media, uh. Daily, daily interaction are also

726
00:44:07.889 --> 00:44:10.360
very polarized. The group of friends that we have

727
00:44:10.360 --> 00:44:14.280
are not usually, uh, very, very different from a

728
00:44:14.280 --> 00:44:16.679
political point of view. Uh, EVEN in, in, in,

729
00:44:16.760 --> 00:44:19.040
in Technopanic in the book I talk about also

730
00:44:19.040 --> 00:44:23.250
the example of, uh, like, like geographic. POLAR polar

731
00:44:23.250 --> 00:44:27.510
uh polarizations like like people live in different neighbors

732
00:44:27.510 --> 00:44:30.270
without even thinking about social media. You just, you

733
00:44:30.270 --> 00:44:33.399
know, there are neighbors for a rich and leftist

734
00:44:33.399 --> 00:44:37.030
people and whatever so it's, it's, I, I, I,

735
00:44:37.110 --> 00:44:40.350
I think the picture is much, much more complicated.

736
00:44:40.810 --> 00:44:43.659
And, and to me now this seems really like

737
00:44:43.659 --> 00:44:45.489
uh not the case, but there is a, I,

738
00:44:45.570 --> 00:44:47.770
I think a nice example is what happened now

739
00:44:47.770 --> 00:44:51.840
with the, with the um the, the division, the

740
00:44:51.840 --> 00:44:57.399
Blue Sky and I like it. I mean, people

741
00:44:57.419 --> 00:45:01.100
aid to create their own echo chambers because uh

742
00:45:01.100 --> 00:45:03.820
social media were too much mixing people so you

743
00:45:03.820 --> 00:45:05.979
say, OK, I want to refund. So, so it's

744
00:45:05.979 --> 00:45:09.050
the opposite of the idea of uh like the,

745
00:45:09.139 --> 00:45:11.739
it, it's probably social media that, that they were

746
00:45:11.739 --> 00:45:15.179
giving too much diversity and so people had to

747
00:45:15.179 --> 00:45:17.590
say, OK, I don't want this diversity. I prefer

748
00:45:17.590 --> 00:45:21.989
my, which is again. Reasonable, uh, for many points

749
00:45:21.989 --> 00:45:25.659
of view, but it's really the opposite of, of,

750
00:45:25.669 --> 00:45:29.510
uh, what the, the echo chambers idea would predict.

751
00:45:31.010 --> 00:45:34.810
And another aspect here that I think is important

752
00:45:34.810 --> 00:45:40.239
is that This this is really an example in

753
00:45:40.239 --> 00:45:42.330
which and, and I think is a problem in

754
00:45:42.330 --> 00:45:46.000
general for studies of, uh, social media is that

755
00:45:46.000 --> 00:45:50.879
they are, uh, generally US centric. So in, in

756
00:45:50.879 --> 00:45:54.149
US there has been clearly an increase in polarization

757
00:45:54.149 --> 00:45:58.199
in the last, uh, actually it predates social media.

758
00:45:58.280 --> 00:46:01.699
The increase in polarization is in the last decades,

759
00:46:02.239 --> 00:46:04.840
um, and then of course you, you link these

760
00:46:04.840 --> 00:46:08.899
two things. Uh, THE point is, is less clear

761
00:46:08.899 --> 00:46:13.139
that the same effort has been in other countries

762
00:46:13.139 --> 00:46:14.979
where of course you still had the, uh, the

763
00:46:14.979 --> 00:46:17.659
diffusion of, uh, social media, but you didn't have

764
00:46:17.659 --> 00:46:23.020
the same, um, the same increasing polarization and so

765
00:46:23.020 --> 00:46:25.340
again this is the same problem I was telling

766
00:46:25.340 --> 00:46:28.389
before about, uh, fake news is easy to say,

767
00:46:28.419 --> 00:46:30.989
OK. There is a problem we don't like the

768
00:46:30.989 --> 00:46:35.469
increasing polarization. Let's identify an easy target, echo chambers

769
00:46:35.469 --> 00:46:40.129
in social media, uh, but, but, uh, you know,

770
00:46:40.530 --> 00:46:43.010
sometimes this cannot be, be the right way to

771
00:46:43.010 --> 00:46:43.590
do it.

772
00:46:44.820 --> 00:46:48.139
So another very common thing that we've heard even

773
00:46:48.139 --> 00:46:53.489
in recent years from very notable people like Tristan

774
00:46:53.489 --> 00:46:57.979
Harris and others where they say that algorithms know

775
00:46:57.979 --> 00:47:01.739
us better than we know ourselves and that we

776
00:47:01.739 --> 00:47:05.969
can, for example, predict someone's personality. Done what they

777
00:47:05.969 --> 00:47:08.729
do on social media, and if, for example, you

778
00:47:08.729 --> 00:47:11.570
go on Netflix or on Amazon or on, on

779
00:47:11.570 --> 00:47:15.810
YouTube, they also, they always know you better than

780
00:47:15.810 --> 00:47:18.729
yourself and they always suggest content for you to

781
00:47:18.729 --> 00:47:23.250
watch that before you know you want to watch

782
00:47:23.250 --> 00:47:25.889
it. Something along those lines. Is, is that really

783
00:47:25.889 --> 00:47:26.090
true?

784
00:47:27.620 --> 00:47:31.649
Uh, WELL, I mean, it is a bit true,

785
00:47:31.820 --> 00:47:33.659
of course, I mean, of course, there is a

786
00:47:33.659 --> 00:47:37.699
lot of uh data that can be collected on

787
00:47:37.699 --> 00:47:42.139
our activities and of course they tell something about

788
00:47:42.139 --> 00:47:47.100
us and about our, our inclinations and it, it,

789
00:47:47.219 --> 00:47:49.899
it, our behavior can be up to a point

790
00:47:49.899 --> 00:47:54.379
predictable if we use our activities. Um, SO, so,

791
00:47:54.629 --> 00:47:56.709
on, on this, I, I'm pretty, I'm, I'm saying,

792
00:47:56.790 --> 00:47:58.889
OK, yeah, we, we have to be careful about

793
00:47:58.889 --> 00:48:01.750
this, uh, this, what, what we are doing online

794
00:48:01.750 --> 00:48:05.389
about how our data are, are, uh, who, who

795
00:48:05.389 --> 00:48:07.310
can have access to our data. So I think

796
00:48:07.310 --> 00:48:10.750
this is, this is true. Uh, THE point for

797
00:48:10.750 --> 00:48:14.010
me is what are the consequences of this. So,

798
00:48:14.280 --> 00:48:17.189
um, if, if we go from this to the

799
00:48:17.189 --> 00:48:23.790
fact that some, uh. Magic, uh, algorithm can change

800
00:48:23.790 --> 00:48:28.469
our political ideas or our behaviors, then, then it's

801
00:48:28.469 --> 00:48:33.060
a problem. The problem is there. So, um, something

802
00:48:33.060 --> 00:48:36.790
can be predicted, um, up to a point I

803
00:48:36.790 --> 00:48:39.610
think this is OK. So it's good if, if

804
00:48:39.659 --> 00:48:43.465
we have some personalization, for example, in, in. Uh,

805
00:48:43.544 --> 00:48:47.304
YOU know, if Amazon advised me some new books

806
00:48:47.304 --> 00:48:49.705
to buy, that's, I mean, the, the, the, the,

807
00:48:49.784 --> 00:48:52.344
the, the catalog of Amazon or the, or the,

808
00:48:52.504 --> 00:48:54.864
the, the, the, the amount of things that are

809
00:48:54.864 --> 00:48:59.614
online would be an algorithm should, should somehow do

810
00:48:59.614 --> 00:49:03.245
something, otherwise we cannot have a random search in

811
00:49:03.334 --> 00:49:06.739
this, uh, big search space. It's just that. An

812
00:49:06.739 --> 00:49:12.120
impossible problem. Um, I have to say, this is,

813
00:49:12.500 --> 00:49:14.500
I, I'm talking as a, as a, as a

814
00:49:14.500 --> 00:49:17.459
normal guy. Uh, MY, my feeling is that they

815
00:49:17.459 --> 00:49:20.100
are not that effective. They, I think the personalization

816
00:49:20.100 --> 00:49:22.699
algorithm should be more effective. I think in the

817
00:49:22.699 --> 00:49:24.860
book, I do this example when, when we moved

818
00:49:24.860 --> 00:49:28.889
in Toronto, I bought a, uh, mattress for the

819
00:49:28.889 --> 00:49:34.020
bed from IKEA. And then I had the 4

820
00:49:34.020 --> 00:49:37.219
weeks like advertisement of mattress of the bed so

821
00:49:37.219 --> 00:49:39.300
it's not that I'm opening a hotel or something.

822
00:49:39.540 --> 00:49:42.020
I bought my mattress, so it's like a not

823
00:49:42.020 --> 00:49:44.500
super smart algorithm should have this or you buy,

824
00:49:44.540 --> 00:49:46.879
you know, some Chinese food and then always sell

825
00:49:47.459 --> 00:49:50.100
it. I, I don't know, it seems pretty, pretty

826
00:49:50.100 --> 00:49:52.899
nothing for I, I never failed to be so

827
00:49:52.899 --> 00:49:56.689
like scared about this, the power of the algorithm.

828
00:49:57.429 --> 00:49:59.939
Say so, it's true to, to, as I was

829
00:49:59.939 --> 00:50:03.219
saying before that, that there is some predictive power

830
00:50:03.219 --> 00:50:06.429
that have been studies that they think about quite

831
00:50:06.429 --> 00:50:09.120
a reasonable that, you know, can predict, for example,

832
00:50:09.379 --> 00:50:14.100
the uh up to a point your personality traits

833
00:50:14.100 --> 00:50:17.219
based on, on your social media activity which is

834
00:50:17.219 --> 00:50:21.489
uh. Uh, REASONABLE, and, uh, it's, it's something that

835
00:50:21.489 --> 00:50:25.050
is quite interesting as I was saying, what can

836
00:50:25.050 --> 00:50:28.090
you do with that, uh, even in this case

837
00:50:28.090 --> 00:50:31.030
again there is the, the, the alarmistic narrative that,

838
00:50:31.090 --> 00:50:35.610
you know, Cambridge Analytica with the, the psychographic, I

839
00:50:35.610 --> 00:50:38.649
think was the term, uh, psychographic was like, you

840
00:50:38.649 --> 00:50:41.610
know, taking the data that having people change their

841
00:50:41.610 --> 00:50:46.070
vote that Brexit was, uh, was due to. To

842
00:50:46.070 --> 00:50:48.879
impart at least to the activity of Cambridge Analytica.

843
00:50:49.510 --> 00:50:52.429
Again, I, I repeat the same story. When we

844
00:50:52.429 --> 00:50:56.070
look at the, at the studies that try to,

845
00:50:56.270 --> 00:51:00.229
to, to look on the, um, for example, the

846
00:51:00.229 --> 00:51:04.820
persuasive effect of uh microtargeting. I, I, how do

847
00:51:04.820 --> 00:51:07.780
you say in English, micro targeting or micro targeting?

848
00:51:07.830 --> 00:51:08.100
Uh,

849
00:51:08.209 --> 00:51:12.179
I usually say micro targeting. I mean, maybe there

850
00:51:12.179 --> 00:51:14.260
are people who say micro targeting. I'm not

851
00:51:14.260 --> 00:51:16.260
sure. I don't know. I, I, I live in

852
00:51:16.260 --> 00:51:19.219
Italy now. I'm forgetting my English. Uh, LET'S say

853
00:51:19.219 --> 00:51:22.540
micro targeting, uh, what, what studies that look at

854
00:51:22.540 --> 00:51:29.340
the effect of microtargeting, political microtargeting. Mhm. Usually, as

855
00:51:29.340 --> 00:51:31.500
I, for all, I mean, I, I have the

856
00:51:31.500 --> 00:51:35.020
same pattern, but they, they don't find uh strong

857
00:51:35.020 --> 00:51:38.540
effort. The effort seems very small and very context-dependent.

858
00:51:39.060 --> 00:51:41.780
Of course, there are studies that show that if

859
00:51:41.780 --> 00:51:45.219
I, if I have some information of your personality

860
00:51:45.580 --> 00:51:49.620
and I put like an advertisement uh related to

861
00:51:49.620 --> 00:51:52.100
the personality, maybe you click a bit more on

862
00:51:52.100 --> 00:51:54.179
this than on the other, that is the opposite.

863
00:51:54.610 --> 00:51:57.989
But going from there to the effect of political

864
00:51:57.989 --> 00:52:02.010
uh microtargeting is really, again, I'm not saying in

865
00:52:02.010 --> 00:52:04.850
this case that uh we know that it doesn't

866
00:52:04.850 --> 00:52:07.929
work. I'm saying that uh we don't know whether

867
00:52:07.929 --> 00:52:13.139
it works. And this is, is very, is very

868
00:52:13.139 --> 00:52:16.060
uh different with the narrative that we have about

869
00:52:16.060 --> 00:52:18.860
the, the power of the algorithms that are changing

870
00:52:18.860 --> 00:52:23.560
our idea and The same logic, I think again

871
00:52:23.620 --> 00:52:25.580
when you look at this, then you are not

872
00:52:25.580 --> 00:52:29.060
looking at the uh more real problem. So like

873
00:52:29.060 --> 00:52:31.760
the, the, the, the Cambridge Analytica scandals, if you

874
00:52:31.760 --> 00:52:36.500
think that, that um changed the result of Brexit

875
00:52:36.500 --> 00:52:40.860
and the power of psychographic advertisement, but then. OK,

876
00:52:40.979 --> 00:52:44.379
maybe let's look at why, uh, Brexit happened more

877
00:52:44.379 --> 00:52:48.219
let's look about the why, uh, the usage of

878
00:52:48.219 --> 00:52:51.179
data in Cambridge Analytica, what was the problem? What

879
00:52:51.179 --> 00:52:54.100
was OK, what was not. If we say they

880
00:52:54.100 --> 00:52:56.540
are destroying the world, we are not really even

881
00:52:56.540 --> 00:53:01.060
criticizing well these, uh, these big corporations, so. Again,

882
00:53:01.110 --> 00:53:04.350
the, the, my, my, my, my more uh nuanced

883
00:53:04.350 --> 00:53:07.070
uh um view is not to say everything is

884
00:53:07.070 --> 00:53:08.830
good and they are doing good. It's, I think

885
00:53:08.830 --> 00:53:13.330
that it's not effective to have this catastrophic and

886
00:53:13.330 --> 00:53:15.689
overly negative, uh, narrative.

887
00:53:17.020 --> 00:53:20.810
So another very common claim, and because I mean,

888
00:53:21.020 --> 00:53:23.939
I spent some time on YouTube also because of

889
00:53:23.939 --> 00:53:27.639
my work, I, I've heard over the past few

890
00:53:27.639 --> 00:53:31.699
years many people talking about rabbit holes and that

891
00:53:31.699 --> 00:53:35.100
people can fall into rabbit holes on social media

892
00:53:35.100 --> 00:53:38.764
and. YouTube particularly, and there are even videos of

893
00:53:38.764 --> 00:53:41.645
people on YouTube talking about how they fell into

894
00:53:41.645 --> 00:53:45.814
a uh alt-right rabbit hole, for example. I mean,

895
00:53:45.955 --> 00:53:50.284
is that, uh, really something supported by the evidence

896
00:53:50.284 --> 00:53:53.084
that something can that uh like that can happen

897
00:53:53.084 --> 00:53:54.274
on social media?

898
00:53:54.764 --> 00:53:56.955
Again, that's, that's the thing. So this is a,

899
00:53:56.995 --> 00:54:01.574
uh, a relatively plausible story, you know, you have

900
00:54:01.574 --> 00:54:04.540
people spend a lot of time on YouTube. Uh,

901
00:54:04.590 --> 00:54:08.669
YouTube advise you other videos and maybe videos that

902
00:54:08.669 --> 00:54:13.199
are more emotionally. Uh, STRONG,

903
00:54:13.370 --> 00:54:14.510
compelling, yeah,

904
00:54:14.689 --> 00:54:15.050
sorry,

905
00:54:15.699 --> 00:54:17.610
um, emotionally compelling,

906
00:54:18.070 --> 00:54:21.030
yeah, exactly, it can be advised more by the

907
00:54:21.030 --> 00:54:25.580
algorithms. There is an increase in, uh, in, uh,

908
00:54:25.590 --> 00:54:29.270
again, the alt-right or whatever for young people. So

909
00:54:29.270 --> 00:54:32.070
that's, that's a very good story. It, it's plausible.

910
00:54:32.389 --> 00:54:36.110
It's a very, uh, make us feel well because

911
00:54:36.120 --> 00:54:40.310
you say, OK, these are good guys. Society is

912
00:54:40.310 --> 00:54:43.580
good, but YouTube is making them going in this

913
00:54:43.580 --> 00:54:48.489
direction. Uh, AGAIN, in this case, uh, there is

914
00:54:48.489 --> 00:54:50.969
about, I mean, usually when you, when you, when

915
00:54:50.969 --> 00:54:54.790
you read about these rabbit hole is generally. You

916
00:54:54.790 --> 00:54:58.669
know, uh, single stories of some people that tells

917
00:54:58.669 --> 00:55:00.739
the story about this, which I mean, I, I,

918
00:55:00.830 --> 00:55:02.840
I don't have a doubt that this story are,

919
00:55:03.070 --> 00:55:06.989
are, uh, real and honest. The point is when

920
00:55:06.989 --> 00:55:11.030
we try to study this with experiments, uh, or

921
00:55:11.030 --> 00:55:16.429
with, with data, it's also very, very, uh, unclear

922
00:55:16.429 --> 00:55:18.110
if there is an effort. In, in the book

923
00:55:18.110 --> 00:55:21.060
I talk about, uh, an experiment they did with

924
00:55:21.060 --> 00:55:26.510
uh what they call, um. Counterfactual bots so they

925
00:55:26.510 --> 00:55:30.489
had some, some bots being trained on the um

926
00:55:30.949 --> 00:55:32.949
because what is difficult in this case is to

927
00:55:32.949 --> 00:55:38.629
distinguish the effect of personal uh usage, personal choices

928
00:55:38.629 --> 00:55:42.419
and the algorithms of course people that go towards

929
00:55:42.419 --> 00:55:45.770
alt right will look at more alt right video,

930
00:55:45.949 --> 00:55:47.909
but it's because they want to or it's because

931
00:55:47.909 --> 00:55:52.070
the algorithm that uh uh push them that, that's

932
00:55:52.070 --> 00:55:55.899
another very difficult question. This, this, this experiment, they

933
00:55:55.899 --> 00:56:00.459
have these counterfactual bots. They were trained on the

934
00:56:00.699 --> 00:56:05.570
um viewing history of real individuals and then at

935
00:56:05.570 --> 00:56:09.540
some point, the, the bots were just following blindly

936
00:56:09.540 --> 00:56:13.510
because it's a bot. The, the, the algorithmic recommendation.

937
00:56:13.889 --> 00:56:17.169
And the users will keep on doing their own

938
00:56:17.169 --> 00:56:19.459
stuff so you can, in this case, you can

939
00:56:19.459 --> 00:56:24.530
compare the, the pure algorithmic recommendation and not the

940
00:56:24.530 --> 00:56:28.850
pure user but user plus algorithmic. If the pure

941
00:56:28.850 --> 00:56:32.510
algorithmic com if the, the, the rabbit hole theory

942
00:56:32.510 --> 00:56:37.679
is correct. The pure algorithmic recommendation should push you

943
00:56:37.679 --> 00:56:41.159
more toward the extreme and uh and again in

944
00:56:41.159 --> 00:56:43.199
this study they found the opposite so they found

945
00:56:43.199 --> 00:56:46.919
that uh the algorithmic recommendation tend to stay where

946
00:56:46.919 --> 00:56:49.159
they were or even go to our more moderate

947
00:56:49.159 --> 00:56:55.500
videos while users were going in their whatever relation

948
00:56:55.500 --> 00:56:58.360
some case in some case toward more extreme view.

949
00:56:58.780 --> 00:57:02.260
And this makes sense because the algorithms usually give

950
00:57:02.260 --> 00:57:06.580
you, OK, uh, emotionally compelling content but also generally

951
00:57:06.580 --> 00:57:09.860
give you content that is already popular and content

952
00:57:09.860 --> 00:57:14.419
that is already popular is mainstream video and, and,

953
00:57:14.500 --> 00:57:17.340
and again of course it happens that sometime you

954
00:57:17.340 --> 00:57:20.379
have, uh, you know, the, the Andrew Tate or

955
00:57:20.379 --> 00:57:24.820
whatever that get very big. Um, BUT I, I,

956
00:57:24.899 --> 00:57:28.020
I think it's interesting because when it happens, everybody

957
00:57:28.020 --> 00:57:31.580
talk about it, so it's not the, what happens

958
00:57:31.580 --> 00:57:34.719
in YouTube, what happened in social media, it's sometimes

959
00:57:34.719 --> 00:57:37.620
this happens, so we are like all, uh, worried,

960
00:57:37.699 --> 00:57:40.169
but when we look at the general pattern, this

961
00:57:40.169 --> 00:57:43.080
is not the case. And even in this case

962
00:57:43.080 --> 00:57:46.100
to understand why some, some young people go to

963
00:57:46.100 --> 00:57:50.610
the alt-right or whatever they go mm. The, the,

964
00:57:50.649 --> 00:57:53.370
the, the rabbit hole uh hypothesis is to me

965
00:57:53.370 --> 00:57:55.929
a bit of a shortcut and a scapegoat to

966
00:57:55.929 --> 00:57:58.969
say, OK, we just, uh, we just, if the,

967
00:57:59.050 --> 00:58:02.050
the algorithm would be different, uh, every, everybody would

968
00:58:02.050 --> 00:58:06.199
be happy which is uh Uh, strange way to

969
00:58:06.199 --> 00:58:07.909
put things for me. Mhm.

970
00:58:09.040 --> 00:58:11.919
So, uh, let me ask you now about uh

971
00:58:11.919 --> 00:58:16.719
the correlation or even the causation some people have

972
00:58:16.719 --> 00:58:21.879
been making between social media use and mental health

973
00:58:21.879 --> 00:58:25.439
outcomes. I mean, even last year, we've had a

974
00:58:25.439 --> 00:58:29.040
new book come out by Jonathan Haidt, The Anxious

975
00:58:29.040 --> 00:58:32.229
Generation. I mean, it, it really didn't convince me

976
00:58:32.229 --> 00:58:35.084
at all, and I I read some critiques of

977
00:58:35.084 --> 00:58:38.725
it and it seems that many of the claims

978
00:58:38.725 --> 00:58:42.245
he makes, there are not really well supported by

979
00:58:42.245 --> 00:58:45.764
the evidence and there were some poor studies cited

980
00:58:45.764 --> 00:58:48.405
there. But I mean, let's go step by step

981
00:58:48.405 --> 00:58:52.364
here. First of all, uh, how can social media

982
00:58:52.364 --> 00:58:56.485
use and its supposed effect on mental health be

983
00:58:56.485 --> 00:58:57.764
properly measured?

984
00:59:01.449 --> 00:59:04.370
That's a difficult question. How can we properly measure?

985
00:59:04.489 --> 00:59:07.570
We don't know. Uh, WHAT we know is that,

986
00:59:07.590 --> 00:59:11.840
uh, uh, a lot of people, including me and

987
00:59:11.840 --> 00:59:14.449
some of my colleagues and other people are trying

988
00:59:14.449 --> 00:59:19.280
to, uh, do this. And um I think if

989
00:59:19.280 --> 00:59:21.979
we are honest we should say that is very

990
00:59:21.979 --> 00:59:27.050
difficult and that so far even in this case

991
00:59:27.050 --> 00:59:30.020
there is not a a clear results that go

992
00:59:30.020 --> 00:59:33.820
in any direction so I, I mean on this

993
00:59:33.820 --> 00:59:37.169
I I am I am. You know, quite open

994
00:59:37.169 --> 00:59:39.689
to see what happened. I, I, I'm happy to

995
00:59:39.889 --> 00:59:42.719
if we will meet in an interview in, uh,

996
00:59:42.729 --> 00:59:45.570
other 5 years. I'm happy to say, OK, I

997
00:59:45.570 --> 00:59:48.870
want to see other 5 years of research and

998
00:59:48.969 --> 00:59:51.129
I may change my mind on this. I, I'm

999
00:59:51.129 --> 00:59:53.649
pretty, I'm pretty happy to say in the other,

1000
00:59:53.689 --> 00:59:56.250
I think I, I, I would bet all my

1001
00:59:56.250 --> 00:59:58.889
money on this. I'm like, let's see what happens.

1002
00:59:59.469 --> 01:00:02.870
But we can say what is happening now and

1003
01:00:02.870 --> 01:00:05.870
uh the situation didn't change from when I finished

1004
01:00:05.870 --> 01:00:09.439
the book one year ago or so is that

1005
01:00:09.439 --> 01:00:14.629
uh we don't have a clear uh there is

1006
01:00:14.629 --> 01:00:19.270
the first problem is the, the, the existence and

1007
01:00:19.270 --> 01:00:23.750
strength of the correlation between social media usage or

1008
01:00:23.750 --> 01:00:29.830
smartphone usage and mental health. So some study. Some

1009
01:00:29.830 --> 01:00:37.870
studies found uh correlation, um, some didn't. uh, THERE

1010
01:00:37.870 --> 01:00:41.590
are several problems. One is that usually these correlations

1011
01:00:41.590 --> 01:00:45.989
are very, very, uh, small, which by itself is

1012
01:00:45.989 --> 01:00:51.110
OK. Human behavior and societies are complicated stuff, so

1013
01:00:51.110 --> 01:00:55.330
small efforts are perfectly fine. Uh, THE problem is

1014
01:00:55.330 --> 01:00:58.209
that uh these small efforts are also linked to

1015
01:00:58.209 --> 01:01:00.750
the fact that, uh, as, as you were saying,

1016
01:01:00.770 --> 01:01:04.770
it's very difficult to measure, uh, both social media

1017
01:01:04.770 --> 01:01:09.810
usage or smartphone usage or digital technology usage and,

1018
01:01:09.889 --> 01:01:13.760
uh, mental health. So you can ask people, uh,

1019
01:01:13.770 --> 01:01:17.530
how many social media do they have in their

1020
01:01:17.530 --> 01:01:20.229
account, how many hours they spend on social media.

1021
01:01:20.610 --> 01:01:23.629
Uh, HOW many hours they spend on the smartphone,

1022
01:01:23.850 --> 01:01:25.850
do they have a smartphone or not. There are

1023
01:01:25.850 --> 01:01:28.850
many. Mental health is the same. You can ask

1024
01:01:28.850 --> 01:01:35.530
them, you can look at the. Medical prescription, uh,

1025
01:01:35.540 --> 01:01:38.379
and the problem is that when you put this

1026
01:01:38.379 --> 01:01:43.899
thing together, you can find always something, um, so

1027
01:01:43.899 --> 01:01:47.320
we need to be very, very careful. To me

1028
01:01:47.320 --> 01:01:50.379
it seems that yes, it's plausible that there is

1029
01:01:50.379 --> 01:01:54.300
some, some small negative correlation, but even if there

1030
01:01:54.300 --> 01:01:58.939
is, there is a big discrepancy between this very

1031
01:01:58.939 --> 01:02:02.510
small effect, if there is. And like the, the,

1032
01:02:02.530 --> 01:02:06.949
the destroying a generation uh thing and, and this

1033
01:02:06.949 --> 01:02:09.229
is just the first aspect. This is the aspect

1034
01:02:09.229 --> 01:02:12.350
about the the correlation. Then the second aspect is

1035
01:02:12.350 --> 01:02:17.629
about the, the, the causality. So what is, I

1036
01:02:17.629 --> 01:02:20.959
mean, it, it's equally plausible that people that uh

1037
01:02:21.209 --> 01:02:24.870
uh for any reason have a worse mental health

1038
01:02:24.870 --> 01:02:29.459
would use more smartphones and social media. Than the

1039
01:02:29.459 --> 01:02:32.090
opposite that that that is the usage of social

1040
01:02:32.090 --> 01:02:35.379
media or smartphone to cause mental health and uh

1041
01:02:35.379 --> 01:02:37.459
even in this case again I, I'm, I'm not

1042
01:02:37.459 --> 01:02:39.699
saying that I am sure that uh it's not

1043
01:02:39.699 --> 01:02:41.820
the case. I'm sure that people that say that

1044
01:02:41.820 --> 01:02:45.780
they are sure they are uh a bit cheating

1045
01:02:45.780 --> 01:02:47.939
I think because it, it's, it's really, it's really

1046
01:02:47.939 --> 01:02:53.520
difficult to do experiments that taste properly this quisition,

1047
01:02:54.459 --> 01:02:58.590
um. Because, uh, because it's very difficult, so you,

1048
01:02:58.629 --> 01:03:01.989
you, you have this, for example, this, this deactivation

1049
01:03:01.989 --> 01:03:04.590
experiment. So you ask a person to say not

1050
01:03:04.590 --> 01:03:08.659
use, uh, a smartphone or a social media for

1051
01:03:08.909 --> 01:03:11.550
a certain period of time and then you measure

1052
01:03:11.550 --> 01:03:14.590
the difference, uh, so you indeed these are kind

1053
01:03:14.590 --> 01:03:17.389
of like standard experiments so you have a control

1054
01:03:17.389 --> 01:03:20.570
group, you can do your stats, everything, but, uh,

1055
01:03:21.229 --> 01:03:25.739
of course, uh, we. How, how, how common, uh,

1056
01:03:25.750 --> 01:03:29.189
is this conversation in society if someone asks me

1057
01:03:29.189 --> 01:03:31.750
to not use my phone for a week, I

1058
01:03:31.750 --> 01:03:35.030
kind of have very strong expectation of how I

1059
01:03:35.030 --> 01:03:37.229
would feel and what I would say is very

1060
01:03:37.229 --> 01:03:40.310
difficult to see if these data are very reliable

1061
01:03:40.310 --> 01:03:42.389
and then it's just one week. What, what does

1062
01:03:42.389 --> 01:03:44.790
it mean to not have it or have it?

1063
01:03:44.870 --> 01:03:46.669
I mean just one week or one month or

1064
01:03:46.669 --> 01:03:50.050
one day can be a little holiday from, from

1065
01:03:50.050 --> 01:03:53.739
the. Social media, it's, it's just complicated and, and

1066
01:03:53.739 --> 01:03:57.340
again it's, it's um. Also, I mean, in, in

1067
01:03:57.340 --> 01:04:00.500
a, in a, in a situation in which everybody

1068
01:04:00.500 --> 01:04:04.459
use uh social media or smartphone is very difficult

1069
01:04:04.459 --> 01:04:07.020
to measure if there is maybe and and now

1070
01:04:07.020 --> 01:04:09.800
I'm saying something against my point so like maybe

1071
01:04:10.100 --> 01:04:13.610
uh even if a person use it not too

1072
01:04:13.610 --> 01:04:17.659
much if everybody around them use it they will

1073
01:04:17.659 --> 01:04:19.870
also have a negative effort so you cannot measure.

1074
01:04:19.939 --> 01:04:23.209
So what I'm saying is that uh. To me,

1075
01:04:23.330 --> 01:04:27.330
the, the, the best, uh, best guess that we

1076
01:04:27.330 --> 01:04:29.870
can have is that, that we don't know now,

1077
01:04:30.449 --> 01:04:33.010
uh, so it's good to have up to a

1078
01:04:33.010 --> 01:04:37.090
point some, some, uh, be, be careful about that,

1079
01:04:37.689 --> 01:04:39.929
but that, that we don't want to go on

1080
01:04:39.929 --> 01:04:43.209
the extreme and, and present this thing as, as

1081
01:04:43.209 --> 01:04:45.729
that we know and that we need to have

1082
01:04:45.729 --> 01:04:49.989
strong, uh, strong reaction to this because. And I'm

1083
01:04:49.989 --> 01:04:53.510
repeating myself, this could also have uh by itself

1084
01:04:53.510 --> 01:04:55.590
a negative effect. We don't know what would happen

1085
01:04:55.590 --> 01:04:59.590
if we take out. I'm, I'm almost uh uh.

1086
01:05:00.610 --> 01:05:03.649
Interested. So in Australia there will be a ban

1087
01:05:03.649 --> 01:05:07.530
for um less than 16 year old of uh

1088
01:05:07.530 --> 01:05:10.520
social media starting I think in the 10th of

1089
01:05:10.520 --> 01:05:16.080
December. You know, can be a, maybe we will

1090
01:05:16.080 --> 01:05:18.679
know more. I, I'm, I'm happy to see, to

1091
01:05:18.679 --> 01:05:19.500
see what happened.

1092
01:05:20.639 --> 01:05:24.050
Yeah, I mean, but there are two other points

1093
01:05:24.050 --> 01:05:26.489
here that I think could be a little bit

1094
01:05:26.489 --> 01:05:28.729
problematic. I mean, one of them has to do

1095
01:05:28.729 --> 01:05:33.530
even with the claim. That uh teen mental health

1096
01:05:33.530 --> 01:05:36.310
is getting worse over time because I've heard and

1097
01:05:36.310 --> 01:05:40.129
read people questioning that claim and they look at

1098
01:05:40.129 --> 01:05:43.790
the data and say that there's not really good

1099
01:05:43.790 --> 01:05:47.750
enough evidence to really make the claim or be

1100
01:05:47.750 --> 01:05:51.469
sure that teen mental health is getting worse over

1101
01:05:51.469 --> 01:05:54.750
time or over the past 15 years or so

1102
01:05:54.750 --> 01:05:58.952
because usually people Start in 2010 with the introduction

1103
01:05:58.952 --> 01:06:03.232
of smartphones and uh the uh widespread social media

1104
01:06:03.232 --> 01:06:06.752
and so on, and then even if it's getting

1105
01:06:06.752 --> 01:06:10.153
worse, I think that, and uh please tell me

1106
01:06:10.153 --> 01:06:12.072
if you agree with this point or not, but

1107
01:06:12.072 --> 01:06:16.732
I think that uh trying to create this unifactorial

1108
01:06:16.732 --> 01:06:21.072
narrative based just on social media use is not

1109
01:06:21.072 --> 01:06:24.016
helpful because There are many other factors that could

1110
01:06:24.016 --> 01:06:26.476
be playing a role here, like, for example, an

1111
01:06:26.476 --> 01:06:29.756
uncertain future for young people having to do with,

1112
01:06:29.835 --> 01:06:34.615
for example, climate change, the job market, the economy,

1113
01:06:34.676 --> 01:06:37.196
and so on. I mean, there are many other

1114
01:06:37.196 --> 01:06:40.775
aspects that could be playing a role here if,

1115
01:06:41.035 --> 01:06:43.115
if mental health is getting worse,

1116
01:06:43.325 --> 01:06:46.355
right? Yes, but it's much easier to say that

1117
01:06:46.355 --> 01:06:50.239
it's a smartphone. You know, it's more difficult to

1118
01:06:50.239 --> 01:06:52.560
solve the job market and climate change where the

1119
01:06:52.560 --> 01:06:54.709
smartphone you can say just OK, don't use the

1120
01:06:54.709 --> 01:06:57.399
smartphone until you are 16 and uh you feel

1121
01:06:57.399 --> 01:07:01.389
good about that. No, uh, excellent point. I, I

1122
01:07:01.389 --> 01:07:05.739
completely agree. So, uh, first thing, uh, that is

1123
01:07:06.159 --> 01:07:08.679
really true that there is this decline in mental

1124
01:07:08.679 --> 01:07:13.790
health, um. Again, one aspect is that usually as

1125
01:07:13.790 --> 01:07:16.989
you were saying, people take the last 20 years

1126
01:07:16.989 --> 01:07:20.389
of data and they observe something like this. Even

1127
01:07:20.389 --> 01:07:22.750
in the US if you take say the last

1128
01:07:22.750 --> 01:07:26.149
40 or 50 years, you have kind of some,

1129
01:07:26.520 --> 01:07:28.399
some ups. And down and if you look at

1130
01:07:28.399 --> 01:07:31.360
all the data, there's nothing special in these last

1131
01:07:31.360 --> 01:07:35.260
uh 20 years. People were, uh for example, much

1132
01:07:35.449 --> 01:07:37.600
uh worse in the early 90s. I don't know.

1133
01:07:37.679 --> 01:07:39.479
I, I remember that now. I don't remember the

1134
01:07:39.479 --> 01:07:42.040
exact data, but if you look, it's nothing special

1135
01:07:42.040 --> 01:07:45.719
now. So, um this is also not clear and

1136
01:07:45.719 --> 01:07:48.040
again, as I was saying before, this is all

1137
01:07:48.040 --> 01:07:52.350
about US data. Uh, IT'S less clear if this

1138
01:07:52.350 --> 01:07:57.709
happens in another country, um. Seen kind of in

1139
01:07:57.709 --> 01:08:01.189
the UK, for example, is less in Italy or

1140
01:08:01.189 --> 01:08:06.689
in more South European country. So it's, it's not

1141
01:08:06.739 --> 01:08:09.350
obvious this, and the second aspect that you were

1142
01:08:09.350 --> 01:08:12.429
saying which I completely agree is this, as you

1143
01:08:12.429 --> 01:08:16.819
were saying this mono, uh, mono causal explanation, uh,

1144
01:08:16.830 --> 01:08:20.470
of course there are so many possible other reasons.

1145
01:08:20.509 --> 01:08:23.270
I mean all that you say, uh, the fact

1146
01:08:23.270 --> 01:08:27.350
that there has been, uh, uh. In some country,

1147
01:08:27.649 --> 01:08:29.709
and these are the country in which you observe

1148
01:08:29.709 --> 01:08:34.890
more clearly, uh, uh, this, this, uh, this effect,

1149
01:08:35.649 --> 01:08:39.149
uh, a change also in the social, uh, fabric

1150
01:08:39.149 --> 01:08:43.299
of the societies and, uh, which I think it's,

1151
01:08:43.310 --> 01:08:47.390
uh, more problematic and interesting also than just the,

1152
01:08:47.709 --> 01:08:52.429
the, the, the, the smartphone. Um, THAT there has

1153
01:08:52.429 --> 01:08:55.249
been, uh, uh, the, the, the way in which,

1154
01:08:55.309 --> 01:09:00.587
uh, mental health is, uh, categorized changes a lot,

1155
01:09:00.688 --> 01:09:04.229
so it's, uh, it's, uh, now, uh, which is

1156
01:09:04.229 --> 01:09:06.229
good, but, uh, uh, it can have some effort

1157
01:09:06.229 --> 01:09:08.349
now it's kind of more OK to say that

1158
01:09:08.349 --> 01:09:12.225
you have, uh, uh, some. Some problem than maybe

1159
01:09:12.225 --> 01:09:16.734
20 years ago again that's good but could could

1160
01:09:16.734 --> 01:09:20.544
create these efforts so uh also this part is

1161
01:09:20.544 --> 01:09:23.944
not is not uh foolproof so we need to

1162
01:09:23.944 --> 01:09:26.955
be careful when we do big claim but again

1163
01:09:26.955 --> 01:09:32.249
these big claims tend to be very. Very attractive

1164
01:09:32.249 --> 01:09:34.269
for, for, for the reason I was saying before,

1165
01:09:34.349 --> 01:09:37.749
they are negative. They talk about uh when I

1166
01:09:37.749 --> 01:09:41.188
was younger, we didn't use the smartphone. Uh, THEY

1167
01:09:41.188 --> 01:09:45.068
find easy solution to problems that are complex and

1168
01:09:45.068 --> 01:09:49.828
are scary. So yeah, they, they travel fast in

1169
01:09:49.837 --> 01:09:51.028
in culture.

1170
01:09:52.000 --> 01:09:56.379
Uh, IS there a good enough evidence to support

1171
01:09:56.379 --> 01:10:02.240
the existence of something like smartphone or social media

1172
01:10:02.240 --> 01:10:05.240
addiction? Because I mean, I've heard people talking about

1173
01:10:05.240 --> 01:10:10.359
that, and then something that really particularly irritates me

1174
01:10:10.359 --> 01:10:14.439
is that people always link it to dopamine and

1175
01:10:14.439 --> 01:10:17.935
then there's, there's always influencers on social media. Media

1176
01:10:17.935 --> 01:10:21.854
and the internet talking about dopamine addiction or that

1177
01:10:21.854 --> 01:10:24.814
every time you use your smartphone or you go

1178
01:10:24.814 --> 01:10:28.174
on social media you get the dopamine hit. I

1179
01:10:28.174 --> 01:10:31.214
mean, is, is there really any basis, uh, any

1180
01:10:31.214 --> 01:10:32.634
scientific basis to that?

1181
01:10:32.935 --> 01:10:36.095
No, no, not really. I mean again, but yeah,

1182
01:10:36.174 --> 01:10:38.484
I, I, I completely agree with you. So it's

1183
01:10:38.484 --> 01:10:41.654
fine because yes, you, you have been addicted by

1184
01:10:41.654 --> 01:10:45.370
dopamine or whatever. Uh, SO, yeah, this, this is,

1185
01:10:45.450 --> 01:10:48.080
I, I mean, you know, dopamine is, uh, I,

1186
01:10:48.169 --> 01:10:50.410
I, I'm a, I'm a, I, I work in

1187
01:10:50.410 --> 01:10:52.330
cultural revolution, so I don't want to enter in

1188
01:10:52.330 --> 01:10:54.890
the detail, but dopamine is a neurotransmitter, so you

1189
01:10:54.890 --> 01:10:58.250
cannot be addicted to a neurotransmitter. It's just something

1190
01:10:58.250 --> 01:11:00.770
that, you know, the level change whenever you do

1191
01:11:00.770 --> 01:11:04.609
something that is pleasurable or not. Uh, SO this

1192
01:11:04.609 --> 01:11:06.689
is the first point, when you say, when someone

1193
01:11:06.689 --> 01:11:09.490
say, OK, I'm addicted to dopamine or whatever, it's

1194
01:11:09.490 --> 01:11:11.770
just say, OK, let's not read the rest of

1195
01:11:11.770 --> 01:11:16.850
the article. Um, uh, THEN of course, uh, the

1196
01:11:16.850 --> 01:11:19.450
fact that we use smartphone and social media means

1197
01:11:19.450 --> 01:11:23.109
that they give us some form of, uh, uh,

1198
01:11:23.930 --> 01:11:27.810
reward, otherwise we would not use them. So, so

1199
01:11:27.810 --> 01:11:30.410
that probably is true. I, again, I, I don't

1200
01:11:30.410 --> 01:11:32.850
know really here the, the detail, but of course

1201
01:11:32.850 --> 01:11:35.290
there will be some increase in dopamine when we

1202
01:11:35.290 --> 01:11:39.370
do something on, on, uh, social media. The point

1203
01:11:39.370 --> 01:11:44.200
is that, uh, uh. To talk about, uh, you

1204
01:11:44.200 --> 01:11:48.240
know, addiction or about problematic behavior, this increase in

1205
01:11:48.240 --> 01:11:51.479
dopamine should be, for example, similar to what is

1206
01:11:51.479 --> 01:11:54.319
the increase that you have for activities for which

1207
01:11:54.319 --> 01:11:58.549
we use, uh, usually the term of addiction, drugs

1208
01:11:58.549 --> 01:12:00.799
or similar thing and when you look at this

1209
01:12:00.799 --> 01:12:05.100
in this way, it's completely uh another, another thing.

1210
01:12:06.029 --> 01:12:10.709
I mean, in general, the, the I, I think

1211
01:12:10.709 --> 01:12:15.680
generally it's problematic to. Move the, the, the, the

1212
01:12:15.680 --> 01:12:20.680
terminology of addiction to uh uh behavioral addiction. I

1213
01:12:20.680 --> 01:12:26.000
mean, people do it, uh. That I like obviously

1214
01:12:26.000 --> 01:12:28.720
the, the, the proper domain of the term addiction

1215
01:12:28.720 --> 01:12:31.799
is about, you know, drugs or alcohol, things that

1216
01:12:31.799 --> 01:12:34.629
there is some, some withdrawal, physical effort. This, this

1217
01:12:34.629 --> 01:12:37.600
you can use addiction, that's a problem. When you

1218
01:12:37.600 --> 01:12:41.319
talk about giving an addiction, my opinion is that

1219
01:12:41.319 --> 01:12:49.799
uh. It makes sense if the er behavior gets

1220
01:12:49.799 --> 01:12:54.979
problematic for everyone that go after a certain threshold.

1221
01:12:55.319 --> 01:12:59.160
So let me explain, let's leave aside any physical

1222
01:12:59.160 --> 01:13:01.839
effort or whatever, but we can say that heroin

1223
01:13:01.839 --> 01:13:06.120
is addictive, because if you start to use heroin,

1224
01:13:06.629 --> 01:13:09.020
er and if you go on for a while

1225
01:13:09.319 --> 01:13:12.049
for the, I don't know. I, I'm, I'm using

1226
01:13:12.049 --> 01:13:14.709
just a random number for the 90% of people,

1227
01:13:14.740 --> 01:13:17.439
this will create problems. So it's not, it's, it's,

1228
01:13:17.490 --> 01:13:23.089
it's generally problematic. Um, SO sometime for behavioral addition

1229
01:13:23.089 --> 01:13:26.759
this could make sense. I, I'm thinking about like,

1230
01:13:26.770 --> 01:13:31.209
uh, um, like betting in a, in a very

1231
01:13:31.209 --> 01:13:33.810
consistent way. I, I would think that if a

1232
01:13:33.810 --> 01:13:39.500
person. Bet, uh, money every, every day for, uh,

1233
01:13:39.589 --> 01:13:42.540
for a long period of time. This would necessarily,

1234
01:13:42.910 --> 01:13:45.939
at least for a big percentage of people, uh,

1235
01:13:45.950 --> 01:13:48.379
a problematic behavior. So I would call it, uh,

1236
01:13:48.589 --> 01:13:50.270
I, I would, I, I don't know, again, I'm

1237
01:13:50.270 --> 01:13:53.970
not an expert but I would accept that, OK,

1238
01:13:54.669 --> 01:13:57.390
can we, we can talk, we can use the

1239
01:13:57.390 --> 01:14:01.149
language of addition, this add something to our knowledge

1240
01:14:01.149 --> 01:14:05.549
of the, of the behavior. Going to the social

1241
01:14:05.549 --> 01:14:08.339
media, I think that for the this is not

1242
01:14:08.339 --> 01:14:11.870
the case. The majority of people have uh an

1243
01:14:11.870 --> 01:14:14.270
OK usage of social media. Of course there is

1244
01:14:14.270 --> 01:14:16.870
problematic. This doesn't mean that there is not problematic

1245
01:14:16.870 --> 01:14:20.790
use. There is some people use, uh, uh, maybe

1246
01:14:20.790 --> 01:14:25.970
they, they need like real, real, um. Real, uh,

1247
01:14:25.979 --> 01:14:29.819
uh, medical or, or, or, or psychological help, that's,

1248
01:14:29.870 --> 01:14:32.390
that's for sure, but uh it's not that the

1249
01:14:32.390 --> 01:14:35.129
majority of people that start to use social media

1250
01:14:35.129 --> 01:14:38.509
end up in a problematic situation. So in this

1251
01:14:38.509 --> 01:14:41.129
case I would not talk about addiction. That's why

1252
01:14:41.339 --> 01:14:44.129
when people use the term addiction, you know, like

1253
01:14:44.350 --> 01:14:47.910
sex addiction or shopping addiction, I, I think these

1254
01:14:47.910 --> 01:14:51.879
are kind of uh. You know, labeled for saying

1255
01:14:51.879 --> 01:14:54.339
something in a, in a funny way that maybe

1256
01:14:54.339 --> 01:14:57.979
describe the behavior of some people, but the, the

1257
01:14:57.979 --> 01:15:00.459
problem is not social media edition, the problem is

1258
01:15:00.459 --> 01:15:04.790
that these people that are. Addicted to social media

1259
01:15:04.790 --> 01:15:07.850
or shopping, whatever, have some, some other problem, but

1260
01:15:07.850 --> 01:15:10.970
the, the, while in the case of heroin and

1261
01:15:10.970 --> 01:15:14.479
maybe betting, these are problematic by itself, in this

1262
01:15:14.479 --> 01:15:16.709
case, they are not, so I will not use

1263
01:15:16.709 --> 01:15:20.009
the, the, uh, the term addiction and I think

1264
01:15:20.009 --> 01:15:23.490
this also is a bit of polluting the, the,

1265
01:15:23.569 --> 01:15:26.490
the conversation about the effect of social media because

1266
01:15:26.490 --> 01:15:31.000
you know. Mhm. So, yeah,

1267
01:15:31.490 --> 01:15:36.000
what do you make of proposals that we've seen

1268
01:15:36.000 --> 01:15:42.399
across the world, uh, for banning, uh, smart smartphones

1269
01:15:42.399 --> 01:15:49.444
from schools and prohibiting social. Media use for children

1270
01:15:49.444 --> 01:15:52.455
under 16, I mean, do you think that there's

1271
01:15:52.714 --> 01:15:58.245
any scientific evidence to support policies and measures like

1272
01:15:58.245 --> 01:15:58.435
that?

1273
01:15:58.564 --> 01:16:02.774
Yes, as I was saying before, the scientific evidence

1274
01:16:02.774 --> 01:16:07.930
is really Not, not there yet, uh, and I

1275
01:16:07.930 --> 01:16:10.770
would say, OK, let's hope that, you know, if

1276
01:16:10.770 --> 01:16:14.209
this thing happened, even if I'm not particularly in

1277
01:16:14.209 --> 01:16:16.770
support of this, maybe we will have some scientific

1278
01:16:16.770 --> 01:16:19.370
evidence, and, uh, you know, I hope in 5

1279
01:16:19.370 --> 01:16:21.850
years to talk about this, and I will answer

1280
01:16:21.850 --> 01:16:23.890
to this question, and I'm really happy in this

1281
01:16:23.890 --> 01:16:27.950
case to, uh, to change my mind with the,

1282
01:16:27.970 --> 01:16:30.799
uh, evidence that we have now about the effect.

1283
01:16:30.850 --> 01:16:36.069
I, I think, uh. Like 16 year old bands

1284
01:16:36.069 --> 01:16:42.060
can be more risky than, than, uh than um

1285
01:16:42.060 --> 01:16:46.189
beneficial. Um, OF course, uh, I'm not claiming that

1286
01:16:46.189 --> 01:16:48.770
we need to give, you know, smartphone to, to

1287
01:16:48.770 --> 01:16:51.830
primary school children and let them abandon and do

1288
01:16:51.830 --> 01:16:55.959
things. I don't think that the way is banning.

1289
01:16:56.270 --> 01:16:58.669
I think the way it should be a society

1290
01:16:58.669 --> 01:17:02.430
that uh accept in full what can be the,

1291
01:17:02.509 --> 01:17:05.950
the effect, the positive and the negative effect of

1292
01:17:05.950 --> 01:17:10.910
smartphone and social media usage and build, uh, structures

1293
01:17:10.910 --> 01:17:14.310
in society, in school, in family that make the

1294
01:17:14.310 --> 01:17:18.000
usage of social media, take the positive part and

1295
01:17:18.000 --> 01:17:22.149
not the, uh, and not the negative part, um.

1296
01:17:22.890 --> 01:17:26.540
Again, because a band can actually for, for some

1297
01:17:26.540 --> 01:17:29.620
teenager, I imagine that social media usage is really

1298
01:17:30.020 --> 01:17:35.700
important. Imagine, you know, uh marginalized communities or uh

1299
01:17:36.259 --> 01:17:40.379
it could be. More problematic for them than beneficial

1300
01:17:40.379 --> 01:17:41.879
for the rest of the people and then you

1301
01:17:41.879 --> 01:17:44.189
would do really damage to someone with a band

1302
01:17:44.560 --> 01:17:48.140
so I'm not particularly in favor, as I'm saying

1303
01:17:48.359 --> 01:17:50.640
we don't know too much so to me it

1304
01:17:50.640 --> 01:17:52.549
would be, I mean I would, I would, you

1305
01:17:52.549 --> 01:17:55.680
know, I, I, I would prefer that uh uh

1306
01:17:55.680 --> 01:17:58.910
the, the, the usage of smartphone is incremental with

1307
01:17:58.910 --> 01:18:02.740
age that should not start too early, um, but

1308
01:18:02.740 --> 01:18:04.629
uh this should be done, I think in a

1309
01:18:04.629 --> 01:18:06.680
different way, should be done in a society that

1310
01:18:06.680 --> 01:18:11.339
give you. Occasions to, you know, do something else

1311
01:18:11.339 --> 01:18:16.009
without prohibiting so you know, middle school kids should,

1312
01:18:16.069 --> 01:18:19.470
uh, not use too much the smartphone, not because

1313
01:18:19.470 --> 01:18:21.990
there is a law against that, but because they

1314
01:18:21.990 --> 01:18:25.790
have other things to do because adults people should

1315
01:18:25.790 --> 01:18:28.549
give the example of using it in a good

1316
01:18:28.549 --> 01:18:30.799
and moderate way again it's easy to say yes,

1317
01:18:30.990 --> 01:18:34.939
it's the kids that are the problem, so. Mm,

1318
01:18:35.180 --> 01:18:37.779
it's, it's difficult. It's, so I, I'm, I'm mostly

1319
01:18:37.779 --> 01:18:40.399
talking about what is a kind of a, a

1320
01:18:40.459 --> 01:18:43.580
common sense thing because we don't have much science

1321
01:18:43.580 --> 01:18:46.459
behind that, but, uh, people that pretend that there

1322
01:18:46.459 --> 01:18:48.580
is a science and in general I don't think

1323
01:18:48.580 --> 01:18:51.060
in, in very good faith. Oh, well, they, they

1324
01:18:51.060 --> 01:18:52.939
are probably in good faith from their point of

1325
01:18:52.939 --> 01:18:56.700
view, but, uh, um. You cannot say that we

1326
01:18:56.700 --> 01:18:59.740
have strong support for these uh these initiatives.

1327
01:19:00.759 --> 01:19:04.600
Yes, and I've heard some arguments made by people

1328
01:19:04.600 --> 01:19:07.680
who are against these kinds of bans that I

1329
01:19:07.680 --> 01:19:11.560
think are really important to consider. At least it's

1330
01:19:11.560 --> 01:19:15.240
not just some of the positive aspects of social

1331
01:19:15.240 --> 01:19:17.600
media like the ones you referred to there and

1332
01:19:17.600 --> 01:19:21.120
the fact that people can connect more easily with

1333
01:19:21.120 --> 01:19:24.240
other people and perhaps it's easier to get in

1334
01:19:24.240 --> 01:19:28.520
touch with people from different cultural backgrounds and have

1335
01:19:28.520 --> 01:19:32.720
those sorts of experiences. But also the fact that,

1336
01:19:32.799 --> 01:19:35.620
and these are the arguments that some people make

1337
01:19:35.620 --> 01:19:38.700
against the bans, the fact that it is important

1338
01:19:38.700 --> 01:19:43.520
even for children to acquire and develop social media

1339
01:19:43.520 --> 01:19:48.600
literacy, uh, online literacy, and also for them because

1340
01:19:48.600 --> 01:19:51.600
like it or not, social media are now part

1341
01:19:51.600 --> 01:19:55.560
of our lives, the lives of everyone. It's important

1342
01:19:55.560 --> 01:19:58.580
for children also to learn how to use them

1343
01:19:58.839 --> 01:20:04.040
properly. Even uh in the future for their future

1344
01:20:04.040 --> 01:20:07.740
in terms of professional development and so on, yes,

1345
01:20:08.000 --> 01:20:10.200
yeah, no, I, I, I agree definitely. I mean,

1346
01:20:10.439 --> 01:20:13.759
it's, it's at least unclear whether, you know, they're

1347
01:20:13.759 --> 01:20:16.200
banned until 16 year old and then a 16

1348
01:20:16.200 --> 01:20:19.069
year old what happened, they just, you know, it,

1349
01:20:19.160 --> 01:20:21.160
it could be even worse because then they have,

1350
01:20:21.240 --> 01:20:24.120
you know, the full access and they didn't, didn't

1351
01:20:24.120 --> 01:20:27.560
have any experience to build some knowledge, yeah, I,

1352
01:20:27.640 --> 01:20:31.669
I completely agree. Another aspect that is more practical.

1353
01:20:33.240 --> 01:20:36.720
I would this work? I mean, in, in, in,

1354
01:20:36.839 --> 01:20:40.759
in how, in how many days, uh, the, the

1355
01:20:40.759 --> 01:20:43.879
kids will find a way to, to fake the,

1356
01:20:43.990 --> 01:20:46.279
the, the age identity. I mean, it seems to

1357
01:20:46.279 --> 01:20:50.439
me like very, very, uh, but, but, OK, this,

1358
01:20:50.680 --> 01:20:52.879
let's not talk about that. It's not, uh, but,

1359
01:20:52.959 --> 01:20:55.600
but is a practical thing, but we see. But

1360
01:20:55.600 --> 01:20:58.490
yeah, I agree completely. Yeah, it's probably makes sense

1361
01:20:58.490 --> 01:21:02.149
to build incremental skills in a. In a, in

1362
01:21:02.149 --> 01:21:05.350
a, in a positive and controlled environment and say

1363
01:21:05.350 --> 01:21:07.189
like, OK, this is bad and you can do

1364
01:21:07.189 --> 01:21:08.290
it only at 160.

1365
01:21:09.629 --> 01:21:11.990
Uh, YEAH, I mean, I was laughing because I

1366
01:21:11.990 --> 01:21:16.379
was an adolescent in the 2000s when the people,

1367
01:21:16.390 --> 01:21:19.709
when the internet became more widespread, and I can

1368
01:21:19.709 --> 01:21:23.430
guarantee you that uh as young people we were

1369
01:21:23.430 --> 01:21:27.069
able to find ways around certain things.

1370
01:21:27.819 --> 01:21:30.589
Exactly, we, we, we all have this experience, so

1371
01:21:30.589 --> 01:21:30.620
yes, yes.

1372
01:21:33.089 --> 01:21:36.029
OK, so, uh, uh, I have two more questions

1373
01:21:36.029 --> 01:21:40.310
then. Uh, WHAT is your take then on communication

1374
01:21:40.310 --> 01:21:45.990
technologies? I mean, are they mostly good, mostly bad?

1375
01:21:46.299 --> 01:21:48.109
How, how should we deal with them?

1376
01:21:49.299 --> 01:21:53.970
Well, uh, that's, that's a, that's a hard question.

1377
01:21:54.350 --> 01:22:00.609
Uh, SO, um, We cannot say if they are

1378
01:22:00.609 --> 01:22:03.930
mostly good or mostly bad. The, the, the classic

1379
01:22:03.930 --> 01:22:06.490
thing is it depends how we use them, but

1380
01:22:06.490 --> 01:22:08.879
also I, I, I, there is also, I think,

1381
01:22:08.890 --> 01:22:11.609
a naive thing that say, OK, uh, they are

1382
01:22:11.609 --> 01:22:14.379
neutral and, uh, you know, it depends what we

1383
01:22:14.379 --> 01:22:16.319
do, but I don't think they are neutral. I,

1384
01:22:16.490 --> 01:22:20.859
I think that uh. Each communication technologies gives some

1385
01:22:20.859 --> 01:22:23.779
kind of affordances, gives some kind of, and, and

1386
01:22:23.779 --> 01:22:28.259
even different social media have different algorithms that you,

1387
01:22:28.299 --> 01:22:30.830
you use them for different things so you use

1388
01:22:30.830 --> 01:22:35.419
uh uh LinkedIn to promote your work and Facebook

1389
01:22:35.419 --> 01:22:39.720
to show the last uh dinner that you did

1390
01:22:40.379 --> 01:22:48.020
uh. Television was one too many, media, uh, visual

1391
01:22:48.560 --> 01:22:52.560
radio was one too many, but without visual, social

1392
01:22:52.560 --> 01:22:55.439
media are 1 to 1 or many to many.

1393
01:22:55.720 --> 01:22:59.959
So uh it's not that. They are neutral. Each

1394
01:22:59.959 --> 01:23:06.390
different communication media have different features, um. And I

1395
01:23:06.390 --> 01:23:07.910
don't even think that I can say if they

1396
01:23:07.910 --> 01:23:09.950
are good or bad. The point is, as I

1397
01:23:09.950 --> 01:23:14.700
was saying probably at the beginning. Each new communication

1398
01:23:14.700 --> 01:23:19.399
media has communication uh technology. Each new media have

1399
01:23:19.399 --> 01:23:22.209
different affordance, different features, and then there is a

1400
01:23:22.520 --> 01:23:27.689
co-adaptation process of the society that tend to use

1401
01:23:27.689 --> 01:23:30.149
this in a way that uh. Up to a

1402
01:23:30.149 --> 01:23:34.720
point serves the society. So, uh, in general, new,

1403
01:23:34.830 --> 01:23:41.459
uh, new communication technologies give more possibility of, uh,

1404
01:23:41.470 --> 01:23:46.709
communicating this thing it's potentially at least uh positive.

1405
01:23:47.029 --> 01:23:49.979
Then of course, depends how we use the photos

1406
01:23:49.979 --> 01:23:53.350
is, how the society adapt to this, uh, to

1407
01:23:53.350 --> 01:23:58.759
me. Like now it seems, seems, uh, seems crazy

1408
01:23:58.759 --> 01:24:01.640
to say that, you know, internet is, uh, it

1409
01:24:01.640 --> 01:24:05.359
was a positive development, but to me it clearly

1410
01:24:05.359 --> 01:24:07.160
is. I mean, we, we, we give us for

1411
01:24:07.160 --> 01:24:09.640
granted the fact that, you know, we, we are

1412
01:24:09.640 --> 01:24:14.120
doing something that until uh uh 30, maybe years

1413
01:24:14.120 --> 01:24:17.279
ago, 20 even was, was impossible. Now it's, it's

1414
01:24:17.279 --> 01:24:21.154
normal and, and. This is increasing, uh, the possibility

1415
01:24:21.154 --> 01:24:23.435
like we, we, we, we take for granted a

1416
01:24:23.435 --> 01:24:26.875
lot of, uh, uh, a lot of innovation, a

1417
01:24:26.875 --> 01:24:30.265
lot of possibilities that uh uh to me are,

1418
01:24:30.314 --> 01:24:35.555
are uh positive so, so it's not that technologies

1419
01:24:35.555 --> 01:24:38.685
are positive or negative and not even neutral technologies

1420
01:24:38.685 --> 01:24:42.390
I was saying, have, have affordances and uh. Um,

1421
01:24:42.450 --> 01:24:45.169
OPEN the new spaces and it's up to the

1422
01:24:45.169 --> 01:24:49.089
society to use them in the, in the better

1423
01:24:49.089 --> 01:24:53.490
ways and, and my point is that whatever people

1424
01:24:53.490 --> 01:24:56.120
would say, I think that the, uh, the net,

1425
01:24:56.129 --> 01:25:02.600
um. Balance of what happened with digital communication technologies

1426
01:25:02.600 --> 01:25:05.439
is, uh, is positive. We, we seem crazy. I

1427
01:25:05.439 --> 01:25:08.520
mean, it's, it's, uh, uh, I, I was, uh,

1428
01:25:08.600 --> 01:25:11.759
I was, uh, this was probably the, the dominant

1429
01:25:11.759 --> 01:25:14.680
idea like 20 years ago or something, and I,

1430
01:25:14.759 --> 01:25:17.600
I found recently a book of, uh, 20 years

1431
01:25:17.600 --> 01:25:19.720
ago that you would, about 15 years ago that

1432
01:25:19.720 --> 01:25:22.640
was like uh Internet will not solve all the

1433
01:25:22.640 --> 01:25:25.160
problem, but it's not, uh, which really now seems

1434
01:25:25.160 --> 01:25:27.680
like, of course, because they not solve the problem,

1435
01:25:27.759 --> 01:25:31.640
but because we really change our perspective on, on

1436
01:25:31.640 --> 01:25:36.879
the effort but uh um it's, it's, um, I

1437
01:25:36.879 --> 01:25:38.640
think there are many things that we take for

1438
01:25:38.640 --> 01:25:41.000
granted that are, that are very important.

1439
01:25:41.689 --> 01:25:45.370
Mhm. And so my last question and then in

1440
01:25:45.370 --> 01:25:48.970
part, we've already explored this question, but I want

1441
01:25:48.970 --> 01:25:52.290
to get, I guess a broader uh or a

1442
01:25:52.290 --> 01:25:55.129
more detailed answer from you. Uh, WHAT do you

1443
01:25:55.129 --> 01:26:01.009
think are the effects of alarmist narrative surrounding social

1444
01:26:01.009 --> 01:26:04.850
media and communication technologies more generally? I mean, earlier

1445
01:26:04.850 --> 01:26:09.564
when we talked about the supposed correlation. Between social

1446
01:26:09.564 --> 01:26:14.294
media use and mental health outcomes we mentioned the

1447
01:26:14.294 --> 01:26:18.694
fact that the sort of monocausal explanations, I mean,

1448
01:26:18.814 --> 01:26:23.654
are not good scientifically, but also they leave other

1449
01:26:23.654 --> 01:26:28.825
possible factors aside that could be considered and tackled.

1450
01:26:29.095 --> 01:26:33.645
So what other effects of these alarmist narratives do,

1451
01:26:33.814 --> 01:26:35.354
do you think we can have?

1452
01:26:36.490 --> 01:26:40.040
Yeah, yeah, that's a, that's an excellent question to

1453
01:26:40.040 --> 01:26:41.689
conclude, and I mean, it's also like a bit

1454
01:26:41.689 --> 01:26:44.830
like the chapter of my book this because usually

1455
01:26:44.839 --> 01:26:47.669
people kind of tell me, well, you are defending,

1456
01:26:47.850 --> 01:26:53.529
uh, Elon Musk or the Zuckerberg, which is unfortunately,

1457
01:26:53.569 --> 01:26:55.240
as you see, I'm here in my kitchen, yeah,

1458
01:26:55.370 --> 01:26:58.040
I'm not on some, some private some

1459
01:26:58.040 --> 01:26:59.089
mention,

1460
01:26:59.689 --> 01:27:01.450
yeah, no, there, there's no mention. It's, it's a

1461
01:27:01.450 --> 01:27:04.370
little kitchen. Treto is very nice, but, uh, no.

1462
01:27:04.899 --> 01:27:08.740
Uh, OK, I decided, so yeah, the, the point

1463
01:27:08.740 --> 01:27:13.660
is that uh I think that this negative alarmistic

1464
01:27:13.660 --> 01:27:18.410
narrative can have. Negative effect by itself. So what

1465
01:27:18.410 --> 01:27:21.209
one could say, OK, well they're not strong supported,

1466
01:27:21.250 --> 01:27:24.049
but still we try to do better. The point

1467
01:27:24.049 --> 01:27:25.689
is that I don't think that this is the

1468
01:27:25.689 --> 01:27:28.410
case. So you, you, you already gave the example

1469
01:27:28.410 --> 01:27:31.729
of, of, uh, mental health, but for example, let's

1470
01:27:31.729 --> 01:27:36.370
go back at the, uh, misinformation case. So 11

1471
01:27:36.370 --> 01:27:40.770
aspect that we already, uh, discussed a bit before

1472
01:27:40.770 --> 01:27:46.600
is that uh if you focus on this. Cause

1473
01:27:46.640 --> 01:27:50.259
or or presumed cause of, of uh various events

1474
01:27:50.259 --> 01:27:53.890
you are not really looking at more uh possibly

1475
01:27:53.890 --> 01:27:58.419
important structural causes, social problem, economical problem, cultural problem

1476
01:27:58.419 --> 01:28:02.020
you just say, OK, people uh vote uh populist

1477
01:28:02.020 --> 01:28:06.660
parties because of misinformation. That's, that's, uh, that's a

1478
01:28:06.660 --> 01:28:10.259
problem with this narrative. There are also other, for

1479
01:28:10.259 --> 01:28:13.689
example, in the case of misinformation, more, more specific

1480
01:28:13.689 --> 01:28:19.020
problem. One, I think that this narrative contributed in

1481
01:28:19.020 --> 01:28:25.009
uh a general loss in trust of uh media

1482
01:28:25.009 --> 01:28:27.640
and institution. I, I, even in this case, I

1483
01:28:27.640 --> 01:28:29.600
would not say that this is the only cause.

1484
01:28:29.759 --> 01:28:32.359
I, I, I would not go for a monocausal

1485
01:28:32.600 --> 01:28:34.694
explanation, but if it. Is one of the cause

1486
01:28:34.694 --> 01:28:39.095
if people keep on repeating, you know, social media,

1487
01:28:39.125 --> 01:28:43.384
uh, it's just a lot of misinformation and then

1488
01:28:43.384 --> 01:28:46.745
we observed that there are now some experiments that

1489
01:28:46.745 --> 01:28:48.944
try to actually test these things. So if you

1490
01:28:48.944 --> 01:28:54.509
tend to participate that, uh, um. That's the, the,

1491
01:28:54.580 --> 01:28:58.540
the, the everything that the misinformation is very widespread,

1492
01:28:58.799 --> 01:29:02.759
they will tend to trust less uh less also

1493
01:29:02.759 --> 01:29:05.740
reliable news. This is also a problem for what

1494
01:29:05.740 --> 01:29:08.279
we said before, so we say that the, the,

1495
01:29:08.799 --> 01:29:11.120
the problem with, with social influence is not too

1496
01:29:11.120 --> 01:29:14.879
much that we are too, too gullible, but that,

1497
01:29:14.919 --> 01:29:17.240
that we do not change our mind when we

1498
01:29:17.240 --> 01:29:22.720
should. So if this negative narrative of misinformation make

1499
01:29:22.720 --> 01:29:26.140
everybody more skeptical, this is probably not the right

1500
01:29:26.140 --> 01:29:30.169
direction. We would like people to be open to

1501
01:29:30.259 --> 01:29:36.859
to uh to. Reliable information like people use less

1502
01:29:36.859 --> 01:29:41.020
and less news, uh, social media, at least some

1503
01:29:41.020 --> 01:29:44.979
of them try to avoid the to, uh, propose

1504
01:29:44.979 --> 01:29:48.060
political news for the fear of being involved in

1505
01:29:48.060 --> 01:29:51.020
some misinformation thing and again the result is that

1506
01:29:51.020 --> 01:29:55.220
people are, uh, less informed. So in general I

1507
01:29:55.220 --> 01:29:59.250
think there is a lot of uh uh. Problems

1508
01:29:59.250 --> 01:30:01.850
that are so that the beside the fact that

1509
01:30:01.850 --> 01:30:06.270
they are not uh uh empirically uh strongly supported

1510
01:30:06.270 --> 01:30:11.009
this narrative have uh um some, some drawback by

1511
01:30:11.009 --> 01:30:13.810
itself and again even if we want, as, as

1512
01:30:13.810 --> 01:30:15.450
I want, if you even if you want to

1513
01:30:15.450 --> 01:30:19.790
be critical about say social media about some aspect

1514
01:30:20.209 --> 01:30:24.240
if, if we have this overall not robust narrative,

1515
01:30:24.330 --> 01:30:28.089
we're not really looking at the, at the, um.

1516
01:30:29.850 --> 01:30:31.839
At the specific aspect of the things that we

1517
01:30:31.839 --> 01:30:34.379
could change. I, I, I see you had two

1518
01:30:34.379 --> 01:30:36.700
minutes for talking about something that I didn't talk

1519
01:30:36.700 --> 01:30:40.540
in the book because I didn't want artificial intelligence

1520
01:30:40.540 --> 01:30:43.279
because I said, OK, let's just not touch this

1521
01:30:43.279 --> 01:30:47.259
because negative narrative, it's, it's, it's complicated, but uh

1522
01:30:47.620 --> 01:30:50.299
I, I was finishing to, to work at the

1523
01:30:50.299 --> 01:30:52.379
book and was the, the time in which there

1524
01:30:52.379 --> 01:30:58.859
were this big um narrative about the existential AI

1525
01:30:58.859 --> 01:31:01.180
risk for humanity and that, you know, there will

1526
01:31:01.180 --> 01:31:05.339
be AGI and then people will uh humanity will

1527
01:31:05.339 --> 01:31:08.859
uh will get extinct or and so, so you

1528
01:31:08.859 --> 01:31:12.049
have in this case like a very big negative

1529
01:31:12.049 --> 01:31:16.660
alarmistic narrative and what I found uh interesting in

1530
01:31:16.660 --> 01:31:20.720
a way that it was supported. A lot by

1531
01:31:20.720 --> 01:31:23.560
the same people like the big guy working in

1532
01:31:23.560 --> 01:31:26.240
AI and I was wondering like why, I mean

1533
01:31:26.240 --> 01:31:28.759
they, they don't have any, any interest in supporting

1534
01:31:28.759 --> 01:31:30.720
this narrative, but then I was thinking about this

1535
01:31:30.720 --> 01:31:32.140
and I was saying, OK, but, but they do

1536
01:31:32.399 --> 01:31:35.200
because if we are focusing about these, you know,

1537
01:31:35.359 --> 01:31:37.520
things that maybe will not happen or maybe they

1538
01:31:37.520 --> 01:31:40.359
will in 50 years, we are not looking how

1539
01:31:40.359 --> 01:31:42.640
they are getting the money, where are the data

1540
01:31:42.640 --> 01:31:45.839
that they're using, so again, I think this big

1541
01:31:45.839 --> 01:31:50.660
narrative are in a way. Paradoxically going in a

1542
01:31:50.660 --> 01:31:55.259
good direction for, for the, the, the, the, the

1543
01:31:55.259 --> 01:31:57.859
companies or the social media. So it's, it's also

1544
01:31:57.859 --> 01:32:00.970
this uh this is a a risk. It's I

1545
01:32:00.970 --> 01:32:04.479
think it's much more productive to focus on a

1546
01:32:04.479 --> 01:32:07.479
realistic thing and change what we can actually change.

1547
01:32:08.620 --> 01:32:09.100
Mhm.

1548
01:32:09.580 --> 01:32:12.529
So let's, let's end on that note, Doctor Rerbi,

1549
01:32:12.540 --> 01:32:15.299
and the book is again Technopanicco. I will be

1550
01:32:15.299 --> 01:32:17.939
leaving a link to it in the description below

1551
01:32:17.939 --> 01:32:21.939
and apart from the book, where can people find

1552
01:32:21.939 --> 01:32:24.000
you and your work on the internet?

1553
01:32:25.339 --> 01:32:28.439
Yes, again, I, I, I try to do one,

1554
01:32:29.129 --> 01:32:32.660
soup stack in Italian which is, uh, which because

1555
01:32:32.660 --> 01:32:34.859
I, I was trying to focus a bit on

1556
01:32:34.859 --> 01:32:37.540
the, on the Italian audience, so there is this,

1557
01:32:37.640 --> 01:32:43.250
uh, it's called chinqueliaima which means, uh, 5, links

1558
01:32:43.250 --> 01:32:46.259
every week. In which I try to put uh

1559
01:32:46.259 --> 01:32:49.220
these links to this kind of topic we are

1560
01:32:49.220 --> 01:32:53.729
talking about and also some uh cultural evolution things.

1561
01:32:53.819 --> 01:32:57.140
So this is something that can be interesting. Again,

1562
01:32:57.270 --> 01:32:59.379
the idea here is also related to the fact

1563
01:32:59.379 --> 01:33:03.279
that uh uh if you have access to all

1564
01:33:03.350 --> 01:33:08.270
this information. Curation is kind of important, so having

1565
01:33:08.270 --> 01:33:10.390
someone that would give you, OK, just read these

1566
01:33:10.390 --> 01:33:12.430
five things for the work that you do, so

1567
01:33:12.430 --> 01:33:14.979
try to, you know, give some, some curated aspect.

1568
01:33:15.029 --> 01:33:19.000
It's, it's getting, uh, more and more important and

1569
01:33:19.000 --> 01:33:22.049
more access do you have? And then I am

1570
01:33:22.049 --> 01:33:25.609
in the, you know, the usual social media.

1571
01:33:26.930 --> 01:33:30.370
Well, and, uh, as I told you yesterday, I

1572
01:33:30.370 --> 01:33:33.759
mean the day before we recorded the interview, uh,

1573
01:33:33.850 --> 01:33:36.930
I'm going to put up your book on my

1574
01:33:36.930 --> 01:33:41.410
list of my favorite nonfiction books of 2025, and

1575
01:33:41.410 --> 01:33:45.450
I'm maybe I can influence a few people to

1576
01:33:45.450 --> 01:33:47.810
learn Italian because I will also

1577
01:33:47.810 --> 01:33:48.629
have another,

1578
01:33:49.209 --> 01:33:51.694
I will also. I have another Italian book on

1579
01:33:51.694 --> 01:33:54.375
the list. This year two Italian books, so I

1580
01:33:54.375 --> 01:33:57.964
mean maybe I, I might influence some people to

1581
01:33:57.964 --> 01:33:59.734
learn another language. That,

1582
01:33:59.834 --> 01:34:01.415
that, that would be good. I mean, I, I,

1583
01:34:01.495 --> 01:34:04.365
I, I translated it. Uh, I have a first

1584
01:34:04.365 --> 01:34:08.854
draft of an English translation. Uh, HOPEFULLY there will

1585
01:34:08.854 --> 01:34:11.794
be an English version maybe, maybe next year, but,

1586
01:34:11.854 --> 01:34:14.814
uh, let's see. People can start to learn Italian

1587
01:34:14.814 --> 01:34:16.604
and then, and then, yeah, I, I,

1588
01:34:16.694 --> 01:34:18.634
I think that, I think that's a good thing.

1589
01:34:20.040 --> 01:34:20.049
OK,

1590
01:34:22.040 --> 01:34:24.620
OK, so Doctor Aservi, thank you very much for

1591
01:34:24.620 --> 01:34:27.140
coming on the show again. It's been fascinating to

1592
01:34:27.140 --> 01:34:27.620
talk with you.

1593
01:34:28.509 --> 01:34:30.509
Thank you very much. Thank you to you. Bye,

1594
01:34:30.750 --> 01:34:31.149
everybody.

1595
01:34:32.379 --> 01:34:34.899
Hi guys, thank you for watching this interview until

1596
01:34:34.899 --> 01:34:37.020
the end. If you liked it, please share it,

1597
01:34:37.220 --> 01:34:39.979
leave a like and hit the subscription button. The

1598
01:34:39.979 --> 01:34:42.240
show is brought to you by Enlights Learning and

1599
01:34:42.240 --> 01:34:46.279
Development done differently. Check their website at enlights.com and

1600
01:34:46.279 --> 01:34:50.009
also please consider supporting the show on Patreon or

1601
01:34:50.009 --> 01:34:52.470
PayPal. I would also like to give a huge

1602
01:34:52.470 --> 01:34:55.589
thank you to my main patrons and PayPal supporters,

1603
01:34:56.009 --> 01:34:59.850
Perergo Larsson, Jerry Muller, Frederick Sundo, Bernard Seyaz Olaf,

1604
01:34:59.890 --> 01:35:03.160
Alex, Adam Cassel, Matthew Whittingberrd, Arnaud Wolff, Tim Hollis,

1605
01:35:03.319 --> 01:35:06.879
Eric Elena, John Connors, Philip Forst Connolly. Then Dmitri

1606
01:35:06.879 --> 01:35:10.910
Robert Windegerru Inai Zu Mark Nevs, Colin Holbrookfield, Governor,

1607
01:35:11.390 --> 01:35:15.189
Michel Stormir, Samuel Andrea, Francis Forti Agnun, Svergoo, and

1608
01:35:15.189 --> 01:35:18.959
Hal Herzognun, Machael Jonathan Labran, John Yardston, and Samuel

1609
01:35:18.959 --> 01:35:22.879
Curric Hines, Mark Smith, John Ware, Tom Hammel, Sardusran,

1610
01:35:23.040 --> 01:35:26.729
David Sloan Wilson, Yasilla Dezaraujo Romain Roach, Diego Londono

1611
01:35:26.729 --> 01:35:31.109
Correa. Yannik Punteran Ruzmani, Charlotte Blis Nicole Barbaro, Adam

1612
01:35:31.109 --> 01:35:35.060
Hunt, Pavlostazevski, Alekbaka Madison, Gary G. Alman, Semov, Zal

1613
01:35:35.060 --> 01:35:39.069
Adrian Yei Poltonin, John Barboza, Julian Price, Edward Hall,

1614
01:35:39.149 --> 01:35:44.390
Edin Bronner, Douglas Fry, Franco Bartolati, Gabriel Pancortezus Suliliski,

1615
01:35:44.870 --> 01:35:47.910
Scott Zachary Fish, Tim Duffy, Sony Smith, and Wisman.

1616
01:35:48.259 --> 01:35:52.290
Daniel Friedman, William Buckner, Paul Georg Jarno, Luke Lovai,

1617
01:35:52.370 --> 01:35:57.109
Georgios Theophanous, Chris Williamson, Peter Wolozin, David Williams, Dio

1618
01:35:57.109 --> 01:36:01.299
Costa, Anton Ericsson, Charles Murray, Alex Shaw, Marie Martinez,

1619
01:36:01.419 --> 01:36:05.689
Coralli Chevalier, Bangalore atheists, Larry D. Lee Jr. Old

1620
01:36:05.689 --> 01:36:10.540
Eringbon. Esterri, Michael Bailey, then Spurber, Robert Grassy, Zigoren,

1621
01:36:10.770 --> 01:36:15.180
Jeff McMahon, Jake Zul, Barnabas Raddix, Mark Kempel, Thomas

1622
01:36:15.180 --> 01:36:19.529
Dovner, Luke Neeson, Chris Story, Kimberly Johnson, Benjamin Galbert,

1623
01:36:19.660 --> 01:36:24.990
Jessica Nowicki, Linda Brendan, Nicholas Carlson, Ismael Bensleyman. George

1624
01:36:24.990 --> 01:36:29.649
Ekoriati, Valentine Steinmann, Per Crawley, Kate Van Goler, Alexander

1625
01:36:29.649 --> 01:36:36.459
Obert, Liam Dunaway, BR, Massoud Ali Mohammadi, Perpendicular, Jannes

1626
01:36:36.459 --> 01:36:41.410
Hetner, Ursula Guinov, Gregory Hastings, David Pinsov, Sean Nelson,

1627
01:36:41.529 --> 01:36:45.140
Mike Levin, and Jos Necht. A special thanks to

1628
01:36:45.140 --> 01:36:48.310
my producers Iar Webb, Jim Frank Lucas Stink, Tom

1629
01:36:48.310 --> 01:36:53.189
Vanneden, Bernardine Curtis Dixon, Benedict Mueller, Thomas Trumbull, Catherine

1630
01:36:53.189 --> 01:36:56.229
and Patrick Tobin, John Carlo Montenegro, Al Nick Cortiz,

1631
01:36:56.270 --> 01:36:59.350
and Nick Golden, and to my executive producers, Matthew

1632
01:36:59.350 --> 01:37:02.910
Lavender, Sergio Quadrian, Bogdan Kanis, and Rosie. Thank you

1633
01:37:02.910 --> 01:37:03.370
for all.

