WEBVTT

1
00:00:00.009 --> 00:00:02.700
Everybody. Welcome to a new episode of the Decent.

2
00:00:02.710 --> 00:00:05.170
I'm your host, as always Ricardo Lob. And today

3
00:00:05.179 --> 00:00:07.610
I'm joined by Dr Seth Robertson. He is a

4
00:00:07.619 --> 00:00:11.619
lecturer in Philosophy and associate Director of undergraduate studies

5
00:00:11.630 --> 00:00:16.350
at Harvard University. His research interests include moral psychology,

6
00:00:16.360 --> 00:00:21.739
the history of ethics, early Chinese ethics, social epistemology,

7
00:00:21.750 --> 00:00:25.319
virtue, ethics, and meta ethics. And today we're talking

8
00:00:25.329 --> 00:00:30.250
about normative theorizing and if it should be informed

9
00:00:30.260 --> 00:00:35.770
by non normative information per perspective or naturalism, Confucian

10
00:00:36.020 --> 00:00:39.520
ethics and some other related topics. So, Doctor Robertson,

11
00:00:39.529 --> 00:00:41.529
welcome to the show. It's a pleasure to everyone.

12
00:00:42.349 --> 00:00:43.919
Thank you so much Ricardo for having me. It's

13
00:00:43.930 --> 00:00:45.700
a pleasure to be here. I'm excited to chat

14
00:00:45.709 --> 00:00:47.180
about some of these topics.

15
00:00:47.889 --> 00:00:50.049
So I would like to start by asking you

16
00:00:50.060 --> 00:00:54.569
about normative theorizing, but perhaps it's better for us

17
00:00:54.580 --> 00:00:57.689
to start with some definitions or with a little

18
00:00:57.700 --> 00:01:01.110
bit of background here. So, uh first of all,

19
00:01:01.119 --> 00:01:04.290
what is normative theorizing? What do you mean by

20
00:01:04.300 --> 00:01:07.360
that? And on the other hand, what is non

21
00:01:07.550 --> 00:01:09.059
normative information?

22
00:01:09.529 --> 00:01:12.419
Yeah, great. So this is something that has been

23
00:01:12.430 --> 00:01:15.150
interesting to me for a very long time even

24
00:01:15.339 --> 00:01:19.069
in my very first undergraduate ethics class. I remember

25
00:01:19.220 --> 00:01:23.620
asking questions about this. And so here's a really

26
00:01:23.629 --> 00:01:26.430
general question that doesn't actually, you know, fully answer

27
00:01:26.440 --> 00:01:28.540
what you asked about, but is a good starting

28
00:01:28.550 --> 00:01:32.209
point. So, yeah, two questions. So one is to

29
00:01:32.220 --> 00:01:36.050
do ethics. Well, as a philosopher, as an ethicist,

30
00:01:36.669 --> 00:01:38.699
do you need to learn about things? Do you

31
00:01:38.709 --> 00:01:41.349
need to study things other than ethics, other than

32
00:01:41.360 --> 00:01:46.080
ethical theory? So that's one question, uh, slightly different.

33
00:01:46.089 --> 00:01:50.220
I maybe substantially different question is to do ethics

34
00:01:50.230 --> 00:01:52.860
well, personally, to be a good person, to make

35
00:01:52.930 --> 00:01:56.239
good moral decisions, what do you need to know

36
00:01:56.250 --> 00:02:00.430
beyond ethical theory? I mean, some philosophers and non

37
00:02:00.440 --> 00:02:02.069
philosophers might ask you to need to know that

38
00:02:02.080 --> 00:02:05.610
much about ethical theory. Um There's interesting research on

39
00:02:05.620 --> 00:02:08.570
that, including some that you've um talked to some

40
00:02:08.580 --> 00:02:13.009
folks doing on your show. Um But sort of

41
00:02:13.020 --> 00:02:15.210
to get at this question, what do we mean

42
00:02:15.220 --> 00:02:18.899
by normative and non, these are terms that we

43
00:02:18.910 --> 00:02:22.929
use in philosophy all the time and don't always

44
00:02:22.940 --> 00:02:25.630
explain exactly what we mean to students when we

45
00:02:25.639 --> 00:02:27.970
say them, I remember a friend of mine in

46
00:02:27.979 --> 00:02:31.380
graduate school and my master's program after studying philosophy

47
00:02:31.389 --> 00:02:33.429
at the graduate level for two years that I

48
00:02:33.440 --> 00:02:37.919
still don't know what normative means. Um But so

49
00:02:37.929 --> 00:02:40.009
what I mean by it and this, I mean,

50
00:02:40.020 --> 00:02:41.830
I don't think, I mean, anything all that special

51
00:02:41.839 --> 00:02:44.029
or different by it, but there are questions about

52
00:02:44.039 --> 00:02:47.460
what should we do, what should we believe? What

53
00:02:47.470 --> 00:02:51.929
should we feel? What should we think? Um Those

54
00:02:51.940 --> 00:02:54.830
are normative questions, some questions that have to do

55
00:02:54.839 --> 00:02:58.960
with what should s goods bads, right or wrong.

56
00:02:59.360 --> 00:03:04.610
Um Typically in philosophy, when we like talk about

57
00:03:04.619 --> 00:03:09.690
normative normativity, we're usually thinking about morality, ethics. But

58
00:03:09.699 --> 00:03:12.809
so like questions about what should we do or,

59
00:03:12.820 --> 00:03:14.759
you know, maybe what kind of person should we

60
00:03:14.770 --> 00:03:17.270
try to become? What kind of character trait should

61
00:03:17.279 --> 00:03:20.089
we try to develop? What kind of motivations should

62
00:03:20.100 --> 00:03:22.610
we have for our actions? What kind of reasons

63
00:03:22.619 --> 00:03:25.910
should we have in our ethical decision making? But

64
00:03:26.190 --> 00:03:29.490
of course, there are lots of shoulds beyond moral,

65
00:03:29.500 --> 00:03:32.619
right? So there's like what we say, like epistemic.

66
00:03:33.110 --> 00:03:35.660
So what should I believe? What am I like?

67
00:03:35.669 --> 00:03:41.000
What am I justified in believing something? Um There

68
00:03:41.009 --> 00:03:44.210
are sort of like prudential should so like given

69
00:03:44.250 --> 00:03:47.380
what I want, given my own life happiness or

70
00:03:47.389 --> 00:03:50.699
flourishing, what should I pursue? Should I take this

71
00:03:50.710 --> 00:03:54.009
job or that job? Right. What should I major

72
00:03:54.020 --> 00:03:57.309
in? Um Should I be friends with this person?

73
00:03:57.320 --> 00:03:59.419
Would they be a good friend or not? So

74
00:03:59.429 --> 00:04:02.779
all of these questions about should, should do, should

75
00:04:02.789 --> 00:04:05.139
decide to do, should believe, should think these are

76
00:04:05.149 --> 00:04:12.110
like normative questions, right? And, and you know, 20th

77
00:04:12.119 --> 00:04:16.000
century thought there was often a kind of strong

78
00:04:16.010 --> 00:04:20.230
divide between normative and non normative questions. And we

79
00:04:20.238 --> 00:04:21.959
have different ways of describing that. Sometimes we see

80
00:04:21.970 --> 00:04:27.010
like prescriptive versus descriptive. Right. And so I guess,

81
00:04:27.019 --> 00:04:29.709
you know, like, as, you know, very well, um,

82
00:04:29.839 --> 00:04:32.839
in the 20th century there's a, there's was sort

83
00:04:32.850 --> 00:04:38.140
of a widespread belief in academia, right, that certain

84
00:04:38.149 --> 00:04:41.510
fields of study are doing mostly non normative work,

85
00:04:41.519 --> 00:04:44.420
they're trying to understand how the world works. Right.

86
00:04:44.429 --> 00:04:46.440
So, like, the sciences are trying to understand how

87
00:04:46.450 --> 00:04:50.239
the world works and normativity, ethics, that sort of

88
00:04:50.250 --> 00:04:53.359
stuff that, like wishy washy stuff. Like maybe they

89
00:04:53.369 --> 00:04:56.329
can talk about that in philosophy departments. But no,

90
00:04:56.339 --> 00:04:59.079
that's not what we're doing. We're trying to figure

91
00:04:59.089 --> 00:05:02.290
out how the world world really works. And so

92
00:05:02.299 --> 00:05:04.529
you get, like people say, there's like this really

93
00:05:04.540 --> 00:05:07.290
strong distinction between like facts and opinions or facts

94
00:05:07.299 --> 00:05:11.089
and values. Um AND things like that. Now more

95
00:05:11.100 --> 00:05:13.809
recently, people have pointed out that, you know, this

96
00:05:13.820 --> 00:05:16.649
is actually really messy sort of thing. Like you

97
00:05:16.660 --> 00:05:20.339
can't do science without values, like, you know, good

98
00:05:20.350 --> 00:05:27.029
evidence, rigor, um appreciation for truth, um appreciation for

99
00:05:27.040 --> 00:05:30.649
honesty, like intellectual honesty, honestly reporting your data, things

100
00:05:30.660 --> 00:05:34.779
like that. So it turns out that probably all

101
00:05:34.790 --> 00:05:37.489
of our lives, including like our research lives in

102
00:05:37.500 --> 00:05:40.390
academia, regardless of our field are infused with Normandy

103
00:05:40.399 --> 00:05:45.399
and normativity in various ways. But so that was

104
00:05:45.410 --> 00:05:47.859
like a very long winded explanation. It still didn't

105
00:05:47.869 --> 00:05:51.339
get to the original question, right? Which was why

106
00:05:51.350 --> 00:05:54.480
do I am I so interested in non normative

107
00:05:54.489 --> 00:05:58.690
information for normative decision making. So even though, I

108
00:05:58.700 --> 00:06:00.200
just sort of said, like it's a really, really

109
00:06:00.209 --> 00:06:05.450
messy, everything is normative in various ways. I think

110
00:06:05.459 --> 00:06:09.049
going back to the original way that the original

111
00:06:09.059 --> 00:06:12.559
questions that I asked so to do ethics well,

112
00:06:12.570 --> 00:06:14.839
to do moral philosophy. Well, what do you need

113
00:06:14.850 --> 00:06:18.070
to know outside of ethical theory to be like

114
00:06:18.079 --> 00:06:20.220
to make moral, your own moral decisions, your own

115
00:06:20.230 --> 00:06:24.570
ethical decisions, um to decide what kind of ethical

116
00:06:24.579 --> 00:06:26.329
person you wanna be. What do you need to

117
00:06:26.339 --> 00:06:33.089
know beyond ethical theory and different philosophers, different people

118
00:06:33.100 --> 00:06:35.209
have thought about this, there's a huge spectrum of

119
00:06:35.220 --> 00:06:39.260
answers, right? Um Some people are very happy to

120
00:06:39.269 --> 00:06:43.239
do like highly abstract philosophy, right? Sitting in their

121
00:06:43.250 --> 00:06:47.260
armchair coming up with really really impressive complex robust

122
00:06:47.269 --> 00:06:52.000
theories. Some people are happy to really, really get

123
00:06:52.010 --> 00:06:55.440
into the weeds of debates about these theories, um

124
00:06:55.450 --> 00:06:59.750
very rigorously and spend a lot of their time

125
00:06:59.760 --> 00:07:02.950
doing ethics thinking about that and you know, very,

126
00:07:02.959 --> 00:07:07.880
very, very abstract theoretical kind of work. Um And

127
00:07:07.890 --> 00:07:09.890
I think that's, that's really, really important. I think

128
00:07:09.899 --> 00:07:12.619
that's a valid, very valid and very important way

129
00:07:12.630 --> 00:07:15.649
of doing moral philosophy, but it's a spectrum many

130
00:07:15.660 --> 00:07:21.230
other philosophers have been interested in. Um WHAT do

131
00:07:21.239 --> 00:07:24.970
we need to know about human psychology, human sociology,

132
00:07:24.980 --> 00:07:29.429
human behavior. Um And how does that influence our

133
00:07:29.440 --> 00:07:34.730
ethical theorizing? So like to give example in Kant's

134
00:07:34.739 --> 00:07:37.619
groundwork for the Metaphysics of morals at the very

135
00:07:37.630 --> 00:07:40.920
beginning, he makes this distinction So Kant write the

136
00:07:40.929 --> 00:07:43.989
moral, like 17 hundreds, one of the great moral

137
00:07:44.000 --> 00:07:48.179
philosophers um says, you know, there's metaphysics of morality

138
00:07:48.190 --> 00:07:50.970
and that's these, you know, big philosophical questions about

139
00:07:50.980 --> 00:07:53.619
how morality works. And there's the anthropology of morality.

140
00:07:53.700 --> 00:07:55.290
And by that, he means like today, we would

141
00:07:55.299 --> 00:07:59.700
probably say moral psychology, moral anthropology, one way of

142
00:07:59.709 --> 00:08:03.579
thinking about this distinction is the philosophy comes first,

143
00:08:03.720 --> 00:08:07.799
then you develop your ethical theory, you do like

144
00:08:07.809 --> 00:08:11.079
really, really good critical thinking and then you figure

145
00:08:11.089 --> 00:08:13.059
out how to apply that in the world. Real

146
00:08:13.070 --> 00:08:16.079
world. So on that way of thinking about things,

147
00:08:16.369 --> 00:08:19.730
the Phil philosophy and the, you know, real world

148
00:08:19.739 --> 00:08:22.119
application are kind of separate and the philosophy is

149
00:08:22.130 --> 00:08:26.940
first, other people throughout history and today have thought

150
00:08:26.950 --> 00:08:32.969
that they're much more intertwined, right? Um And maybe

151
00:08:32.979 --> 00:08:35.780
it's not as clear which one should have priority.

152
00:08:35.789 --> 00:08:39.619
So for various reasons, um Aristotle and the Nicko

153
00:08:39.659 --> 00:08:42.808
Ma and ethics famously said we're not trying to

154
00:08:42.820 --> 00:08:46.090
just merely understand like what virtue is, the definition

155
00:08:46.099 --> 00:08:49.989
of virtue, but to become virtuous. Um And so

156
00:08:50.929 --> 00:08:53.289
the last thing that I'll say about this, I

157
00:08:53.299 --> 00:08:59.200
sort of give an example, right? Um There are

158
00:08:59.210 --> 00:09:01.619
sort of two big things where none yet to

159
00:09:01.630 --> 00:09:04.419
get to the original question, non normative information matters.

160
00:09:04.549 --> 00:09:07.039
So one is I think maybe not controversial at

161
00:09:07.049 --> 00:09:09.510
all to make good decisions in general, you need

162
00:09:09.520 --> 00:09:13.729
good information, right? Um So like when you're deciding

163
00:09:13.739 --> 00:09:16.869
which politician to support, which policies do they support?

164
00:09:17.090 --> 00:09:20.229
You need? No, like not like regular good information

165
00:09:20.239 --> 00:09:23.530
about like will this policy actually be effective, will

166
00:09:23.539 --> 00:09:27.419
trickle down economics work? Right? So regardless of what

167
00:09:27.429 --> 00:09:29.419
you think about like the end goal, the morality

168
00:09:29.429 --> 00:09:32.500
of the end goal will like that policy work.

169
00:09:33.150 --> 00:09:37.070
Um And then there's sort of messy questions that

170
00:09:37.080 --> 00:09:39.039
are kind of normative, non normative, like, OK, do

171
00:09:39.049 --> 00:09:41.750
I trust this person? Right. So they say they're

172
00:09:41.760 --> 00:09:44.179
gonna do this, these policies? Are they actually going

173
00:09:44.190 --> 00:09:47.640
to do it or not? Right. Um So there's

174
00:09:47.650 --> 00:09:49.989
this and I think that the vast majority of

175
00:09:50.000 --> 00:09:52.960
philosophers throughout history would be very happy to grant

176
00:09:52.969 --> 00:09:55.969
this. Um Then there's, and this is where I

177
00:09:55.979 --> 00:09:58.280
think it's really interesting. Then there are sort of

178
00:09:58.289 --> 00:10:02.619
deeper questions about um how much should that kind

179
00:10:02.630 --> 00:10:06.539
of thing go feed back into our ethical theorizing.

180
00:10:06.989 --> 00:10:09.929
Um How much should we be concerned about how

181
00:10:09.940 --> 00:10:13.989
human moral psychology works? And I think the answer

182
00:10:14.000 --> 00:10:16.179
and I'm not the only person I'm, you know,

183
00:10:16.190 --> 00:10:17.770
at one end of the spectrum of philosophers, I

184
00:10:17.780 --> 00:10:19.130
think it matters quite a bit.

185
00:10:21.080 --> 00:10:24.169
Uh So o OK, so I have 21 or

186
00:10:24.179 --> 00:10:27.250
two more questions about that. But let me just

187
00:10:27.260 --> 00:10:29.700
say, and perhaps you can comment on it that

188
00:10:29.710 --> 00:10:32.059
of course, on the show, I've already talked many

189
00:10:32.070 --> 00:10:35.900
times with moral psychologists with uh uh with people

190
00:10:35.909 --> 00:10:38.989
who work on morality from an anthropological perspective, from

191
00:10:39.000 --> 00:10:42.179
the perspective of of game theory. And I guess

192
00:10:42.190 --> 00:10:45.280
we could put all of this in the descriptive

193
00:10:45.289 --> 00:10:50.054
camp of things, in the non normative informational camp

194
00:10:50.064 --> 00:10:52.734
of things. And I, and I guess that when

195
00:10:52.744 --> 00:10:56.193
it comes to our ethicists or moral philosophers, then

196
00:10:56.203 --> 00:10:59.314
deal with this information. I mean, one of the

197
00:10:59.323 --> 00:11:01.893
debates that interests me the most, for example, is

198
00:11:01.903 --> 00:11:05.093
the debate in meta ethics between the moral realists

199
00:11:05.104 --> 00:11:09.304
and the moral anti realists. And they pick uh

200
00:11:09.314 --> 00:11:11.783
sometimes on the same information but use it in

201
00:11:11.794 --> 00:11:15.497
different. So for example, the, the moral realists, they,

202
00:11:15.507 --> 00:11:18.867
they already have some sort of uh framework in

203
00:11:18.877 --> 00:11:21.538
mind and they say, oh, this is what we

204
00:11:21.547 --> 00:11:25.047
should strive for, this is what's good to attain

205
00:11:25.057 --> 00:11:28.408
and so on. And so with the constraints that

206
00:11:28.418 --> 00:11:32.888
we know of about our uh moral psychology, then

207
00:11:32.898 --> 00:11:37.168
this is the way we should strive toward these

208
00:11:37.177 --> 00:11:40.892
goals, good, whatever and the moral anti or uh

209
00:11:40.901 --> 00:11:45.091
or the, yeah, the moral anti realists, what they

210
00:11:45.101 --> 00:11:47.781
do many times is that they bring to the

211
00:11:47.791 --> 00:11:52.401
table information or knowledge from psychology, anthropology, sociology and

212
00:11:52.411 --> 00:11:56.101
so on to build the case that there's really

213
00:11:56.111 --> 00:12:01.062
not anything objective about morality out there, there's no

214
00:12:01.072 --> 00:12:04.796
objective moral values. It's just a man of different

215
00:12:04.895 --> 00:12:09.916
uh preferences, different psychological predispositions, people being exposed to

216
00:12:09.926 --> 00:12:14.145
different cultural environments and so on. So, uh that's

217
00:12:14.156 --> 00:12:17.656
for example, one of the ways that this sort

218
00:12:17.666 --> 00:12:22.515
of um question that we're addressing here manifests in

219
00:12:22.606 --> 00:12:26.515
both science and then how it gets applied to

220
00:12:26.526 --> 00:12:28.476
philosophy or moral philosophy, more

221
00:12:28.486 --> 00:12:33.940
specific. Right. Yeah. Absolutely. And I mean, I think

222
00:12:33.950 --> 00:12:37.059
this is fascinating and I've also personally been there.

223
00:12:37.070 --> 00:12:40.419
Right. So, um, I sort of switched in when

224
00:12:40.429 --> 00:12:42.349
I was in graduate school from a moral realist

225
00:12:42.359 --> 00:12:45.770
to a moral anti realist in large part because

226
00:12:45.780 --> 00:12:47.919
of these concerns. And so I was one of

227
00:12:47.929 --> 00:12:49.469
the people who was, like, you know, using some

228
00:12:49.479 --> 00:12:54.549
of this, um, research from outside of psychology to

229
00:12:54.640 --> 00:12:56.909
um argue both ways. I don't know that I'm,

230
00:12:56.919 --> 00:12:59.640
you know, more correct now than I was in

231
00:12:59.650 --> 00:13:01.570
the past. But yeah,

232
00:13:02.859 --> 00:13:04.659
uh and so, uh but I mean, when you

233
00:13:04.669 --> 00:13:08.909
talk about non normative information and it's possibly constraining

234
00:13:09.000 --> 00:13:12.909
our no normative theorizing. What kinds of, of course,

235
00:13:12.919 --> 00:13:15.869
I think you've already gave some examples then. Uh

236
00:13:15.880 --> 00:13:20.919
But what kinds of non normative information are you?

237
00:13:20.929 --> 00:13:21.640
Right. Exactly.

238
00:13:22.210 --> 00:13:25.349
So, as I mentioned before, um I've for a

239
00:13:25.359 --> 00:13:27.789
long time, been quite interested in, you know, empirical

240
00:13:27.799 --> 00:13:32.000
research and moral psychology and related fields. Um And,

241
00:13:32.010 --> 00:13:33.989
you know, just having a better understanding of how

242
00:13:34.000 --> 00:13:37.190
our moral minds work and that's absolutely no easy

243
00:13:37.200 --> 00:13:40.849
feat for multiple reasons. Um But I have for

244
00:13:40.859 --> 00:13:43.400
a long time really thought that that's quite important,

245
00:13:43.409 --> 00:13:45.909
but I don't think that's the only thing that's

246
00:13:46.210 --> 00:13:50.119
really, really interesting and really, really useful for ethicist.

247
00:13:50.450 --> 00:13:53.820
Um So for example, I also think that what

248
00:13:53.830 --> 00:13:56.969
we might call like sort of philosophical anthropology is

249
00:13:56.979 --> 00:13:58.440
really useful and what I mean by this and

250
00:13:58.450 --> 00:14:02.119
things like moral slang. Um If you start thinking

251
00:14:02.130 --> 00:14:05.979
about it, our moral vocabularies are just absolutely full

252
00:14:05.989 --> 00:14:09.080
of ethical terms, right? And I think, you know,

253
00:14:09.090 --> 00:14:13.200
especially characterological terms, terms, like describing robust traits. So

254
00:14:13.210 --> 00:14:16.119
somebody, um and these are things that have, you

255
00:14:16.130 --> 00:14:18.340
know, somebody is a jerk, right? Things that have

256
00:14:18.349 --> 00:14:21.349
been explored by philosophers, like including philosophers that you

257
00:14:21.359 --> 00:14:24.659
venture read, right? Um There's a really good and

258
00:14:24.669 --> 00:14:26.900
that's like Eric Schwitz Gables works. There's a really

259
00:14:26.909 --> 00:14:29.919
good little paper by the philosopher Bonnie Man on

260
00:14:30.390 --> 00:14:32.369
being a creep. Like what does it mean to

261
00:14:32.380 --> 00:14:38.000
be a creep? Um You know, the there's so

262
00:14:38.010 --> 00:14:41.289
much really interesting work about like, yeah, moral slang

263
00:14:41.419 --> 00:14:44.809
and yeah, you start to like how many words

264
00:14:44.820 --> 00:14:48.119
we have like a rat, a snitch, a mensch

265
00:14:48.200 --> 00:14:51.559
an OG, right? There, we have so many words

266
00:14:51.570 --> 00:14:55.580
I'm describing people. Um And, and traits. And so

267
00:14:55.590 --> 00:14:58.869
I think that's really interesting. Um A recent paper

268
00:14:58.880 --> 00:15:00.840
that I had on this was like, what's wrong

269
00:15:00.849 --> 00:15:03.919
with unhelpful comments? Right? So I got really interested

270
00:15:03.929 --> 00:15:07.679
in when people describe that was a really unhelpful

271
00:15:07.690 --> 00:15:09.460
thing to say or that was an unhelpful comment.

272
00:15:09.469 --> 00:15:12.010
What were like, what are they really saying? Um

273
00:15:12.179 --> 00:15:16.570
So for me, I like people are constantly making

274
00:15:16.580 --> 00:15:21.219
moral claims, epistemic claims, normative claims in general, they're

275
00:15:21.229 --> 00:15:25.859
using normative vocabulary and often in really, really interesting

276
00:15:25.869 --> 00:15:31.020
ways. Um So I think it's worth investigating that.

277
00:15:31.030 --> 00:15:32.440
And how does that work? I think there's often

278
00:15:32.450 --> 00:15:35.619
lots of really interesting lessons there. And the last

279
00:15:35.630 --> 00:15:39.200
thing that I wanted to mention um specifically, and

280
00:15:39.210 --> 00:15:42.820
this is something that's really influenced my work is

281
00:15:42.880 --> 00:15:46.520
um just the fact that we're socially situated creatures,

282
00:15:46.530 --> 00:15:50.739
we live in societies and those societies have entrenched

283
00:15:50.750 --> 00:15:54.520
power structures, right? So, um various people and groups

284
00:15:54.530 --> 00:15:57.159
have more power than others and they build that

285
00:15:57.169 --> 00:16:02.039
into our systems and societies, right? And so I

286
00:16:02.049 --> 00:16:04.739
think, and this is again, it's not just me

287
00:16:04.750 --> 00:16:06.880
and there's a spectrum of philosophers who are concerned

288
00:16:06.890 --> 00:16:09.039
about this and philosophy, we often call this like

289
00:16:09.070 --> 00:16:13.880
non ideal approaches, right? So um when we do

290
00:16:13.890 --> 00:16:17.380
our theorizing and ethics or epistemology, we're really, really

291
00:16:17.390 --> 00:16:20.880
concerned and think we need to take account of

292
00:16:20.890 --> 00:16:24.190
the fact that um different groups and people have

293
00:16:24.200 --> 00:16:26.400
more power than others and use that power in

294
00:16:26.409 --> 00:16:30.219
various ways. And we need to have our theories

295
00:16:30.229 --> 00:16:32.640
like really, really being paying attention to that.

296
00:16:33.900 --> 00:16:37.030
So, keeping in mind one of the points you

297
00:16:37.039 --> 00:16:39.710
made earlier that and this is something that I've

298
00:16:39.719 --> 00:16:42.059
already talked with other people on the show that

299
00:16:42.070 --> 00:16:45.619
uh values also play a role in science and

300
00:16:45.630 --> 00:16:49.000
in the production of scientific knowledge. So keeping that

301
00:16:49.010 --> 00:16:53.130
caveat in mind uh just to close the section

302
00:16:53.140 --> 00:16:57.580
of the uh so should you then should you

303
00:16:57.590 --> 00:16:59.960
uh uh do you think then that we should

304
00:16:59.969 --> 00:17:05.530
or should non normative information, constrain or normative theorizing.

305
00:17:05.670 --> 00:17:09.880
Yeah. Yeah, I think absolutely, it should in a

306
00:17:09.890 --> 00:17:12.670
lot of different ways. Right. So, and I mean,

307
00:17:12.680 --> 00:17:17.030
again, with the caveat that, you know, the, the

308
00:17:17.040 --> 00:17:20.339
distinction is incredibly messy and, you know, maybe it's

309
00:17:20.348 --> 00:17:24.920
a spectrum at best. Um But, and some of

310
00:17:24.930 --> 00:17:27.050
this sort of might make more sense once we

311
00:17:27.060 --> 00:17:30.209
see applications of like, what would this actually look

312
00:17:30.219 --> 00:17:33.660
like in practice? Um But yeah, so if you

313
00:17:33.670 --> 00:17:35.449
sort of take it as a like a descriptive

314
00:17:35.459 --> 00:17:37.530
claim that in fact, the way that, you know,

315
00:17:37.540 --> 00:17:41.130
human societies work is that they're quite often hierarchical,

316
00:17:41.260 --> 00:17:45.329
they often have entrenched hierarchies of power. Um If

317
00:17:45.339 --> 00:17:47.050
you could say that that's non no, like just

318
00:17:47.060 --> 00:17:49.550
grant, that's more on the non normative side, that's

319
00:17:49.560 --> 00:17:52.510
more on the descriptive side. I think that um

320
00:17:52.550 --> 00:17:55.699
theories that don't pay serious attention to that maybe

321
00:17:55.709 --> 00:17:59.329
even in the very first steps of theorizing, um

322
00:17:59.599 --> 00:18:05.349
it's just a potentially dangerous move. Um You might

323
00:18:05.359 --> 00:18:09.579
be missing out on incredibly important things and your

324
00:18:09.589 --> 00:18:12.030
theories might not be well placed to actually address

325
00:18:12.040 --> 00:18:14.040
these problems when we get there. And so this

326
00:18:14.050 --> 00:18:17.160
is again, um for philosophers, this is the non

327
00:18:17.170 --> 00:18:18.760
ideal stuff, right?

328
00:18:19.270 --> 00:18:22.489
And of course, I guess that you would agree

329
00:18:22.500 --> 00:18:25.619
if I say that here when it comes to

330
00:18:25.630 --> 00:18:29.156
the kinds of non normative information that we deal

331
00:18:29.166 --> 00:18:34.115
with it and then uh normative rise about ethics

332
00:18:34.125 --> 00:18:37.046
and moral philosophy, we have to keep in mind

333
00:18:37.056 --> 00:18:40.375
that of course, it's not as simple as we

334
00:18:40.385 --> 00:18:44.355
have and um a sort of human nature that

335
00:18:44.365 --> 00:18:48.355
is fixed because, I mean, that learned through human

336
00:18:48.365 --> 00:18:53.875
behavioral ecology and anthropology and some other disciplines, uh

337
00:18:53.885 --> 00:18:56.682
I mean, of course, we have certain evolved through

338
00:18:56.692 --> 00:19:00.332
these positions, but they are much more, much more

339
00:19:00.342 --> 00:19:05.911
malleable than uh just uh like evolutionary theory just

340
00:19:05.921 --> 00:19:07.891
by self would tell you.

341
00:19:08.442 --> 00:19:11.862
Absolutely. And to me, this is an incredibly important

342
00:19:11.871 --> 00:19:16.572
point. Um JUST this, like, even from a, like

343
00:19:16.582 --> 00:19:20.712
a purely evolutionary standpoint, even that is incredibly messy,

344
00:19:21.010 --> 00:19:23.410
right? So, yeah, it's sort of like very simplistic

345
00:19:23.420 --> 00:19:26.170
claims about human nature. I mean, one of my

346
00:19:26.180 --> 00:19:28.650
favorite, like, least favorite, you get like an intro

347
00:19:28.699 --> 00:19:31.849
of philosophy class, right? Students, you know, human beings

348
00:19:31.859 --> 00:19:35.209
are inherently selfish, right? That's like one of the

349
00:19:35.219 --> 00:19:36.989
more like, well, what does it mean to be

350
00:19:37.000 --> 00:19:40.030
inherently like from an evolutionary perspective? Well, we have

351
00:19:40.040 --> 00:19:42.219
all sorts of things that, you know, maybe are

352
00:19:42.699 --> 00:19:45.680
more hardwired, but that's like there's tons of different

353
00:19:45.689 --> 00:19:48.290
stuff going on in our brains and that's not

354
00:19:48.300 --> 00:19:50.229
even, well, like once we add in culture to

355
00:19:50.239 --> 00:19:54.880
the mix and like how those things interact. Um

356
00:19:55.250 --> 00:19:57.599
So I think it's incredibly important that this is

357
00:19:57.609 --> 00:20:01.430
all incredibly messy um because that makes it more

358
00:20:01.439 --> 00:20:03.750
difficult to have a nice, simple theory that can

359
00:20:03.760 --> 00:20:05.430
explain it all.

360
00:20:06.079 --> 00:20:09.069
Mhm Yeah. And I mean, uh that came to

361
00:20:09.079 --> 00:20:11.619
my mind also because since you mentioned, for example,

362
00:20:11.630 --> 00:20:16.250
people thinking that we are innately predisposed toward the

363
00:20:16.260 --> 00:20:20.650
building hierarchical societies. I mean, yeah, that might be

364
00:20:20.660 --> 00:20:24.160
true to some extent. But then if you assume

365
00:20:24.170 --> 00:20:27.640
that there's no malleability to that, then you will

366
00:20:27.650 --> 00:20:32.300
inevitably build the hierarchical society when perhaps in certain

367
00:20:32.310 --> 00:20:35.699
contexts that it could not apply.

368
00:20:35.810 --> 00:20:41.689
Right. Exactly. Yeah. So it, yeah. And doesn't imply

369
00:20:41.699 --> 00:20:43.339
that, you know, we need to do things in

370
00:20:43.349 --> 00:20:46.020
a certain way or the Yeah, because, yeah. So

371
00:20:46.030 --> 00:20:50.250
I think it's is always a complex question and

372
00:20:50.260 --> 00:20:53.530
answer and it needs a lot of investigation figuring.

373
00:20:53.540 --> 00:20:55.780
OK, so what actually are we supposed to do?

374
00:20:55.790 --> 00:20:58.020
That's incredibly difficult to figure out. And then there's

375
00:20:58.030 --> 00:21:00.209
sort of like given that what can we do

376
00:21:00.219 --> 00:21:03.760
about it, that's also incredibly difficult to figure out

377
00:21:03.770 --> 00:21:06.160
and people will have a tendency to oversimplify these

378
00:21:06.170 --> 00:21:06.640
things.

379
00:21:07.709 --> 00:21:10.540
And so I guess that related to what we've

380
00:21:10.550 --> 00:21:13.229
been talking about, could you tell us a little

381
00:21:13.239 --> 00:21:17.300
bit about the sort of novel version of meta

382
00:21:17.489 --> 00:21:23.420
ethical, human, human constructivism that you called perspective or

383
00:21:23.449 --> 00:21:24.550
naturalism?

384
00:21:24.930 --> 00:21:29.310
Is that good? Yeah. So, so this is, so

385
00:21:29.319 --> 00:21:33.650
I should say, um this was work in my

386
00:21:33.660 --> 00:21:36.800
dissertation. Um So it's something that's been in sort

387
00:21:36.810 --> 00:21:38.680
of in the back of my mind for the

388
00:21:38.689 --> 00:21:41.869
past four or five years. And what I was

389
00:21:41.880 --> 00:21:44.260
really trying to do there is sort of figure

390
00:21:44.270 --> 00:21:47.969
out what do I really believe about all of

391
00:21:47.979 --> 00:21:51.189
this, what my own approach because I was really

392
00:21:51.199 --> 00:21:56.339
torn. Right. So you've had so many great philosophers

393
00:21:56.349 --> 00:21:59.489
and people specialize in meta ethics who can talk

394
00:21:59.500 --> 00:22:02.579
about this much more fluently and correctly than I

395
00:22:02.589 --> 00:22:05.290
can. But what I like what I wanna say,

396
00:22:05.300 --> 00:22:08.880
so it's really interesting to me that we're so,

397
00:22:08.890 --> 00:22:12.719
like, meta ethically muddled. On the one hand, like,

398
00:22:12.729 --> 00:22:16.180
students come into my ethics classes, they have such

399
00:22:16.189 --> 00:22:20.550
strong political opinions, they have strong ethical opinions, but

400
00:22:20.560 --> 00:22:23.709
at the same time, they'll like, really strongly assert

401
00:22:23.719 --> 00:22:26.579
things like, well, morality is relative or this is

402
00:22:26.589 --> 00:22:29.069
all subjective. And it's like, well, how do you

403
00:22:29.079 --> 00:22:33.250
square away this really strong assertion of about like

404
00:22:33.260 --> 00:22:36.219
morality, it's relative, it's subjective, but at the same

405
00:22:36.229 --> 00:22:39.689
time having really, really, really strong moral or political

406
00:22:39.699 --> 00:22:46.739
opinions. Um AND, you know, those are the questions

407
00:22:46.750 --> 00:22:49.329
at the heart of meta ethics, right? And so

408
00:22:49.339 --> 00:22:51.739
like long time listeners of your show, like we

409
00:22:51.750 --> 00:22:56.000
have heard, you know, this distinction, moral realism and

410
00:22:56.010 --> 00:23:02.650
anti realism and the like the and history of

411
00:23:02.660 --> 00:23:05.369
that is maybe early on in the 20th century

412
00:23:05.380 --> 00:23:08.140
by realism. Are there moral facts or their moral

413
00:23:08.150 --> 00:23:12.349
truths? But then as the discussion went on and

414
00:23:12.359 --> 00:23:14.290
on and on, sort of like, as you've already

415
00:23:14.300 --> 00:23:16.890
sort of mentioned, the discussion shifted to thinking about

416
00:23:16.900 --> 00:23:20.469
objectivity, right? Are there objective moral truths or objective

417
00:23:20.479 --> 00:23:23.869
moral facts? And what do we mean by objective

418
00:23:23.880 --> 00:23:28.739
that's incredibly difficult to say. But the general sort

419
00:23:28.750 --> 00:23:32.319
of idea that developed is like, sort of compare

420
00:23:33.010 --> 00:23:36.589
certain other fields of study, like certain sciences or

421
00:23:36.599 --> 00:23:40.400
history where the facts that they're like, the fact

422
00:23:40.410 --> 00:23:42.260
of the matter it's out there, it doesn't depend

423
00:23:42.270 --> 00:23:44.819
on anything about our minds. Maybe the way that

424
00:23:44.829 --> 00:23:48.020
we explain, it depends on our minds. But the

425
00:23:48.030 --> 00:23:50.390
thing that is being studying is like, it's out

426
00:23:50.400 --> 00:23:55.619
there. Um That's what we mean roughly by very,

427
00:23:55.630 --> 00:23:59.819
incredibly, roughly by objective and anti realist of various

428
00:23:59.829 --> 00:24:04.959
stripes. Um We say no morality, moral claims, moral

429
00:24:04.969 --> 00:24:09.270
and moral claims depend in some way or they

430
00:24:09.319 --> 00:24:11.530
weren't, you know, the old school version was they

431
00:24:11.540 --> 00:24:13.829
aren't factual at all. Right. They're just sort of

432
00:24:13.839 --> 00:24:17.189
expressing our emotions or something like that. Um But

433
00:24:17.199 --> 00:24:19.979
the sort of, many of the newer versions are

434
00:24:19.989 --> 00:24:23.170
saying, you know, they're not objective, that's that they

435
00:24:23.180 --> 00:24:27.349
aren't based entirely on stuff outside of our minds.

436
00:24:29.119 --> 00:24:34.989
So, and this is something when people start learning

437
00:24:35.000 --> 00:24:37.280
about this, they sort of, you know, been told

438
00:24:37.290 --> 00:24:39.390
they've heard so many times that morality is subjective,

439
00:24:39.400 --> 00:24:42.699
morality is relative. And, you know, there is like

440
00:24:42.709 --> 00:24:46.579
something to that in the sense that, yeah, if

441
00:24:46.589 --> 00:24:48.699
you studied abroad, if you lived in different cultures,

442
00:24:48.709 --> 00:24:51.359
you know, that, you know, people's behaviors and opinions

443
00:24:51.369 --> 00:24:55.569
and things like that vary quite a bit. Um

444
00:24:55.650 --> 00:24:57.400
But again, at the same time, we have these

445
00:24:57.410 --> 00:25:01.400
really strong like political beliefs, moral beliefs about various

446
00:25:01.410 --> 00:25:06.930
things. So philosophers have been really like more recently,

447
00:25:06.949 --> 00:25:08.390
the past 30 or 40 years have been really

448
00:25:08.400 --> 00:25:10.420
attracted to moral realism. It makes sense of things

449
00:25:10.430 --> 00:25:15.739
like, um the civil, right? Like, racism is genuinely

450
00:25:15.750 --> 00:25:19.119
unjust, sexism is genuinely unjust. If you believe those

451
00:25:19.130 --> 00:25:21.660
sorts of things, how can you make sense of

452
00:25:21.670 --> 00:25:25.609
that without being a realist? And so I was,

453
00:25:25.619 --> 00:25:28.609
like, very much in that camp and then I

454
00:25:28.619 --> 00:25:32.770
encountered evolutionary debunking arguments and to defer a long

455
00:25:32.780 --> 00:25:34.369
and, you know, you've talked about this on your

456
00:25:34.380 --> 00:25:36.430
show, right? So I won't like go into tons

457
00:25:36.439 --> 00:25:38.630
of detail about this. But for a long time,

458
00:25:38.640 --> 00:25:39.849
I was like, no, this can't be right. But

459
00:25:39.859 --> 00:25:42.709
this general idea really was in the back of

460
00:25:42.719 --> 00:25:45.290
my mind that if you start with the assumption

461
00:25:45.300 --> 00:25:49.130
that morality, ethics could have been, if it's objective,

462
00:25:49.140 --> 00:25:52.439
it could have been anything, right? Like what would

463
00:25:52.449 --> 00:25:53.949
be the objective moral truth? Well, they could be

464
00:25:53.959 --> 00:25:56.800
anything. How would we figure them out? And the

465
00:25:56.810 --> 00:25:59.449
idea of evolutionary debunking arguments and again, long time

466
00:25:59.459 --> 00:26:03.500
listeners know this, but like the very, very general

467
00:26:03.510 --> 00:26:07.410
and maybe overly simplistic idea is, well, our intuitions

468
00:26:07.420 --> 00:26:09.630
about what we morally should do are shaped by

469
00:26:09.640 --> 00:26:13.459
many factors and like important set of them are

470
00:26:13.469 --> 00:26:16.640
evolutionary, the kind of creatures that we are, right?

471
00:26:16.650 --> 00:26:19.229
And so my example of this, which is like,

472
00:26:19.680 --> 00:26:21.859
you know, kind of strong, but I think is

473
00:26:21.880 --> 00:26:27.449
relevant is um if we were highly evolved spiders,

474
00:26:28.069 --> 00:26:31.170
our ethical systems would look entirely different. We wouldn't

475
00:26:31.180 --> 00:26:35.310
care so much about raising, like raising our Children

476
00:26:35.459 --> 00:26:37.619
and protecting Children. We wouldn't care so much about

477
00:26:37.630 --> 00:26:42.839
that. Um And so the, the very rough intuition

478
00:26:42.849 --> 00:26:46.719
underlying at the like evolutionary debunking arguments is that

479
00:26:47.439 --> 00:26:51.770
one of the key factors shaping our ethical, like

480
00:26:51.780 --> 00:26:56.589
our ethical intuitions is our evolutionary history. They weren't

481
00:26:56.599 --> 00:27:00.280
necessarily tracking truth and I came to think more

482
00:27:00.290 --> 00:27:02.300
and more and more. There's something to that now

483
00:27:02.310 --> 00:27:08.800
you might say in response, um Well, that might

484
00:27:09.109 --> 00:27:12.689
um pose some trouble for some approaches and meta

485
00:27:12.699 --> 00:27:16.530
ethics. But here's, there's other approaches that are perfectly

486
00:27:16.540 --> 00:27:19.400
safe. So like the word here is like intuition

487
00:27:19.439 --> 00:27:22.479
stick approaches, right? So you might say something like,

488
00:27:22.489 --> 00:27:26.079
OK, this is kind of like probability. We're naturally

489
00:27:26.089 --> 00:27:29.130
not very good at doing probability, right? That's why

490
00:27:29.140 --> 00:27:30.939
it took so long to figure out, but we

491
00:27:30.949 --> 00:27:33.119
could figure it out, right? We figured out how

492
00:27:33.130 --> 00:27:35.180
to do math in general. We do the math,

493
00:27:35.260 --> 00:27:38.060
we do the logic. Like once you've got rationality

494
00:27:38.069 --> 00:27:40.199
and logic on the table, you can figure things

495
00:27:40.209 --> 00:27:44.130
out. And so even if like we were evolutionary

496
00:27:44.140 --> 00:27:48.670
not predisposed to figuring out objective moral truth, we

497
00:27:48.680 --> 00:27:51.000
could figure it out using rationality and logic or

498
00:27:51.010 --> 00:27:56.430
something like that, I was initially attracted to that

499
00:27:56.439 --> 00:27:58.859
view quite a bit. But then, you know, and

500
00:27:58.869 --> 00:28:00.699
like a lot of these views, well, maybe some

501
00:28:00.709 --> 00:28:03.829
truths are self evident, right? But then I realize

502
00:28:03.849 --> 00:28:06.790
this, like I call it like the unicorn or

503
00:28:06.800 --> 00:28:10.140
the hobbit problem, right? So claims like unicorns have

504
00:28:10.150 --> 00:28:13.790
horns or hobbits have big feet, right? Like Lord

505
00:28:13.800 --> 00:28:16.410
of the Rings, right? So these are like fiction

506
00:28:16.420 --> 00:28:21.000
entities, like are those claims true? Um Like are

507
00:28:21.010 --> 00:28:23.689
they self evidence? But yeah, sort of like if

508
00:28:23.699 --> 00:28:26.010
you understand what the words mean. Yeah, unicorns have,

509
00:28:27.510 --> 00:28:29.790
but that doesn't mean that the property is being

510
00:28:29.800 --> 00:28:35.109
described actually exist in the world, right? So I

511
00:28:35.260 --> 00:28:38.550
started to worry that moral claims like murder is

512
00:28:38.560 --> 00:28:40.839
wrong is the go to example of like, that's

513
00:28:40.849 --> 00:28:43.469
self evident. Like it just means murder is wrongful

514
00:28:43.479 --> 00:28:47.880
killing. Um But if you're saying that that um

515
00:28:47.890 --> 00:28:53.729
like murder means objectively wrongful killing. The fact that

516
00:28:53.739 --> 00:28:55.849
it's like a self evident truth. If it is

517
00:28:55.859 --> 00:28:57.859
self, if that is self evident, I'm not sure

518
00:28:57.869 --> 00:29:00.520
that the objective part there is self evident to

519
00:29:00.530 --> 00:29:03.119
anyone other than certain philosophers. But even if you

520
00:29:03.130 --> 00:29:06.390
grant that and to me that doesn't imply that

521
00:29:06.400 --> 00:29:10.420
that property being described exists in this world, right?

522
00:29:10.430 --> 00:29:13.550
So I think that there is um a dangerous

523
00:29:13.560 --> 00:29:15.930
leap there. And again, like I'm not a meta

524
00:29:15.939 --> 00:29:17.910
ethics specialist. I was just trying to figure this

525
00:29:17.920 --> 00:29:21.089
out for myself. So I started to think like

526
00:29:21.609 --> 00:29:26.060
um the strong, like the strongest versions of moral

527
00:29:26.069 --> 00:29:29.770
realism or morality is really, really objective. I just

528
00:29:29.780 --> 00:29:32.579
couldn't see my way to thinking that those were

529
00:29:32.589 --> 00:29:36.040
correct anymore. Now, throughout history, there's been like two

530
00:29:36.050 --> 00:29:38.430
big lines of responses to this one is a

531
00:29:38.439 --> 00:29:42.459
really strong anti realist kind of response. And I

532
00:29:42.469 --> 00:29:45.969
think you like in Ancient India there's like a

533
00:29:45.979 --> 00:29:48.430
school called the Charva. And they say this, you

534
00:29:48.439 --> 00:29:52.050
see this in some of Plato's like dial of

535
00:29:52.069 --> 00:29:53.790
their Simic. You see this in the like the

536
00:29:53.800 --> 00:29:57.670
gorgeous um you see this in um the History

537
00:29:57.680 --> 00:30:00.540
of Thucydides, history of the Peloponnesian war, the Melian

538
00:30:00.550 --> 00:30:03.199
dialogue, the dialogue between the Athenians and the millions,

539
00:30:03.209 --> 00:30:06.160
the Athenian side, people say, you know, ethics, it's

540
00:30:06.170 --> 00:30:10.280
just a invention, reality is just an invention of

541
00:30:10.290 --> 00:30:13.479
some sort. And so anyone who believes in it

542
00:30:13.489 --> 00:30:15.520
that is just being silly. Um I think you

543
00:30:15.530 --> 00:30:18.239
also, there's a Chin ancient Chinese text called Robert

544
00:30:18.250 --> 00:30:21.680
J. Um You also get an argument like that.

545
00:30:21.689 --> 00:30:24.250
So anyone who believes in morality, come on, it's

546
00:30:24.260 --> 00:30:27.510
all made up, it's all constructed. You're just a

547
00:30:27.520 --> 00:30:30.599
fool for believing in it. That's one route that

548
00:30:30.609 --> 00:30:34.010
people have gone another route that is maybe like

549
00:30:34.020 --> 00:30:36.959
technically anti realist in the sense that it says

550
00:30:36.969 --> 00:30:40.130
the morality isn't like fully objective like this says,

551
00:30:40.140 --> 00:30:43.119
OK, yeah, but we care about all this stuff

552
00:30:43.800 --> 00:30:46.079
that's kind of like a basic premise in evolutionary

553
00:30:46.089 --> 00:30:48.930
debunking arguments, right, that we do care about. And

554
00:30:48.939 --> 00:30:51.489
so there's some the idea is like given that

555
00:30:51.500 --> 00:30:53.579
we care about it, are there better or worse

556
00:30:53.589 --> 00:30:56.699
ways of doing it? And that in the most

557
00:30:56.709 --> 00:31:01.089
general terms is kind of the constructivist picture in

558
00:31:01.099 --> 00:31:03.030
meta ethics and now there are, you know, much

559
00:31:03.040 --> 00:31:06.300
more theoretically robust and precise and rigorous ways of

560
00:31:06.310 --> 00:31:09.270
stating it, but that's the sort of general intuition

561
00:31:09.569 --> 00:31:13.300
that we care about things like we would say

562
00:31:13.310 --> 00:31:18.449
that we have a valuated standpoint. Um And what

563
00:31:18.459 --> 00:31:21.229
can we do given that? Now, there are different

564
00:31:21.239 --> 00:31:25.209
ways of cashing that out. So the, and this

565
00:31:25.219 --> 00:31:29.380
work here is the Phil um Sharon Street's excellent

566
00:31:29.390 --> 00:31:34.000
work um exploring this. And so there's like a

567
00:31:34.369 --> 00:31:38.180
Kent thought that, you know, all of these standpoints

568
00:31:38.189 --> 00:31:41.410
where we care about things should probably be governed

569
00:31:41.420 --> 00:31:43.959
by the basic rules of logic. Like, don't contradict

570
00:31:43.969 --> 00:31:46.900
yourself and write K thinks that you can get

571
00:31:46.910 --> 00:31:49.400
quite a bit of morality out of that basic

572
00:31:49.410 --> 00:31:52.069
intuition. And so it isn't quite objective, but it

573
00:31:52.079 --> 00:31:55.180
is that like every standpoint should have be concerned

574
00:31:55.189 --> 00:31:57.300
with these sorts of things. So it's kind of

575
00:31:57.310 --> 00:32:01.199
like, um pretty close to objective without being objective.

576
00:32:01.209 --> 00:32:04.680
It's still based in our standpoints and our concerns.

577
00:32:04.739 --> 00:32:09.099
Um THE human approach. All right. So like David

578
00:32:09.140 --> 00:32:15.170
Hume, um 1700 Scottish philosopher sort of says that

579
00:32:15.640 --> 00:32:19.530
there are so many different practical standpoints that not

580
00:32:19.540 --> 00:32:21.270
only people could have, but all sorts of like

581
00:32:21.280 --> 00:32:24.219
moral creatures that aliens or something like that could

582
00:32:24.229 --> 00:32:26.520
have had. And so there isn't going to be

583
00:32:26.530 --> 00:32:30.449
a single theory that applies to all of them.

584
00:32:32.050 --> 00:32:34.819
Um And so he humans, so like quick examples

585
00:32:34.829 --> 00:32:38.119
are like, he thinks we're naturally like sympathetic empathetic,

586
00:32:38.130 --> 00:32:41.530
we care about others well being in various ways.

587
00:32:41.540 --> 00:32:43.709
So that's a concern. Then he also sorts of

588
00:32:43.719 --> 00:32:48.130
thinks that they're um given our various concerns, there

589
00:32:48.140 --> 00:32:50.910
might be values that we need to develop, maybe

590
00:32:50.920 --> 00:32:53.520
invent our artificial virtues. So he thinks like justice

591
00:32:53.530 --> 00:32:56.780
is something like this. Um And that it's like,

592
00:32:56.790 --> 00:32:58.560
not maybe we, he thinks, you know, maybe we

593
00:32:58.569 --> 00:33:03.119
don't have naturally strong inclinations about justice or dispositions

594
00:33:03.130 --> 00:33:05.160
to care about it, but to get society to

595
00:33:05.170 --> 00:33:07.050
function, given the things we do care about it,

596
00:33:07.510 --> 00:33:10.109
we need to create something like that. The approach

597
00:33:10.119 --> 00:33:13.750
that I kind of took on today, I would

598
00:33:13.760 --> 00:33:17.489
probably call it something like a Confucian constructivism. Um

599
00:33:19.199 --> 00:33:21.430
Yeah, we're going to talk about it. So I

600
00:33:21.939 --> 00:33:25.229
kind of knew that I was like, thinking about

601
00:33:25.239 --> 00:33:29.390
this, but it didn't really strike me until my

602
00:33:29.400 --> 00:33:32.290
dissertation defense when one of my teachers, one wonderful,

603
00:33:32.300 --> 00:33:35.449
wonderful teacher, Amy Oberon was like, this is like,

604
00:33:35.459 --> 00:33:38.390
Shun, the great, one of the greatest Confucian philosophers

605
00:33:38.400 --> 00:33:39.709
like this is Shun are web born. I was

606
00:33:39.719 --> 00:33:45.339
like, oh, yeah, it actually is. Um So, um

607
00:33:47.030 --> 00:33:53.089
my thought and was one of the sort of

608
00:33:53.099 --> 00:33:58.239
like anti realist moves that wants to keep doing

609
00:33:58.250 --> 00:34:00.930
mostly what we've been doing, not throw it all

610
00:34:00.939 --> 00:34:05.699
away, sort of his thought about philos um ethics

611
00:34:05.829 --> 00:34:10.090
as a kind of technology or an invention. And

612
00:34:10.100 --> 00:34:12.530
that's a good thing. Right. That's, that doesn't mean

613
00:34:12.540 --> 00:34:15.208
that we should just throw it all away. Um

614
00:34:16.148 --> 00:34:18.648
So, one of the earliest statements of this is

615
00:34:18.658 --> 00:34:20.918
the person I just mentioned, Shinza who is a

616
00:34:20.958 --> 00:34:25.279
ancient Confucian philosopher, he literally says the ancient SAGES

617
00:34:25.329 --> 00:34:29.728
invented ritual and rightness and so kind of like

618
00:34:29.739 --> 00:34:33.309
small ethical things and big ethical things that the

619
00:34:33.319 --> 00:34:35.908
ancient SAGES invented it. Like that's like the term

620
00:34:35.918 --> 00:34:40.860
that he uses. Um And I already mentioned Hume's

621
00:34:40.870 --> 00:34:46.168
artificial um versus famously. Um John Mackey's invent ethics

622
00:34:46.179 --> 00:34:48.030
inventing right and wrong. One of the most famous

623
00:34:48.040 --> 00:34:52.489
important influential works of anti realism. Um He says,

624
00:34:52.500 --> 00:34:55.458
like, you know, the morality is not to be

625
00:34:55.469 --> 00:35:01.969
discovered but made. Um There's in Plato's dialogue, the

626
00:35:01.979 --> 00:35:05.560
Protagoras um Protagoras gives us, it was, I guess

627
00:35:05.570 --> 00:35:07.610
a quite a famous speech in the ancient world.

628
00:35:07.620 --> 00:35:09.449
It's not so f as f quite as famous

629
00:35:09.459 --> 00:35:11.979
today in contemporary philosophy, but he sort of says

630
00:35:11.989 --> 00:35:15.100
that um when the gods created humans first, it

631
00:35:15.110 --> 00:35:17.629
was a mess, they couldn't get anything to stay,

632
00:35:17.639 --> 00:35:19.889
the humans just fought each other. And so then

633
00:35:19.899 --> 00:35:22.000
the gods decided to give them a sense of

634
00:35:22.010 --> 00:35:25.699
justice and a sense of shame and then humans

635
00:35:25.709 --> 00:35:29.449
could cooper and society and like live together in

636
00:35:29.459 --> 00:35:33.810
cities. And one that was really influential for me

637
00:35:33.820 --> 00:35:37.260
and kind of where that name perspective, naturalism came

638
00:35:37.270 --> 00:35:42.540
from. Um WAS a philosopher um Philip Kit's um

639
00:35:42.550 --> 00:35:45.929
book, The Ethical Project, which came out almost. Well,

640
00:35:45.939 --> 00:35:49.719
I guess, yeah, 10 a little bit more than

641
00:35:49.729 --> 00:35:52.939
10 years ago. Now and he developed this view

642
00:35:52.949 --> 00:35:56.409
called pragmatic naturalism. So he was interested in the

643
00:35:56.419 --> 00:36:00.330
American pragmatist. So, like John Dewey, um Ker really

644
00:36:00.340 --> 00:36:04.000
likes this quote from Dewey that um moral conceptions

645
00:36:04.010 --> 00:36:06.629
and processes grow naturally out of the very conditions

646
00:36:06.639 --> 00:36:11.239
of human life, right? We're just moral creatures if

647
00:36:11.250 --> 00:36:13.419
we don't think too much about what morality means

648
00:36:13.429 --> 00:36:17.260
there. But um so kids think like, yeah, it's,

649
00:36:17.270 --> 00:36:21.959
we're just more naturally morally concerned. And so what

650
00:36:21.969 --> 00:36:25.449
Kitcher thinks is like the, the view that he

651
00:36:25.459 --> 00:36:29.399
developed there in that book was basically something like

652
00:36:29.409 --> 00:36:34.580
this that were kind of naturally altruistic. And that's,

653
00:36:34.590 --> 00:36:36.560
you know, he spends a lot of time sorting

654
00:36:36.570 --> 00:36:38.669
out what do we mean by altruism? If you

655
00:36:38.679 --> 00:36:40.639
were like a certain kind of scientist, especially in

656
00:36:40.649 --> 00:36:43.939
the, you know, sixties, seventies, that would be preposterous.

657
00:36:43.969 --> 00:36:49.100
Um But for like kids, you know, we are

658
00:36:49.110 --> 00:36:53.919
naturally like other concerns in various ways, but we

659
00:36:53.929 --> 00:36:58.570
have many altruism like he calls altruism failures. So

660
00:36:58.580 --> 00:37:01.610
we're concerned with others, but not that well. And

661
00:37:01.620 --> 00:37:03.699
this leads to sort of like an internal tension,

662
00:37:03.709 --> 00:37:08.510
a felt tension. And so he thinks that ethics

663
00:37:08.520 --> 00:37:11.479
like ethical theorizing and it's like earliest forms is

664
00:37:11.489 --> 00:37:14.629
developing out of this and we develop systems to

665
00:37:14.639 --> 00:37:17.419
try to cope with this tension and none of

666
00:37:17.429 --> 00:37:19.500
the systems are perfect. They have their own internal

667
00:37:19.510 --> 00:37:22.979
tensions. And so we update them to fix those

668
00:37:22.989 --> 00:37:25.030
internal tensions or to alleviate those. And we just

669
00:37:25.040 --> 00:37:29.669
sort of keep doing that. Um But the original,

670
00:37:29.679 --> 00:37:32.340
the idea here and for many of these views

671
00:37:32.350 --> 00:37:37.840
is there's a specific function of ethics. And so

672
00:37:37.850 --> 00:37:41.540
the sort of way of thinking about it, the

673
00:37:41.550 --> 00:37:45.300
um metaphor is sort of like ethics as a

674
00:37:45.310 --> 00:37:48.919
technology, right, where technology is, you know, like a

675
00:37:48.929 --> 00:37:52.179
can opener, can opener. It's, there's a specific problem

676
00:37:52.189 --> 00:37:54.040
that you need to solve and the, and the

677
00:37:54.050 --> 00:37:57.729
technology is designed to solve it. And so do

678
00:37:57.739 --> 00:38:01.550
you evaluate whether the instance of the technology is

679
00:38:01.560 --> 00:38:03.530
good or bad based on how well it serves

680
00:38:03.540 --> 00:38:06.209
that function? So does the can opener open cans?

681
00:38:06.219 --> 00:38:10.689
Well, right. Um I thought there was something really,

682
00:38:10.699 --> 00:38:15.100
really fascinating and probably correct about this. But as

683
00:38:15.110 --> 00:38:18.989
we alluded towards earlier, morality is so messy, right?

684
00:38:19.429 --> 00:38:23.209
Even like from, you know, purely evolutionary perspective, there's

685
00:38:23.219 --> 00:38:26.080
so much going on. So many of the moral

686
00:38:26.090 --> 00:38:29.909
intuitions that we have feelings and attitudes, like morally

687
00:38:29.919 --> 00:38:35.110
relevant, feelings, attitudes, dispositions, um emotions evolved at different

688
00:38:35.120 --> 00:38:38.080
times and for different reasons. And then once you

689
00:38:38.090 --> 00:38:41.909
add culture into the mix um personal upbringing, um

690
00:38:41.919 --> 00:38:45.629
even like philosophical theorizing, it's just a complete mess.

691
00:38:45.810 --> 00:38:50.250
And so I thought sort of thinking about is

692
00:38:50.389 --> 00:38:52.169
much as that. I thought that these views were

693
00:38:52.179 --> 00:38:56.479
getting something right? Thinking about morality is having kind

694
00:38:56.489 --> 00:39:00.000
of a single underlying function function or a small

695
00:39:00.010 --> 00:39:01.780
set of them wasn't the right way to go.

696
00:39:02.300 --> 00:39:06.080
And so I started thinking about um ethics I

697
00:39:06.090 --> 00:39:07.520
could at the time, I called it like ethics

698
00:39:07.530 --> 00:39:10.800
as design today, I would say like cosign. And

699
00:39:10.810 --> 00:39:13.330
so the idea is like a slightly different metaphor

700
00:39:13.340 --> 00:39:16.850
where when you're designing something, there isn't like one

701
00:39:16.860 --> 00:39:20.479
consideration that you're trying to maximize you have multiple

702
00:39:20.489 --> 00:39:23.639
considerations that you're trying to weigh. Um There isn't

703
00:39:23.649 --> 00:39:26.350
necessarily one best way of doing it. There's multiple

704
00:39:26.360 --> 00:39:30.000
different ways that you can balance these multiple considerations.

705
00:39:32.209 --> 00:39:34.899
But then where does this perspective a bit come

706
00:39:34.949 --> 00:39:39.090
in? This is one of the things that I,

707
00:39:39.100 --> 00:39:42.290
so I should say there's been really, really, really

708
00:39:42.300 --> 00:39:46.729
cool work on perspective um by philosophers like Elizabeth

709
00:39:46.739 --> 00:39:51.139
Camp and um Kate Elgin um in the past,

710
00:39:51.149 --> 00:39:57.840
you know, 1015 years. And so by perspective, what's

711
00:39:57.850 --> 00:39:59.560
really interesting to me about that? What I mean

712
00:39:59.570 --> 00:40:04.040
by that is like perspectives, filter information. And we

713
00:40:04.050 --> 00:40:06.280
think in terms of perspective that's we don't have

714
00:40:06.290 --> 00:40:09.270
access to all of the information out there, right?

715
00:40:09.429 --> 00:40:11.129
We don't have access to all the information in

716
00:40:11.139 --> 00:40:13.709
our mind when we have a perspective, a perspective,

717
00:40:13.719 --> 00:40:17.750
sort of highlights some information it hides or includes

718
00:40:17.760 --> 00:40:21.080
some information. And it's an essential feature of how

719
00:40:21.090 --> 00:40:24.969
we think we think in terms of perspectives. So

720
00:40:24.979 --> 00:40:27.770
I've been interested in this for a long time.

721
00:40:28.310 --> 00:40:31.790
Um I also got interested in thinking about like

722
00:40:31.800 --> 00:40:35.219
what does this mean for ethical theorizing, right? And

723
00:40:35.419 --> 00:40:38.620
so, and this might like might be a segue

724
00:40:38.639 --> 00:40:40.540
to some of the other stuff to talk about.

725
00:40:40.550 --> 00:40:43.550
But I was really struck by and, like, recently

726
00:40:43.560 --> 00:40:45.419
gave a talk and I started, but, like, have

727
00:40:45.429 --> 00:40:49.540
you ever noticed that what's obvious varies from academic

728
00:40:49.550 --> 00:40:54.820
department to department? Um. Right. And so I was,

729
00:40:55.139 --> 00:40:58.949
you know, my master's program, I was, like, doing

730
00:40:59.010 --> 00:41:02.479
lots of utilitarian stuff. My intuitions were really utilitarian.

731
00:41:02.689 --> 00:41:06.169
My phd program, I was suddenly surrounded by virtue

732
00:41:06.179 --> 00:41:09.439
ethicists and it was just as intuitive to them

733
00:41:09.729 --> 00:41:13.419
that virtue, ethics made sense and was approached. Now

734
00:41:13.429 --> 00:41:16.979
I'm surrounded by contents and it's, you know, the

735
00:41:16.989 --> 00:41:18.709
ways that people talk like it just as it's

736
00:41:18.719 --> 00:41:20.679
obvious for some people to talk about morality in

737
00:41:20.689 --> 00:41:23.330
terms of principles, it was just as obvious to

738
00:41:23.340 --> 00:41:27.780
talk about morality in terms of virtues and that,

739
00:41:28.449 --> 00:41:32.229
you know, really struck like, why are people so

740
00:41:32.239 --> 00:41:35.330
confident in their intuitions? Well, partly the reason is

741
00:41:35.399 --> 00:41:38.070
when you occupy a perspective for a long time

742
00:41:38.080 --> 00:41:43.010
perspective, makes perspectives make thing out things obvious, right?

743
00:41:43.129 --> 00:41:47.179
So I think that has lots of implications that

744
00:41:47.189 --> 00:41:49.350
one of the implications that will make sense at

745
00:41:49.360 --> 00:41:53.469
this point in our discussion is even the complexity

746
00:41:53.479 --> 00:41:56.860
of morality that we've been like mentioned several times.

747
00:41:57.820 --> 00:42:03.600
My thought was that there couldn't be a like

748
00:42:03.969 --> 00:42:07.959
we wouldn't, we shouldn't want a single correct moral

749
00:42:07.969 --> 00:42:14.270
theory because the idea is if we occupy perspectives,

750
00:42:15.699 --> 00:42:18.909
a perspective is like going to highlight some things

751
00:42:18.919 --> 00:42:21.080
that's very useful, but it's also going to hide

752
00:42:21.090 --> 00:42:24.979
some things and there's more information than any psychological

753
00:42:24.989 --> 00:42:28.520
perspective that we could actually occupy or moral information

754
00:42:28.889 --> 00:42:31.830
that we would want our moral theories um in

755
00:42:31.840 --> 00:42:34.060
general to be able to capture that any single

756
00:42:34.070 --> 00:42:37.080
one perspective could. And so the thought was that

757
00:42:37.090 --> 00:42:41.179
actually we should be pluralist when it comes to

758
00:42:41.189 --> 00:42:45.699
doing moral theory, because no single approach is going

759
00:42:45.709 --> 00:42:47.919
to capture everything that we needed to capture.

760
00:42:49.340 --> 00:42:52.699
So by being pluralist, in the specific case of

761
00:42:52.709 --> 00:42:56.780
ethical theory, you're talking about bringing together, for example,

762
00:42:56.790 --> 00:43:00.469
consequentialism, virtue, virtue, ethics, the ontology.

763
00:43:01.300 --> 00:43:04.739
Yeah, exactly. Things like that, that um it might

764
00:43:04.750 --> 00:43:07.590
be really, really useful to, you know, be a

765
00:43:07.600 --> 00:43:09.689
content about certain things, but it should not be

766
00:43:09.699 --> 00:43:12.679
the only perspective because again, it highlights certain things

767
00:43:12.689 --> 00:43:17.510
really well and that's incredibly useful. Um But, and

768
00:43:17.520 --> 00:43:20.030
yeah, and this is something that, you know, people

769
00:43:20.040 --> 00:43:22.300
who have, you know, like, well, I'm a utilitarian,

770
00:43:22.310 --> 00:43:24.159
I'm afraid like the people who think that is

771
00:43:24.169 --> 00:43:26.800
the best, best theory. Um YOU know, they'll often

772
00:43:26.810 --> 00:43:29.129
sort of say, well, I can capture all of

773
00:43:29.139 --> 00:43:32.719
those intuitions that you say my theory doesn't and

774
00:43:33.179 --> 00:43:34.780
you know, it's bugged me for a while that

775
00:43:34.790 --> 00:43:37.030
people like we will make this move and but

776
00:43:37.040 --> 00:43:39.040
then they never actually care about. So like, you

777
00:43:39.050 --> 00:43:41.379
know, de onto contents will say, you know, we

778
00:43:41.389 --> 00:43:44.959
care about like virtue, ethics, like content, the metaphysics

779
00:43:44.969 --> 00:43:47.899
of moral spent the entire second half of the

780
00:43:47.909 --> 00:43:50.870
book talking about virtue and then they like never

781
00:43:50.879 --> 00:43:56.310
actually talk about it, right. Um And the idea

782
00:43:56.320 --> 00:43:59.580
is like, actually, you know, it's fine in so

783
00:43:59.600 --> 00:44:04.229
far as we take multiple approaches. Um AS long

784
00:44:04.239 --> 00:44:06.870
as we don't, you know, think that one approach

785
00:44:06.879 --> 00:44:09.689
has to be like the single correct approach because

786
00:44:09.820 --> 00:44:13.350
again, I think that given the real, yeah, the

787
00:44:13.610 --> 00:44:16.639
genuine descriptive fact, the non normative fact that we

788
00:44:16.649 --> 00:44:18.939
think in terms of perspectives and that we're, you

789
00:44:18.949 --> 00:44:24.000
know, not like omniscient or like epistemic omnipotent. Um

790
00:44:24.010 --> 00:44:27.330
We can't have a single correct ethical theory, an

791
00:44:27.340 --> 00:44:29.729
ethical theory that will capture everything we needed to

792
00:44:29.739 --> 00:44:33.250
capture. So we need um multiple ones

793
00:44:33.959 --> 00:44:37.159
would one of the ways that these more pluralistic

794
00:44:37.169 --> 00:44:41.879
perspective would work. Uh Is I, I mean, I'm

795
00:44:41.889 --> 00:44:45.679
asking you this because actually, uh actually recently, once

796
00:44:45.689 --> 00:44:47.919
it came to my mind, this idea, I mean,

797
00:44:47.989 --> 00:44:52.110
would it be, for example that for uh individual

798
00:44:52.120 --> 00:44:56.959
ethical problems, you would apply a specific perspective without

799
00:44:56.969 --> 00:45:00.462
having to like the same perspective to every single

800
00:45:00.472 --> 00:45:03.452
issue? Because it came to my mind actually, recently

801
00:45:03.462 --> 00:45:07.212
that uh I mean, just by noticing how people

802
00:45:07.222 --> 00:45:11.232
many times shift perspectives when dealing with different ethical

803
00:45:11.242 --> 00:45:13.583
problems. Oh The uh uh now you're being a

804
00:45:13.593 --> 00:45:19.393
consequentialist there, you the onto whatever uh is that

805
00:45:19.413 --> 00:45:24.256
something, right? Uh I mean, that's one of the

806
00:45:24.266 --> 00:45:26.285
ways how we would manifest.

807
00:45:26.855 --> 00:45:29.726
Yeah. So, and I think this is a really,

808
00:45:29.736 --> 00:45:31.085
it's a great question and I think it's a

809
00:45:31.095 --> 00:45:34.065
really complex question. So I think, you know, for

810
00:45:34.075 --> 00:45:36.795
me, one of the, the lessons from this approach

811
00:45:36.805 --> 00:45:39.666
is kind of like a personal intellectual humility, right?

812
00:45:39.676 --> 00:45:42.916
So when I'm making decisions, like I should be

813
00:45:42.926 --> 00:45:46.186
listening to others because I should know that my

814
00:45:46.196 --> 00:45:49.615
perspective is limited. Um OTHER people might be tracking

815
00:45:49.625 --> 00:45:53.570
things that I'm not tracking. So like uh an

816
00:45:53.580 --> 00:45:55.850
example of this, um I was like, sort of,

817
00:45:55.860 --> 00:45:58.590
we were like doing a workshop in my department

818
00:45:58.659 --> 00:46:01.080
and um there was sort of like a, a

819
00:46:01.090 --> 00:46:04.290
question, the question was like, you know, you know,

820
00:46:04.300 --> 00:46:07.489
you're a faculty member and one of your graduate

821
00:46:07.500 --> 00:46:09.560
students that you're advising comes up to you on

822
00:46:09.570 --> 00:46:12.560
campus and they, like, seem very flustered and they

823
00:46:12.570 --> 00:46:14.899
ask like, um can you meet with me this

824
00:46:14.909 --> 00:46:17.379
afternoon? And these sort of the idea is like,

825
00:46:17.389 --> 00:46:19.620
and yeah, you sort of had already blocked off

826
00:46:19.629 --> 00:46:22.419
the time to do something that, you know, like

827
00:46:22.429 --> 00:46:24.489
you could set it aside, but you kind of

828
00:46:24.500 --> 00:46:27.699
don't want to, what should you do? And I

829
00:46:27.709 --> 00:46:30.389
sort of said, well, like for me personally, like,

830
00:46:30.399 --> 00:46:33.000
use some social intelligence, like, how flustered do they

831
00:46:33.010 --> 00:46:35.739
look? Does it look like a genuine emergency or

832
00:46:35.750 --> 00:46:37.959
not? And one of my colleagues, like a great

833
00:46:37.969 --> 00:46:42.370
philosopher, philosopher, I'm good friend. She had like entirely

834
00:46:42.379 --> 00:46:46.429
different approach and she was like, well, would it

835
00:46:46.439 --> 00:46:49.159
be re like, is it a reasonable ask for

836
00:46:49.169 --> 00:46:51.310
everyone to do that, like to use their social

837
00:46:51.320 --> 00:46:54.669
intelligence, what would be reasonable for like any faculty

838
00:46:54.679 --> 00:46:56.889
member to do in this situation? And to me,

839
00:46:56.899 --> 00:46:58.010
and I was like, oh, this is kind of

840
00:46:58.020 --> 00:47:01.090
like a con and virtue ethics sort of thing.

841
00:47:01.860 --> 00:47:04.909
Um And it was like, philosophically interesting because we

842
00:47:04.919 --> 00:47:07.300
were asking sort of two different, like ethical questions.

843
00:47:07.310 --> 00:47:09.620
So like, one, I was asking like, what would

844
00:47:09.629 --> 00:47:12.080
be the best thing for me to do, given

845
00:47:12.090 --> 00:47:16.100
my personality and my abilities? Um AND she was

846
00:47:16.110 --> 00:47:21.179
asking then like, what would be a reasonable expectation

847
00:47:21.469 --> 00:47:24.560
to have of others like a at a workplace

848
00:47:24.570 --> 00:47:28.439
like ours? And, you know, at first I was

849
00:47:28.449 --> 00:47:30.800
like, well, I my perspective is correct, but I

850
00:47:30.810 --> 00:47:33.179
thought like, no, that's also an important aspect of

851
00:47:33.189 --> 00:47:36.639
this question, right? So there's this sort of like

852
00:47:36.649 --> 00:47:39.699
intellectual humility point that by having and listening to

853
00:47:39.709 --> 00:47:42.169
other perspective, like there's more moral information that are

854
00:47:42.389 --> 00:47:47.000
tracking in the other side to this though is

855
00:47:47.629 --> 00:47:49.000
at the end of the day, we do need

856
00:47:49.010 --> 00:47:50.729
to make a decision and then if we're just

857
00:47:50.739 --> 00:47:53.919
sort of like, get stuck listening to other perspectives

858
00:47:54.270 --> 00:47:58.270
that doesn't help us very much. And I think

859
00:47:58.280 --> 00:48:03.570
it's incredibly important um for us like a to

860
00:48:04.350 --> 00:48:07.979
develop our own personal moral approaches, to think about

861
00:48:07.989 --> 00:48:10.669
like, what commitments do we want to develop in

862
00:48:10.679 --> 00:48:13.729
our lives to have our own moral projects. So

863
00:48:14.610 --> 00:48:17.629
while we should listen to others, I personally, I

864
00:48:17.639 --> 00:48:20.610
mean, and a person maybe could their, their life,

865
00:48:20.620 --> 00:48:23.340
moral project might be like relativist, maybe Zhuang's of

866
00:48:23.350 --> 00:48:26.060
the ancient Chinese philosopher was like this. But for

867
00:48:26.070 --> 00:48:28.360
most of us, I think it's important to sort

868
00:48:28.370 --> 00:48:31.989
of develop our own appro like, what do we

869
00:48:32.000 --> 00:48:34.760
value, what are our deepest commitments as a way

870
00:48:34.770 --> 00:48:36.399
of, sort of making sense of the world and

871
00:48:36.409 --> 00:48:39.439
finding meaning in our moral lives? So, it's a

872
00:48:39.449 --> 00:48:43.479
tension that I don't think it's, like, easily or

873
00:48:43.489 --> 00:48:45.320
at all resolvable, but I think it's a felt

874
00:48:45.330 --> 00:48:46.530
tension of human life.

875
00:48:46.909 --> 00:48:50.189
Mhm. Yeah. And actually related to what I just

876
00:48:50.199 --> 00:48:54.469
comment a minute ago, uh, I've noticed many times.

877
00:48:54.479 --> 00:48:57.129
I, I actually don't know if there's any scientific

878
00:48:57.139 --> 00:48:59.040
work done on this or not. I mean, if

879
00:48:59.050 --> 00:49:02.469
people in terms of their preferred ethical theory, uh,

880
00:49:02.479 --> 00:49:06.280
tend to be consistent or not. But since, since

881
00:49:06.290 --> 00:49:10.350
I, I mean, I work in science communication, basically

882
00:49:10.360 --> 00:49:13.699
I move in science communication circles and I don't

883
00:49:13.709 --> 00:49:17.584
know exactly why, but apparently there's many science communicators

884
00:49:17.594 --> 00:49:21.635
who are fond of utilitarianism. But, yeah. But, but

885
00:49:21.945 --> 00:49:25.495
just to finish. But then I, I asked them

886
00:49:25.504 --> 00:49:28.264
just to tease them a little bit. Oh, so,

887
00:49:28.465 --> 00:49:32.245
so if you're a pure utilitarian, why do you

888
00:49:32.254 --> 00:49:33.814
believe in human rights?

889
00:49:34.514 --> 00:49:39.040
Yeah. Exactly. Yeah. And I think, um, and this

890
00:49:39.050 --> 00:49:43.659
is like, what my, I found utilitarianism so intuitive

891
00:49:43.669 --> 00:49:46.790
until these sorts of questions came up and I

892
00:49:46.800 --> 00:49:50.820
still find it very, like, very intuitive, um, in

893
00:49:50.830 --> 00:49:53.550
so many areas. But and I think that's probably

894
00:49:53.560 --> 00:49:55.969
part of the story here is, you know, like,

895
00:49:56.000 --> 00:49:59.080
oh, this gives us a nice answer and a

896
00:49:59.090 --> 00:50:01.520
very compelling answer for a lot of kinds of

897
00:50:01.530 --> 00:50:04.260
cases, right? It seems to be tracking exactly what

898
00:50:04.270 --> 00:50:07.250
we should be tracking in many cases. But then

899
00:50:07.260 --> 00:50:10.280
when you start asking these other questions, um a

900
00:50:10.290 --> 00:50:11.679
lot of people's intuition in the show.

901
00:50:13.320 --> 00:50:16.020
And so how does all of this link to

902
00:50:16.030 --> 00:50:19.699
your interest in Confucian ethics? And, and by the

903
00:50:19.709 --> 00:50:23.929
way for the audience, mainly, what is Confucian ethics,

904
00:50:23.939 --> 00:50:25.370
what characterizes it?

905
00:50:26.330 --> 00:50:29.979
Yeah. So this is something that I didn't expect

906
00:50:29.989 --> 00:50:32.439
to be interested in before I started graduate school

907
00:50:32.449 --> 00:50:34.649
in philosophy. I lived in South Korea for three

908
00:50:34.659 --> 00:50:38.090
years. And when I was there, especially among young

909
00:50:38.100 --> 00:50:40.760
people there, Confucianism was kind of a, well, that

910
00:50:40.770 --> 00:50:43.110
kind of negative word. It was a negative word

911
00:50:43.120 --> 00:50:45.860
where they meant just sort of this old traditionalist

912
00:50:45.959 --> 00:50:48.870
way of thinking where you have to be respectful

913
00:50:48.879 --> 00:50:51.550
to your parents and old people no matter how

914
00:50:51.560 --> 00:50:54.860
ridiculous or mean they are. Um And you have

915
00:50:54.870 --> 00:50:56.360
to be. And so it was like a really

916
00:50:56.370 --> 00:51:01.659
negative sort of um idea about it. Then when

917
00:51:01.669 --> 00:51:04.199
I started my phd program, I was sort of

918
00:51:04.209 --> 00:51:06.600
assigned to be a graduate, teaching assistant to an

919
00:51:06.610 --> 00:51:09.520
introduction to age and philosophy class. And I started

920
00:51:09.530 --> 00:51:11.239
to learn about, you know, what did the original

921
00:51:11.250 --> 00:51:14.320
text actually say? And I was really blown away

922
00:51:14.330 --> 00:51:16.760
by by that. Um And that's in, you know,

923
00:51:16.770 --> 00:51:19.459
large parts of like, wonderful teacher, Amy Oberon. But

924
00:51:20.280 --> 00:51:25.020
what's going on here? So this is um some

925
00:51:25.030 --> 00:51:28.639
of the oldest, like robust ethical theory and debates

926
00:51:28.649 --> 00:51:33.889
that we have. Yeah, around 3000 years ago in

927
00:51:33.899 --> 00:51:37.459
like West today China, there was a highly successful,

928
00:51:37.469 --> 00:51:43.669
like, highly academic, highly bureaucratic um government, like then

929
00:51:44.310 --> 00:51:48.129
a dynasty called the Zhou dynasty. And, you know,

930
00:51:48.139 --> 00:51:51.669
they just sort of have their problems, but for

931
00:51:51.679 --> 00:51:54.169
the most part, they have like a period of

932
00:51:54.179 --> 00:51:58.929
relative peace, relative prosperity. It's unlike sort of like

933
00:51:58.939 --> 00:52:01.500
ancient Greek philosophy which develops in like small city

934
00:52:01.510 --> 00:52:04.469
states. It's over a huge swath of land. So

935
00:52:04.479 --> 00:52:08.370
there's a huge bureaucracy, there's a large educational system.

936
00:52:09.010 --> 00:52:11.810
Um So like public, like small, like public schools

937
00:52:11.820 --> 00:52:15.429
and villages, towns and cities. Um AND that training

938
00:52:15.439 --> 00:52:17.820
system sort of feeds into like government officials and

939
00:52:17.830 --> 00:52:21.459
you need a lot of government officials. Um The

940
00:52:21.469 --> 00:52:23.909
in part because you need lots of food for

941
00:52:23.919 --> 00:52:26.159
this, you know, like large area and you have

942
00:52:26.169 --> 00:52:28.550
these floodplains and you, so you need lots of

943
00:52:28.560 --> 00:52:32.030
big government projects to deal with all of the

944
00:52:32.040 --> 00:52:36.459
flooding. Like so you have this, they start to

945
00:52:36.469 --> 00:52:42.500
sort of lose power, um start to decline. And

946
00:52:42.510 --> 00:52:48.020
so around 500 BC, they've sort of lost all

947
00:52:48.030 --> 00:52:51.379
of their power. But people have been like, kept

948
00:52:51.389 --> 00:52:55.030
records of these, you know, golden ages and you

949
00:52:55.040 --> 00:52:58.580
still sort of have this big bureaucracy, um including

950
00:52:58.590 --> 00:53:01.429
like one group of people that are scholar officials.

951
00:53:01.439 --> 00:53:04.229
And these scholar officials have lots of different ideas

952
00:53:04.239 --> 00:53:07.370
about how to run government, um how to live

953
00:53:07.379 --> 00:53:12.790
our lives. One of the most influential factions of

954
00:53:12.800 --> 00:53:16.280
these government officials, these scholar officials were what we

955
00:53:16.290 --> 00:53:18.790
call English, the Confucians. Um IN Chinese, they were

956
00:53:18.800 --> 00:53:21.010
called the rule. And they sort of seem to

957
00:53:21.020 --> 00:53:25.260
be originally kind of like the priest class. So

958
00:53:25.270 --> 00:53:28.949
they were in charge of religious rituals and ceremonies,

959
00:53:29.260 --> 00:53:33.800
um kind of like courtly ceremonies, things like that.

960
00:53:33.810 --> 00:53:35.209
They were the ones who were experts and in

961
00:53:35.219 --> 00:53:37.379
charge of that, they were also as part of

962
00:53:37.389 --> 00:53:40.469
their training. They also had to be experts in

963
00:53:40.479 --> 00:53:43.090
history, kind of like the history of public policy

964
00:53:43.100 --> 00:53:45.320
and things like that and also like the history

965
00:53:45.330 --> 00:53:54.449
of literature. So like ancient poems um around, yeah,

966
00:53:54.479 --> 00:53:59.070
right around 500 BC, like around that era. Um

967
00:53:59.860 --> 00:54:05.600
WE have Confucius uh Konsa and Confucius is not

968
00:54:05.610 --> 00:54:09.639
the first like member of this group, but he

969
00:54:09.649 --> 00:54:13.120
seems to be able to articulate the underlying ethos

970
00:54:13.129 --> 00:54:16.189
of this group in an incredi in a way

971
00:54:16.199 --> 00:54:22.669
that's sort of incredibly compelling and influential. And so

972
00:54:22.679 --> 00:54:25.510
he says that he like, he doesn't invent anything,

973
00:54:25.520 --> 00:54:27.909
he just sort of transmit. So the idea is

974
00:54:27.919 --> 00:54:31.600
sort of that in the past, there was a

975
00:54:31.610 --> 00:54:35.179
golden age, we figured it out, we figured out

976
00:54:35.189 --> 00:54:38.909
how to have a good, stable, prosperous society where

977
00:54:38.919 --> 00:54:43.739
people live flourishing lives. We've lost track of that.

978
00:54:43.750 --> 00:54:45.550
But what did they do? What was so special

979
00:54:45.560 --> 00:54:48.840
about that? And so by Confucius's life, things were

980
00:54:48.850 --> 00:54:52.510
like, going terribly civil war was on the horizon

981
00:54:52.520 --> 00:54:55.189
very clearly. And he wants to, like, stop this,

982
00:54:55.199 --> 00:54:57.350
like, go back to what was working in the

983
00:54:57.360 --> 00:54:59.350
golden age. And so if you, like, say, like,

984
00:54:59.360 --> 00:55:01.840
well, how do we stop this crisis? How do

985
00:55:01.850 --> 00:55:03.090
we get back to the golden age? You might

986
00:55:03.100 --> 00:55:05.399
say, well, what were the, the laws that they

987
00:55:05.409 --> 00:55:08.270
used? What were the policies that they used? What

988
00:55:08.280 --> 00:55:10.979
did they do? That was so special. Confucius is

989
00:55:10.989 --> 00:55:15.159
kind of shocking answer was what was special about

990
00:55:15.169 --> 00:55:18.080
what they did was were their rituals sort of

991
00:55:18.090 --> 00:55:23.320
like their system of manners and etiquette. And that,

992
00:55:23.330 --> 00:55:25.090
like, it should be a very, like the first

993
00:55:25.100 --> 00:55:27.510
time I heard that's a like etiquette, like, come

994
00:55:27.520 --> 00:55:30.250
on. Right. That doesn't matter all that much. Like,

995
00:55:30.260 --> 00:55:32.580
it's nice, I guess to be polite, right? But

996
00:55:32.590 --> 00:55:34.179
it doesn't matter all that much and think about

997
00:55:34.189 --> 00:55:36.620
like in western philosophy, some of our heroes, Socratic

998
00:55:36.629 --> 00:55:40.189
Gadfly, like according to the, I think the stereotypes

999
00:55:40.199 --> 00:55:42.909
are incorrect, but according to that stereotype, you know,

1000
00:55:42.919 --> 00:55:47.239
he's far from polite. Um AND maybe like socially

1001
00:55:47.250 --> 00:55:49.889
oblivious. I think that's an incorrect understanding of Socrates,

1002
00:55:49.899 --> 00:55:53.879
but that's like a pretty common view. So sort

1003
00:55:53.889 --> 00:55:58.530
of the way that Confucius explains this puts serves,

1004
00:55:58.540 --> 00:56:03.889
puts this all together is something like this. It

1005
00:56:03.899 --> 00:56:06.379
all starts and ends with good government and by

1006
00:56:06.389 --> 00:56:10.300
good government, I, he means something like non corrupt

1007
00:56:10.629 --> 00:56:13.330
and well informed. So they sort of like, make

1008
00:56:13.340 --> 00:56:18.159
good decisions. Um So they have good information and

1009
00:56:18.169 --> 00:56:20.239
you can think about like, they needed to take

1010
00:56:20.250 --> 00:56:22.219
care of all this flooding and things like that

1011
00:56:22.350 --> 00:56:24.280
and you had to like, keep an empire going

1012
00:56:24.290 --> 00:56:27.050
over, you know, huge lots of land. Um, SO

1013
00:56:27.060 --> 00:56:30.290
if you don't have that, you know, good luck.

1014
00:56:30.389 --> 00:56:32.209
Right. Which to me seems, you know, like sensible,

1015
00:56:32.219 --> 00:56:33.989
if you have, are living in an area with

1016
00:56:34.000 --> 00:56:37.250
a highly corrupt or um, a highly, a government

1017
00:56:37.260 --> 00:56:40.050
that makes absolutely terrible choices. Like, yeah, good luck

1018
00:56:40.060 --> 00:56:42.810
you might be able to get by, um, if

1019
00:56:42.820 --> 00:56:45.030
you're like, really clever or really lucky. But again,

1020
00:56:45.040 --> 00:56:46.810
good luck. But then the question is, how do

1021
00:56:46.820 --> 00:56:51.149
you get good government? Well, he says we start

1022
00:56:51.159 --> 00:56:55.280
with family. You need a good upbringing, right? You

1023
00:56:55.290 --> 00:56:57.840
need families to raise their Children. Well, how do

1024
00:56:57.850 --> 00:56:59.280
you do that? Well, you need a good education

1025
00:56:59.290 --> 00:57:04.050
system. Well, how do you do that? Um, WELL,

1026
00:57:04.060 --> 00:57:05.870
the education system has to be tied back into

1027
00:57:05.879 --> 00:57:07.949
the government. The education system has to train us

1028
00:57:07.959 --> 00:57:12.280
to be like, good smart, non corrupt government officials.

1029
00:57:12.290 --> 00:57:14.280
And so like, we've got this cycle. How do

1030
00:57:14.290 --> 00:57:17.770
you do all of this? And he says what

1031
00:57:17.780 --> 00:57:20.219
the successful dynasty of the past, what they did

1032
00:57:20.229 --> 00:57:24.239
ingeniously was they invented a system of social etiquette

1033
00:57:24.250 --> 00:57:28.659
of social norms, rituals that at every step of

1034
00:57:28.669 --> 00:57:34.290
the way helped to do these things. Um And

1035
00:57:34.959 --> 00:57:38.820
so just, and the idea is like, he has

1036
00:57:38.830 --> 00:57:42.139
stories about like every literal ritual and how it

1037
00:57:42.149 --> 00:57:45.050
helps us um like, be a better person in

1038
00:57:45.060 --> 00:57:48.959
like little ways um or make better decisions. So

1039
00:57:48.969 --> 00:57:51.010
you give like one example that I found like,

1040
00:57:51.020 --> 00:57:54.330
really interesting um when you're a government official and

1041
00:57:54.340 --> 00:57:56.669
you have like a tablet that has like a

1042
00:57:56.679 --> 00:58:00.010
new law or policy, the ritual is you act

1043
00:58:00.020 --> 00:58:02.810
as if that tablet is very, very heavy, much

1044
00:58:02.820 --> 00:58:06.199
heavier than it actually is. So why do you

1045
00:58:06.209 --> 00:58:07.699
do that? And this is to me where it

1046
00:58:07.709 --> 00:58:10.169
gets really ingenious because for the Confucians, they thought

1047
00:58:10.889 --> 00:58:14.659
the ancient SAGES developed this based on a really,

1048
00:58:14.669 --> 00:58:19.100
really good understanding of human psychology. So they thought

1049
00:58:19.429 --> 00:58:24.159
in human psychology, heaviness is associated with seriousness. So

1050
00:58:24.169 --> 00:58:26.889
in English, we say like gravitas, right? Gravity gravitas.

1051
00:58:26.919 --> 00:58:31.239
So if you like actors of this policy is

1052
00:58:31.250 --> 00:58:33.919
very heavy, it reminds you this is serious, this

1053
00:58:33.929 --> 00:58:39.219
policy will affect people, right? Um And so the

1054
00:58:39.229 --> 00:58:41.169
idea is they have all of these rituals and

1055
00:58:41.179 --> 00:58:44.219
all arenas of life that sort of provide this

1056
00:58:44.229 --> 00:58:46.760
like script, this sort of directly helps us to

1057
00:58:46.770 --> 00:58:50.350
do something that's really good, but also trains us

1058
00:58:50.360 --> 00:58:54.790
over time to associate different things together. And so

1059
00:58:54.800 --> 00:58:58.320
it ends up being this really, really robust, psychologically

1060
00:58:58.330 --> 00:59:02.469
rich picture of moral development. And I found that

1061
00:59:02.479 --> 00:59:04.429
absolutely fascinating.

1062
00:59:05.520 --> 00:59:10.060
So, but what is then exactly the connection between

1063
00:59:10.070 --> 00:59:14.149
that those aspects of Confucian ethics and what we

1064
00:59:14.159 --> 00:59:17.239
talked about just a minute ago in terms of

1065
00:59:17.250 --> 00:59:19.520
your perspective on ethics.

1066
00:59:19.969 --> 00:59:25.510
Yeah, absolutely. So the, like the philosophy that was

1067
00:59:25.520 --> 00:59:28.580
emerging at this time obviously is not the philosophy

1068
00:59:28.590 --> 00:59:31.620
that emerged in like 20th century English speaking philosophy

1069
00:59:31.629 --> 00:59:33.929
world, right? So there wasn't, you know, a clear,

1070
00:59:33.939 --> 00:59:36.620
I mean, there wasn't always a might not be

1071
00:59:36.629 --> 00:59:38.330
a clear distinction between what we would call meta

1072
00:59:38.340 --> 00:59:41.969
ethics and normative ethics today. But there definitely wasn't

1073
00:59:42.129 --> 00:59:45.250
back then. So like, were the Ancient Confucians more

1074
00:59:45.260 --> 00:59:47.810
realist or anti realist or something like, I don't

1075
00:59:47.820 --> 00:59:50.510
know, but there are little hit like things that

1076
00:59:50.520 --> 00:59:53.149
they say every once in a while that to

1077
00:59:53.159 --> 00:59:57.030
me suggests that it is something kind of like

1078
00:59:57.040 --> 01:00:00.739
this constructivist picture or at least very useful for

1079
01:00:00.750 --> 01:00:04.810
contemporary philosophers. So I mentioned earlier, there's um a

1080
01:00:04.879 --> 01:00:08.979
couple 100 years, several 100 years after Confucius. Um

1081
01:00:08.989 --> 01:00:13.439
AND Confucian philosopher named Shimza lived and he like

1082
01:00:13.449 --> 01:00:16.350
literally says the ancient stages invented ritual en, right?

1083
01:00:17.260 --> 01:00:22.469
And so, um his, and you know, there's a

1084
01:00:22.479 --> 01:00:24.669
lot of, he was also famous for saying that

1085
01:00:24.679 --> 01:00:28.250
human nature is inherently bad or evil or something

1086
01:00:28.260 --> 01:00:31.379
like that. Um He lost a lot of favor

1087
01:00:31.389 --> 01:00:33.070
for some of his beliefs and some of his

1088
01:00:33.080 --> 01:00:35.949
students and what they went on to do. Um

1089
01:00:36.989 --> 01:00:42.330
But to me, it seemed like either they did

1090
01:00:42.340 --> 01:00:46.379
believe or their approach was really useful for people

1091
01:00:46.389 --> 01:00:49.300
who are trying to develop a constructivist view, sort

1092
01:00:49.310 --> 01:00:52.469
of they thought that we have these different, like

1093
01:00:52.479 --> 01:00:58.199
numerous, like moral intuitions, emotions, dispositions. And we're trying

1094
01:00:58.209 --> 01:01:00.409
to make sense of them in the best way

1095
01:01:00.419 --> 01:01:05.030
that we can. And now what is the origin

1096
01:01:05.040 --> 01:01:07.270
of those? So some of the Confucian philosophers said,

1097
01:01:07.310 --> 01:01:10.379
you know, like there's heaven, right? This kind of

1098
01:01:10.389 --> 01:01:14.620
like godlike ish figure. Um AND heaven gives us

1099
01:01:14.629 --> 01:01:17.939
these dispositions or something like that. Um But at

1100
01:01:17.949 --> 01:01:21.060
the same time, there's this element in Confucian ethics

1101
01:01:21.070 --> 01:01:24.149
that is not always super interested in the supernatural

1102
01:01:24.159 --> 01:01:29.760
aspects of their views. So, um Confucius would refuse

1103
01:01:29.770 --> 01:01:31.739
to talk about. And so like students would ask

1104
01:01:31.750 --> 01:01:34.540
him like, how do I serve the ghost? And

1105
01:01:34.550 --> 01:01:36.090
he says, like, we'll figure out how to serve

1106
01:01:36.100 --> 01:01:39.360
the living. First. One of the Confucian's earliest critics,

1107
01:01:39.439 --> 01:01:42.290
the Moest say the Confucian don't actually believe in

1108
01:01:42.300 --> 01:01:45.800
any of the supernatural stuff. Shu I mentioned actually

1109
01:01:45.810 --> 01:01:48.239
comes out and says, there's no such thing as

1110
01:01:48.250 --> 01:01:52.370
like supernatural, like none of it actually exists. So

1111
01:01:52.379 --> 01:01:55.419
they, even though many of them are and might

1112
01:01:55.429 --> 01:01:57.149
be committed to sort of saying there is this

1113
01:01:57.159 --> 01:01:59.699
sort of supernatural origin, what they're really interested in

1114
01:01:59.709 --> 01:02:03.179
is given all of these different emotions and dispositions

1115
01:02:03.189 --> 01:02:05.149
that we have. How do we make sense of

1116
01:02:05.159 --> 01:02:08.909
it? How do we um construct a society and

1117
01:02:08.919 --> 01:02:12.379
construct our own lives and So, for people who

1118
01:02:12.389 --> 01:02:15.500
are um familiar with early Confucian ethics here might

1119
01:02:15.510 --> 01:02:17.439
be thinking of one of the most philosophers in

1120
01:02:17.449 --> 01:02:20.370
all, all the world, but especially East Asia menus

1121
01:02:20.439 --> 01:02:25.350
among who thought specifically they're like four like moral

1122
01:02:25.360 --> 01:02:30.260
sprouts is like probably a good translation, like these,

1123
01:02:30.270 --> 01:02:33.350
a set of inclinations that we just naturally have.

1124
01:02:34.229 --> 01:02:36.429
And, and to me, it's interesting because it's not

1125
01:02:36.439 --> 01:02:37.979
just one. So it's not like the ethics of

1126
01:02:37.989 --> 01:02:42.199
technology view, but like we have multiple um dispositions

1127
01:02:42.209 --> 01:02:43.620
and we have to like what's the best way

1128
01:02:43.629 --> 01:02:46.550
of making sense of fitting them all together. And

1129
01:02:46.560 --> 01:02:49.399
to me that's useful for constructiveness, whether the yearly

1130
01:02:49.409 --> 01:02:53.300
Confucians actually could best be described as something along

1131
01:02:53.310 --> 01:02:54.260
those lines or not.

1132
01:02:55.270 --> 01:02:58.530
You know, that is actually very interesting, at least

1133
01:02:58.540 --> 01:03:01.189
in two different ways. So one of them is

1134
01:03:01.199 --> 01:03:05.169
that uh at least in the western philosophical tradition,

1135
01:03:05.179 --> 01:03:07.770
I, I mean, perhaps there would be different readings

1136
01:03:07.780 --> 01:03:10.370
of these, but at least my reading is that

1137
01:03:10.959 --> 01:03:13.689
uh whether you're, you fall more on the camp

1138
01:03:13.699 --> 01:03:17.780
of the ontology or consequential, more virtue ethics or

1139
01:03:17.790 --> 01:03:21.580
contractual or something like that, there's a sort of

1140
01:03:21.590 --> 01:03:27.550
moral, moral realist assumption underlying all of those different

1141
01:03:27.560 --> 01:03:31.000
ethical systems, right? But in that specific case, if

1142
01:03:31.010 --> 01:03:35.189
you're reading of Confucian ethics, you, right, there's at

1143
01:03:35.199 --> 01:03:39.332
least some and elements there. And then, uh I,

1144
01:03:39.342 --> 01:03:41.731
I mean, the other thing that I wanted to

1145
01:03:41.741 --> 01:03:46.112
mention is that there's also some interesting anthropological work

1146
01:03:46.122 --> 01:03:48.951
on. Uh I can't remember exactly if this was

1147
01:03:48.961 --> 01:03:52.441
done on western societies or some traditional societies. But

1148
01:03:52.451 --> 01:03:56.731
at least there are societies out there where uh

1149
01:03:56.741 --> 01:04:00.791
uh uh a majority of people uh tend toward

1150
01:04:00.802 --> 01:04:04.803
the relativist perspective that is they think that the

1151
01:04:04.813 --> 01:04:09.224
norms that operate in their society are objective and

1152
01:04:09.233 --> 01:04:12.583
question it. But they are, but they also allow

1153
01:04:12.593 --> 01:04:16.394
for other societies to have different norms and they

1154
01:04:16.404 --> 01:04:21.023
think that whatever norms they have are as objective

1155
01:04:21.033 --> 01:04:25.543
for them as their own norms are for themselves.

1156
01:04:25.553 --> 01:04:27.706
So I I mean, just to say that uh

1157
01:04:28.095 --> 01:04:34.025
we shouldn't assume neither moral realism for human nature,

1158
01:04:34.035 --> 01:04:38.216
even for philosophy in general, nor universalism. That is

1159
01:04:38.226 --> 01:04:43.855
assuming that uh our own nor moral values, moral

1160
01:04:43.865 --> 01:04:47.115
preferences lie to everyone out there a

1161
01:04:47.125 --> 01:04:49.476
absolutely. And I think this is so important. This

1162
01:04:49.486 --> 01:04:51.426
is one of the things that kind of really

1163
01:04:51.436 --> 01:04:54.500
struck me as I was learning about classical Chinese

1164
01:04:54.510 --> 01:04:57.570
philosophy because there are often like many ass some

1165
01:04:57.580 --> 01:04:59.770
like shared assumptions or shared views. But there are

1166
01:04:59.780 --> 01:05:04.209
also like little but very important differences. So for,

1167
01:05:04.219 --> 01:05:08.159
you know, I mentioned earlier, the students who come

1168
01:05:08.169 --> 01:05:10.129
into like internet ethics, it's like human beings are

1169
01:05:10.139 --> 01:05:15.530
fundamentally selfish. Um THAT'S actually not a common view

1170
01:05:15.540 --> 01:05:18.370
in classical Chinese philosophy whatsoever. And in fact, the

1171
01:05:18.379 --> 01:05:21.659
like one person who's the most associated with that

1172
01:05:21.669 --> 01:05:25.850
view, that sort of gets treated as radically like

1173
01:05:25.860 --> 01:05:27.969
a very radical view, the thought seems to be

1174
01:05:27.979 --> 01:05:33.439
that we're sort of inherently more interested in those

1175
01:05:33.449 --> 01:05:35.820
we care about. So like our family members, our

1176
01:05:35.830 --> 01:05:38.510
friends, our neighbors or stuff like that. And we're

1177
01:05:38.520 --> 01:05:42.320
also like selfish sometimes, but we are like other

1178
01:05:42.330 --> 01:05:45.300
interested in and with the I and so it

1179
01:05:45.310 --> 01:05:47.510
often gets translated like partial. So we're like, inherently

1180
01:05:47.520 --> 01:05:50.429
partial, but that doesn't mean we're like, necessarily inherently

1181
01:05:50.439 --> 01:05:52.850
selfish and like, you know, that actually seems to

1182
01:05:52.860 --> 01:05:57.820
be more like descriptively accurate than the sort of

1183
01:05:57.830 --> 01:06:01.899
like radical egoism that we often um find in

1184
01:06:01.909 --> 01:06:05.050
American discussions especially.

1185
01:06:06.060 --> 01:06:09.000
So let's move on to another topic. Now, you

1186
01:06:09.010 --> 01:06:13.830
also do work on epistemic injustice. So tell us

1187
01:06:13.840 --> 01:06:15.780
first of all what it is about.

1188
01:06:16.790 --> 01:06:21.959
Yeah. So this has been one of the hottest

1189
01:06:21.969 --> 01:06:24.350
topics in philosophy in the past 15 or 20

1190
01:06:24.360 --> 01:06:27.610
years. Um And I think incredibly important. So first,

1191
01:06:27.620 --> 01:06:30.739
like by epistemic, we mean things having to do

1192
01:06:30.750 --> 01:06:34.540
with knowledge, having to do with learning, understanding, share

1193
01:06:34.550 --> 01:06:37.750
it like good information, getting good information, sharing good

1194
01:06:37.760 --> 01:06:44.500
information, all of that sort of stuff. And there

1195
01:06:44.510 --> 01:06:48.020
are forms of injustice or forms of oppression or

1196
01:06:48.030 --> 01:06:54.550
marginalization that clearly have epistemic um impacts. So for

1197
01:06:54.560 --> 01:06:58.540
example, if schools, like if you live in a

1198
01:06:58.550 --> 01:07:01.719
racist society and schools and neighborhoods that are more

1199
01:07:01.729 --> 01:07:04.860
popular, densely populated by people in marginalized groups, get

1200
01:07:04.870 --> 01:07:08.350
less funding students who goes to those schools like

1201
01:07:08.360 --> 01:07:11.580
get less good edu like not as good of

1202
01:07:11.590 --> 01:07:14.989
an education, they have fewer opportunities, fewer, like less

1203
01:07:15.000 --> 01:07:19.360
support. They are epistemic worse off than they could

1204
01:07:19.370 --> 01:07:23.010
have been because of oppression, right? Um And so

1205
01:07:23.020 --> 01:07:26.120
that's something that I think philosophers for a long

1206
01:07:26.129 --> 01:07:31.550
time they like very aware of. Um In 2007,

1207
01:07:31.560 --> 01:07:34.340
a philosopher named Miranda Fricker um published a book

1208
01:07:34.350 --> 01:07:39.469
called epistemic Injustice. And this became a like huge

1209
01:07:39.479 --> 01:07:41.830
deal, a huge topic. And in that book, she

1210
01:07:41.840 --> 01:07:46.389
argued that over and above that there's a distinctive

1211
01:07:46.399 --> 01:07:50.010
epistemic form of injustice or oppression. There's a way

1212
01:07:50.020 --> 01:07:55.709
that injustice suppression, marginalization affects people as knower as

1213
01:07:55.719 --> 01:08:01.350
learners as sharers of knowledge and information. And that's

1214
01:08:01.360 --> 01:08:04.030
really, really, really interesting and part of what makes

1215
01:08:04.040 --> 01:08:07.189
it interesting might come through with some examples. So

1216
01:08:07.199 --> 01:08:10.439
I'll just like mention quickly three examples. Two of

1217
01:08:10.449 --> 01:08:13.360
these Miranda Fricker discussed in her work. Um These

1218
01:08:13.370 --> 01:08:20.209
are testimonial, injustice and hermeneutic injustice. Testimonial injustice is

1219
01:08:20.220 --> 01:08:24.529
kind of roughly when um like the main case

1220
01:08:24.540 --> 01:08:27.609
of testimonial justice is when a person trusts you

1221
01:08:27.620 --> 01:08:31.379
less takes you as less credible because of your

1222
01:08:31.390 --> 01:08:35.560
social identity, right? Um So your gender or your

1223
01:08:35.569 --> 01:08:39.100
race or something like that. Um And so we

1224
01:08:39.109 --> 01:08:41.709
know that this is a feature of how oppression

1225
01:08:41.720 --> 01:08:45.839
and marginalization works. So her examples of this, you

1226
01:08:45.850 --> 01:08:49.990
know, the, in the original book, um the most

1227
01:08:50.000 --> 01:08:52.950
famous example is like To Kill a Mockingbird, right?

1228
01:08:52.959 --> 01:08:57.129
So people dismissing someone's testimony like literal testimony in

1229
01:08:57.140 --> 01:09:00.060
a court case, uh just because of their racial

1230
01:09:00.069 --> 01:09:05.020
stereotypes Um And so this is something that like

1231
01:09:05.049 --> 01:09:08.188
when like from my experience teaching about this, the

1232
01:09:08.198 --> 01:09:11.309
students here about like many students, like I've experienced

1233
01:09:11.318 --> 01:09:13.648
that now I have a word for this, right?

1234
01:09:13.658 --> 01:09:17.948
So um this happens in various different ways um

1235
01:09:17.957 --> 01:09:20.519
And like very like complex ways that is like

1236
01:09:20.568 --> 01:09:23.188
been explored in a lot of detail since the

1237
01:09:23.198 --> 01:09:26.798
book came out. So that's testimonial injustice, hermeneutic injustice,

1238
01:09:26.809 --> 01:09:29.028
which yeah, a big word. But for something that's

1239
01:09:29.038 --> 01:09:31.438
a really, really fascinating idea, like having to do

1240
01:09:31.448 --> 01:09:33.658
with how we interpret the world. And the, the

1241
01:09:33.667 --> 01:09:38.009
rough idea is something like this. There are things

1242
01:09:38.020 --> 01:09:40.450
that happened to us that we don't have words

1243
01:09:40.459 --> 01:09:46.950
for yet. Sometimes that's just bad luck, right? So

1244
01:09:46.959 --> 01:09:49.939
imagine before we had a word for depression or

1245
01:09:49.950 --> 01:09:52.470
had a word for anxiety or a word for

1246
01:09:52.479 --> 01:09:55.299
um PTSD or something like that, we don't have

1247
01:09:55.310 --> 01:09:57.549
a word for it yet. Now, like we figured

1248
01:09:57.560 --> 01:09:58.870
out this is a thing, this is the thing

1249
01:09:58.879 --> 01:10:00.709
that happens. We've got a term for it that's

1250
01:10:00.720 --> 01:10:05.819
incredibly useful and helpful in various ways. It helps

1251
01:10:05.830 --> 01:10:08.490
us as we like, interpret the world, interpret our

1252
01:10:08.500 --> 01:10:14.419
experiences better. Now, there's questions like, why don't we

1253
01:10:14.430 --> 01:10:17.759
have a word for something? Well, a lot of

1254
01:10:17.770 --> 01:10:18.959
times it's just sort of like, I don't know,

1255
01:10:18.970 --> 01:10:20.850
bad luck. There's not really much of a story

1256
01:10:20.859 --> 01:10:23.160
there, but there are some cases where we don't

1257
01:10:23.169 --> 01:10:25.470
really have a good word. A good term. For

1258
01:10:25.479 --> 01:10:30.229
something because of oppression and marginalization. So Fricker's example

1259
01:10:30.240 --> 01:10:33.680
of this was the term sexual harassment. And so

1260
01:10:33.689 --> 01:10:39.319
the story behind that where um and we actually

1261
01:10:39.330 --> 01:10:41.540
like have a lot of detail about the development

1262
01:10:41.549 --> 01:10:46.680
of this term, right? And sort of the relevant

1263
01:10:47.040 --> 01:10:51.419
information here is that like sort of when it

1264
01:10:51.430 --> 01:10:53.899
was being described in the group that like, really

1265
01:10:53.910 --> 01:10:57.029
got this, like the terminology up and going, what

1266
01:10:57.040 --> 01:11:00.229
happened was everyone in the group was like all

1267
01:11:00.240 --> 01:11:01.740
the women in the group that were discussing this,

1268
01:11:01.750 --> 01:11:04.330
like, yeah, that happened to me, but I never

1269
01:11:04.339 --> 01:11:07.310
wanted to talk about like, I blamed myself or

1270
01:11:07.319 --> 01:11:09.759
I was too embarrassed or ashamed to talk about

1271
01:11:09.770 --> 01:11:13.250
this. And so because of social norms, like oppressive

1272
01:11:13.259 --> 01:11:16.729
prejudiced social norms, people didn't talk about it. And

1273
01:11:16.740 --> 01:11:18.450
so it took a lot longer to develop this

1274
01:11:18.459 --> 01:11:22.930
really important, powerful idea than it otherwise might have.

1275
01:11:22.939 --> 01:11:25.709
Another example of this is like postpartum depression, right?

1276
01:11:26.029 --> 01:11:30.470
Um If our sort of sexist expectations and norms

1277
01:11:30.479 --> 01:11:32.339
of that, you know, like as a woman, you

1278
01:11:32.350 --> 01:11:38.339
should be absolutely like in like, so in joyful

1279
01:11:38.350 --> 01:11:41.009
to have a baby and like this is, you

1280
01:11:41.020 --> 01:11:43.709
know, like the big goal of your life to

1281
01:11:43.720 --> 01:11:45.970
be a good mother. Like, why are you down

1282
01:11:46.879 --> 01:11:48.450
and you know, now we know like all these

1283
01:11:48.459 --> 01:11:52.540
really, really as problematic and informed views make it

1284
01:11:52.709 --> 01:11:55.540
difficult for pe for some people to notice and

1285
01:11:55.549 --> 01:11:59.109
track and explain what like something that happens and,

1286
01:12:00.020 --> 01:12:03.339
yeah, sorry for interrupting you. But actually, just to

1287
01:12:03.350 --> 01:12:06.359
add to that because postpartum depression of course, is

1288
01:12:06.370 --> 01:12:10.975
associated with sexism. But there's also, uh a broader,

1289
01:12:10.984 --> 01:12:14.214
uh, social theme, let's say that applies to both

1290
01:12:14.225 --> 01:12:16.765
men and women that I was just reading a

1291
01:12:16.774 --> 01:12:19.154
few minutes before we started the interview, uh, uh,

1292
01:12:19.165 --> 01:12:23.814
uh, an article on psychology today about people who

1293
01:12:23.825 --> 01:12:29.354
regret becoming parents and, and that's something that we

1294
01:12:29.365 --> 01:12:31.345
almost never hear about.

1295
01:12:32.345 --> 01:12:34.544
Yeah. Good. Yeah. There's lots of interesting stuff about

1296
01:12:34.555 --> 01:12:39.810
this where um yeah, it's the my understanding is

1297
01:12:39.819 --> 01:12:41.870
the research suggests that yeah, becoming a parent does

1298
01:12:41.879 --> 01:12:44.569
not make you happier. Now, you might find like

1299
01:12:44.580 --> 01:12:46.700
more meaning in it or like, you know, many

1300
01:12:46.709 --> 01:12:48.430
people say they don't regret doing it. But yeah,

1301
01:12:48.439 --> 01:12:50.459
it's stressful like I don't, yeah, you don't sleep

1302
01:12:50.470 --> 01:12:53.819
for years. Um And yes, and this is like,

1303
01:12:54.479 --> 01:12:58.970
um I think something that like the first time

1304
01:12:58.979 --> 01:13:01.180
I encountered it, like the very first feminist philosophy

1305
01:13:01.189 --> 01:13:03.049
class that I took in graduate school, it took

1306
01:13:03.060 --> 01:13:04.450
a lot of bit to click. But now it's

1307
01:13:04.459 --> 01:13:08.060
so obvious you have such strong narratives about parenting

1308
01:13:08.319 --> 01:13:11.319
and, you know, especially motherhood, but parenting in general

1309
01:13:11.700 --> 01:13:15.540
that just are incredibly dangerous, like in some cases,

1310
01:13:15.549 --> 01:13:17.839
just not true. But I think even if true,

1311
01:13:17.910 --> 01:13:21.009
incredibly dangerous because, you know, if the narrative is

1312
01:13:21.020 --> 01:13:22.669
like, this is one of the most important and

1313
01:13:22.680 --> 01:13:25.750
meaningful and joyful experiences of life, if you aren't

1314
01:13:25.759 --> 01:13:28.229
feeling joy in a particular moment. If you're feeling

1315
01:13:28.240 --> 01:13:31.640
stressed, what do you like? You can't deal with

1316
01:13:31.649 --> 01:13:34.939
that, you can't understand it, right? Um And that's

1317
01:13:34.950 --> 01:13:37.080
one of the reasons that this idea of hermeneutic

1318
01:13:37.100 --> 01:13:40.120
injustice is so interesting and powerful and the idea

1319
01:13:40.129 --> 01:13:44.879
like it's an incredibly stressful disorienting thing to experience

1320
01:13:44.890 --> 01:13:47.259
something but not have words for it. And it's

1321
01:13:47.270 --> 01:13:49.919
incredibly empowering thing to get a word for it.

1322
01:13:50.709 --> 01:13:52.370
OK. The last thing that I wanted to mention

1323
01:13:52.379 --> 01:13:56.720
about epistemic injustice in general. And this is um

1324
01:13:56.910 --> 01:14:00.250
work by like great philosophers like Gil Pole House

1325
01:14:00.259 --> 01:14:03.700
Junior and Christy Dodson who have said this idea

1326
01:14:03.709 --> 01:14:06.629
of hermeneutic injustice, we don't have a word for

1327
01:14:06.640 --> 01:14:10.299
some phenomenon that is affecting people. That's something that

1328
01:14:10.310 --> 01:14:13.270
you know, happens, but something else that happens, maybe

1329
01:14:13.279 --> 01:14:17.810
happens more often is actually the oppressed group has

1330
01:14:17.819 --> 01:14:21.129
words and has terminology and has been talking about

1331
01:14:21.140 --> 01:14:24.129
this stuff for a long time and people from

1332
01:14:24.140 --> 01:14:27.560
the in like oppressive groups from the dominant groups

1333
01:14:28.560 --> 01:14:32.700
can't and won't listen to it. And so you

1334
01:14:32.709 --> 01:14:36.459
can't contribute to the conversations because people just won't

1335
01:14:36.470 --> 01:14:39.970
believe you. And so for example, if you sort

1336
01:14:39.979 --> 01:14:43.279
of um have an encounter at your workplace that

1337
01:14:43.290 --> 01:14:46.040
strikes you as like kind of racist and you

1338
01:14:46.049 --> 01:14:49.490
bring it up and someone um replies and sort

1339
01:14:49.500 --> 01:14:51.129
of dismisses you and say, well, you're just playing

1340
01:14:51.140 --> 01:14:55.729
the race card. Mhm You can't really contribute to

1341
01:14:55.740 --> 01:14:58.109
the conversations even though you've been talking about it.

1342
01:14:58.330 --> 01:15:01.189
And so um many philosophers have pointed out that

1343
01:15:01.200 --> 01:15:04.660
this con like this, the phenomenon that people are

1344
01:15:04.669 --> 01:15:07.319
talking about when they're talking about epistemic injustice that

1345
01:15:07.330 --> 01:15:11.450
Fricker was talking about. Actually, people have been talking

1346
01:15:11.459 --> 01:15:14.450
about for a very, very long time. Um Feminist

1347
01:15:14.459 --> 01:15:17.720
philosophers have been talking about black feminist philosophers have

1348
01:15:17.729 --> 01:15:19.359
been talking about her for a long time. I

1349
01:15:19.370 --> 01:15:24.439
think um there are really, really like, I think

1350
01:15:24.450 --> 01:15:26.910
one of the best works on epistemic injustice is

1351
01:15:26.919 --> 01:15:29.669
Frederick douglas' speech. What to the slave is the

1352
01:15:29.680 --> 01:15:32.930
fourth of July um back in the 18 fifties,

1353
01:15:32.939 --> 01:15:37.600
right? So, um and so the last thing I'll

1354
01:15:37.609 --> 01:15:39.810
say this is, yeah, just in the last 20

1355
01:15:39.819 --> 01:15:42.459
or so years, 1520 years, there's just been an

1356
01:15:42.470 --> 01:15:46.000
explosion of interest in these topics and people thinking

1357
01:15:46.009 --> 01:15:48.899
through all of the different ways that this affects

1358
01:15:49.160 --> 01:15:49.299
us.

1359
01:15:49.310 --> 01:15:51.549
Mhm Yeah. So in the interest of time, I

1360
01:15:51.560 --> 01:15:53.740
have one more question that, that, that I would

1361
01:15:53.750 --> 01:15:55.910
like to ask you and perhaps later on in

1362
01:15:55.919 --> 01:15:58.339
the year, if you agree, we can have another

1363
01:15:58.350 --> 01:16:01.799
conversation to, to cover some of the other topics

1364
01:16:01.810 --> 01:16:03.850
that I have prepared. But I mean, just before

1365
01:16:03.859 --> 01:16:06.819
I get into the last question, uh I also

1366
01:16:06.830 --> 01:16:09.779
wanted to comment that uh I mean, this uh

1367
01:16:09.790 --> 01:16:14.020
epistemic injustice uh thing. I mean, it's very interesting

1368
01:16:14.029 --> 01:16:17.850
because while we were talking, for example, about postpartum

1369
01:16:17.859 --> 01:16:21.399
depression and stuff like that, I was thinking that

1370
01:16:21.660 --> 01:16:25.299
there are many ways in which people sort of

1371
01:16:25.310 --> 01:16:29.979
naturalize their own societal norms. And sometimes they, they

1372
01:16:29.990 --> 01:16:33.319
try to find in a sort of more natural

1373
01:16:33.330 --> 01:16:36.571
way or in nature, it's self a justification for

1374
01:16:36.582 --> 01:16:41.441
their norms. Like, for example, until a few decades

1375
01:16:41.452 --> 01:16:45.582
ago, no, probably virtually, no one used the term

1376
01:16:45.611 --> 01:16:50.562
almost sexual or gay. And then people started to

1377
01:16:50.571 --> 01:16:54.452
understand that there was probably nothing wrong with them.

1378
01:16:54.461 --> 01:16:57.481
They were just attracted to the same to people

1379
01:16:57.492 --> 01:17:00.301
of the same sex. And so now we have

1380
01:17:00.312 --> 01:17:05.054
this understanding of almost sexuality and what sexual is

1381
01:17:05.063 --> 01:17:08.983
and the same thing applies to probably a sexual

1382
01:17:08.994 --> 01:17:12.193
people because we just assume that everyone, uh, uh,

1383
01:17:12.253 --> 01:17:16.994
is, uh, sexually attracted to one or the other

1384
01:17:17.003 --> 01:17:19.644
sex or to anyone out there. And now we

1385
01:17:19.653 --> 01:17:22.713
know that there is a minority of people who

1386
01:17:22.724 --> 01:17:26.244
don't even, they are not interested in sex, they

1387
01:17:26.253 --> 01:17:28.124
don't want to have sex all of that. And,

1388
01:17:28.363 --> 01:17:31.724
and I guess that this is also important because,

1389
01:17:31.733 --> 01:17:35.496
uh, I, I brought up the example of people

1390
01:17:35.505 --> 01:17:40.655
who regret having kids and many, and perhaps it

1391
01:17:40.666 --> 01:17:42.996
would be interesting to know what would happen in

1392
01:17:43.005 --> 01:17:47.076
society if this sort of societal coercion to make

1393
01:17:47.085 --> 01:17:50.436
people have kids. Because I mean, sometimes people might

1394
01:17:50.445 --> 01:17:53.065
say, oh, you're complaining about that you're just a

1395
01:17:53.076 --> 01:17:57.686
cry baby. But yeah, it's sexual coercion because women

1396
01:17:58.076 --> 01:18:02.428
more than men, I guess, uh, might get ostracized

1397
01:18:02.437 --> 01:18:04.487
and all of that people might be down on

1398
01:18:04.498 --> 01:18:07.777
you if you're child was particularly past a certain

1399
01:18:07.788 --> 01:18:10.498
age and it would be very interesting to know

1400
01:18:10.507 --> 01:18:15.288
how many people would feel more at staying childless

1401
01:18:15.297 --> 01:18:18.067
throughout their entire life if you were not exposed

1402
01:18:18.078 --> 01:18:19.627
to this sort of social.

1403
01:18:20.458 --> 01:18:23.958
Right. Absolutely. I think these are so the first

1404
01:18:23.967 --> 01:18:26.587
thing is these are fantastic examples. I think of

1405
01:18:26.598 --> 01:18:31.140
something like hermeneutic justice, right? Improvements. Um, SO, yeah,

1406
01:18:31.149 --> 01:18:34.459
I think this is, and I think a really,

1407
01:18:34.470 --> 01:18:39.080
really unfortunate toxic element of like American society, many

1408
01:18:39.089 --> 01:18:41.740
other societies is sort of being really dismissive and

1409
01:18:41.750 --> 01:18:44.540
demeaning. Like people make fun of people of young

1410
01:18:44.549 --> 01:18:46.509
people. And now for saying like, oh, they have

1411
01:18:46.520 --> 01:18:48.850
all of these different genders and they're like constantly

1412
01:18:48.859 --> 01:18:51.470
changing their gender or things like that. It's so

1413
01:18:51.479 --> 01:18:55.089
so incredibly frustrating because to me, like, what's going

1414
01:18:55.100 --> 01:18:59.790
on is people are finally like better understanding their

1415
01:18:59.799 --> 01:19:02.569
own experience and coming up with words for their

1416
01:19:02.580 --> 01:19:05.029
own experience and finding that there are plenty of

1417
01:19:05.040 --> 01:19:08.529
others out there who like encounter these were like

1418
01:19:08.540 --> 01:19:11.859
asexuals and like, oh my gosh, that describes me

1419
01:19:11.870 --> 01:19:15.229
perfectly or bis, like there's so many different um

1420
01:19:15.959 --> 01:19:17.709
better words and this is like, you know, it's

1421
01:19:17.720 --> 01:19:21.209
actually been happening for much longer time, but now

1422
01:19:21.220 --> 01:19:23.549
it's um people are more and more aware of

1423
01:19:23.560 --> 01:19:25.600
it because of the internet and social media. And

1424
01:19:25.609 --> 01:19:27.459
so to me, this is like a great triumph

1425
01:19:27.470 --> 01:19:32.129
of hermeneutic injustice of hermeneutic justice. Sorry. Um Like

1426
01:19:32.140 --> 01:19:34.850
having these terms of people personally like encounter that.

1427
01:19:34.859 --> 01:19:40.049
Oh my gosh, that's me. Um And yes, so

1428
01:19:40.060 --> 01:19:42.870
I think that's a fantastic, a fantastic example of

1429
01:19:42.879 --> 01:19:45.509
this sort of phenomenon and why it matters so

1430
01:19:45.520 --> 01:19:46.520
much to people.

1431
01:19:46.819 --> 01:19:49.729
So my last question then, and I guess that

1432
01:19:49.740 --> 01:19:52.770
probably to a certain extent, this would also connect

1433
01:19:52.779 --> 01:19:56.029
to epistemic injustice. So you've done work on the

1434
01:19:56.040 --> 01:20:00.589
continuation for women in academic philosophy. So what could

1435
01:20:00.600 --> 01:20:01.740
you tell us about that?

1436
01:20:02.250 --> 01:20:05.459
Yeah, this is something that I've been. This is

1437
01:20:05.470 --> 01:20:08.379
actually like one of my first semesters in my

1438
01:20:08.390 --> 01:20:11.600
phd program was when I started doing this. And

1439
01:20:11.609 --> 01:20:14.839
so, yeah, the background here is there are many

1440
01:20:14.850 --> 01:20:18.680
academic fields that have diversity problems. That's no surprise

1441
01:20:18.689 --> 01:20:21.790
for academics on many fields that are um the

1442
01:20:21.799 --> 01:20:24.720
vast majority of faculty are white or white men

1443
01:20:24.729 --> 01:20:30.959
philosophy, historically had been um one of the least

1444
01:20:30.970 --> 01:20:34.899
diverse academic fields. I'm at the level of faculty

1445
01:20:34.910 --> 01:20:42.080
and especially tenured faculty. Um And people had been

1446
01:20:42.089 --> 01:20:44.000
concerned about this for a while and talked about

1447
01:20:44.009 --> 01:20:46.560
it for a while. Um But they hadn't necessarily

1448
01:20:46.569 --> 01:20:48.979
done a ton of actual empirical research for it,

1449
01:20:48.990 --> 01:20:51.540
which isn't that surprising given that we're philosophers, like

1450
01:20:51.549 --> 01:20:53.759
we sit in our armchair and try to figure

1451
01:20:53.770 --> 01:20:58.850
out um problems. And then, um you know, over

1452
01:20:58.859 --> 01:21:02.200
the past especially 10 years, people have started to

1453
01:21:02.750 --> 01:21:05.000
look at this empirically, try to get some data

1454
01:21:05.009 --> 01:21:07.819
about this. Um And like in general, people have

1455
01:21:07.830 --> 01:21:09.950
been in like the sciences have been doing this

1456
01:21:09.959 --> 01:21:12.279
for a while and doing lots of good research,

1457
01:21:12.290 --> 01:21:16.060
but people started to do this in philosophy. Um

1458
01:21:16.069 --> 01:21:18.680
One of the common stories is we say something

1459
01:21:18.689 --> 01:21:23.060
like a pipeline problem, right? So that fewer women

1460
01:21:23.069 --> 01:21:27.299
take philosophy classes than fewer women gr um major

1461
01:21:27.310 --> 01:21:30.200
in philosophy. And so fewer women study philosophy in

1462
01:21:30.209 --> 01:21:34.040
graduate school. And then like when you graduate school,

1463
01:21:34.049 --> 01:21:37.350
people don't finish graduate school for various reasons. So

1464
01:21:37.359 --> 01:21:40.899
they don't become that right? So um there's that

1465
01:21:40.910 --> 01:21:44.620
story, there was some research that and so this

1466
01:21:45.270 --> 01:21:47.919
the initial research that we published as a group

1467
01:21:47.930 --> 01:21:50.359
of scholars I like, was definitely not the only

1468
01:21:50.370 --> 01:21:54.990
person Heather dearest really led that team. Um And

1469
01:21:55.000 --> 01:21:58.100
we were interested in some research suggesting that in

1470
01:21:58.109 --> 01:22:03.470
philosophy, um actually, one of the biggest drop offs

1471
01:22:03.479 --> 01:22:07.089
occurred not in graduate school, which is what many

1472
01:22:07.100 --> 01:22:11.089
academic philosophers would assume. But right after introduction to

1473
01:22:11.100 --> 01:22:15.279
philosophy classes, and that was really interesting to us,

1474
01:22:15.290 --> 01:22:17.790
like, what is going on in intro of philosophy

1475
01:22:17.830 --> 01:22:22.250
that is making a philosophy very homogeneous. So what

1476
01:22:22.259 --> 01:22:26.859
we did was we sort of, we gave out

1477
01:22:26.870 --> 01:22:29.180
surveys at the beginning and end of every seme

1478
01:22:29.209 --> 01:22:32.069
uh of a semester in as many different intro

1479
01:22:32.080 --> 01:22:36.250
to philosophy type classes as we could and what

1480
01:22:36.259 --> 01:22:37.529
we found. And this is, you know, it was

1481
01:22:37.540 --> 01:22:41.029
just one study we had, you know, around, I

1482
01:22:41.080 --> 01:22:44.049
don't remember exactly 250. I want to say people,

1483
01:22:44.060 --> 01:22:47.529
which is, you know, 1000 would be better, but

1484
01:22:47.569 --> 01:22:51.209
it was not nothing. Um And what we found

1485
01:22:51.220 --> 01:22:55.580
two really interesting things. So, like we asked many

1486
01:22:55.589 --> 01:22:58.290
questions about their opinions and their feelings and their

1487
01:22:58.299 --> 01:23:03.069
experiences with the philosophy classes. Um AND some of

1488
01:23:03.080 --> 01:23:05.459
them, you know, maybe expected there to be a

1489
01:23:05.470 --> 01:23:09.919
big role in predicting how like would they take?

1490
01:23:09.930 --> 01:23:12.830
Were they interested in taking future philosophy classes? It

1491
01:23:12.839 --> 01:23:15.100
turned out that there were two questions we found

1492
01:23:15.109 --> 01:23:19.000
that had a shockingly big impact on whether they

1493
01:23:19.009 --> 01:23:21.600
reported that they wanted to take more philosophy classes.

1494
01:23:22.779 --> 01:23:26.600
And the one that has like been like, has

1495
01:23:26.609 --> 01:23:29.970
really, really been influential for me is this question,

1496
01:23:30.350 --> 01:23:34.169
I feel similar to the kinds of people who

1497
01:23:34.180 --> 01:23:41.009
become philosophers that was incredibly like from a social

1498
01:23:41.020 --> 01:23:45.299
science perspective, shockingly predictive of whether they said they

1499
01:23:45.310 --> 01:23:49.080
would take future philosophy classes at the beginning of

1500
01:23:49.089 --> 01:23:52.200
the semester. There was no difference between like self

1501
01:23:52.220 --> 01:23:54.600
identified men and women students at the end of

1502
01:23:54.609 --> 01:23:59.979
the semester. There was, and you know, it was

1503
01:23:59.990 --> 01:24:02.399
sort of like right on the border of like,

1504
01:24:02.479 --> 01:24:05.020
are we absolutely certain that this is like, you

1505
01:24:05.029 --> 01:24:08.620
know, um in fact, but so like, we need

1506
01:24:08.629 --> 01:24:14.350
more information, but it looks like that seems to

1507
01:24:14.359 --> 01:24:17.509
play a big role. The other question was I

1508
01:24:17.520 --> 01:24:21.220
enjoy thinking about philosophical questions and puzzles and now

1509
01:24:21.229 --> 01:24:22.950
I wish that we would have said, like, I

1510
01:24:22.959 --> 01:24:25.700
enjoy thinking about philosophical issues or something like that.

1511
01:24:25.709 --> 01:24:28.390
I wonder what the questions and puzzles, but the

1512
01:24:28.399 --> 01:24:30.350
way that we, you know, phrased that was doing

1513
01:24:30.359 --> 01:24:33.930
because we didn't expect that one to actually do

1514
01:24:33.939 --> 01:24:37.000
anything. Um, THAT one had a gender difference at

1515
01:24:37.009 --> 01:24:39.310
the beginning of the semester, the widened over the

1516
01:24:39.319 --> 01:24:42.180
course of the semester. But the thing that, yeah,

1517
01:24:42.470 --> 01:24:45.410
it really struck me as like, what kind of

1518
01:24:45.419 --> 01:24:48.470
signals are we sending in our classes about who

1519
01:24:48.479 --> 01:24:53.990
belongs in philosophy? And so I've come to think,

1520
01:24:54.000 --> 01:24:58.939
you know, um why be surprised if your syllabus

1521
01:24:58.950 --> 01:25:01.589
is all dead white men that your students are

1522
01:25:01.600 --> 01:25:05.709
all live white men, like live white men. There's

1523
01:25:05.720 --> 01:25:07.709
so many little signals that we send about who

1524
01:25:07.720 --> 01:25:11.790
does philosophy, who doesn't do it. Um And part

1525
01:25:11.799 --> 01:25:13.589
of that is like who's on the reading list,

1526
01:25:13.640 --> 01:25:18.180
but they're also like much subtler things. So my

1527
01:25:18.339 --> 01:25:23.379
go to example of this is, well, two of

1528
01:25:23.390 --> 01:25:26.430
them, one thing we come like frequently say philosophy

1529
01:25:26.439 --> 01:25:30.259
began in Ancient Greece. That's a very common thing

1530
01:25:30.270 --> 01:25:32.669
that people will say an introduction to philosophy classes.

1531
01:25:33.120 --> 01:25:38.089
So, I mean, philosophy probably began about five minutes

1532
01:25:38.100 --> 01:25:41.709
after the first verbal disagreement that human beings had,

1533
01:25:41.720 --> 01:25:44.240
right? Um Like, what do you mean that I'm

1534
01:25:44.250 --> 01:25:46.609
not a good hunter? I don't know, like, what

1535
01:25:46.620 --> 01:25:48.890
do you mean these berries aren't good enough?

1536
01:25:49.040 --> 01:25:52.069
Um I mean, then when it comes even to

1537
01:25:52.080 --> 01:25:55.379
let, let's, even if we talk just about people

1538
01:25:55.479 --> 01:26:00.020
who lived as philosophers, we, we have done some

1539
01:26:00.029 --> 01:26:03.229
intellectual work of that, of that kind. I mean,

1540
01:26:03.240 --> 01:26:08.209
even now we know that that Ancient Athenian and

1541
01:26:08.220 --> 01:26:12.350
Greek philosophers were probably influenced by the Persians, the

1542
01:26:12.359 --> 01:26:13.220
Indians. And

1543
01:26:13.229 --> 01:26:16.430
exactly, exactly. And so in the sense that the

1544
01:26:16.439 --> 01:26:20.330
Ancient Greeks thought that philosophy probably originated in Ancient

1545
01:26:20.339 --> 01:26:24.419
Egypt, um many of them like literally say that

1546
01:26:24.430 --> 01:26:27.450
and like relatedly and again, the little signals we

1547
01:26:27.459 --> 01:26:32.879
send in philosophy. If you specialize in the subfield,

1548
01:26:32.890 --> 01:26:36.319
ancient Greek and Roman philosophy, we use shorthand, we

1549
01:26:36.330 --> 01:26:40.569
say ancient. So when we say ancient in academic

1550
01:26:40.580 --> 01:26:43.209
philosophy, at least in the US and UK, that

1551
01:26:43.220 --> 01:26:46.370
means ancient Greek, it really means kind of Plato

1552
01:26:46.379 --> 01:26:49.000
and Aristotle. But it's like changing a little bit.

1553
01:26:49.009 --> 01:26:52.680
It really means like ancient Greek. But philosophy existed

1554
01:26:52.689 --> 01:26:55.509
in like that we have in Ancient India and

1555
01:26:55.520 --> 01:26:59.140
Ancient China, ancient Egypt. Um But like we know

1556
01:26:59.149 --> 01:27:01.930
it. So when we like send the subtle signals

1557
01:27:01.939 --> 01:27:05.250
that we send about who does philosophy and why

1558
01:27:05.459 --> 01:27:08.549
I think end up having like, yeah, the subtle

1559
01:27:08.560 --> 01:27:10.649
and then the not so subtle signal, like again,

1560
01:27:10.709 --> 01:27:13.250
is the syllabus, like only you know, dead white

1561
01:27:13.259 --> 01:27:17.490
guys or people who even though like the ancient

1562
01:27:17.500 --> 01:27:21.180
Greeks were not Western Europeans in any way, shape

1563
01:27:21.189 --> 01:27:23.979
or form, but they get kind of whitewashed in

1564
01:27:23.990 --> 01:27:26.200
some ways of talking and teaching about them.

1565
01:27:26.575 --> 01:27:31.564
Maybe, maybe in the university, the philosophy departments where

1566
01:27:31.575 --> 01:27:37.674
people do not teach Chinese Arabic, Indian African philosophy,

1567
01:27:37.685 --> 01:27:40.825
they should be called western philosophy.

1568
01:27:41.884 --> 01:27:45.104
I mean, you know, like color horse to horse.

1569
01:27:46.620 --> 01:27:49.970
Yeah. And so, uh, were you about to say

1570
01:27:49.979 --> 01:27:55.209
something else about this issue of women not continuing

1571
01:27:55.299 --> 01:27:56.810
in their philosophy?

1572
01:27:56.959 --> 01:27:59.759
And I think that there's so much research like

1573
01:27:59.770 --> 01:28:01.220
this still needs to be done about that. It's

1574
01:28:01.229 --> 01:28:04.040
a very complex problem. Um, AND this is like,

1575
01:28:04.049 --> 01:28:05.859
far from the only, like there are tons of

1576
01:28:05.870 --> 01:28:09.350
well known issues about, you know, sexism and academia

1577
01:28:09.359 --> 01:28:11.649
and academic philosophy. So, you know, it's a big

1578
01:28:11.660 --> 01:28:16.140
problem with multiple causes. But I think one of

1579
01:28:16.149 --> 01:28:19.990
the important issues that people should be aware of,

1580
01:28:20.000 --> 01:28:22.930
like in their own teaching and presentation of philosophy

1581
01:28:23.149 --> 01:28:25.830
is the signals they send about who belongs, who

1582
01:28:25.839 --> 01:28:29.430
does philosophy, the big signals, the direct signals and

1583
01:28:29.439 --> 01:28:32.109
the indirect signals that we signed about who does

1584
01:28:32.120 --> 01:28:35.189
philosophy because students pick up on that.

1585
01:28:36.500 --> 01:28:39.790
And, and so in your mind, what would be

1586
01:28:39.799 --> 01:28:44.319
some possible solutions for that if, if those are

1587
01:28:44.330 --> 01:28:47.479
the causes? I mean, would we need for example,

1588
01:28:47.529 --> 01:28:53.000
to talk more about women philosophers in history or

1589
01:28:53.009 --> 01:28:57.390
even contemporary women philosophers or to have more women

1590
01:28:57.399 --> 01:29:00.799
philosopher, role models in academia?

1591
01:29:01.649 --> 01:29:03.919
Yeah. And so this is like in that original

1592
01:29:03.930 --> 01:29:07.939
paper that um that came out um an analysis

1593
01:29:07.950 --> 01:29:10.169
of those years ago, one of the things we

1594
01:29:10.180 --> 01:29:13.649
suggested what we call them, like counter stereotypical exemplars,

1595
01:29:13.660 --> 01:29:16.779
right? So like in your class, emphasizing people who

1596
01:29:16.790 --> 01:29:20.169
don't fit that mold of, like the stereotypes of

1597
01:29:20.180 --> 01:29:23.060
a lot, you know, an old white lonely hermit

1598
01:29:23.069 --> 01:29:25.189
with a beard sort of thing. Um And there

1599
01:29:25.200 --> 01:29:28.859
are many, like, in fact, maybe more canal philosophers

1600
01:29:28.870 --> 01:29:32.450
than most don't fit that stereotype. Um I think

1601
01:29:32.459 --> 01:29:36.020
again, it's a multifaceted problem with multifaceted solutions. So

1602
01:29:36.240 --> 01:29:39.830
departments should be hiring people that can teach non

1603
01:29:39.850 --> 01:29:45.580
western philosophy um philosophers, you know, we're, everyone's overextended

1604
01:29:45.589 --> 01:29:47.620
and overworked and we don't have enough time to

1605
01:29:47.629 --> 01:29:50.479
learn about our own specialization. But I mean, in

1606
01:29:50.490 --> 01:29:54.080
my own department here at Harvard, so many people

1607
01:29:54.089 --> 01:29:57.729
have been like slowly including more and more non

1608
01:29:57.740 --> 01:30:01.970
western philosophy in their classes, especially like intro classes

1609
01:30:02.020 --> 01:30:05.810
um in ways that like they find very rewarding

1610
01:30:05.819 --> 01:30:12.620
and interesting. Um And or is often less difficult

1611
01:30:12.629 --> 01:30:16.220
than people expect to do um when they, you

1612
01:30:16.229 --> 01:30:19.779
know, put off doing it. And yeah, so I

1613
01:30:19.790 --> 01:30:22.459
think it's a multifaceted problem and it needs a

1614
01:30:22.470 --> 01:30:26.620
multifaceted solution, but a big part of it is

1615
01:30:27.310 --> 01:30:32.140
um just do like just doing and having people

1616
01:30:32.149 --> 01:30:35.689
that can talk and teach about philosophy sort of

1617
01:30:35.700 --> 01:30:38.879
outside and also sort of like problematic the can.

1618
01:30:38.890 --> 01:30:42.000
So like some of the um philosophers here who

1619
01:30:42.009 --> 01:30:45.430
specialize in early modern philosophy have done really, really

1620
01:30:45.439 --> 01:30:49.419
cool work. People who maybe they were very important

1621
01:30:49.430 --> 01:30:51.640
in their own, like Mary Shepherd, Mary Estelle and

1622
01:30:52.000 --> 01:30:55.939
um but didn't get included in the Canon for

1623
01:30:55.950 --> 01:30:59.189
various reasons. Um Francois Poulain de la Barre is

1624
01:30:59.200 --> 01:31:02.740
one who just blew my mind. Um And so

1625
01:31:03.089 --> 01:31:06.100
there's so many different options that people have for

1626
01:31:06.109 --> 01:31:09.500
teaching philosophy in a more welcoming way. And it's,

1627
01:31:09.509 --> 01:31:13.500
it's healthier. It presents philosophy as something that people

1628
01:31:13.509 --> 01:31:16.180
everywhere all over the world have been concerned about.

1629
01:31:16.189 --> 01:31:19.470
And students really respond to that and react to

1630
01:31:19.479 --> 01:31:22.520
that much better than this story. We used to

1631
01:31:22.529 --> 01:31:26.020
tell that philosophy began in Ancient Greece and it's

1632
01:31:26.029 --> 01:31:30.245
this one great conversation that like eight white people

1633
01:31:30.254 --> 01:31:32.435
had that we're going to learn about.

1634
01:31:33.024 --> 01:31:36.254
Yeah. No, I definitely think that people should listen

1635
01:31:36.265 --> 01:31:41.615
to Peter Adamson's History of Philosophy without any gaps.

1636
01:31:41.625 --> 01:31:42.595
It's fantastic.

1637
01:31:42.685 --> 01:31:44.354
Yeah, it, it's phenomenal. Right?

1638
01:31:45.000 --> 01:31:48.060
And, and so Doctor Robertson, I'm getting really mindful

1639
01:31:48.069 --> 01:31:50.790
of your time now. So probably it's better for

1640
01:31:50.799 --> 01:31:53.560
us to wrap up the interview here just before

1641
01:31:53.569 --> 01:31:55.580
we go. Would you like to tell people where

1642
01:31:55.589 --> 01:31:57.580
they can find you and your work on the

1643
01:31:57.589 --> 01:31:58.100
internet?

1644
01:31:58.799 --> 01:32:01.020
Yeah. So I have a website that I haven't

1645
01:32:01.029 --> 01:32:04.100
updated in a while that at robertson.net. And then

1646
01:32:04.109 --> 01:32:05.930
I tried to update that with some of my

1647
01:32:05.939 --> 01:32:10.029
current work and recent publications. Um But yeah, thank

1648
01:32:10.040 --> 01:32:12.370
you so much for having me. It was really,

1649
01:32:12.379 --> 01:32:15.379
really wonderful to talk about these um topics with

1650
01:32:15.390 --> 01:32:15.609
you.

1651
01:32:15.799 --> 01:32:18.149
No, thank you so much. It's been really fun

1652
01:32:18.160 --> 01:32:21.359
to talk to you. Hi guys. Thank you for

1653
01:32:21.370 --> 01:32:23.680
watching this interview until the end. If you liked

1654
01:32:23.689 --> 01:32:25.979
it, please share it. Leave a like and hit

1655
01:32:25.990 --> 01:32:28.390
the subscription button. The show is brought to you

1656
01:32:28.399 --> 01:32:31.370
by the N Lights learning and development. Then differently

1657
01:32:31.379 --> 01:32:34.600
check the website at N lights.com and also please

1658
01:32:34.609 --> 01:32:38.680
consider supporting the show on Patreon or paypal. I

1659
01:32:38.689 --> 01:32:40.660
would also like to give a huge thank you

1660
01:32:40.669 --> 01:32:44.700
to my main patrons and paypal supporters, Perera Larson,

1661
01:32:44.709 --> 01:32:47.589
Jerry Muller and Frederick Suno Bernard Seche O of

1662
01:32:47.600 --> 01:32:50.500
Alex Adam Kel Matthew Whitten B are no wt

1663
01:32:50.580 --> 01:32:54.319
Ho Erica LJ Connors Philip Forrest Connolly. Then the

1664
01:32:54.330 --> 01:32:57.330
Met Robert Wine in NAI Z Mark Nevs calling

1665
01:32:57.339 --> 01:33:01.399
Hol Brookfield governor Mikel Stormer Samuel Andre Francis for

1666
01:33:02.069 --> 01:33:05.390
Agns Ferus and H her me and Lain Jung

1667
01:33:05.549 --> 01:33:09.479
Y and the K Hes Mar Smith J Tom

1668
01:33:09.620 --> 01:33:15.029
Humble S David Sloan Wilson de Ro Ro Diego,

1669
01:33:15.470 --> 01:33:19.140
Jan Punter, Romani Charlotte bli Nico Barba, Adam Hunt,

1670
01:33:19.359 --> 01:33:22.529
Pavlo Stassi Nale medicine, Gary G Alman Sam of

1671
01:33:22.939 --> 01:33:27.430
Zed YPJ Barboza Julian Price Edward Hall. Eden Broner

1672
01:33:28.430 --> 01:33:32.930
Douglas Fry Franca, Beto Lati Cortez or Solis Scott

1673
01:33:33.140 --> 01:33:39.660
Zachary ftdw Daniel Friedman, William Buckner, Paul Giorgio, Luke

1674
01:33:39.669 --> 01:33:43.879
Loki, Georgio Theophano Chris Williams and Peter Wo David

1675
01:33:43.890 --> 01:33:47.990
Williams, the Ausa Anton Erickson Charles Murray, Alex Shaw,

1676
01:33:48.089 --> 01:33:54.200
Marie Martinez, Coralie Chevalier, Bangalore Dey junior, Old Ebon,

1677
01:33:54.680 --> 01:33:58.100
Starry Michael Bailey then Spur by Robert Grassy Zorn,

1678
01:33:58.319 --> 01:34:02.870
Jeff mcmahon, Jake Zul Barnabas Radis, Mark Kemple Thomas

1679
01:34:02.879 --> 01:34:07.350
Dvor Luke Neeson, Chris to Kimberley Johnson, Benjamin Gilbert

1680
01:34:07.359 --> 01:34:11.629
Jessica. No, Linda Brendan Nicholas Carlson, Ismael Bensley Man

1681
01:34:12.089 --> 01:34:17.319
George Katis Valentine Steinman, Perras, Kate Van Goler, Alexander

1682
01:34:17.990 --> 01:34:23.890
Abert Liam Dan Biar Masoud Ali Mohammadi Perpendicular Jer

1683
01:34:24.649 --> 01:34:28.140
Urla. Good enough, Gregory Hastings David Pins of Sean

1684
01:34:28.600 --> 01:34:32.790
Nelson, Mike Levin and Jos Net. A special thanks

1685
01:34:32.799 --> 01:34:35.169
to my producers is our web, Jim Frank Luca

1686
01:34:35.689 --> 01:34:39.069
Stina, Tom Vig and Bernard N Cortes Dixon, Bendik

1687
01:34:39.080 --> 01:34:42.770
Muller Thomas Trumble, Catherine and Patrick Tobin, John Carlman,

1688
01:34:43.140 --> 01:34:45.500
Negro, Nick Ortiz and Nick Golden. And to my

1689
01:34:45.509 --> 01:34:49.729
executive producers, Matthew Lavender, Sergi, Adrian Bogdan Knits and

1690
01:34:49.740 --> 01:34:51.129
Rosie. Thank you for all.

