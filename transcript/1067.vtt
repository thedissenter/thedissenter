WEBVTT

1
00:00:00.230 --> 00:00:02.819
Hello, everyone. Welcome to a new episode of the

2
00:00:02.819 --> 00:00:05.659
Dissenter. I'm your host, as always, Ricardo Lopez, and

3
00:00:05.659 --> 00:00:08.439
today I'm joined by for a 4th time by

4
00:00:08.439 --> 00:00:12.260
Doctor David Benatar. And today we're talking about his

5
00:00:12.260 --> 00:00:17.780
latest book, Very Practical Ethics Engaging Everyday Moral Questions.

6
00:00:17.899 --> 00:00:20.500
So, Doctor Benatar, welcome back on the show. It's

7
00:00:20.500 --> 00:00:21.840
always a pleasure to everyone.

8
00:00:22.540 --> 00:00:23.899
Thank you very much. It was a pleasure to

9
00:00:23.899 --> 00:00:24.379
be with you.

10
00:00:25.409 --> 00:00:28.000
So, I mean, it's very interesting because you go

11
00:00:28.000 --> 00:00:31.760
through a lot of different kinds of, uh, ethical

12
00:00:31.760 --> 00:00:35.180
questions that people might face in their everyday lives.

13
00:00:35.400 --> 00:00:38.759
You talk, for example, about sex, the environment, smoking,

14
00:00:38.919 --> 00:00:42.959
giving aid, consumption of animal products, the certain uses

15
00:00:42.959 --> 00:00:46.799
of language. But before we get into some of

16
00:00:46.799 --> 00:00:50.040
those examples, let me start by, uh, let me

17
00:00:50.040 --> 00:00:52.529
start by asking. Asking you some general questions. So

18
00:00:52.529 --> 00:00:56.490
first of all, what is practical ethics and how

19
00:00:56.490 --> 00:01:00.689
do common problems that people face differ from, for

20
00:01:00.689 --> 00:01:05.650
example, scenarios, explore in ethics that affect maybe only

21
00:01:05.650 --> 00:01:09.769
a select number of people or even thought experiments

22
00:01:09.769 --> 00:01:13.879
involving events that problem, that probably no one is

23
00:01:13.879 --> 00:01:15.650
ever going to be faced with.

24
00:01:17.480 --> 00:01:20.040
Sure, well, the area of practical ethics is much

25
00:01:20.040 --> 00:01:23.000
broader than I'm covering in this particular book. Practical

26
00:01:23.000 --> 00:01:25.879
ethics is concerned, as the name suggests, with practical

27
00:01:25.879 --> 00:01:29.260
ethical problems. There are areas of ethics that look

28
00:01:29.260 --> 00:01:32.860
into questions, for example, about what makes a right

29
00:01:32.860 --> 00:01:36.080
action right or a wrong action wrong, and they

30
00:01:36.080 --> 00:01:38.629
will sometimes refer to examples and as you know,

31
00:01:38.959 --> 00:01:41.800
sometimes those examples are going to be highly theorized

32
00:01:41.800 --> 00:01:45.019
and theoretical ones, hypothetical ones. But the area of

33
00:01:45.019 --> 00:01:48.459
practical practical ethics is really interested in what we

34
00:01:48.459 --> 00:01:53.730
ought practically to do. But it does include Questions

35
00:01:53.730 --> 00:01:55.410
that go beyond what I look at in this

36
00:01:55.410 --> 00:01:57.690
book because what I'm interested in or what I'm

37
00:01:57.690 --> 00:02:01.970
calling very practical ethical questions or to use a

38
00:02:01.970 --> 00:02:06.410
term that I've used elsewhere, quotidian ethical problems. These

39
00:02:06.410 --> 00:02:09.369
are problems that confront ordinary people in their everyday

40
00:02:09.369 --> 00:02:11.649
lives. So if you think about the question of

41
00:02:11.649 --> 00:02:15.690
abortion or capital punishment, these are practical ethical questions.

42
00:02:16.190 --> 00:02:19.350
But they're not quotidian problems. They're not problems that

43
00:02:19.350 --> 00:02:21.830
an individual is going to face in their everyday

44
00:02:21.830 --> 00:02:25.380
life. Hopefully, questions about abortion will be relatively rare

45
00:02:25.669 --> 00:02:28.470
in any individual's life and perhaps absent in many

46
00:02:28.470 --> 00:02:32.440
other people's lives. Questions about the moral justice viability

47
00:02:32.440 --> 00:02:35.490
of capital punishment. These are questions that the state

48
00:02:35.490 --> 00:02:37.839
really needs to engage. These are not questions that

49
00:02:37.839 --> 00:02:40.119
people are going to be confronting in their ordinary

50
00:02:40.119 --> 00:02:42.399
life. And simply they're going to be questions about

51
00:02:42.399 --> 00:02:46.279
professional ethics, ethical questions that arise within particular roles,

52
00:02:46.440 --> 00:02:49.720
public roles, and that falls beyond the scope of

53
00:02:49.720 --> 00:02:53.059
what I'm looking at here. I'm, I'm interested in

54
00:02:53.059 --> 00:02:55.100
these quotidian ethical questions in this book.

55
00:02:56.500 --> 00:02:59.580
And do you approach the ethical problems you deal

56
00:02:59.580 --> 00:03:01.750
with in the book from the point of view

57
00:03:01.750 --> 00:03:06.710
of any specific ethical theory, like, for example, consequentialism,

58
00:03:06.750 --> 00:03:09.029
the ontology or virtue ethics?

59
00:03:10.179 --> 00:03:13.339
No, in fact, I specifically try to bracket the

60
00:03:13.339 --> 00:03:17.940
disagreements there are between those different theoretical positions because

61
00:03:17.940 --> 00:03:19.899
my worry is that if I adopt a particular

62
00:03:19.899 --> 00:03:22.500
theoretical view, and then I spell out what the

63
00:03:22.500 --> 00:03:25.699
implications of that would be for the practical issues,

64
00:03:26.059 --> 00:03:28.220
that this will then not be of interest to

65
00:03:28.220 --> 00:03:31.199
people who don't accept the background theoretical view. So

66
00:03:31.199 --> 00:03:34.160
I try to engage the practical issues uh in

67
00:03:34.160 --> 00:03:37.080
their own terms. Sometimes it will make a difference

68
00:03:37.080 --> 00:03:39.089
what your background theory is, and then I might

69
00:03:39.089 --> 00:03:42.160
uh point to those differences. But in general, I'm

70
00:03:42.160 --> 00:03:44.720
not wanting to assume a particular ethical theory.

71
00:03:46.070 --> 00:03:50.369
And what do right and wrong mean in the

72
00:03:50.369 --> 00:03:52.610
context of practical ethics?

73
00:03:54.100 --> 00:03:55.669
Well, that's one of the questions that I look

74
00:03:55.669 --> 00:03:57.979
at in the introduction to the book, and I

75
00:03:57.979 --> 00:04:00.570
point out that those terms like right and wrong

76
00:04:00.570 --> 00:04:04.610
are actually multiply ambiguous. They can mean different things.

77
00:04:05.100 --> 00:04:08.449
So the word right for could mean, for example,

78
00:04:08.500 --> 00:04:11.130
something that is required, an action that is morally

79
00:04:11.130 --> 00:04:13.740
required, or it could mean simply an action that

80
00:04:13.740 --> 00:04:18.329
is morally permissible, so not required and not prohibited,

81
00:04:18.738 --> 00:04:21.820
but it might also mean an action that has

82
00:04:21.820 --> 00:04:25.690
some kind of moral value. Without being morally required.

83
00:04:25.859 --> 00:04:27.260
So if you, sometimes if you say it's the

84
00:04:27.260 --> 00:04:30.220
right thing to do, it's not that you're technically

85
00:04:30.220 --> 00:04:32.779
required to do it, but you get moral credit

86
00:04:32.779 --> 00:04:35.380
for doing it. And then there's also the abstract

87
00:04:35.380 --> 00:04:38.350
noun version of right that is a right, a

88
00:04:38.350 --> 00:04:42.329
right to not be killed, for example. And so

89
00:04:42.329 --> 00:04:45.010
these are very ambiguous terms, and one of the

90
00:04:45.010 --> 00:04:46.529
things I try to do in the introduction is

91
00:04:46.529 --> 00:04:48.679
to disambiguate them and get some clarity.

92
00:04:50.540 --> 00:04:53.500
So one more general question before we get into

93
00:04:53.500 --> 00:04:57.769
a specific example you exploring the book. So, um,

94
00:04:57.779 --> 00:05:02.220
how does ethics differ from norms in a particular

95
00:05:02.220 --> 00:05:02.739
society?

96
00:05:04.730 --> 00:05:10.609
Well, norms in a particular society. Uh, Social perceptions

97
00:05:10.609 --> 00:05:13.290
about what you ought to do. And some of

98
00:05:13.290 --> 00:05:16.130
those norms may be perceived to be moral norms,

99
00:05:16.209 --> 00:05:19.320
but they, they, they needn't always be. So sometimes

100
00:05:19.320 --> 00:05:21.970
the society might just think that certain things are

101
00:05:21.970 --> 00:05:25.100
not to be done, but not that they are

102
00:05:25.100 --> 00:05:27.190
morally wrong. It's just that they would let's say

103
00:05:27.190 --> 00:05:31.279
violate etiquette or good manners in a given society.

104
00:05:31.899 --> 00:05:34.989
But it is true, as I've suggested, that sometimes

105
00:05:34.989 --> 00:05:38.019
the society has a perception of what is right

106
00:05:38.019 --> 00:05:40.799
or wrong and will then have an according norm

107
00:05:41.149 --> 00:05:43.989
that societies can be wrong in their judgments just

108
00:05:43.989 --> 00:05:47.500
as individuals can be wrong in their judgments about

109
00:05:47.670 --> 00:05:51.029
what norms ought to prevail. And so what the

110
00:05:51.029 --> 00:05:53.739
philosopher tries to do is to step back from

111
00:05:53.989 --> 00:05:56.470
perceptions about what we ought to do, to see

112
00:05:56.470 --> 00:05:59.190
what we've got best reason for thinking we ought

113
00:05:59.190 --> 00:05:59.589
to do.

114
00:06:01.450 --> 00:06:04.089
So one of the topics as I mentioned uh

115
00:06:04.089 --> 00:06:06.929
in the beginning that you include in your book

116
00:06:06.929 --> 00:06:09.820
has to do with sex. So, and, and you,

117
00:06:09.839 --> 00:06:13.089
you explore different views, I think we could call

118
00:06:13.089 --> 00:06:16.570
them ethical views of sex and uh a pair

119
00:06:16.570 --> 00:06:20.529
of them are the significance view and the casual

120
00:06:20.529 --> 00:06:23.049
view of sex. Could you explain them and do

121
00:06:23.049 --> 00:06:25.760
you prefer one over the other?

122
00:06:27.160 --> 00:06:29.429
Well, it's interesting because the chapter on sex is

123
00:06:29.429 --> 00:06:32.160
the chapter in which I give the least direction

124
00:06:32.160 --> 00:06:37.279
because I'm actually ultimately agnostic on uh which these

125
00:06:37.279 --> 00:06:39.519
two views we ought to take. So to sketch

126
00:06:39.519 --> 00:06:42.760
them briefly, the significance view suggests that in order

127
00:06:42.760 --> 00:06:45.420
for sex to be morally permissible, it must be

128
00:06:45.420 --> 00:06:49.850
the expression of mutual romantic love. And the casual

129
00:06:49.850 --> 00:06:52.070
view would deny that. The casual view of sex

130
00:06:52.070 --> 00:06:55.250
is that sex is just like any other activity

131
00:06:55.250 --> 00:06:59.000
that is prone to cause pleasure, and it, it

132
00:06:59.000 --> 00:07:01.839
doesn't have any special requirements, just as you can

133
00:07:02.089 --> 00:07:06.489
eat a meal, uh, or enjoy an art exhibition

134
00:07:06.489 --> 00:07:09.600
together with somebody who you don't have a deep

135
00:07:10.339 --> 00:07:13.660
affection towards. So you could have sex with somebody

136
00:07:13.809 --> 00:07:19.920
in permissibly without uh having mutual romantic love. And

137
00:07:20.130 --> 00:07:22.649
uh I think at the outset, many people will

138
00:07:22.649 --> 00:07:25.290
be immediately inclined to one or other of these

139
00:07:25.290 --> 00:07:26.809
views. And one of the things I try to

140
00:07:26.809 --> 00:07:29.450
spell out in the chapter is that there's actually

141
00:07:29.450 --> 00:07:33.690
bad news for both of these views, that the

142
00:07:33.690 --> 00:07:36.690
full set of views that any one individual holds

143
00:07:37.119 --> 00:07:40.410
about sexual practices and array of sexual practices is

144
00:07:40.410 --> 00:07:43.480
unlikely to fit with any one of these views.

145
00:07:43.529 --> 00:07:45.250
So something is going to have to be given

146
00:07:45.250 --> 00:07:47.899
up. And so I present really a kind of

147
00:07:47.899 --> 00:07:52.119
dilemma. There is a fallback position that I refer

148
00:07:52.119 --> 00:07:56.079
to. It's not the direct topic of, of most

149
00:07:56.079 --> 00:07:58.890
of my discussion in, in that chapter, but I

150
00:07:58.890 --> 00:08:00.290
do refer to it, and that is the view

151
00:08:00.290 --> 00:08:02.730
that perhaps the significance view is a view not

152
00:08:02.730 --> 00:08:06.130
about what sex is permissible, but a view about

153
00:08:06.130 --> 00:08:09.929
what sex is morally best. So it's not that

154
00:08:09.929 --> 00:08:13.089
it would be impermissible to have sex with somebody

155
00:08:13.420 --> 00:08:16.200
for whom you did not have, uh, with whom

156
00:08:16.200 --> 00:08:19.209
you did not have mutual romantic love, but it

157
00:08:19.209 --> 00:08:22.369
might just be less morally desirable. That's another possible

158
00:08:22.369 --> 00:08:24.250
position. Mhm.

159
00:08:25.420 --> 00:08:29.809
So let's perhaps you explore several different examples of

160
00:08:29.809 --> 00:08:32.900
sexual activities in the book. Let's perhaps pick one

161
00:08:32.900 --> 00:08:35.770
of them and talk about it. When it comes

162
00:08:35.770 --> 00:08:40.890
to rape specifically, on what grounds can it be

163
00:08:40.890 --> 00:08:42.340
morally condemned?

164
00:08:43.510 --> 00:08:46.630
Well, this is a case where both the advocates

165
00:08:46.630 --> 00:08:49.450
of the significance view and the casual view uh

166
00:08:49.450 --> 00:08:52.460
can condemn rape. Both of them can do that.

167
00:08:53.030 --> 00:08:56.429
But it's only the significance view that can show

168
00:08:56.429 --> 00:09:00.710
that there's a special wrong with rape. Because, uh,

169
00:09:00.809 --> 00:09:03.409
they're going to be lots of activities where you

170
00:09:03.409 --> 00:09:07.159
force somebody to do something and the casual view

171
00:09:07.159 --> 00:09:09.090
can say, well, it's wrong. You can force somebody

172
00:09:09.090 --> 00:09:11.570
to eat some food, for example, you can force

173
00:09:11.570 --> 00:09:14.869
them to go to the opera with you, uh,

174
00:09:14.890 --> 00:09:17.849
and that would be wrong to do. And the

175
00:09:17.849 --> 00:09:20.250
advocate of the casualty would similarly be able, similarly

176
00:09:20.250 --> 00:09:22.609
be able to say that forcing somebody to have

177
00:09:22.609 --> 00:09:25.140
sex would be morally wrong. And it might even

178
00:09:25.140 --> 00:09:28.000
be pretty seriously morally wrong, but it wouldn't be

179
00:09:28.000 --> 00:09:31.599
more wrong on the casual view than with forcing

180
00:09:31.599 --> 00:09:35.760
somebody to eat something, for example. And that is

181
00:09:35.760 --> 00:09:38.679
going to strike many people as odd. And the

182
00:09:38.679 --> 00:09:42.159
way to avoid that implication is really by going

183
00:09:42.159 --> 00:09:45.369
for the significance view. Because the significance is wanting

184
00:09:45.369 --> 00:09:49.200
to set sex apart from these other activities that

185
00:09:49.200 --> 00:09:52.229
are prone to cause, uh, pleasure. And I'm, I'm

186
00:09:52.400 --> 00:09:54.320
quite specifically saying prone to cause. It's not that

187
00:09:54.320 --> 00:09:57.640
they have to cause pleasure in every individual instance.

188
00:09:58.489 --> 00:10:01.460
And, uh, so the significance you can very easily

189
00:10:01.460 --> 00:10:04.020
say why there's a special wrong in rape. There's

190
00:10:04.020 --> 00:10:06.500
going to be a much bigger challenge for the

191
00:10:06.500 --> 00:10:08.400
advocate of the casual view to show why that's

192
00:10:08.400 --> 00:10:08.909
the case.

193
00:10:11.030 --> 00:10:15.109
So, is there anything that could be wrong about

194
00:10:15.109 --> 00:10:17.070
masturbation specifically?

195
00:10:18.919 --> 00:10:20.830
Well, again, I think this is a case where

196
00:10:20.830 --> 00:10:23.770
it depends on how exactly you're going to understand

197
00:10:23.770 --> 00:10:26.450
the significance view. On the casual view, obviously there's

198
00:10:26.450 --> 00:10:28.570
nothing wrong and advocates of the casual view are

199
00:10:28.570 --> 00:10:30.880
going to say that that's absolutely fine, that's entirely

200
00:10:30.880 --> 00:10:34.659
intuitive. I think advocates of the significance you could

201
00:10:34.659 --> 00:10:37.059
go in different ways. So one thing you can

202
00:10:37.059 --> 00:10:39.820
say is, well, there's no mutuality in this sexual

203
00:10:39.820 --> 00:10:43.059
activity. So assuming you're using the term masturbation to

204
00:10:43.059 --> 00:10:45.140
refer to what somebody does to themselves as opposed

205
00:10:45.140 --> 00:10:49.869
to, uh, perhaps a mutual masturbation. Uh, AND so

206
00:10:49.869 --> 00:10:51.830
you may say, well, there's no mutuality here and

207
00:10:51.830 --> 00:10:54.190
so it's failing to meet the requirements of this

208
00:10:54.190 --> 00:10:57.950
significance view and therefore it's either impermissible or at

209
00:10:57.950 --> 00:11:02.109
the very least morally less good. But another possibility

210
00:11:02.109 --> 00:11:05.020
is to say, well, because it only involves one

211
00:11:05.020 --> 00:11:08.979
individual, there are no two individuals here and so

212
00:11:09.270 --> 00:11:12.190
the mutuality requirement would fall away. And so the

213
00:11:12.190 --> 00:11:14.890
significance we simply wouldn't apply there and then it

214
00:11:14.890 --> 00:11:17.330
might be permissible on that view. So I can

215
00:11:17.330 --> 00:11:20.400
see different ways in which a significance theorist would

216
00:11:20.400 --> 00:11:21.729
approach that particular question.

217
00:11:23.109 --> 00:11:26.469
And then you also explore another pair of views

218
00:11:26.469 --> 00:11:30.729
that are the reproductive and the anti-reproductive views of

219
00:11:30.729 --> 00:11:32.289
sex. Could you tell us about them?

220
00:11:33.359 --> 00:11:38.030
Yes, a very common view often held among conservatives

221
00:11:38.030 --> 00:11:40.070
is that for sex to be morally permissible, it

222
00:11:40.070 --> 00:11:43.630
must stand the chance of being reproductive. That is

223
00:11:43.630 --> 00:11:46.059
to say, it must stand the chance of producing

224
00:11:46.349 --> 00:11:50.150
offspring. And some conservatives want to use this, for

225
00:11:50.150 --> 00:11:54.150
example, to rule out a homosexual sex, which they

226
00:11:54.150 --> 00:11:57.789
say has no option of, of being reproductive, no

227
00:11:57.789 --> 00:12:02.109
opportunity to be reproductive, would also apply to bestiality.

228
00:12:02.599 --> 00:12:06.599
Um, TO sex with a child who's not gone

229
00:12:06.599 --> 00:12:08.599
through puberty. So they believe that they could say

230
00:12:08.599 --> 00:12:12.320
it could rule out all of these activities. But

231
00:12:12.320 --> 00:12:15.200
they do face a problem because it seems to

232
00:12:15.200 --> 00:12:20.000
also rule out sex between married people who are

233
00:12:20.000 --> 00:12:23.229
for some reason infertile. So if a woman is,

234
00:12:23.299 --> 00:12:28.520
um, A postmenopausal Uh, she and her husband might

235
00:12:28.809 --> 00:12:30.609
not be able to have permissible sex on this

236
00:12:30.609 --> 00:12:33.650
view. Sometimes advocates of the significance view want to

237
00:12:33.650 --> 00:12:36.250
contort themselves in a way to say, well, this

238
00:12:36.250 --> 00:12:39.219
is between an adult male and a female. And

239
00:12:39.219 --> 00:12:41.969
so in principle, it could be reproductive, but that

240
00:12:41.969 --> 00:12:48.280
seems like a highly strained uh rationalization of um

241
00:12:48.530 --> 00:12:50.330
of their view. So it's gonna have that kind

242
00:12:50.330 --> 00:12:53.739
of odd, uh, odd implication. Now, of course, there

243
00:12:53.739 --> 00:12:55.950
are many people who reject the reproductive view, say

244
00:12:55.950 --> 00:12:58.830
sex does not need to be reproductive. And I

245
00:12:58.830 --> 00:13:02.590
think that's an entirely a plausible position. My own

246
00:13:02.590 --> 00:13:05.340
view, for reasons we've discussed in an earlier interview,

247
00:13:05.630 --> 00:13:09.789
is that actually sex should not be reproductive. Uh,

248
00:13:09.909 --> 00:13:11.859
SO I have the anti-reproductive view because I'm an

249
00:13:11.869 --> 00:13:14.549
anti-atalist and think that it's wrong to bring new

250
00:13:14.549 --> 00:13:18.469
sentient beings into existence. I think sex is gonna

251
00:13:18.469 --> 00:13:22.099
be fine, uh, if it's not reproductive. I mean,

252
00:13:22.309 --> 00:13:24.390
there are other conditions as well that you might

253
00:13:24.390 --> 00:13:29.150
apply, including possibly the significance of view, but certainly

254
00:13:29.150 --> 00:13:31.429
a requirement for permissible sex on my view is

255
00:13:31.429 --> 00:13:33.179
that it would not be reproductive.

256
00:13:34.640 --> 00:13:38.820
So antenatalism is part of the set of ethical

257
00:13:38.820 --> 00:13:41.270
questions surrounding sexual practices. Right.

258
00:13:42.580 --> 00:13:45.020
Correct, I think so. But I, I, I do

259
00:13:45.020 --> 00:13:48.739
acknowledge that my views on this are controversial. And

260
00:13:48.739 --> 00:13:50.820
so, and I, this is not, this is not

261
00:13:50.820 --> 00:13:55.150
a tract on, on antenatalism. So I draw attention

262
00:13:55.150 --> 00:13:58.849
to what antenatalists would say about this, but I

263
00:13:58.849 --> 00:14:02.260
also allow the scope that somebody might reject that

264
00:14:02.260 --> 00:14:05.450
particular view. And then assemble a collection of other

265
00:14:05.450 --> 00:14:07.809
views about sexual ethics in order to generate a

266
00:14:07.809 --> 00:14:09.169
practical ethical conclusion.

267
00:14:09.880 --> 00:14:13.710
Mhm. So before we move on to another topic,

268
00:14:13.880 --> 00:14:15.719
let me just ask you, do you think that

269
00:14:15.719 --> 00:14:18.599
it is possible from any of the views we

270
00:14:18.599 --> 00:14:23.159
explore there, the reproductive and the reproductive significant schedule

271
00:14:23.159 --> 00:14:26.599
view or even a combination of them to justify

272
00:14:26.599 --> 00:14:29.549
that all incest is wrong?

273
00:14:31.950 --> 00:14:35.270
Um, THAT'S hard to do. I mean, on the

274
00:14:35.270 --> 00:14:38.229
casual view, there's no way of saying why, why

275
00:14:38.229 --> 00:14:41.150
incest will be, will be wrong. And on the

276
00:14:41.150 --> 00:14:43.280
significance view, of course, it's possible for let's say,

277
00:14:43.429 --> 00:14:47.750
adult siblings to have um mutual romantic love. And

278
00:14:47.750 --> 00:14:49.669
if they're of the opposite sex, then their sex

279
00:14:49.669 --> 00:14:52.789
could be reproductive. But there is this challenge in

280
00:14:52.789 --> 00:14:56.070
that uh sex between closely related people, if it

281
00:14:56.070 --> 00:15:00.580
is reproductive, is much more likely to lead to

282
00:15:00.580 --> 00:15:04.020
abnormalities in the offspring, and that would then, of

283
00:15:04.020 --> 00:15:07.700
course, harm those offspring. And so it may be

284
00:15:07.700 --> 00:15:11.299
that on those grounds, you would say that the

285
00:15:11.299 --> 00:15:15.619
reproductive incestuous sex is wrong, but it can't be

286
00:15:15.619 --> 00:15:19.619
on the grounds of being incestuous per se, because

287
00:15:19.619 --> 00:15:21.630
that's going to meet the significance requirement. It would

288
00:15:21.630 --> 00:15:24.140
be because it actually causes harm that it would

289
00:15:24.140 --> 00:15:25.340
be, that would be wrong.

290
00:15:26.719 --> 00:15:29.799
Mhm. So, now I would like to ask you

291
00:15:29.799 --> 00:15:33.280
a little bit about questions surrounding the environment. So

292
00:15:33.280 --> 00:15:36.880
when it comes to the environment, with ethical questions

293
00:15:36.880 --> 00:15:39.760
or some examples of ethical questions that fall under

294
00:15:39.760 --> 00:15:43.260
the rubric of practical ethics, could you tell us

295
00:15:43.260 --> 00:15:44.080
about some of that?

296
00:15:45.380 --> 00:15:47.809
Well, for a very long time there have been

297
00:15:47.809 --> 00:15:52.289
questions about local pollution, so one could pollute one's

298
00:15:52.289 --> 00:15:57.380
environment locally by littering, for example. And uh polluting

299
00:15:57.380 --> 00:16:00.229
the local water and those are gonna be practical

300
00:16:00.229 --> 00:16:04.659
ethical questions. But in recent decades, a much more

301
00:16:05.359 --> 00:16:09.900
catastrophic issue in a sense is the, the global

302
00:16:09.900 --> 00:16:13.599
environment and our contribution to the global environment. So

303
00:16:13.599 --> 00:16:16.340
if you think about carbon and other greenhouse gas

304
00:16:16.340 --> 00:16:20.840
emissions that we individuals engage in, these contribute in

305
00:16:20.840 --> 00:16:26.400
aggregation to a potentially catastrophic effects from environmental change.

306
00:16:26.440 --> 00:16:29.039
And so that does raise a question, but it's

307
00:16:29.039 --> 00:16:32.950
an interesting question because the emissions of no individual

308
00:16:32.950 --> 00:16:36.599
are going to cause these harms. It's these emissions

309
00:16:36.599 --> 00:16:40.039
in aggregation with others. And that's, this leads to

310
00:16:40.039 --> 00:16:43.719
a very interesting uh ethical problem that actually recurs

311
00:16:43.719 --> 00:16:46.309
in a number of other chapters in the book.

312
00:16:46.559 --> 00:16:50.349
It's the problem of what's known as causal inefficacy

313
00:16:50.640 --> 00:16:55.280
or sometimes inconsequentialism. And this is really about the

314
00:16:55.280 --> 00:16:58.039
question, does it make any difference whether or not

315
00:16:58.039 --> 00:17:01.700
I engage in this activity? Because if I do,

316
00:17:02.270 --> 00:17:05.229
barely any difference of a of a negative kind

317
00:17:05.229 --> 00:17:07.839
will be, uh, will result. And if I don't,

318
00:17:08.030 --> 00:17:11.388
there'll be barely any benefit. But if everybody reasons

319
00:17:11.388 --> 00:17:13.729
that way, then we've got a real problem.

320
00:17:16.160 --> 00:17:19.199
Right, but, uh, but with that in mind, do

321
00:17:19.199 --> 00:17:22.670
you think that even if the vast majority of

322
00:17:22.670 --> 00:17:25.319
individuals are not doing what they can to reduce

323
00:17:25.319 --> 00:17:30.359
emissions, for example, should I as an individual still

324
00:17:30.359 --> 00:17:33.229
have a a duty to do it myself?

325
00:17:34.469 --> 00:17:38.239
Well, I argue that there is a duty, but

326
00:17:38.239 --> 00:17:40.680
that it's a limited duty. So I'm steering a

327
00:17:40.680 --> 00:17:43.640
path between those people who say, because of this

328
00:17:43.640 --> 00:17:46.800
problem of inconsequentialism, there's absolutely no duty at all.

329
00:17:47.540 --> 00:17:50.140
Uh, AND those people who think that we have

330
00:17:50.140 --> 00:17:54.579
very, very substantial duties, uh, with regards to our

331
00:17:54.579 --> 00:17:57.660
individual actions. Uh, I, I think that neither of

332
00:17:57.660 --> 00:18:01.400
those positions are plausible. Uh, MY worry with the

333
00:18:01.400 --> 00:18:03.660
first position is that if we say people have

334
00:18:03.660 --> 00:18:07.449
no duty whatsoever, well, then that is very likely

335
00:18:07.449 --> 00:18:10.040
just to exacerbate the problem because everybody will then

336
00:18:10.040 --> 00:18:13.459
just emmit with abandon. But on the other hand,

337
00:18:13.609 --> 00:18:17.569
to require some individual to stop emitting at fairly

338
00:18:17.569 --> 00:18:20.680
high levels when that comes at quite significant cost

339
00:18:20.680 --> 00:18:24.849
to them. And yet doesn't secure any meaningful good.

340
00:18:25.260 --> 00:18:27.380
It's also hard to say that that person is

341
00:18:27.380 --> 00:18:29.859
required to do that. And so I think our

342
00:18:29.859 --> 00:18:33.660
primary duties are really to support a collective action.

343
00:18:34.209 --> 00:18:39.530
That would result in lowering emissions. So that's where

344
00:18:39.530 --> 00:18:42.130
our strongest duties lie. But we also have some

345
00:18:42.130 --> 00:18:45.810
duties I think, to consider the contribution that we

346
00:18:45.810 --> 00:18:49.810
make to global warming and to try to limit

347
00:18:49.810 --> 00:18:52.489
that. So it's, it's not that you have to

348
00:18:52.489 --> 00:18:57.089
limit it to, to zero net emissions, but you

349
00:18:57.089 --> 00:18:59.569
need to do what you can within reason to,

350
00:18:59.650 --> 00:19:01.050
uh, to limit your contribution.

351
00:19:02.250 --> 00:19:04.589
OK, so let me ask you about an activity

352
00:19:04.589 --> 00:19:07.949
that many, many people out there participate in. Do

353
00:19:07.949 --> 00:19:11.670
you think that in order to reduce emissions, we

354
00:19:11.670 --> 00:19:15.430
should be morally obliged to stop driving?

355
00:19:17.290 --> 00:19:19.650
Well, again, I don't think so because sometimes they're

356
00:19:19.650 --> 00:19:22.890
very important benefits to be derived from, from driving.

357
00:19:23.050 --> 00:19:26.040
Some people, for example, can't get to work without

358
00:19:26.300 --> 00:19:28.530
driving. Uh IF they go to work, they're able

359
00:19:28.530 --> 00:19:30.609
to earn a salary which can support them and

360
00:19:30.609 --> 00:19:33.599
their family. It, they can also contribute to aid,

361
00:19:33.689 --> 00:19:38.250
the subject of, of another chapter. And I think

362
00:19:38.250 --> 00:19:40.579
one's gonna be hard pressed to say that all

363
00:19:40.579 --> 00:19:43.650
of those things must be given up because you

364
00:19:43.650 --> 00:19:47.069
shouldn't be driving. Uh, BUT I do think it's

365
00:19:47.069 --> 00:19:49.750
gonna have an impact on how much driving you

366
00:19:49.750 --> 00:19:52.680
do. So it should impact on decisions, to be

367
00:19:52.680 --> 00:19:56.229
relevant to decisions, for example, to how close to

368
00:19:56.229 --> 00:19:59.510
your work you live. It should be relevant to

369
00:19:59.510 --> 00:20:02.750
question about joy rides. And even joy rides are

370
00:20:02.750 --> 00:20:07.550
going to make relatively little contribution, but you can

371
00:20:07.550 --> 00:20:09.589
have fewer of them rather than more of them.

372
00:20:10.140 --> 00:20:11.770
And so I think we should do what we

373
00:20:11.770 --> 00:20:13.890
can to limit those, but it's a, it's a

374
00:20:13.890 --> 00:20:18.709
constant balance balancing between The amount of sacrifice that

375
00:20:18.709 --> 00:20:21.469
you're making and the amount of harm that you're

376
00:20:21.469 --> 00:20:24.630
preventing. But also recognizing that you are one of

377
00:20:24.630 --> 00:20:27.550
many, many people making that kind of decision. And

378
00:20:27.550 --> 00:20:30.540
so you can't just view it as an individual

379
00:20:30.540 --> 00:20:32.829
decision, you have to recognize it as a decision

380
00:20:32.829 --> 00:20:34.229
that you and others are making.

381
00:20:35.439 --> 00:20:40.109
Mhm. So, to establish here a small link, I

382
00:20:40.109 --> 00:20:44.150
think between the previous topic, sex and the environment,

383
00:20:44.189 --> 00:20:47.040
and going back to the topic of antenatalism, I

384
00:20:47.040 --> 00:20:51.520
guess, how bad is procreation for the environment?

385
00:20:52.589 --> 00:20:54.189
Well, it's pretty bad. One of the things I

386
00:20:54.189 --> 00:20:56.150
point out in the book is that one of

387
00:20:56.150 --> 00:21:00.829
the biggest contributions you can make to, uh, to

388
00:21:00.829 --> 00:21:03.640
global warming is just by having a child. And

389
00:21:03.949 --> 00:21:07.300
I really, I do the mathematics and show how

390
00:21:07.829 --> 00:21:13.520
recycling has a relatively small impact and uh your,

391
00:21:13.650 --> 00:21:16.030
your driving has a greater impact and your flying

392
00:21:16.030 --> 00:21:17.819
is a greater impact, but more than all of

393
00:21:17.819 --> 00:21:21.670
these things is, uh, having a child. With having

394
00:21:21.670 --> 00:21:25.189
a child, certainly in the developed world where average

395
00:21:25.189 --> 00:21:28.550
emission rates are, are really high, is going to

396
00:21:28.550 --> 00:21:33.630
lead to uh a significant environmental impact. Now, I

397
00:21:33.630 --> 00:21:36.349
also look at details about whether you're sharing this

398
00:21:36.349 --> 00:21:40.150
cost, as it were, with your procreative partner. But

399
00:21:40.150 --> 00:21:43.189
even when you take that into account, this is

400
00:21:43.189 --> 00:21:46.709
perhaps the highest emission activity that you can Engage

401
00:21:46.709 --> 00:21:49.390
in. Now, does this tell you to say that

402
00:21:49.390 --> 00:21:51.589
procreation is wrong? No, I don't think it does.

403
00:21:51.680 --> 00:21:54.670
I don't think that this is a definitive argument

404
00:21:54.670 --> 00:21:58.550
against procreation. It's probably an argument for reproducing less

405
00:21:58.550 --> 00:22:01.569
than you otherwise would. Uh, BUT the arguments are

406
00:22:01.569 --> 00:22:04.099
advanced in the earlier books since most especially better

407
00:22:04.099 --> 00:22:06.670
than never to have been, those are arguments that

408
00:22:06.670 --> 00:22:11.550
I think produce a categorical, uh, injunction against procreating.

409
00:22:11.829 --> 00:22:13.800
And so the interesting thing is when you combine

410
00:22:13.800 --> 00:22:17.359
that argument with the environment, environmental argument, then you

411
00:22:17.359 --> 00:22:22.359
get a somewhat overdetermined argument against procreation. It's not

412
00:22:22.359 --> 00:22:24.400
that the environmental argument by itself will do the

413
00:22:24.400 --> 00:22:29.010
work, but it certainly bolsters up the antenatalist argument

414
00:22:29.560 --> 00:22:31.439
uh that I advanced in earlier books.

415
00:22:31.839 --> 00:22:34.239
Mhm. So, I would like to ask you now

416
00:22:34.239 --> 00:22:36.760
a little bit about smoking. I mean, this is

417
00:22:36.760 --> 00:22:39.790
a topic that uh here in Portugal, for example,

418
00:22:39.930 --> 00:22:43.359
has been very relevant in recent years because there

419
00:22:43.359 --> 00:22:47.839
have been restrictions enacted on it in particular places

420
00:22:47.839 --> 00:22:50.839
and I would like to ask you, are there

421
00:22:50.839 --> 00:22:56.369
ethical questions associated with smoking are these are the

422
00:22:56.369 --> 00:23:00.849
ethical questions associated with smoking just about the harm

423
00:23:01.050 --> 00:23:05.209
it can cause to non-smokers, or are there also

424
00:23:05.209 --> 00:23:09.130
ethical questions to consider in regarding to the acts

425
00:23:09.130 --> 00:23:12.969
of smoking alone and the potential harms to the

426
00:23:12.969 --> 00:23:13.369
smoker?

427
00:23:15.099 --> 00:23:17.540
Thank you. Yes, in the chapter, I do start

428
00:23:17.540 --> 00:23:22.579
out by looking at ethical questions. That arise from

429
00:23:22.579 --> 00:23:24.939
the harm that smoking does to the individual who

430
00:23:24.939 --> 00:23:28.180
is smoking. And there are some ethical views that

431
00:23:28.180 --> 00:23:30.619
would deny that that's relevant. In other words, some

432
00:23:30.619 --> 00:23:33.540
people think that ethics is really just about what

433
00:23:33.540 --> 00:23:37.380
you do to other people. Uh, AND I reject

434
00:23:37.380 --> 00:23:38.780
that view. It may be that some of the

435
00:23:38.780 --> 00:23:41.569
more important ethical issues are of that kind, but

436
00:23:41.989 --> 00:23:44.619
I do think that if we understand ethics as

437
00:23:44.619 --> 00:23:48.060
how you ought to live, then there can be

438
00:23:48.060 --> 00:23:52.939
questions. ABOUT purely self-regarding actions, actions that affect only

439
00:23:52.939 --> 00:23:55.849
yourself that fall within the uh scope of ethics.

440
00:23:56.260 --> 00:23:59.140
And my suggestion here is that if you're not

441
00:23:59.140 --> 00:24:02.459
already a smoker, then you ought not to become

442
00:24:02.459 --> 00:24:06.219
a smoker because this is a dangerous activity to

443
00:24:06.219 --> 00:24:08.819
yourself and to others. Uh, IT'S also an expensive

444
00:24:08.819 --> 00:24:10.859
activity and it's an addictive activity. It's not as

445
00:24:10.859 --> 00:24:14.380
though you can readily revisit this decision later once

446
00:24:14.380 --> 00:24:16.819
you've become addicted. And so I think there are

447
00:24:16.819 --> 00:24:19.359
very good reasons not to start smoking. But I

448
00:24:19.359 --> 00:24:22.479
also exhibit an awareness of what it's like for

449
00:24:22.479 --> 00:24:25.599
a smoker, somebody who's already smoking, and I certainly

450
00:24:25.599 --> 00:24:29.160
can imagine circumstances in which somebody who's already a

451
00:24:29.160 --> 00:24:31.630
smoker may be under no obligation to give it

452
00:24:31.630 --> 00:24:35.719
up, uh, based on the effect on themselves, but

453
00:24:35.719 --> 00:24:38.000
there would still be questions about the effect of

454
00:24:38.000 --> 00:24:41.069
their smoking on other people. Mhm.

455
00:24:42.579 --> 00:24:47.660
Sometimes smokers who are not fond of restrictions evoke

456
00:24:47.660 --> 00:24:51.250
examples of other activities that most people participate in

457
00:24:51.250 --> 00:24:55.219
like driving that they say are even more harmful

458
00:24:55.219 --> 00:24:58.300
to humans and the environment, but no one cares

459
00:24:58.300 --> 00:25:01.339
about restricting them. Do they have a point here?

460
00:25:02.560 --> 00:25:05.229
I don't think so. And the driving analogy is

461
00:25:05.229 --> 00:25:07.189
one that I looked at in this chapter, or

462
00:25:07.189 --> 00:25:10.150
more specifically in an appendix to the chapter. I'm

463
00:25:10.150 --> 00:25:13.790
afraid my empirical inquiries here have not yielded a

464
00:25:13.790 --> 00:25:19.349
definitive answer on whether driving, the collective effects of

465
00:25:19.349 --> 00:25:23.589
driving are worse than the collective effects of, of

466
00:25:23.589 --> 00:25:27.369
smoking. It's because it's really hard to get comparative

467
00:25:27.369 --> 00:25:30.930
data and hard to know what the appropriate denominator

468
00:25:30.930 --> 00:25:34.550
is sometimes in these, uh, in these comparisons. Uh,

469
00:25:34.609 --> 00:25:39.290
BUT Uh, even if we set that aside, driving

470
00:25:39.290 --> 00:25:42.800
has a benefit. Very often, not always, but very

471
00:25:42.800 --> 00:25:46.479
often, that smoking doesn't have. And so to ask

472
00:25:46.479 --> 00:25:50.930
people not to smoke in the presence of non-smokers

473
00:25:50.930 --> 00:25:54.719
is not like asking somebody not to drive. Not

474
00:25:54.719 --> 00:25:57.040
to drive can involve a much greater sacrifice for

475
00:25:57.040 --> 00:25:59.000
people. That's not to say that they're gonna be

476
00:25:59.000 --> 00:26:02.199
no restrictions on how we may drive and Uh,

477
00:26:02.290 --> 00:26:04.050
what kind of engines we should have and what

478
00:26:04.050 --> 00:26:06.729
kind of uh petroleum or gas should be used

479
00:26:06.729 --> 00:26:09.050
in, in the, in the running of those cars.

480
00:26:09.209 --> 00:26:12.479
All of those questions are still entirely pertinent. But

481
00:26:12.479 --> 00:26:16.640
I, I don't think that appealing to the acceptability

482
00:26:16.640 --> 00:26:19.209
of car driving or the widespread acceptability of car

483
00:26:19.209 --> 00:26:23.550
driving as a basis for condoning cigarette smoking is

484
00:26:23.550 --> 00:26:25.250
a plausible argument.

485
00:26:26.290 --> 00:26:29.819
And, and by the way, how about electronic cigarettes?

486
00:26:29.939 --> 00:26:33.939
Are they any better than regular cigarettes at this

487
00:26:33.939 --> 00:26:37.380
point? What do we know about their potential harms?

488
00:26:38.520 --> 00:26:42.579
Again, uh, my empirical investigation there suggests that we

489
00:26:42.579 --> 00:26:44.699
don't yet have all the data that we need

490
00:26:44.699 --> 00:26:47.979
to, uh, to make comparisons in part because electronic

491
00:26:47.979 --> 00:26:50.449
cigarettes have just not been around for as long

492
00:26:50.819 --> 00:26:55.459
as tobacco cigarettes have been. And so, and also

493
00:26:55.459 --> 00:26:58.569
there are lots of variations between different forms of,

494
00:26:58.579 --> 00:27:01.640
uh, of e uh smoking. So that's hard to

495
00:27:01.640 --> 00:27:05.640
know, but again, I would encourage people not to

496
00:27:05.640 --> 00:27:10.479
start uh smoking if they're not already smokers, because

497
00:27:10.479 --> 00:27:12.599
there seems to be very little, in fact, nothing

498
00:27:12.599 --> 00:27:14.670
to be gained and lots to be lost, uh,

499
00:27:14.680 --> 00:27:19.250
from, uh from starting off with, with e-cigarettes. And

500
00:27:19.250 --> 00:27:22.130
then again, uh probably best not to smoke in

501
00:27:22.130 --> 00:27:24.770
the presence of smoking those kinds of cigarettes in

502
00:27:24.770 --> 00:27:27.650
the presence of non-smokers, uh, if for no other

503
00:27:27.650 --> 00:27:30.209
reason than just out of caution, but there is

504
00:27:30.209 --> 00:27:32.089
some evidence as well that this might be harmful

505
00:27:32.089 --> 00:27:32.569
to others.

506
00:27:33.949 --> 00:27:38.060
So shifting gears to giving aid, which is another

507
00:27:38.060 --> 00:27:40.969
topic you explored in the book. First of all,

508
00:27:41.219 --> 00:27:45.260
how do we identify people who need aid and

509
00:27:45.260 --> 00:27:48.020
the ones that should be the recipients of it?

510
00:27:50.500 --> 00:27:55.780
Well, that's a really hard question. But perhaps a

511
00:27:55.780 --> 00:27:58.260
harder question still is, of all the people who

512
00:27:58.260 --> 00:28:02.020
need aid, who should we be helping? Because the

513
00:28:02.020 --> 00:28:05.760
need probably outstrips the resources. Certainly, it's gonna outstrip

514
00:28:05.760 --> 00:28:09.420
the resources of any individual. And so you need

515
00:28:09.420 --> 00:28:15.130
to decide how best to allocate your resources. And

516
00:28:15.660 --> 00:28:18.420
that's a practical question that really varies from moment

517
00:28:18.420 --> 00:28:22.859
to moment. I can't give Uh, any guidance here

518
00:28:22.859 --> 00:28:25.339
either in the book or now, that would be

519
00:28:25.339 --> 00:28:30.900
enduring. And many of these charity guidance organizations like

520
00:28:30.900 --> 00:28:35.780
Give Well are well known for varying their recommendations.

521
00:28:35.900 --> 00:28:38.329
So at a given time, they'll make one recommendation

522
00:28:38.660 --> 00:28:41.290
and at another time, they'll make another recommendation because

523
00:28:41.510 --> 00:28:45.599
the relevant facts might change. For example, if they

524
00:28:45.599 --> 00:28:48.180
recommend aid to a particular organization, there may be

525
00:28:48.180 --> 00:28:51.599
a, a burst of aid to that organization. And,

526
00:28:51.760 --> 00:28:54.560
and then that organization has got more for the

527
00:28:54.560 --> 00:28:57.040
moment than it can actually process. And so it

528
00:28:57.040 --> 00:28:59.719
may then make more sense to divert some resources

529
00:29:00.079 --> 00:29:04.000
uh to, uh, to another, another organization. And so

530
00:29:04.000 --> 00:29:07.520
I don't think that highly specific advice can be

531
00:29:07.520 --> 00:29:10.319
given, yeah, but there's no shortage of people who

532
00:29:10.319 --> 00:29:13.989
are in dire need, people who, people and animals,

533
00:29:14.069 --> 00:29:18.319
by the way, uh, who are suffering unbearably and

534
00:29:18.319 --> 00:29:20.400
could, could benefit from, from assistance.

535
00:29:21.569 --> 00:29:24.780
And how do we determine to what point we

536
00:29:24.780 --> 00:29:28.729
should be morally obliged to give aid to someone

537
00:29:28.989 --> 00:29:32.339
and when does it become too demanding?

538
00:29:33.699 --> 00:29:36.089
Well, this is a very deep question in ethics

539
00:29:36.089 --> 00:29:39.449
more generally, even if we move beyond practical ethics.

540
00:29:39.609 --> 00:29:42.849
And I do through these practical ethical issues, uh

541
00:29:42.849 --> 00:29:46.479
try to formulate a view about this. And again,

542
00:29:46.810 --> 00:29:50.729
I opt for something of a middle path between

543
00:29:50.729 --> 00:29:52.770
two kinds of extreme views. I, I don't always

544
00:29:52.770 --> 00:29:55.489
think middle paths are the correct parts, uh, but

545
00:29:55.489 --> 00:29:58.569
in this case, among some others, uh, I think

546
00:29:58.569 --> 00:30:01.780
that The very, very demanding views that people like

547
00:30:01.780 --> 00:30:04.900
Peter Singer have advanced, I think are implausible, but

548
00:30:04.900 --> 00:30:06.979
so too are the view, uh, are the views

549
00:30:06.979 --> 00:30:10.530
that we have no duties uh to positively aid

550
00:30:10.819 --> 00:30:13.060
uh people who are in need. I think there's

551
00:30:13.060 --> 00:30:15.739
some kind of intermediate position, and I think the

552
00:30:15.739 --> 00:30:18.979
intermediate position is the most reasonable one, and that's

553
00:30:18.979 --> 00:30:22.300
the reason why we should settle upon it. And

554
00:30:22.310 --> 00:30:24.150
I go into quite some detail in the book

555
00:30:24.150 --> 00:30:27.550
trying to gauge what that might be, where that

556
00:30:27.550 --> 00:30:30.069
middle part might lie. But it might not be

557
00:30:30.069 --> 00:30:30.510
a strict.

558
00:30:32.420 --> 00:30:35.479
Uh, COULD you just tell us about uh Peter

559
00:30:35.479 --> 00:30:39.170
Singer's position and why you think it is implausible?

560
00:30:42.300 --> 00:30:45.660
Well, it's hard to summarize my responses in a

561
00:30:45.660 --> 00:30:47.339
short space of time. I go into quite a

562
00:30:47.339 --> 00:30:51.699
lot of detail in the book trying to imagine

563
00:30:51.699 --> 00:30:55.300
various interpretations of what he's saying and then considering

564
00:30:55.300 --> 00:31:00.260
various objections to it. But uh his view is

565
00:31:00.260 --> 00:31:02.459
really rests on a principle. And the principle is

566
00:31:02.459 --> 00:31:05.140
that if you can prevent something very bad from

567
00:31:05.140 --> 00:31:09.699
happening without sacrificing anything of comparable moral significance, then

568
00:31:09.699 --> 00:31:13.420
you ought to do it. And the example that

569
00:31:13.420 --> 00:31:16.369
he often uses to illustrate this or to generate

570
00:31:16.369 --> 00:31:19.020
the principle, there's some dispute about what exactly he's,

571
00:31:19.099 --> 00:31:21.579
he's doing there, uh, is the case of a

572
00:31:21.579 --> 00:31:24.369
child drowning in a pond. You're going to work

573
00:31:24.369 --> 00:31:26.449
and you see this child drowning in a pond,

574
00:31:26.780 --> 00:31:28.680
and you could save the child, but that would

575
00:31:28.680 --> 00:31:32.410
involve wading into the pond, damaging your fairly expensive

576
00:31:32.410 --> 00:31:34.780
shoes and making the bottom of your pants wet

577
00:31:34.780 --> 00:31:37.660
and have you arrive at work a little late.

578
00:31:38.219 --> 00:31:40.839
And most people would reach the conclusion that it

579
00:31:40.839 --> 00:31:43.800
would be morally indecent, morally wrong, if you were

580
00:31:43.800 --> 00:31:46.439
to just walk by the child and allow the

581
00:31:46.439 --> 00:31:48.119
child to die. What you ought to do is

582
00:31:48.119 --> 00:31:51.079
step in. Because you are in this case preventing

583
00:31:51.079 --> 00:31:53.859
something very bad from happening, uh, and it comes

584
00:31:53.859 --> 00:31:55.739
only at the cost of something that is not

585
00:31:55.739 --> 00:32:01.250
of comparable moral significance. And that seems perfectly reasonable

586
00:32:01.250 --> 00:32:04.589
in the single case. But the worry is that

587
00:32:04.589 --> 00:32:08.109
if you aggregate that over all the possible instances

588
00:32:08.109 --> 00:32:11.800
that you could save somebody uh through charitable donations,

589
00:32:12.150 --> 00:32:15.550
then you reach his conclusion that you ought to

590
00:32:15.550 --> 00:32:19.670
give away most of what you have. And indeed

591
00:32:19.670 --> 00:32:22.459
beyond that, in fact, even choose your profession based

592
00:32:22.459 --> 00:32:26.989
on what can generate the most resources. So perhaps

593
00:32:26.989 --> 00:32:29.829
you shouldn't be a podcaster. Uh, PERHAPS you should

594
00:32:29.829 --> 00:32:32.910
work in a higher paying, uh, profession like dentistry,

595
00:32:32.989 --> 00:32:36.229
for example. And, um, and then you could generate

596
00:32:36.229 --> 00:32:40.069
more resources in order to, uh, to feed the

597
00:32:40.069 --> 00:32:44.420
world's poor. And my view is that this really

598
00:32:44.420 --> 00:32:49.910
amounts to a kind of voluntary self-enslavement. Because if

599
00:32:49.910 --> 00:32:52.670
you now need to do the kind of work

600
00:32:52.670 --> 00:32:54.189
that you don't want to do, that you don't

601
00:32:54.189 --> 00:32:57.630
get fulfillment from doing, and you have to do

602
00:32:57.630 --> 00:33:02.469
something else, then, uh, that's, that's like a kind

603
00:33:02.469 --> 00:33:06.630
of enslavement. And that's a pretty significant cost. It

604
00:33:06.630 --> 00:33:09.150
may, it may not be comparable, but it's nonetheless

605
00:33:09.150 --> 00:33:11.750
pretty significant. And I think it's unreasonable to ask

606
00:33:11.750 --> 00:33:15.869
that of individuals, especially when you're not responsible for

607
00:33:15.869 --> 00:33:19.260
having caused all of these problems. If if if

608
00:33:19.260 --> 00:33:22.959
you'd cause somebody's absolute poverty, then I think you'd

609
00:33:22.959 --> 00:33:25.819
have very robust duties to, to bail them out

610
00:33:25.819 --> 00:33:28.579
of that. But if this is a result of

611
00:33:28.579 --> 00:33:31.859
multiple other causes, including, by the way, their parents

612
00:33:31.859 --> 00:33:35.339
producing them. Then it's not clear to me that

613
00:33:35.339 --> 00:33:37.569
the extent of your duty is anything like as

614
00:33:37.579 --> 00:33:40.739
as serious as, as, as he suggests.

615
00:33:42.530 --> 00:33:46.599
When it comes specifically to our consumption of meat

616
00:33:46.599 --> 00:33:50.959
and other animal products, how much suffering and death

617
00:33:51.209 --> 00:33:54.810
are we causing per year? I mean, what numbers

618
00:33:54.810 --> 00:33:56.109
are we talking about?

619
00:33:56.449 --> 00:33:59.449
Billions of animals. In fact, probably billions of animals

620
00:33:59.449 --> 00:34:02.510
every day. Uh, IF you just think about how

621
00:34:02.510 --> 00:34:05.069
many fish are drawn, tiny fish are drawn out

622
00:34:05.069 --> 00:34:07.349
of the ocean, bigger fish as well, the number

623
00:34:07.349 --> 00:34:11.750
of chickens that are killed, uh, cows, sheep, goats,

624
00:34:11.830 --> 00:34:15.070
pigs, uh, there are 8 billion people on the

625
00:34:15.070 --> 00:34:18.500
planet and the vast majority of those eat animals.

626
00:34:18.790 --> 00:34:21.510
We're causing a vast amount of, of death and

627
00:34:21.510 --> 00:34:27.379
suffering. And this has really been exacerbated by the

628
00:34:27.379 --> 00:34:30.469
increase in the number of intensive farms, factory farms.

629
00:34:31.049 --> 00:34:34.398
Where animals suffer even more than they would ordinarily.

630
00:34:36.678 --> 00:34:39.768
And how do we determine if some of these

631
00:34:39.768 --> 00:34:42.248
suffering, and I mean by this suffering, I'm not

632
00:34:42.248 --> 00:34:46.678
only limiting it to the suffering of non-human animals,

633
00:34:46.688 --> 00:34:51.089
but also humans, uh, like for example, extreme poverty

634
00:34:51.089 --> 00:34:54.128
and other things that we talked about when we

635
00:34:54.128 --> 00:34:57.289
were talking about giving aid. I mean, how do

636
00:34:57.289 --> 00:35:01.289
we determine if some of this suffering is justified?

637
00:35:04.399 --> 00:35:07.600
Well, it's on what basis would you think that

638
00:35:07.600 --> 00:35:09.830
any of this suffering is justified?

639
00:35:10.399 --> 00:35:13.360
I mean, I don't know, perhaps because some people

640
00:35:13.360 --> 00:35:16.600
would argue that we get some benefit from it

641
00:35:16.600 --> 00:35:21.235
or more more other people get. Some benefit from

642
00:35:21.235 --> 00:35:21.875
it. Well,

643
00:35:22.114 --> 00:35:25.675
let's separate out I think the, the famine cases,

644
00:35:25.824 --> 00:35:29.435
the absolute poverty cases from the animal cases. The

645
00:35:29.435 --> 00:35:31.915
vast majority of people on earth do not need

646
00:35:31.915 --> 00:35:34.314
to eat animals or animal products in order to

647
00:35:34.314 --> 00:35:37.554
lead healthy lives. And in fact, would probably, probably

648
00:35:37.554 --> 00:35:43.000
lead healthier lives without consuming those products. Now, that's

649
00:35:43.000 --> 00:35:46.159
not true for every last human being, but it's

650
00:35:46.159 --> 00:35:49.159
also not true for every last human being that

651
00:35:49.159 --> 00:35:53.459
failing to eat human beings is uh uh is

652
00:35:53.459 --> 00:35:57.989
uh unnecessary because there are gonna be circumstances where

653
00:35:57.989 --> 00:36:01.189
cannibalism may be necessary to, to save somebody's life.

654
00:36:01.600 --> 00:36:04.360
But that's not ordinarily true and so it's not

655
00:36:04.360 --> 00:36:08.199
a case for ordinarily consuming human flesh. And ditto,

656
00:36:08.489 --> 00:36:10.770
there may be some circumstances in which you need

657
00:36:10.770 --> 00:36:13.250
to eat animal flesh in order to survive, but

658
00:36:13.250 --> 00:36:15.649
that's not true in most cases.

659
00:36:17.100 --> 00:36:21.709
Mhm. So, uh, but do you think that when

660
00:36:21.709 --> 00:36:25.750
it comes to using animal products or, or, or

661
00:36:25.750 --> 00:36:30.620
using animals themselves, like, for example, not eating them,

662
00:36:30.709 --> 00:36:35.189
but in testing new drugs that uh uh a

663
00:36:35.189 --> 00:36:39.030
case like that is morally justified or not.

664
00:36:39.340 --> 00:36:42.010
Well, first of all, animal experimentation falls, I think

665
00:36:42.010 --> 00:36:44.729
outside of the purview of this particular book because

666
00:36:44.729 --> 00:36:46.929
this is now no longer a quotidian ethical issue.

667
00:36:46.969 --> 00:36:50.689
It's an issue in, in scientific ethics or bioethics

668
00:36:50.689 --> 00:36:53.610
more generally. Uh, AND there are lots of questions

669
00:36:53.610 --> 00:36:57.949
there, uh. It's a much more complicated story precisely

670
00:36:57.949 --> 00:37:00.239
because people think that something more important is at

671
00:37:00.239 --> 00:37:03.510
stake than is at stake, if you excuse that

672
00:37:03.510 --> 00:37:07.590
expression, uh, when people are eating steaks. But there

673
00:37:07.590 --> 00:37:09.379
are lots of problems. I mean, first of all,

674
00:37:11.689 --> 00:37:14.000
Animals are not the best models for human disease,

675
00:37:14.370 --> 00:37:17.409
precisely because they're a different species. The the best

676
00:37:17.409 --> 00:37:21.830
models for human disease are gonna be humans. And

677
00:37:22.159 --> 00:37:23.520
a lot of people want to say, well, but

678
00:37:23.520 --> 00:37:27.239
humans may not be experimented on. And a lot

679
00:37:27.239 --> 00:37:29.310
of animal rights advocates are going to say, yeah,

680
00:37:29.320 --> 00:37:32.510
and animals may also not be experimented on. So,

681
00:37:33.280 --> 00:37:35.560
If you're justifying this on the basis of a

682
00:37:35.560 --> 00:37:39.469
utilitarian calculus, you're saying that sacrificing some small number

683
00:37:39.469 --> 00:37:42.719
could produce a great amount of good. It may

684
00:37:42.719 --> 00:37:45.000
well be that by sacrificing a small number of

685
00:37:45.000 --> 00:37:47.969
human beings, you'd actually produce more good because you'd

686
00:37:47.969 --> 00:37:50.439
have a much more reliable model for what you're

687
00:37:50.439 --> 00:37:54.120
testing. If you're not willing to follow through with

688
00:37:54.120 --> 00:37:55.959
the argument in the case of humans, then it

689
00:37:55.959 --> 00:37:58.679
suggests that you're not merely doing a utilitarian calculation.

690
00:37:58.719 --> 00:38:02.010
There's something else that's going on when you Uh,

691
00:38:02.090 --> 00:38:05.530
when you invoke that argument to justify animal experimentation,

692
00:38:05.689 --> 00:38:10.350
non-human animal experimentation. So there, there, there are lots

693
00:38:10.350 --> 00:38:12.340
of issues there which I suppose we shouldn't really

694
00:38:12.340 --> 00:38:15.149
enter into now because they do indeed go beyond

695
00:38:15.149 --> 00:38:17.550
the domain of quotidian ethics.

696
00:38:18.229 --> 00:38:21.260
Right. So, I would like to get now a

697
00:38:21.260 --> 00:38:24.689
little bit into the domain of language and certain

698
00:38:24.689 --> 00:38:28.969
uses of language. So, are there instances where it

699
00:38:28.969 --> 00:38:33.770
can be immoral, immoral to utter or write certain

700
00:38:33.770 --> 00:38:35.169
words or expressions?

701
00:38:37.169 --> 00:38:40.639
Well, I do distinguish as many philosophers do between

702
00:38:40.639 --> 00:38:45.370
using and mentioning a word. And I think that

703
00:38:45.370 --> 00:38:47.810
very often using certain words is going to be

704
00:38:47.810 --> 00:38:53.639
wrong because certain words are intended as insults. And

705
00:38:53.889 --> 00:38:56.489
unless you've got good reason for insulting somebody, you

706
00:38:56.489 --> 00:38:59.260
shouldn't be doing it. I'm, I'm not saying insulting

707
00:38:59.260 --> 00:39:02.689
somebody is never justified, but it's typically not going

708
00:39:02.689 --> 00:39:04.449
to be justified on the basis, let's say of

709
00:39:04.449 --> 00:39:08.479
somebody's race or their sex or their sexual orientation.

710
00:39:09.010 --> 00:39:12.330
Those kinds of slurs or insults, very hard to

711
00:39:12.330 --> 00:39:14.689
see how they could be, uh, they could be

712
00:39:14.689 --> 00:39:18.229
justified. And so I think that That those are

713
00:39:18.229 --> 00:39:21.229
gonna be wrong. But what many people don't do

714
00:39:21.229 --> 00:39:25.459
is they don't recognize the distinction between using uh

715
00:39:25.459 --> 00:39:30.030
a slur, for example, and mentioning it. When you

716
00:39:30.030 --> 00:39:32.800
mention the slurs, you're not using it. You're not

717
00:39:33.879 --> 00:39:36.620
directing a slur at somebody, there's a sense in

718
00:39:36.620 --> 00:39:42.750
which you're really referring to it. And uh I

719
00:39:42.750 --> 00:39:45.340
think the mistake that people make here is they

720
00:39:45.870 --> 00:39:51.060
attribute a kind of magical power to either the

721
00:39:51.310 --> 00:39:56.189
auditory sounds or to the scratchings that make up

722
00:39:56.189 --> 00:40:00.540
a word. They think that merely uttering it, merely

723
00:40:00.540 --> 00:40:03.540
making that sound or merely making those scratchings by

724
00:40:03.540 --> 00:40:06.350
writing the word has a kind of magical power

725
00:40:06.659 --> 00:40:09.780
that can be harmful and therefore one ought not

726
00:40:09.780 --> 00:40:12.500
to do that. And that to me is just

727
00:40:12.500 --> 00:40:15.439
ridiculous. That is magical thinking. That is no different,

728
00:40:15.540 --> 00:40:19.939
let's say, from people who want to invoke blasphemy

729
00:40:19.939 --> 00:40:24.080
laws and say, well, merely mentioning The name of

730
00:40:24.080 --> 00:40:27.550
God, for example, in the wrong context, amounts to,

731
00:40:27.879 --> 00:40:30.239
amounts to blasphemy. And I do in the book

732
00:40:30.239 --> 00:40:32.959
refer to that wonderful scene in Monty Python's The

733
00:40:32.959 --> 00:40:36.229
Life of Brian, uh, where there, there's a character

734
00:40:36.229 --> 00:40:40.320
who's being stoned for, for blaspheming. And it was

735
00:40:40.320 --> 00:40:44.080
very quickly becomes meta. And when his action is

736
00:40:44.080 --> 00:40:48.330
described, The person who's overseeing the the, the stoning

737
00:40:48.540 --> 00:40:52.090
is now mentioning the name of God rather than

738
00:40:52.090 --> 00:40:54.419
using it in a blasphemous way, and then he

739
00:40:54.419 --> 00:40:56.860
gets stoned. And we can all look on this

740
00:40:56.860 --> 00:40:58.620
and laugh at it, but I think a lot

741
00:40:58.620 --> 00:41:01.070
of people are not going to recognize that the

742
00:41:01.070 --> 00:41:03.899
very same dynamic is at work, the very same

743
00:41:03.899 --> 00:41:07.939
mistake is at work, uh, in, in the mention

744
00:41:07.939 --> 00:41:10.810
of other words today. Not typically the, the name

745
00:41:10.810 --> 00:41:12.449
of God, but other words.

746
00:41:14.370 --> 00:41:18.939
What about talking about today? How about preferred pronouns?

747
00:41:18.969 --> 00:41:23.129
Do you think that people who use nonconventional pronouns

748
00:41:23.129 --> 00:41:27.250
can demand that other people use them? And if

749
00:41:27.250 --> 00:41:32.250
so, would that make intentionally using the incorrect pronouns

750
00:41:32.250 --> 00:41:34.120
something immoral or not?

751
00:41:35.340 --> 00:41:37.449
Well, I do look into this in quite some

752
00:41:37.449 --> 00:41:40.000
detail and with quite some nuance. I do think

753
00:41:40.000 --> 00:41:45.290
that there's a general, although defeasible principle that we

754
00:41:45.290 --> 00:41:47.649
ought to refer to people the way they would

755
00:41:47.649 --> 00:41:51.449
like to be referred to. So, uh, if somebody

756
00:41:51.449 --> 00:41:53.489
has a preference for a particular pronoun, I think

757
00:41:53.489 --> 00:41:57.159
that there is a defeasible presumption in favor of,

758
00:41:57.169 --> 00:42:02.389
of doing that. But there may be circumstances where

759
00:42:02.679 --> 00:42:04.600
you're not morally obliged. I mean, let's imagine somebody

760
00:42:04.600 --> 00:42:09.040
is changing their pronouns every second day and let's

761
00:42:09.040 --> 00:42:13.510
imagine the pronouns they're choosing are highly unusual ones,

762
00:42:14.219 --> 00:42:16.530
uh, sort of a weird constructs of their own

763
00:42:16.530 --> 00:42:20.290
making. Uh, AND it's really hard, especially for people

764
00:42:20.290 --> 00:42:23.010
who struggle even to remember people's names, to now

765
00:42:23.010 --> 00:42:26.169
remember these pronouns as well. The suggestion that you

766
00:42:26.169 --> 00:42:29.889
might not be required, morally required under these circumstances

767
00:42:30.250 --> 00:42:32.649
might be just asking too much of people. So

768
00:42:32.649 --> 00:42:36.419
I can imagine circumstances where you wouldn't be obliged

769
00:42:36.419 --> 00:42:40.209
to do that. But uh the general presumption I

770
00:42:40.209 --> 00:42:42.090
think is referred to people the way they would

771
00:42:42.090 --> 00:42:44.129
like to be uh referred to.

772
00:42:45.169 --> 00:42:47.969
Mhm. So, uh, I would like to ask you

773
00:42:47.969 --> 00:42:51.689
now about some, uh some debates that have been

774
00:42:51.689 --> 00:42:55.770
occurring surrounding the limits of humor. So in what

775
00:42:55.770 --> 00:42:59.969
ways could humor raise moral questions? Are there specific

776
00:42:59.969 --> 00:43:04.129
kinds of humor that should be considered immoral, and

777
00:43:04.129 --> 00:43:07.050
why should we even care about the ethics of

778
00:43:07.050 --> 00:43:07.610
humor?

779
00:43:09.820 --> 00:43:12.620
Good. Well, I think there are two possible grounds

780
00:43:12.620 --> 00:43:15.199
on which we could find fault with humor. The

781
00:43:15.199 --> 00:43:18.179
one is if the humor is an expression of

782
00:43:18.179 --> 00:43:21.689
some defect in the agent. And by the agent,

783
00:43:21.699 --> 00:43:25.419
I mean, either the person who's telling the joke

784
00:43:25.419 --> 00:43:28.750
or the uh or offering the humor or the

785
00:43:28.750 --> 00:43:31.919
person who is appreciating it. So that's one is

786
00:43:31.919 --> 00:43:34.909
the a defect in the agent. The other is

787
00:43:35.520 --> 00:43:41.760
some adverse consequence, and unjustified adverse consequence that results

788
00:43:41.760 --> 00:43:44.409
from some humor. So let me give you examples

789
00:43:44.409 --> 00:43:45.929
of each of them. I mean, let's imagine you

790
00:43:45.929 --> 00:43:48.330
have somebody who is deeply prejudiced, let's say about

791
00:43:48.330 --> 00:43:51.929
a particular racial group. And they now start telling

792
00:43:51.929 --> 00:43:55.800
jokes about that racial group. And other people around

793
00:43:55.800 --> 00:43:58.489
them are appreciating those jokes precisely because they share

794
00:43:58.489 --> 00:44:01.570
those prejudices. Well, now we can find fault with

795
00:44:01.570 --> 00:44:05.250
that humor because it is an expression of uh

796
00:44:05.250 --> 00:44:09.439
the, of the prejudice. But the very same jokes

797
00:44:09.439 --> 00:44:12.879
might not be an expression of prejudice in the

798
00:44:12.879 --> 00:44:14.679
mouths of other people or in the heads of

799
00:44:14.679 --> 00:44:17.199
other people. Other people might enjoy those same jokes

800
00:44:17.199 --> 00:44:20.479
for for another reason. And then I think we

801
00:44:20.479 --> 00:44:24.080
would, we would be justified in not finding fault

802
00:44:24.080 --> 00:44:27.879
with uh, with their appreciation or their utterance of

803
00:44:27.879 --> 00:44:30.800
the, of the jokes. We look at the consequences.

804
00:44:32.110 --> 00:44:35.969
You can imagine a highly combustible environment. Imagine it's

805
00:44:35.969 --> 00:44:38.250
let's say a meeting of the Ku Klux Klux

806
00:44:38.250 --> 00:44:43.050
Klan, and they're telling anti-African American jokes. This could

807
00:44:43.050 --> 00:44:46.090
fuel up, uh, the, the people who are gathered

808
00:44:46.090 --> 00:44:48.610
around and they could go out and they could

809
00:44:48.610 --> 00:44:52.489
perpetrate some real wrongful harm. And I think we

810
00:44:52.489 --> 00:44:54.449
would have a very different and should have a

811
00:44:54.449 --> 00:44:58.060
very different attitude, uh, to The telling of a

812
00:44:58.060 --> 00:45:01.100
joke in that context, then let's say if Dave

813
00:45:01.100 --> 00:45:05.209
Chappelle, the African American comedian, is telling the same

814
00:45:05.209 --> 00:45:07.659
joke or offering the same kind of humor to

815
00:45:07.659 --> 00:45:10.620
a relatively enlightened audience where we are not going

816
00:45:10.620 --> 00:45:14.060
to expect and shouldn't reasonably expect a consequence of,

817
00:45:14.090 --> 00:45:17.580
of that kind to uh to result. So my

818
00:45:17.580 --> 00:45:22.219
sense is generally, we need to be highly context

819
00:45:22.219 --> 00:45:25.419
sensitive in our evaluation of uh of humor.

820
00:45:27.229 --> 00:45:31.709
But uh just by someone getting offended by a

821
00:45:31.709 --> 00:45:35.830
particular joke, does that make it morally problematic?

822
00:45:36.649 --> 00:45:38.560
No, I don't think so, because I think people

823
00:45:38.560 --> 00:45:42.760
can be hypersensitive. They can be misguided. Uh, AND

824
00:45:42.889 --> 00:45:46.399
we, we're not obliged to just to defer to

825
00:45:46.889 --> 00:45:51.610
any inappropriate, uh, and wrongful perception of something. Now

826
00:45:51.610 --> 00:45:54.370
it's not that it may not factor in. It

827
00:45:54.370 --> 00:45:56.330
may be that if you know somebody gets offended

828
00:45:56.330 --> 00:45:58.209
by a particular kind of joke, you should just

829
00:45:58.209 --> 00:46:00.969
rather not tell it to them if, if they're

830
00:46:00.969 --> 00:46:05.560
the intended audience for your joke. But let's imagine

831
00:46:05.889 --> 00:46:09.870
You're broadcasting a joke over a public broadcaster, and

832
00:46:09.870 --> 00:46:11.110
you know that there are lots of people who

833
00:46:11.110 --> 00:46:13.229
are going to appreciate it. The mere fact that

834
00:46:13.229 --> 00:46:15.949
there's an individual or some small number of individuals

835
00:46:15.949 --> 00:46:18.229
out there who will take offense at it. That

836
00:46:18.229 --> 00:46:24.110
by itself is not a definitive. Indication that you

837
00:46:24.110 --> 00:46:25.030
should not tell the joke.

838
00:46:25.830 --> 00:46:29.500
Mhm. So let me ask you then just one

839
00:46:29.500 --> 00:46:34.010
last question. Uh, WE'VE talked about some of these

840
00:46:34.010 --> 00:46:37.239
related to some pairs of topics that you explore

841
00:46:37.239 --> 00:46:39.300
in the book, but do you think that there

842
00:46:39.300 --> 00:46:42.860
are any common threads that run through all the

843
00:46:42.860 --> 00:46:45.659
ethical issues you explore in your book?

844
00:46:47.429 --> 00:46:49.979
I'm not sure about all, but there's certainly threads

845
00:46:49.979 --> 00:46:53.620
that run through a number of, of topics. So

846
00:46:53.620 --> 00:46:57.179
for example, the question about causal inefficacy that I

847
00:46:57.179 --> 00:47:02.699
mentioned earlier, that arises in the environmental case as

848
00:47:02.699 --> 00:47:05.020
we discussed. It also arises in the case of

849
00:47:05.020 --> 00:47:08.370
eating meat because if you go into a supermarket

850
00:47:08.370 --> 00:47:12.969
and you have purchased a steak, let's say, It's

851
00:47:12.969 --> 00:47:16.489
not clear that that by itself is going to

852
00:47:16.489 --> 00:47:18.689
result in increased demand for steaks and leads to

853
00:47:18.689 --> 00:47:21.689
the suffering of any further animals. And so the

854
00:47:21.689 --> 00:47:27.280
question arises there. Uh, IT also, uh, arises in

855
00:47:28.110 --> 00:47:30.500
Uh, in, in, in an array of, of other

856
00:47:30.500 --> 00:47:35.179
topics. So smoking, for example, if you, if you

857
00:47:35.179 --> 00:47:38.510
smoke in the presence of a non-smoker, the effect,

858
00:47:38.860 --> 00:47:41.010
the ill effect that that's going to have on

859
00:47:41.260 --> 00:47:44.929
the non-smoker is negligible. There's a, it's a very

860
00:47:44.929 --> 00:47:49.500
minor increase in, in risk. But obviously, if this

861
00:47:49.500 --> 00:47:52.620
is aggregated across many smokers, then you're going to

862
00:47:52.620 --> 00:47:56.270
have uh much higher increased levels of risk for,

863
00:47:56.310 --> 00:47:59.050
for the non-smokers. And so one of the things

864
00:47:59.050 --> 00:48:01.169
that I do in the conclusion is look at

865
00:48:01.169 --> 00:48:04.050
that problem of causal inefficacy. I point out not

866
00:48:04.050 --> 00:48:06.850
only the similarities of how it arises in these

867
00:48:06.850 --> 00:48:10.250
different cases, but also differences between them. And there

868
00:48:10.250 --> 00:48:13.689
are quite, quite crucial differences between them that I

869
00:48:13.689 --> 00:48:16.489
think should lead us to evaluate these different cases

870
00:48:16.489 --> 00:48:20.489
in different ways. So for example, in the environmental

871
00:48:20.489 --> 00:48:26.709
case, The, the harm arises through an aggregation of

872
00:48:26.709 --> 00:48:32.469
harmless activities. Whereas in the animal case, the harm

873
00:48:32.469 --> 00:48:38.030
gets obscured by an aggregation of individual cases. If

874
00:48:38.030 --> 00:48:39.709
you had to kill a cow in order to

875
00:48:39.709 --> 00:48:42.110
eat it, the connection between the eating and the

876
00:48:42.110 --> 00:48:45.030
killing would be very obvious. It's when you have

877
00:48:45.030 --> 00:48:49.979
an aggregation of, of meat eating and the outsourcing,

878
00:48:49.989 --> 00:48:52.550
as it were, of the killing and butchering and

879
00:48:52.560 --> 00:48:55.070
and preparation of the meat. That the harm that

880
00:48:55.070 --> 00:48:58.310
you do now gets obscured, uh, through the aggregation

881
00:48:58.310 --> 00:49:00.790
of multiple activities. And I think that is a

882
00:49:00.790 --> 00:49:02.989
relevant difference to what we should in fact do

883
00:49:02.989 --> 00:49:06.969
in practice. I think it suggests we should not

884
00:49:06.969 --> 00:49:09.689
be eating animals, uh, ever. I mean, well, almost

885
00:49:09.689 --> 00:49:13.370
ever. There may be highly exceptional cases, but we

886
00:49:13.370 --> 00:49:17.100
should generally not be eating animals, whereas it's permissible

887
00:49:17.100 --> 00:49:21.639
to engage in some carbon emitting uh practices because

888
00:49:21.639 --> 00:49:24.679
that is in itself harmless. It's when it aggregates

889
00:49:24.679 --> 00:49:26.080
that it becomes problematic.

890
00:49:26.850 --> 00:49:30.949
Mhm. So, the book is again very practical ethics,

891
00:49:30.959 --> 00:49:33.929
engaging everyday moral questions, I'm leaving a link to

892
00:49:33.929 --> 00:49:37.060
it in the description of the interview. And Doctor

893
00:49:37.060 --> 00:49:41.489
Benatar, just before we go, are there any particular

894
00:49:41.489 --> 00:49:44.090
places on the internet where people can find your

895
00:49:44.090 --> 00:49:44.520
work?

896
00:49:47.250 --> 00:49:51.429
None that I control. So I, I'm afraid I'm

897
00:49:51.429 --> 00:49:54.790
not, I'm not very much online. So people just

898
00:49:54.790 --> 00:49:56.570
have to look, but thank

899
00:49:56.570 --> 00:49:59.199
you. OK, fair enough. So thank you so much

900
00:49:59.199 --> 00:50:01.129
for taking the time to come on the show

901
00:50:01.129 --> 00:50:02.929
again. It's always a pleasure to talk with you.

902
00:50:03.739 --> 00:50:05.179
Thank you for having me. Always a pleasure to

903
00:50:05.179 --> 00:50:05.810
speak with you.

904
00:50:06.969 --> 00:50:09.459
Hi guys, thank you for watching this interview until

905
00:50:09.459 --> 00:50:11.639
the end. If you liked it, please share it,

906
00:50:11.810 --> 00:50:14.600
leave a like and hit the subscription button. The

907
00:50:14.600 --> 00:50:16.800
show is brought to you by Nights Learning and

908
00:50:16.800 --> 00:50:20.879
Development done differently, check their website at Nights.com and

909
00:50:20.879 --> 00:50:24.600
also please consider supporting the show on Patreon or

910
00:50:24.600 --> 00:50:27.080
PayPal. I would also like to give a huge

911
00:50:27.080 --> 00:50:30.189
thank you to my main patrons and PayPal supporters

912
00:50:30.600 --> 00:50:34.399
Pergo Larsson, Jerry Mullerns, Fredrik Sundo, Bernard Seyches Olaf,

913
00:50:34.520 --> 00:50:38.239
Alexandam Castle, Matthew Whitting Berarna Wolf, Tim Hollis, Erika

914
00:50:38.239 --> 00:50:41.449
Lenny, John Connors, Philip Fors Connolly. Then the Matter

915
00:50:41.449 --> 00:50:46.270
Robert Windegaruyasi Zu Mark Neevs called Holbrookfield governor Michael

916
00:50:46.270 --> 00:50:50.669
Stormir, Samuel Andre, Francis Forti Agnsergoro and Hal Herzognun

917
00:50:50.669 --> 00:50:54.830
Macha Joan Labran Juarsent and Samuel Corriere, Heinz, Mark

918
00:50:54.830 --> 00:50:58.750
Smith, Jore, Tom Hummel, Sardus France David Sloan Wilson,

919
00:50:58.750 --> 00:51:03.840
asilla dearraujurumen ro Diego Londono Correa. Yannick Punteran Rosmani

920
00:51:03.840 --> 00:51:09.020
Charlotte blinikolbar Adamhn Pavlostaevsky nale back medicine, Gary Galman

921
00:51:09.020 --> 00:51:13.500
Sam of Zallidrianei Poltonin John Barboza, Julian Price, Edward

922
00:51:13.500 --> 00:51:18.340
Hall Edin Bronner, Douglas Fry, Franco Bartolotti Gabrielon Corteseus

923
00:51:18.340 --> 00:51:22.500
Slelitsky, Scott Zachary Fish Tim Duffyani Smith John Wieman.

924
00:51:22.850 --> 00:51:27.399
Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai Giorgio

925
00:51:27.399 --> 00:51:32.530
Theophanous, Chris Williamson, Peter Vozin, David Williams, Diocosta, Anton

926
00:51:32.530 --> 00:51:37.129
Eriksson, Charles Murray, Alex Shaw, Marie Martinez, Coralli Chevalier,

927
00:51:37.280 --> 00:51:42.239
bungalow atheists, Larry D. Lee Junior, old Eringbo. Sterry

928
00:51:42.239 --> 00:51:46.229
Michael Bailey, then Sperber, Robert Gray Zigoren, Jeff McMann,

929
00:51:46.399 --> 00:51:50.560
Jake Zu, Barnabas radix, Mark Campbell, Thomas Dovner, Luke

930
00:51:50.560 --> 00:51:55.080
Neeson, Chris Stor, Kimberly Johnson, Benjamin Gilbert, Jessica Nowicki,

931
00:51:55.129 --> 00:52:00.889
Linda Brendon, Nicholas Carlsson, Ismael Bensleyman. George Eoriatis, Valentin

932
00:52:00.889 --> 00:52:07.010
Steinman, Perkrolis, Kate van Goller, Alexander Hubbert, Liam Dunaway,

933
00:52:07.120 --> 00:52:12.649
BR Masoud Ali Mohammadi, Perpendicular John Nertner, Ursula Gudinov,

934
00:52:12.800 --> 00:52:17.439
Gregory Hastings, David Pinsoff Sean Nelson, Mike Levine, and

935
00:52:17.439 --> 00:52:20.620
Jos Net. A special thanks to my producers. These

936
00:52:20.620 --> 00:52:24.149
are Webb, Jim, Frank Lucas Steffinik, Tom Venneden, Bernard

937
00:52:24.149 --> 00:52:28.379
Curtis Dixon, Benedic Muller, Thomas Trumbull, Catherine and Patrick

938
00:52:28.379 --> 00:52:31.709
Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick Golden,

939
00:52:31.979 --> 00:52:35.219
and to my executive producers, Matthew Levender, Sergio Quadrian,

940
00:52:35.340 --> 00:52:37.969
Bogdan Kanivets, and Rosie. Thank you for all.

