WEBVTT

1
00:00:00.479 --> 00:00:03.190
Hello, everyone. Welcome to a new episode of the

2
00:00:03.190 --> 00:00:06.099
Dissenter. I'm your host, as always, Ricardo Lopez, and

3
00:00:06.099 --> 00:00:09.029
today I'm joined by Doctor Elizabeth Bick. She's a

4
00:00:09.029 --> 00:00:13.590
microbiologist and scientific integrity consultant. And today we're going

5
00:00:13.590 --> 00:00:17.500
to talk about mainly the topics of scientific integrity

6
00:00:17.709 --> 00:00:20.909
and scientific fraud. So, Doctor Bick, welcome to the

7
00:00:20.909 --> 00:00:22.819
show. It's a huge pleasure to everyone.

8
00:00:23.190 --> 00:00:25.069
My pleasure to be here with you, Ricardo.

9
00:00:25.899 --> 00:00:30.059
So let's start perhaps with some definitions or uh

10
00:00:30.059 --> 00:00:33.180
to explain some terms here, some basic terms. So

11
00:00:33.180 --> 00:00:36.650
where is integrity in the context of science?

12
00:00:37.779 --> 00:00:43.439
Well, integrity is about honesty, about, uh, truthfully reporting

13
00:00:43.439 --> 00:00:46.939
what you have found during your experiments. So, and

14
00:00:46.939 --> 00:00:49.259
that, that is actually, I think for me, the

15
00:00:49.259 --> 00:00:52.540
heart of what science should be. It's, it's about

16
00:00:52.540 --> 00:00:56.819
finding the truth. So we do interviews, we research,

17
00:00:56.900 --> 00:00:59.540
we are in the lab, uh, we try to

18
00:00:59.540 --> 00:01:02.662
cure patients. But I feel when we write our

19
00:01:02.662 --> 00:01:06.532
scientific papers about it, we need to truthfully report

20
00:01:06.532 --> 00:01:09.693
on what we have found. So basically, it's about

21
00:01:09.693 --> 00:01:13.413
honesty and, and not lying about really reporting what

22
00:01:13.413 --> 00:01:16.212
you found, not leaving out the results you didn't

23
00:01:16.212 --> 00:01:22.172
want, um, not cutting corners, also being um ethical

24
00:01:22.172 --> 00:01:24.585
in a way to your uh to your patients,

25
00:01:24.725 --> 00:01:27.286
to your animals that you might use in animal

26
00:01:27.286 --> 00:01:31.816
experiments, to your co-authors. Now, usually, science integrity is

27
00:01:31.816 --> 00:01:33.886
not, it, it, it depends a little bit on

28
00:01:33.886 --> 00:01:36.966
the definition, but for me, it's this whole, this

29
00:01:36.966 --> 00:01:40.075
whole range, uh, this whole spectrum of being honest

30
00:01:40.475 --> 00:01:44.725
towards your data and your colleagues and your, um,

31
00:01:44.926 --> 00:01:46.435
yeah, the experiments that you do.

32
00:01:47.055 --> 00:01:50.365
And on the other hand, what is scientific fraud?

33
00:01:50.445 --> 00:01:52.566
What counts as scientific fraud?

34
00:01:53.069 --> 00:01:56.720
Well, there, uh, it's, it's basically cheating and lying.

35
00:01:56.809 --> 00:02:01.959
Um, THERE'S, uh, there's different, different definitions for countries

36
00:02:01.959 --> 00:02:05.250
in the United States where I live. Um, IT'S

37
00:02:05.250 --> 00:02:08.449
usually one of three things. So there's, there's 3

38
00:02:08.449 --> 00:02:11.970
types of science fraud that you could, um, that

39
00:02:11.970 --> 00:02:13.770
you could, could look at. So, the first one

40
00:02:13.770 --> 00:02:18.320
is plagiarism. Where you would, um, you would copy

41
00:02:18.320 --> 00:02:21.750
somebody else's text or ideas, but not give credit

42
00:02:21.750 --> 00:02:24.750
to the person who was, who was first. So

43
00:02:24.750 --> 00:02:28.119
that is one type of misconduct. The second type

44
00:02:28.119 --> 00:02:33.080
is falsification, where a person obtains results, but changes

45
00:02:33.080 --> 00:02:36.679
them a little bit. Let's say. Um, A particular

46
00:02:36.679 --> 00:02:40.960
patient or a particular experiment had an outcome, but

47
00:02:40.960 --> 00:02:44.240
it didn't quite was the outcome you had hoped

48
00:02:44.240 --> 00:02:46.240
as a researcher. So you change some of the

49
00:02:46.240 --> 00:02:48.910
data and now, oh, suddenly it's a positive or

50
00:02:48.910 --> 00:02:52.639
a negative, and that would be falsification. And the

51
00:02:52.639 --> 00:02:57.240
third type of misconduct is fabrication, and that is

52
00:02:57.240 --> 00:03:00.979
completely making up results. So typing in some numbers

53
00:03:00.979 --> 00:03:04.669
in a spreadsheet, making a graph, etc. WITHOUT actually

54
00:03:04.919 --> 00:03:07.279
going in the lab and making, uh, and, and

55
00:03:07.279 --> 00:03:10.119
doing measurements and things like that. So completely making

56
00:03:10.119 --> 00:03:12.309
up results. So those are the, the three types

57
00:03:12.309 --> 00:03:16.830
of science misconduct, but Those are the more extreme

58
00:03:16.830 --> 00:03:19.210
types. There's, there's a lot of in between, in

59
00:03:19.210 --> 00:03:23.169
between being completely honest and completely fraudulent. There's a

60
00:03:23.169 --> 00:03:26.139
lot of gray zone. So usually we call those

61
00:03:26.139 --> 00:03:29.899
questionable research practices, uh, where, let's say you're a

62
00:03:29.899 --> 00:03:33.339
little bit sloppy, you don't label your, your data

63
00:03:33.339 --> 00:03:36.139
very well, and so you, you make an error

64
00:03:36.139 --> 00:03:38.059
when you report on the data. And, and that's

65
00:03:38.059 --> 00:03:40.619
a very big middle zone. And it's sometimes very

66
00:03:40.619 --> 00:03:43.139
hard to know if a person really had an

67
00:03:43.139 --> 00:03:47.220
intention to mislead, which would be misconduct, or was

68
00:03:47.220 --> 00:03:50.779
just sloppy. There's, it, it's very hard to distinguish

69
00:03:50.779 --> 00:03:52.330
between those, uh those things.

70
00:03:53.240 --> 00:03:56.050
So I also want to understand here what are

71
00:03:56.050 --> 00:03:59.809
some of the factors behind scientific fraud, what leads

72
00:03:59.809 --> 00:04:02.970
to scientific fraud and stuff like that. So, uh

73
00:04:02.970 --> 00:04:05.970
let me start by asking you about perhaps the

74
00:04:05.970 --> 00:04:10.210
individual incentives that people might have in science. So

75
00:04:10.210 --> 00:04:14.850
what individual incentives actually do scientists have to commit

76
00:04:14.850 --> 00:04:15.369
fraud?

77
00:04:16.529 --> 00:04:20.850
Well, we love metrics as scientists, um, and our

78
00:04:20.850 --> 00:04:24.250
bosses love metrics. We love to, uh, to ask

79
00:04:24.250 --> 00:04:27.089
people how many papers did you publish and what

80
00:04:27.089 --> 00:04:29.829
was their impact factor and what was their, uh,

81
00:04:29.899 --> 00:04:33.149
how many times were they cited, your age index,

82
00:04:33.329 --> 00:04:35.540
uh, how many of your papers were in nature

83
00:04:35.540 --> 00:04:39.369
or in science. So scientists, but also the people

84
00:04:39.369 --> 00:04:44.570
above them, the institutions. Um, THE hiring committees, people

85
00:04:44.570 --> 00:04:47.529
who hire new faculty, for example, they love to

86
00:04:47.529 --> 00:04:51.489
look at those numbers, those metrics. And, and yeah,

87
00:04:51.609 --> 00:04:55.769
if you, if you don't have very good metrics,

88
00:04:55.809 --> 00:04:57.850
if you didn't publish that, that much, or you

89
00:04:57.850 --> 00:05:00.510
weren't cited that much, or your results are just

90
00:05:00.510 --> 00:05:03.250
not very nice, so it's hard to publish, it

91
00:05:03.250 --> 00:05:07.130
is very tempting to cheat. It's like doping in

92
00:05:07.130 --> 00:05:10.950
sports, right? Like if you see Everybody using doping

93
00:05:10.950 --> 00:05:13.790
and everybody winning, um, and you're trying to be

94
00:05:13.790 --> 00:05:16.820
honest, but you see everybody around you uses doping,

95
00:05:17.109 --> 00:05:21.230
uh, or in science is cheating. It's very tempting

96
00:05:21.230 --> 00:05:24.059
to also cheat because you see people winning races.

97
00:05:24.109 --> 00:05:27.899
You see people in science publishing papers who cheat.

98
00:05:28.109 --> 00:05:31.299
And so it is. Uh, AS long as there's

99
00:05:31.299 --> 00:05:35.450
no, um, uh, consequences for cheating, and you see

100
00:05:35.450 --> 00:05:37.660
a lot of cheating around you, I think other

101
00:05:37.660 --> 00:05:39.899
people are starting to cheat. So it depends on

102
00:05:39.899 --> 00:05:42.739
your environment. If you, if you grow up as

103
00:05:42.739 --> 00:05:44.739
a scientist, if you work in a lab where

104
00:05:44.739 --> 00:05:48.049
everybody cheats, you're tempted to do it as well,

105
00:05:48.059 --> 00:05:51.010
because otherwise your results are not as great as

106
00:05:51.010 --> 00:05:54.170
those folks around you and And yeah, I think

107
00:05:54.170 --> 00:05:58.130
we put too much emphasis on positive results, on

108
00:05:58.130 --> 00:06:02.010
amazing findings, uh, on, on numbers, on metrics, and

109
00:06:02.010 --> 00:06:05.250
that is ultimately what leads to cheating, but it

110
00:06:05.250 --> 00:06:08.690
also differs from country to country. So, in some

111
00:06:08.690 --> 00:06:12.239
countries, the pressure to publish is higher. There's even

112
00:06:12.480 --> 00:06:15.070
Um, money you can, you can earn if you

113
00:06:15.070 --> 00:06:18.260
publish a paper, um, you might get a promotion

114
00:06:18.260 --> 00:06:20.589
and, and so it depends per country, and that

115
00:06:20.589 --> 00:06:22.709
is why we see that in some countries, there

116
00:06:22.709 --> 00:06:24.869
seem to be a little bit more cheating than

117
00:06:24.869 --> 00:06:27.910
in other countries. It's not because those, those people

118
00:06:27.910 --> 00:06:31.269
are more, uh, you know, are better cheaters or

119
00:06:31.269 --> 00:06:35.390
bigger cheaters. It's because of these monetary incentives or

120
00:06:35.390 --> 00:06:38.670
these other, uh, incentive structures that lead people to

121
00:06:38.670 --> 00:06:39.109
cheat.

122
00:06:40.070 --> 00:06:42.730
This is also tied at least to some extent

123
00:06:42.730 --> 00:06:47.170
with uh career advancements, right, because since the scientists

124
00:06:47.170 --> 00:06:50.730
are sort of forced to publish X number of

125
00:06:50.730 --> 00:06:54.269
papers per year depending on the country, uh, then

126
00:06:54.269 --> 00:06:57.494
that also went. HAS to do with them being

127
00:06:57.494 --> 00:06:59.385
able to advance in their career,

128
00:06:59.494 --> 00:07:02.684
right? Yeah, absolutely. If we, if we really hold

129
00:07:02.684 --> 00:07:06.614
scientists accountable to how many papers they publish, then

130
00:07:06.614 --> 00:07:09.575
people are going to find ways to to make

131
00:07:09.575 --> 00:07:13.549
that happen. But in science, you don't always get

132
00:07:13.549 --> 00:07:15.799
the results you want, uh, you know, your cells

133
00:07:15.799 --> 00:07:18.239
don't want to grow or, or the results are

134
00:07:18.239 --> 00:07:20.640
different than you expected. And I, I feel it's

135
00:07:20.640 --> 00:07:23.959
not the right way to measure if a scientist

136
00:07:23.959 --> 00:07:26.579
is good. Um, WE should not be looking too

137
00:07:26.579 --> 00:07:29.299
much at, at how many. Did you publish and

138
00:07:29.299 --> 00:07:31.950
how many were in science. I don't think that

139
00:07:31.950 --> 00:07:34.339
necessarily tells you if that person is a better

140
00:07:34.339 --> 00:07:37.779
scientist than a person who works slowly and only

141
00:07:37.779 --> 00:07:41.420
publishes maybe one paper for 2 years or, or

142
00:07:41.420 --> 00:07:44.609
even longer. Like, I think good science needs time.

143
00:07:44.640 --> 00:07:48.100
And if we, if we force people to publish

144
00:07:48.100 --> 00:07:52.059
once a year, people are gonna find creative ways

145
00:07:52.059 --> 00:07:54.220
to, to get, make that happen, and that might

146
00:07:54.220 --> 00:07:55.179
involve cheating.

147
00:07:56.070 --> 00:07:59.570
And do you think that there's anything about how

148
00:07:59.570 --> 00:08:04.170
science as an institution is designed that promotes wrong

149
00:08:04.170 --> 00:08:04.959
or not?

150
00:08:05.570 --> 00:08:08.799
Yeah, it, it is because those that focus on,

151
00:08:08.890 --> 00:08:13.029
on, uh on numbers and I think most scientists

152
00:08:13.029 --> 00:08:16.950
who are successful will have um a page saying

153
00:08:16.950 --> 00:08:18.670
how many, you know, a list of all their

154
00:08:18.670 --> 00:08:21.230
papers and a list of all their citations and

155
00:08:21.429 --> 00:08:23.350
all the prizes that they would that they won.

156
00:08:23.429 --> 00:08:26.359
And I think this is how society. Is that,

157
00:08:26.500 --> 00:08:28.339
that's just the way it is. But I think

158
00:08:28.339 --> 00:08:30.380
we focus a little bit too much. And, and

159
00:08:30.380 --> 00:08:34.058
especially when we, let's say, hire, uh, I don't

160
00:08:34.058 --> 00:08:36.580
say we because I don't actually hire people, but

161
00:08:36.580 --> 00:08:41.690
when, when um people are Looking for a new

162
00:08:41.690 --> 00:08:44.679
professor at the university, they will look at resumes

163
00:08:44.890 --> 00:08:47.400
and, you know, as in any other job, obviously,

164
00:08:47.609 --> 00:08:50.010
but we, we put a lot of focus on

165
00:08:50.010 --> 00:08:52.969
how many papers did somebody publish, uh, all these

166
00:08:52.969 --> 00:08:55.929
metrics, all these citations, and universities do the same.

167
00:08:55.969 --> 00:08:57.849
They. They love to be the number one in

168
00:08:57.849 --> 00:09:01.010
their country. They love and those uh lists of

169
00:09:01.010 --> 00:09:04.450
what is the best university is often those lists

170
00:09:04.450 --> 00:09:07.969
are often based also on number of citations, numbers

171
00:09:07.969 --> 00:09:12.010
of Nobel Prizes a university has one number of.

172
00:09:13.070 --> 00:09:16.849
Yeah, uh, science publications or nature publications. So all

173
00:09:16.849 --> 00:09:19.289
of us are in this rat race to look

174
00:09:19.289 --> 00:09:21.450
at these numbers. And yeah, that is just the

175
00:09:21.450 --> 00:09:23.960
way that science is organized. And there's just not

176
00:09:23.960 --> 00:09:28.840
enough focus on things like open science on rigor,

177
00:09:28.969 --> 00:09:33.179
on how, how, uh, you know, how well we,

178
00:09:33.229 --> 00:09:37.010
we, uh, report on our data, uh, not on.

179
00:09:37.210 --> 00:09:40.489
Reproducibility and those are also very important factors of

180
00:09:40.489 --> 00:09:43.690
science, but if you do them as a scientist,

181
00:09:43.770 --> 00:09:47.169
if you let's say work on really good methods

182
00:09:47.169 --> 00:09:49.049
to publish your paper so that other people can

183
00:09:49.049 --> 00:09:52.369
reproduce it or you share all your data, those

184
00:09:52.369 --> 00:09:55.890
things cost time and they're not always rewarded by

185
00:09:55.890 --> 00:09:58.609
the institutions who just focuses on a number of

186
00:09:58.609 --> 00:10:03.070
publications, things like mentorship. Uh, HOW do you, how

187
00:10:03.070 --> 00:10:05.710
do you guide your graduate students, uh, through the

188
00:10:05.710 --> 00:10:08.669
early phases of their career, or how many papers

189
00:10:08.669 --> 00:10:10.869
did you peer review, or how many times did

190
00:10:10.869 --> 00:10:14.500
you do social outreach or, uh, try to, uh,

191
00:10:14.510 --> 00:10:17.010
go to a pub and explain to people. Science

192
00:10:17.010 --> 00:10:20.010
and how great it is, those things are often

193
00:10:20.010 --> 00:10:23.090
not rewarded for us as scientists, and they're, they're

194
00:10:23.090 --> 00:10:26.690
equally good. I feel that those things also make

195
00:10:26.690 --> 00:10:28.929
a really great scientist, but it's really hard to

196
00:10:28.929 --> 00:10:31.210
put them on your resume and to make them

197
00:10:31.210 --> 00:10:31.570
count.

198
00:10:32.590 --> 00:10:35.890
And when it comes to the publication system itself,

199
00:10:36.020 --> 00:10:39.659
is the fact that negative results tend to not

200
00:10:39.659 --> 00:10:42.059
get published or at least not as much as

201
00:10:42.059 --> 00:10:44.940
the positive ones also a big problem?

202
00:10:45.739 --> 00:10:47.820
Yes, that, that is a big problem. And, and

203
00:10:47.820 --> 00:10:51.969
that, that ties back to reproducibility. BUT because sometimes

204
00:10:52.460 --> 00:10:56.900
there, there's a publication with amazing findings, and people

205
00:10:56.900 --> 00:10:59.020
try to reproduce it, and they, they want to

206
00:10:59.020 --> 00:11:01.030
build on that work, and this is how science

207
00:11:01.030 --> 00:11:03.619
work. We build on each other's work, but they

208
00:11:03.619 --> 00:11:06.570
cannot, they cannot seem to get it to work.

209
00:11:06.859 --> 00:11:10.080
And I think I've been in that situation and

210
00:11:10.080 --> 00:11:13.719
probably every scientist, you're like, why doesn't the experiment

211
00:11:13.719 --> 00:11:16.960
work? And you, you start to think of yourself

212
00:11:16.960 --> 00:11:19.599
as a failure. It's very rare that you think,

213
00:11:19.719 --> 00:11:21.880
well, the other people didn't describe it well. You

214
00:11:21.880 --> 00:11:25.119
usually will blame it on yourself. Um, I guess

215
00:11:25.119 --> 00:11:27.750
I didn't do it correctly, uh, you know, I,

216
00:11:27.880 --> 00:11:30.640
I, I was just a couple of seconds too

217
00:11:30.640 --> 00:11:33.000
late, and that's why the experiment failed or things

218
00:11:33.000 --> 00:11:36.775
like that. So we tend to, to Maybe be

219
00:11:36.775 --> 00:11:39.625
ashamed of those results and not publish them. Most

220
00:11:39.625 --> 00:11:42.575
journals will not accept them. Journals are looking for

221
00:11:42.575 --> 00:11:45.705
novel findings that other people would love to hear

222
00:11:45.705 --> 00:11:48.784
about, and there's just, it's very hard to publish

223
00:11:48.784 --> 00:11:52.344
negative results to say, well, we, we couldn't reproduce

224
00:11:52.344 --> 00:11:55.219
this, we couldn't make this work. And very often,

225
00:11:55.349 --> 00:11:59.099
um, there's many reasons why experiments don't work. Sometimes

226
00:11:59.099 --> 00:12:02.429
it's just this little, this little tiny thing that

227
00:12:02.429 --> 00:12:04.950
you didn't think would be important. Let's say the,

228
00:12:05.099 --> 00:12:08.390
the, the temperature in your room is just one

229
00:12:08.390 --> 00:12:11.190
degree too hot or too low, or the shaker

230
00:12:11.190 --> 00:12:13.539
doesn't shake the same way as the other people,

231
00:12:13.789 --> 00:12:18.330
uh shaker or. There, the ozone concentration in the

232
00:12:18.330 --> 00:12:20.179
air is too high and you had no idea

233
00:12:20.179 --> 00:12:22.859
that that was important. But sometimes it could be

234
00:12:22.859 --> 00:12:25.020
fraud. And so there have been a couple of

235
00:12:25.020 --> 00:12:28.700
fraud cases where I was involved in where then

236
00:12:28.700 --> 00:12:31.340
later people said, yeah, we could never reproduce that

237
00:12:31.340 --> 00:12:35.049
paper. And then And and then you have to

238
00:12:35.049 --> 00:12:37.570
think how many people try to reproduce it? How

239
00:12:37.570 --> 00:12:41.789
many people spend money and time and. And tears

240
00:12:41.789 --> 00:12:44.489
trying to make it to work. And, uh, and

241
00:12:44.489 --> 00:12:47.590
that's just such a shame that we don't talk

242
00:12:47.590 --> 00:12:50.229
about these things, that we don't have platforms to

243
00:12:50.229 --> 00:12:53.109
publish negative results. There's a couple of journals who

244
00:12:53.109 --> 00:12:54.950
start to do this, and that's great, but we

245
00:12:54.950 --> 00:12:58.909
need more. And yeah, uh, I think 90% of

246
00:12:58.909 --> 00:13:00.909
the, the things we do don't work and we

247
00:13:00.909 --> 00:13:04.030
never publish them, and it's only the 10% that

248
00:13:04.030 --> 00:13:06.270
does work, it gets published. So there's, there's so

249
00:13:06.270 --> 00:13:09.539
much money being wasted on, on those negative results.

250
00:13:10.530 --> 00:13:12.150
So I would like to ask you a little

251
00:13:12.150 --> 00:13:15.469
bit more about citations now. Are there ways by

252
00:13:15.469 --> 00:13:21.039
which researchers can artificially increase their citation metrics? And

253
00:13:21.039 --> 00:13:23.270
in this particular case, I would like for you

254
00:13:23.270 --> 00:13:27.150
to tell us specifically about the case of citation

255
00:13:27.150 --> 00:13:28.140
rings.

256
00:13:28.869 --> 00:13:32.630
Right. And, uh, I mean, you could, of course,

257
00:13:32.840 --> 00:13:35.630
always sneak in that one extra paper that is

258
00:13:35.630 --> 00:13:38.640
not maybe that relevant to your new paper and

259
00:13:38.640 --> 00:13:40.830
sneak in one of your old papers and, oh,

260
00:13:40.919 --> 00:13:43.599
now you have an extra citation. So I think

261
00:13:43.599 --> 00:13:46.919
most people cite their own work because you, like

262
00:13:46.919 --> 00:13:49.070
I said before, you build on each other's work

263
00:13:49.070 --> 00:13:50.880
and you build on your own work. So sometimes

264
00:13:50.880 --> 00:13:52.880
you will say, oh, we did, we did this

265
00:13:52.880 --> 00:13:54.960
experiment exactly the way we did it in that

266
00:13:54.960 --> 00:13:58.419
older paper. So that would count towards your citations.

267
00:13:58.719 --> 00:14:02.989
Um. I've seen papers where people cites, uh, let's

268
00:14:02.989 --> 00:14:07.820
say they have 50 citations, 50 references, and uh

269
00:14:07.820 --> 00:14:10.229
20 of them are by their own from their

270
00:14:10.229 --> 00:14:12.590
own lab or from their own. So that is

271
00:14:12.590 --> 00:14:16.400
obviously frowned upon, but there's actually no rule and

272
00:14:16.400 --> 00:14:19.909
then reviewer might not even see that. Citation rings,

273
00:14:19.989 --> 00:14:22.830
that's a very creative way. So these are, I'm

274
00:14:22.830 --> 00:14:24.830
not quite sure how it works, but I think

275
00:14:24.830 --> 00:14:27.549
the way it works is that let's say a

276
00:14:27.549 --> 00:14:30.270
group of people, I don't know, 50 people, 50

277
00:14:30.270 --> 00:14:33.510
researchers form a secret group and they say we're

278
00:14:33.510 --> 00:14:36.544
all gonna cite each other's papers. And so it's,

279
00:14:36.614 --> 00:14:40.655
it doesn't really get noticed because, you know, these

280
00:14:40.655 --> 00:14:43.494
people cite other people's papers, but the other folks

281
00:14:43.494 --> 00:14:45.934
in the group cite their papers. And so it

282
00:14:45.934 --> 00:14:48.734
all, in that way you all increase each other's

283
00:14:48.734 --> 00:14:52.184
papers. And, and sometimes we see these papers where

284
00:14:52.794 --> 00:14:57.500
um there's a statement, um. Like some, some general

285
00:14:57.500 --> 00:15:00.460
statement, like I don't know, DNA sequencing has greatly

286
00:15:00.460 --> 00:15:04.179
increased something, uh, and then there's, there's 10 references.

287
00:15:04.260 --> 00:15:09.179
So between brackets it said reference 37 to 46

288
00:15:09.179 --> 00:15:13.299
or so. And all those references are from, from

289
00:15:13.299 --> 00:15:16.419
certain authors and, and if you really notice that

290
00:15:16.700 --> 00:15:19.929
they might actually not. Be related to the topic

291
00:15:19.929 --> 00:15:22.919
that they're cited in the context of. So it,

292
00:15:23.090 --> 00:15:25.489
it's, it's a way that people sneak in references

293
00:15:25.489 --> 00:15:28.159
and very often these are citation rings, but they're

294
00:15:28.159 --> 00:15:31.049
sometimes very hard to recognize if a person, you

295
00:15:31.049 --> 00:15:33.169
know, cites uh 20 of their own papers, of

296
00:15:33.169 --> 00:15:35.260
course, we can see that. But if a person

297
00:15:35.260 --> 00:15:38.890
cites 20 papers that are not very related and

298
00:15:38.890 --> 00:15:41.969
are from that secret pact of, of, uh, of

299
00:15:41.969 --> 00:15:44.969
the citation rings, it's very hard to notice that.

300
00:15:45.989 --> 00:15:46.000
Mhm.

301
00:15:46.570 --> 00:15:50.609
How about plagiarism? Is there lots of plagiarism in

302
00:15:50.609 --> 00:15:52.200
the scientific literature?

303
00:15:53.440 --> 00:15:56.380
Not as much as there was, let's say in

304
00:15:56.380 --> 00:16:00.820
the early 2000s. So I actually started to become

305
00:16:00.820 --> 00:16:04.900
a science detective by, because somebody had plagiarized my

306
00:16:04.900 --> 00:16:07.299
work and I, I found out about it and

307
00:16:07.299 --> 00:16:10.500
I was very mad and, um, and the paper

308
00:16:10.500 --> 00:16:14.090
was I think around, published around maybe 2010 or

309
00:16:14.090 --> 00:16:18.239
so, so I found it in 2013 and I

310
00:16:18.239 --> 00:16:21.710
think in that time journals weren't really checking for

311
00:16:21.710 --> 00:16:25.309
plagiarism very often. Um, IT was started that there

312
00:16:25.309 --> 00:16:28.020
are some tools to check for plagiarism. It's basically

313
00:16:28.020 --> 00:16:30.549
a glorified Google search where you just search if

314
00:16:30.549 --> 00:16:33.950
somebody else used your text and um so later

315
00:16:33.950 --> 00:16:39.200
journals, let's say around 20. Maybe 2010 or 2015

316
00:16:39.200 --> 00:16:41.640
journals were starting more and more to use plagiarism

317
00:16:41.640 --> 00:16:45.719
checkers. So I think real plagiarism is pretty rare

318
00:16:45.719 --> 00:16:50.219
because it gets caught by both journals. Um, AND

319
00:16:50.219 --> 00:16:53.719
I'm not talking about one or two sentences or

320
00:16:53.719 --> 00:16:56.599
a couple of sentences in the methods where you

321
00:16:56.599 --> 00:16:59.799
just described how you did a particular standard thing.

322
00:17:00.119 --> 00:17:02.960
Those things I don't call plagiarism. I'm talking about

323
00:17:02.960 --> 00:17:05.760
complete review articles that were completely plagiarized or were

324
00:17:05.760 --> 00:17:08.614
a whole pair. GRAPHS were copy pasted. So that

325
00:17:08.614 --> 00:17:11.354
is, that is, you know, the really bad plagiarism

326
00:17:11.574 --> 00:17:14.525
that you don't find, uh, very often anymore, but,

327
00:17:14.854 --> 00:17:19.055
um, there's, there's novel techniques to hide plagiarism and

328
00:17:19.055 --> 00:17:21.935
in particularly, I need to mention, of course, artificial

329
00:17:21.935 --> 00:17:25.765
intelligence because you can take a paragraph that somebody

330
00:17:25.765 --> 00:17:28.535
else has written and if you're lazy, you just

331
00:17:28.535 --> 00:17:33.780
say rewrite that paragraph in using check GBT. Um,

332
00:17:33.839 --> 00:17:36.959
OR some other, uh, generative AI, and it will

333
00:17:36.959 --> 00:17:39.920
generate a new text that is exactly the same

334
00:17:39.920 --> 00:17:42.479
as the old, but it's revered. So you can

335
00:17:42.479 --> 00:17:46.000
use your old plagiarism detector tools and you're, it's

336
00:17:46.000 --> 00:17:49.010
not gonna flag it as plagiarism. But in reality,

337
00:17:49.079 --> 00:17:52.079
it still is. It's still the same text, it's

338
00:17:52.079 --> 00:17:55.175
reworded, but especially if you look at References, you

339
00:17:55.175 --> 00:17:57.755
can see it's still using the same references. So,

340
00:17:58.165 --> 00:18:01.714
it is very hard to recognize that. And, and,

341
00:18:01.915 --> 00:18:03.444
and, you know, you can always say, well, I

342
00:18:03.444 --> 00:18:06.194
just rewrote it, so it's not plagiarism. But with

343
00:18:06.194 --> 00:18:09.045
AI you can, you can very quickly write an

344
00:18:09.045 --> 00:18:12.844
article without doing anything yourself than just copy pasting

345
00:18:12.844 --> 00:18:16.339
text that somebody else has written. And so it's,

346
00:18:16.390 --> 00:18:21.439
it's increasingly hard to recognize plagiarized text. Um, AND

347
00:18:21.609 --> 00:18:23.839
yeah, I'm a bit worried about that, but maybe

348
00:18:24.329 --> 00:18:27.449
that's just the new tool we have and uh

349
00:18:27.449 --> 00:18:30.050
maybe we shouldn't worry too much about how we're

350
00:18:30.050 --> 00:18:33.170
gonna rewrite the same thing again because there's only

351
00:18:33.170 --> 00:18:35.489
so many ways you can say, uh, you can

352
00:18:35.489 --> 00:18:38.300
write an introduction, let's say about breast cancer or

353
00:18:38.300 --> 00:18:40.439
so. I mean, you're gonna use the same terms,

354
00:18:40.609 --> 00:18:44.089
you're gonna use the same statistics. And so maybe.

355
00:18:44.920 --> 00:18:47.099
In the end, we, we shouldn't worry too much

356
00:18:47.099 --> 00:18:49.800
about that type of plagiarism. But of course, when

357
00:18:49.800 --> 00:18:52.640
it's plagiarism of data when you just steal somebody

358
00:18:52.640 --> 00:18:55.910
else's data or even figures or photos, that is,

359
00:18:56.160 --> 00:18:58.839
that is still wrong, of course. But I think

360
00:18:58.839 --> 00:19:03.239
when you describe a particular problem in science, a

361
00:19:03.239 --> 00:19:06.000
particular, like for the introduction, for example, you would

362
00:19:06.000 --> 00:19:09.099
describe, you know, you would. Give the facts and

363
00:19:09.099 --> 00:19:11.420
so I can sort of see that maybe we

364
00:19:11.420 --> 00:19:14.300
shouldn't worry too much about pleasure. We cannot recognize

365
00:19:14.300 --> 00:19:17.260
it anymore. If, if you're a smart fraudster, you'll

366
00:19:17.260 --> 00:19:20.849
just use AI and you're gonna rewrite that paragraph.

367
00:19:21.500 --> 00:19:21.890
Mhm.

368
00:19:22.459 --> 00:19:25.140
So tell us a little bit about uh how

369
00:19:25.140 --> 00:19:27.219
you do your work. I mean, what are the

370
00:19:27.219 --> 00:19:30.660
most common ways you discover scientific fraud?

371
00:19:31.770 --> 00:19:35.819
So I focus on images, on photos and uh

372
00:19:35.819 --> 00:19:39.489
so I will look at scientific papers and And

373
00:19:39.489 --> 00:19:43.050
search for duplicated images. Let's say within a paper

374
00:19:43.050 --> 00:19:46.699
there's a bunch of photos, little panels, very often

375
00:19:46.699 --> 00:19:51.329
western blots or photos of tissues or cells, and,

376
00:19:51.400 --> 00:19:54.290
and the same photo is used twice. So it's

377
00:19:54.290 --> 00:19:58.170
a duplicated photo, but one is experiment A and

378
00:19:58.170 --> 00:20:00.329
one is experiment B, so they should not be

379
00:20:00.329 --> 00:20:02.569
the same. That could be a sloppy, of course,

380
00:20:02.650 --> 00:20:06.680
a sloppy, uh, you know, the researcher who just

381
00:20:06.680 --> 00:20:10.959
grabbed the wrong photo. Sometimes photos overlap. So you

382
00:20:10.959 --> 00:20:14.349
see that let's say one photo is experiment A,

383
00:20:14.479 --> 00:20:18.089
one photo is labeled as a different experiment, experiment

384
00:20:18.089 --> 00:20:20.319
B, but there's a little overlap that you can

385
00:20:20.319 --> 00:20:23.000
find. And so that looks like it was the

386
00:20:23.000 --> 00:20:25.319
same sample, maybe it was moved under a microscope

387
00:20:25.319 --> 00:20:28.719
a little bit. So those are overlapping images, or

388
00:20:28.719 --> 00:20:32.750
sometimes images are, are mirrored or, or rotated, things

389
00:20:32.750 --> 00:20:35.260
like that. Uh, AND then the third type of

390
00:20:35.260 --> 00:20:38.290
image duplication that I look for are photos that

391
00:20:38.500 --> 00:20:41.540
within the photo there's duplicated elements. So it's one

392
00:20:41.540 --> 00:20:45.160
photo. And you see the same cell 3 times

393
00:20:45.160 --> 00:20:47.800
or the same type of tissue copy pasted a

394
00:20:47.800 --> 00:20:50.390
couple of times. And yeah, that is, of course,

395
00:20:50.479 --> 00:20:53.829
that's very bad because uh people do that intentionally.

396
00:20:54.079 --> 00:20:57.199
Maybe to hide a crack or maybe to make

397
00:20:57.199 --> 00:21:00.760
it look like uh there's more cells growing with

398
00:21:00.760 --> 00:21:03.359
this drug or or fewer cells or things like

399
00:21:03.359 --> 00:21:06.599
that. So, so those are the type of things

400
00:21:06.599 --> 00:21:09.199
I'm, I'm looking for. I'm using my eyes, but

401
00:21:09.199 --> 00:21:12.520
I'm also using tools. Uh, AND, and the tools

402
00:21:12.520 --> 00:21:16.180
I'm using, uh, have databases of, of images in

403
00:21:16.180 --> 00:21:19.270
other papers. So I might be able to find

404
00:21:19.719 --> 00:21:23.959
papers that, um, that have stolen an image from

405
00:21:23.959 --> 00:21:27.280
a different paper, or that the authors just reuse

406
00:21:27.280 --> 00:21:31.069
their own. Uh, PHOTO again and make it look

407
00:21:31.069 --> 00:21:34.089
like a different experiment. I need to very carefully

408
00:21:34.089 --> 00:21:38.069
look, because sometimes the duplication is quite OK. Sometimes

409
00:21:38.069 --> 00:21:41.180
it's quite OK to use the same control experiment.

410
00:21:41.630 --> 00:21:43.630
Let's say you test two different drugs and you

411
00:21:43.630 --> 00:21:46.630
have, you know, the, the, the, the not treated

412
00:21:46.630 --> 00:21:49.229
cells versus drug one, and then the not treated

413
00:21:49.229 --> 00:21:52.729
cells versus drug 2. So the, the control, the

414
00:21:52.729 --> 00:21:54.969
not treated sample might be the same and that

415
00:21:54.969 --> 00:21:58.489
is quite OK. So those are, are fine. I'm

416
00:21:58.489 --> 00:22:01.290
not gonna flag those, um, but I'm trying to

417
00:22:01.290 --> 00:22:03.400
find inappropriate duplications.

418
00:22:04.290 --> 00:22:08.400
And what about using fake images? I mean, what

419
00:22:08.400 --> 00:22:12.199
kinds of fake images do people use and where

420
00:22:12.199 --> 00:22:13.199
do they get them?

421
00:22:14.040 --> 00:22:18.119
Um, WELL, we have seen, uh, Western blots that

422
00:22:18.119 --> 00:22:21.119
look very unrealistic, and we think they were generated

423
00:22:21.119 --> 00:22:28.500
using artificial intelligence. Now, AI has, of course, Greatly

424
00:22:28.500 --> 00:22:31.119
improved over the last 2 or 3 years. Uh,

425
00:22:31.180 --> 00:22:34.050
BUT there was a period when AI was not

426
00:22:34.050 --> 00:22:36.380
that good yet, I guess. And, and we saw

427
00:22:36.380 --> 00:22:39.099
these weird blots, and I wasn't the first one

428
00:22:39.099 --> 00:22:41.510
seeing it, but, but many other people had noticed

429
00:22:41.510 --> 00:22:44.859
them. And all the, the, the western blots, they

430
00:22:44.859 --> 00:22:47.930
look like horizontal stripes, but the backgrounds of these

431
00:22:47.930 --> 00:22:50.329
blots was always the same. So it was, it

432
00:22:50.329 --> 00:22:52.739
was hundreds and hundreds and hundreds of papers that

433
00:22:52.739 --> 00:22:55.260
all had photos with the same backgrounds. And we're

434
00:22:55.260 --> 00:22:58.079
like, that is weird. Uh, THAT they have to

435
00:22:58.079 --> 00:23:00.839
have a common source. Uh, NOW, of course, the

436
00:23:00.839 --> 00:23:03.599
technique of AI is so much better that you

437
00:23:03.599 --> 00:23:07.380
can teach your, your artificial intelligence. You can just

438
00:23:07.380 --> 00:23:09.150
give it a bunch of real photos and say,

439
00:23:09.920 --> 00:23:13.050
generate a new photo for me. And I cannot

440
00:23:13.050 --> 00:23:15.640
no longer use my software because it's not a

441
00:23:15.640 --> 00:23:19.853
duplicated image. It's a novel, unique image. So these

442
00:23:19.853 --> 00:23:22.213
are really, really hard to find. And I think

443
00:23:22.213 --> 00:23:25.213
all of us have seen photos generated by AI

444
00:23:25.213 --> 00:23:28.692
that look very realistic. Uh, SOMETIMES you can still

445
00:23:28.692 --> 00:23:31.613
recognize that it's fake, but I think as the

446
00:23:31.613 --> 00:23:35.532
technique gets better, it's very easy to generate photos

447
00:23:35.532 --> 00:23:38.812
of, of tissues and cells and, and even of

448
00:23:38.812 --> 00:23:41.946
human faces that are in. Distinguishable from, from the

449
00:23:41.946 --> 00:23:44.345
real thing. And of course, if you are a

450
00:23:44.345 --> 00:23:47.306
science fraudster, you will be very happy with this

451
00:23:47.306 --> 00:23:49.865
technique because you don't no longer have to work

452
00:23:49.865 --> 00:23:51.786
in the lab and you can crank out fake

453
00:23:51.786 --> 00:23:55.826
paper after fake paper. So generative AI, especially when

454
00:23:55.826 --> 00:23:58.186
it comes to image, you can, uh, when you

455
00:23:58.186 --> 00:24:01.745
think about science integrity, I think it allows fraudsters

456
00:24:01.745 --> 00:24:05.250
to. To generate more and more fake papers, and

457
00:24:05.250 --> 00:24:07.890
I'm not quite sure how detect that. And if

458
00:24:07.890 --> 00:24:10.329
we, I don't think we're ready for for that

459
00:24:10.329 --> 00:24:11.170
amount of fraud.

460
00:24:11.969 --> 00:24:16.079
Mhm. So tell us specifically about the case of

461
00:24:16.079 --> 00:24:19.770
the rat with the big balls and the enormous

462
00:24:19.770 --> 00:24:20.530
pins.

463
00:24:21.689 --> 00:24:23.780
Yes, I, I wrote a blog post about it.

464
00:24:23.890 --> 00:24:26.260
I mean, that was, uh, when was that? Last

465
00:24:26.260 --> 00:24:30.119
summer or so. Uh, THAT was a very funny

466
00:24:30.119 --> 00:24:32.729
image. It was not a photo, but it was

467
00:24:32.729 --> 00:24:37.660
clearly generated by AI because it showed indeed a

468
00:24:37.660 --> 00:24:41.180
rat with a giant penis and giant balls. And

469
00:24:41.180 --> 00:24:43.979
it was supposed to be, it was published in

470
00:24:43.979 --> 00:24:46.739
scientific papers, a paper in, in one of the

471
00:24:46.739 --> 00:24:50.780
Frontier's journals. And, and it, I think it was

472
00:24:50.780 --> 00:24:56.939
supposed to show how stem cells were isolated from

473
00:24:56.939 --> 00:25:00.890
testicles of rats, which You know, that's a scientific

474
00:25:00.890 --> 00:25:03.510
technique, but it wasn't very helpful. First of all,

475
00:25:03.680 --> 00:25:08.400
because the, the anatomic um proportions of the rats

476
00:25:08.400 --> 00:25:11.560
were quite wrong. I don't think rats have penises

477
00:25:11.560 --> 00:25:14.719
like that. It rose into the, the skies. It

478
00:25:14.719 --> 00:25:17.459
was just so big. Um. But also you couldn't,

479
00:25:17.530 --> 00:25:19.410
I mean, that was funny, but obviously, we were

480
00:25:19.410 --> 00:25:21.849
all having a hard, very, you know, good laugh

481
00:25:21.849 --> 00:25:24.130
with it, but you couldn't read the labels, it

482
00:25:24.130 --> 00:25:28.500
was just AI cannot really generate uh letters yet

483
00:25:28.500 --> 00:25:31.449
very well, so you couldn't, it was just, you

484
00:25:31.449 --> 00:25:35.000
couldn't read the letters, um. And uh yeah, it

485
00:25:35.000 --> 00:25:38.119
was just um funny and not very useful from

486
00:25:38.119 --> 00:25:40.400
a scientific point of view. That would not have

487
00:25:40.400 --> 00:25:43.119
been that bad. And actually, the authors did disclose,

488
00:25:43.199 --> 00:25:45.949
they used mid journey, which is one of the

489
00:25:45.959 --> 00:25:50.589
generative AI uh tools to to generate images. But,

490
00:25:50.650 --> 00:25:54.420
um, also the text of the whole article appeared

491
00:25:54.420 --> 00:25:57.800
to have been generated using AI and, and they

492
00:25:57.800 --> 00:25:59.910
had not disclosed that, and so the paper was

493
00:25:59.910 --> 00:26:03.770
retracted. Very quickly, actually may ahead, but we all,

494
00:26:03.979 --> 00:26:06.140
it was very funny. We all had a very

495
00:26:06.140 --> 00:26:09.439
good laugh about it on, on, uh, on Twitter

496
00:26:09.439 --> 00:26:12.579
and other social media. I think, and most scientists

497
00:26:12.579 --> 00:26:14.689
probably have seen that image. And I wrote a,

498
00:26:14.780 --> 00:26:17.650
a blog post about it. And, um, yeah, it's,

499
00:26:17.660 --> 00:26:20.189
uh, so it's good that it was retracted, but

500
00:26:20.189 --> 00:26:22.369
uh, it generated a lot of joy, I think,

501
00:26:22.380 --> 00:26:24.650
in most people's hearts. Yeah.

502
00:26:25.079 --> 00:26:27.880
There are also been cases of fake images of

503
00:26:27.880 --> 00:26:31.670
signaling pathways. What is the problem with such

504
00:26:31.670 --> 00:26:37.520
fakes? Um, YEAH, there's, there's several, um, science fields

505
00:26:37.520 --> 00:26:41.000
that have been infected, if you will, with fake

506
00:26:41.000 --> 00:26:45.160
images with, um, in the fields there have been

507
00:26:45.160 --> 00:26:49.849
many papers that we think. HAVE been mass produced

508
00:26:49.849 --> 00:26:52.729
by what we call paper mills, and, and these

509
00:26:52.729 --> 00:26:55.729
are fake papers. And, and in some of the

510
00:26:55.729 --> 00:26:59.329
fields in biology, it's, there's, let's say many different

511
00:26:59.329 --> 00:27:02.969
molecules or there's many different types of cancers and

512
00:27:02.969 --> 00:27:05.589
so you can just write an. AND then you

513
00:27:05.589 --> 00:27:08.790
change one molecule into another molecule or you change

514
00:27:08.790 --> 00:27:11.910
one cancer into another cancer, but you basically and

515
00:27:11.910 --> 00:27:14.310
you rewrite the text a little bit, so nobody

516
00:27:14.310 --> 00:27:17.069
would think it's the same text. But because you

517
00:27:17.069 --> 00:27:20.229
have all these different combinations of different molecules or

518
00:27:20.229 --> 00:27:23.829
pathways or cancers. It looks like a new paper

519
00:27:23.829 --> 00:27:27.239
and uh we can almost recognize this by the

520
00:27:27.239 --> 00:27:33.150
titles like um inhibition of uh noncoding RNA 123

521
00:27:33.510 --> 00:27:39.380
by uh this pathway uh will decrease uh cell

522
00:27:39.380 --> 00:27:43.280
growth in cancer through this pathway or something like

523
00:27:43.280 --> 00:27:46.359
that. The, the tidal structures were very similar. Um,

524
00:27:46.439 --> 00:27:50.160
SO, uh, the, the, uh, tadpole paper mill that

525
00:27:50.160 --> 00:27:52.839
I referred to earlier, where all the Western blos

526
00:27:52.839 --> 00:27:55.719
had exactly the same backgrounds, those all had very

527
00:27:55.719 --> 00:27:58.359
similar title structures to the point where you could

528
00:27:58.359 --> 00:28:01.040
just see the title and know it was the

529
00:28:01.040 --> 00:28:04.030
same group who generated that. And these papers were,

530
00:28:04.119 --> 00:28:07.405
were, were sold to. Different authors. These came all

531
00:28:07.405 --> 00:28:12.525
from, uh, uh, people working in Chinese hospitals. And

532
00:28:12.525 --> 00:28:14.885
yeah, it was basically the same paper written over

533
00:28:14.885 --> 00:28:19.234
and over again. Text slightly changed, images slightly changed.

534
00:28:19.604 --> 00:28:22.714
Um, BUT, but yeah, 6400 papers that we found

535
00:28:22.714 --> 00:28:25.005
to all have this, use the same template.

536
00:28:26.160 --> 00:28:30.060
And what happens to scientists who are caught committing

537
00:28:30.060 --> 00:28:30.550
fraud?

538
00:28:31.750 --> 00:28:36.630
Well, very often, almost nothing or nothing. Uh, UNFORTUNATELY,

539
00:28:37.239 --> 00:28:39.880
uh, science is, is sort of like the Tour

540
00:28:39.880 --> 00:28:44.030
de France, the big biking race where At some

541
00:28:44.030 --> 00:28:46.180
point, a lot of people used doping and nobody

542
00:28:46.180 --> 00:28:49.069
seemed to care and people were biking faster and

543
00:28:49.069 --> 00:28:51.989
faster because they use doping and if you didn't

544
00:28:51.989 --> 00:28:54.699
use doping, you would definitely not win the race

545
00:28:54.699 --> 00:28:57.510
and nobody seemed to care. And of course at

546
00:28:57.510 --> 00:28:59.750
some point people said we need to care about

547
00:28:59.750 --> 00:29:02.750
the sport if we really care about biking and

548
00:29:02.750 --> 00:29:05.890
you know, how good is an athlete versus how

549
00:29:06.410 --> 00:29:08.430
How much doping do we use? We need to

550
00:29:08.430 --> 00:29:11.310
check for doping. And so it changed the sports,

551
00:29:11.430 --> 00:29:13.959
I think for the better. Um, BUT in science,

552
00:29:14.030 --> 00:29:16.790
we're not at that stage yet. We, I think

553
00:29:16.790 --> 00:29:18.790
we're starting to realize that there's a lot of

554
00:29:18.790 --> 00:29:22.869
fraud, but very little happened. Occasionally a paper gets

555
00:29:22.869 --> 00:29:27.150
retracted, uh, but it's not enough. Um, AND, and

556
00:29:27.150 --> 00:29:30.709
especially the, the serial fraudsters, the people have produced

557
00:29:30.709 --> 00:29:34.709
multiple papers with, with fraud, those folks seem to.

558
00:29:35.180 --> 00:29:39.489
Very often not get uh punished, punished in any

559
00:29:39.489 --> 00:29:43.180
way. They, there's very little consequences. Sometimes, especially when

560
00:29:43.180 --> 00:29:47.140
it's a senior researcher, they usually will blame their

561
00:29:47.140 --> 00:29:51.900
junior research assistants or grad students and maybe those

562
00:29:51.900 --> 00:29:56.479
folks get fired, but the senior person. Keeps offloading.

563
00:29:56.520 --> 00:29:59.800
That's, that's what we see. Uh, PAPERS often don't

564
00:29:59.800 --> 00:30:04.119
get retracted or even corrected. Authors come up with

565
00:30:04.119 --> 00:30:06.800
all kinds of stories trying to explain why this

566
00:30:06.800 --> 00:30:10.040
cell is visible twice or 3 times or 100

567
00:30:10.040 --> 00:30:13.199
times in the same photo, and the editors believe

568
00:30:13.199 --> 00:30:16.959
it and they just issue a correction. It's just,

569
00:30:17.199 --> 00:30:20.130
it seems that nobody, uh, at least until a

570
00:30:20.130 --> 00:30:22.839
couple of years ago, that, that editors and journals

571
00:30:22.839 --> 00:30:26.630
and publishers didn't really. Realize how big of a

572
00:30:26.630 --> 00:30:29.829
problem it has had become. Now they are starting

573
00:30:29.829 --> 00:30:32.189
to realize, but it's um yeah, there has been

574
00:30:32.189 --> 00:30:36.069
a time where people were too naive. uh AND

575
00:30:36.069 --> 00:30:39.349
institutions seem to also do very little. They um

576
00:30:40.709 --> 00:30:43.550
Uh, very often senior research bringing a lot of

577
00:30:43.550 --> 00:30:47.719
money, so an institution might have to decide like,

578
00:30:47.750 --> 00:30:50.589
OK, are we gonna, let's say fire the fraudulent

579
00:30:50.589 --> 00:30:55.260
professor. Or are we gonna keep him or her

580
00:30:55.260 --> 00:30:57.500
on because they bring in so much money, so

581
00:30:57.500 --> 00:30:59.979
many grants, and, and, and I think that a

582
00:30:59.979 --> 00:31:04.099
lot of universities will, will just tell the, the

583
00:31:04.099 --> 00:31:07.589
fraudulent person, oh, don't fraud anymore, or fraud better,

584
00:31:07.599 --> 00:31:10.510
at least that we cannot catch you. Um, AND,

585
00:31:10.619 --> 00:31:12.979
and because they, they really love, of course, the

586
00:31:12.979 --> 00:31:15.599
money that some of these professors bring in and

587
00:31:15.859 --> 00:31:18.520
Um, and the, you know, they have high publication

588
00:31:18.520 --> 00:31:21.719
records, and that makes the university end up higher

589
00:31:21.719 --> 00:31:23.949
in their rankings. And we all love the rankings,

590
00:31:23.959 --> 00:31:28.069
and it's just, there's so much conflict of interest.

591
00:31:28.400 --> 00:31:30.439
Um, YOU want these folks to be, you know,

592
00:31:30.599 --> 00:31:33.839
leave signs, please, and don't fraud anymore. But in

593
00:31:33.839 --> 00:31:37.599
reality, these people are not held accountable, and they

594
00:31:37.599 --> 00:31:39.989
keep on frauding, and there's too many examples of

595
00:31:39.989 --> 00:31:40.280
that.

596
00:31:41.160 --> 00:31:43.020
So, let me ask you just a little bit

597
00:31:43.020 --> 00:31:47.900
about retraction. When are papers retracted and, I mean,

598
00:31:48.099 --> 00:31:51.739
how many among the ones that should be retracted

599
00:31:51.739 --> 00:31:53.420
really get retracted?

600
00:31:54.069 --> 00:31:57.910
Oh yeah. So, um, so, uh, taking a step

601
00:31:57.910 --> 00:32:00.319
back, uh, a scientific paper, once it is peer

602
00:32:00.319 --> 00:32:03.319
reviewed and published it, and, and somebody finds an

603
00:32:03.319 --> 00:32:05.920
error in it. There's, there's several things that can

604
00:32:05.920 --> 00:32:07.640
be done then to the paper. So the first

605
00:32:07.640 --> 00:32:10.349
one is a correction. Uh, IT can be called

606
00:32:10.349 --> 00:32:13.150
an aratum or a corrigenum, but it's basically all

607
00:32:13.150 --> 00:32:15.989
the same thing. Um, THAT is usually used, let's

608
00:32:15.989 --> 00:32:18.920
say there's a small error, 22 little photos were,

609
00:32:19.030 --> 00:32:21.910
were switched, or there's a misspelling in the name

610
00:32:21.910 --> 00:32:24.150
of one of the authors that happens a lot,

611
00:32:24.469 --> 00:32:27.829
um, or they forgot to add, uh, I don't

612
00:32:27.829 --> 00:32:30.829
know, the, the funder wasn't mentioned or things like

613
00:32:30.829 --> 00:32:35.229
that. Small errors that don't really, yeah, influence the

614
00:32:35.229 --> 00:32:38.660
paper. Um, AND then there can be an expression

615
00:32:38.660 --> 00:32:41.300
of concern, which is usually, it's very often a

616
00:32:41.300 --> 00:32:44.020
temporary thing, and it's good because it, it tells

617
00:32:44.020 --> 00:32:47.219
you if the publishers puts that on the paper,

618
00:32:47.260 --> 00:32:50.140
it knows there, it's usually because there's an ongoing

619
00:32:50.140 --> 00:32:53.290
investigation or there are some bigger errors, we're still

620
00:32:53.290 --> 00:32:55.229
thinking about it, but at least in the meantime,

621
00:32:55.339 --> 00:32:57.949
we're warning the readers that there's a problem. And

622
00:32:57.949 --> 00:33:00.939
then the, the, the most severe step that can

623
00:33:00.939 --> 00:33:03.989
happen to a paper is a retraction. So the

624
00:33:03.989 --> 00:33:07.270
paper will remain online. It's not that it's removed

625
00:33:07.270 --> 00:33:10.510
from the interwebs. It will be labeled usually with

626
00:33:10.510 --> 00:33:13.140
a watermark, retracted, will be, or like a big

627
00:33:13.140 --> 00:33:15.430
red sign at the top. You can still read

628
00:33:15.430 --> 00:33:18.829
it. Um, BUT basically it tells you that the

629
00:33:18.829 --> 00:33:21.469
publisher or the, the journal editor no longer has

630
00:33:21.469 --> 00:33:24.339
faith in the journal, and it's very often for

631
00:33:24.339 --> 00:33:27.969
a fatal. Or, um, it could be because an

632
00:33:27.969 --> 00:33:30.209
author, one of the authors found that one of

633
00:33:30.209 --> 00:33:34.079
the calculations was completely wrong, completely changing the outcome.

634
00:33:34.369 --> 00:33:36.810
So it can be started by an author who

635
00:33:36.810 --> 00:33:40.170
says, Sorry, we found a big error. We're gonna

636
00:33:40.170 --> 00:33:43.130
retract the paper, we're gonna redo the experiments, republish

637
00:33:43.130 --> 00:33:45.530
it, maybe. And I think that is, that is

638
00:33:45.530 --> 00:33:48.459
great when the authors actually recognized that there was

639
00:33:48.459 --> 00:33:51.329
a big error and retract the paper, but most

640
00:33:51.329 --> 00:33:55.420
often it is because of all kinds of Suspicions

641
00:33:55.420 --> 00:33:58.300
of misconduct or big errors, let's say there's many

642
00:33:58.300 --> 00:34:03.209
papers, many, uh, panel image panels that overlap. Uh,

643
00:34:03.270 --> 00:34:08.790
MANIPULATED photos, uh, duplications, uh, to the extent where

644
00:34:08.790 --> 00:34:11.300
you no longer have trust in the data, and

645
00:34:11.510 --> 00:34:15.679
that is often by whistleblowers, by external people, maybe

646
00:34:15.949 --> 00:34:17.909
by me when I see a big problem in

647
00:34:17.909 --> 00:34:20.188
the paper, I will write to the editors and

648
00:34:20.188 --> 00:34:24.070
then the editors will, uh, perhaps decide to retract

649
00:34:24.070 --> 00:34:27.188
the paper. Now it doesn't happen as often as

650
00:34:27.188 --> 00:34:29.860
I would like to see it. I I had

651
00:34:29.860 --> 00:34:32.540
an I initially when I started to do this

652
00:34:32.540 --> 00:34:34.458
work, I had a set of 800 papers in

653
00:34:34.458 --> 00:34:36.860
which I found problems. Um, I wrote to the

654
00:34:36.860 --> 00:34:40.458
editors and not all of these should be retracted,

655
00:34:40.540 --> 00:34:45.340
obviously, some were small errors, but only 13 uh

656
00:34:45.340 --> 00:34:49.010
was either corrected or retracted after waiting 5 years

657
00:34:49.010 --> 00:34:51.938
and sending a reminder. And so 5 years seems

658
00:34:51.938 --> 00:34:54.050
a very long time to take a decision, uh,

659
00:34:54.139 --> 00:34:57.178
if a paper. You know, should be corrected or

660
00:34:57.178 --> 00:35:00.188
retracted. So only 1/3 of these papers have been

661
00:35:00.188 --> 00:35:02.468
taken action on and now we're in about 10

662
00:35:02.468 --> 00:35:05.188
years later, it's only a little bit more than

663
00:35:05.188 --> 00:35:08.309
half of the papers have either correction or retraction.

664
00:35:08.388 --> 00:35:11.948
So it's very slow, uh, and in half of

665
00:35:11.948 --> 00:35:16.069
the cases, um, even after waiting 10 years, nothing

666
00:35:16.069 --> 00:35:18.668
happens, and it's so frustrating because we see a

667
00:35:18.668 --> 00:35:22.358
big problem in the paper and the journals looked

668
00:35:22.358 --> 00:35:26.040
out of way, and That is bad. If you,

669
00:35:26.239 --> 00:35:28.679
if you know that your airbag of your car

670
00:35:28.679 --> 00:35:31.120
is, uh, is, is faulty, you would hope that

671
00:35:31.120 --> 00:35:33.560
there's a recall and that, you know, you, you

672
00:35:33.560 --> 00:35:37.000
don't have to pressure the car manufacturer to, to

673
00:35:37.000 --> 00:35:39.040
get a new car or to get it fixed.

674
00:35:39.080 --> 00:35:42.760
And it seems that publishers in many cases, and

675
00:35:42.760 --> 00:35:45.320
there's good exceptions, there's good publishers who really do

676
00:35:45.320 --> 00:35:50.020
the right thing, but There's too many, uh, publishers

677
00:35:50.020 --> 00:35:52.379
who don't seem to care about quality control. They

678
00:35:52.379 --> 00:35:55.860
publish bad papers and then when people complain about

679
00:35:55.860 --> 00:35:58.820
it, the customer service is also very bad and

680
00:35:58.820 --> 00:36:01.409
it will be like buying a very expensive car,

681
00:36:02.100 --> 00:36:04.620
realizing there's a problem, and then the dealer not

682
00:36:04.620 --> 00:36:08.129
wanting to take any action. I think as customers

683
00:36:08.379 --> 00:36:10.340
uh of buying, you know, people who bought a

684
00:36:10.340 --> 00:36:12.750
car, you would not accept that. But in science,

685
00:36:12.860 --> 00:36:15.810
we seem to all. Be OK with that, and

686
00:36:15.810 --> 00:36:18.570
we should not be OK because we pay the

687
00:36:18.570 --> 00:36:21.620
publishers a lot of money. And I'm not quite

688
00:36:21.620 --> 00:36:23.350
sure what all that money goes to, but it

689
00:36:23.350 --> 00:36:26.149
doesn't seem to go to quality control or customer

690
00:36:26.149 --> 00:36:29.909
service, so that needs to change. And again, there

691
00:36:29.909 --> 00:36:32.070
are good publishers who do the right thing, but

692
00:36:32.070 --> 00:36:35.270
there's still too many who are too focused on

693
00:36:35.270 --> 00:36:38.149
making money and not on uh quality control and

694
00:36:38.149 --> 00:36:39.189
customer service.

695
00:36:39.989 --> 00:36:42.639
And in the meantime, while we wait for the

696
00:36:42.639 --> 00:36:45.669
paper to get corrected or retracted, it can be

697
00:36:45.669 --> 00:36:46.770
cited several times,

698
00:36:47.639 --> 00:36:50.879
yeah, that's a very good point. We, the reader

699
00:36:50.879 --> 00:36:53.040
might not know that there's a big problem with

700
00:36:53.040 --> 00:36:56.159
this paper. If I write to the editor, none

701
00:36:56.159 --> 00:36:58.469
of the readers would know there's a big problem,

702
00:36:58.840 --> 00:37:03.310
and that is why most. PEOPLE who work like

703
00:37:03.489 --> 00:37:06.110
me who find problems in papers, we will post

704
00:37:06.110 --> 00:37:11.800
on a website called Puppeer.com, like publication and peer

705
00:37:11.800 --> 00:37:15.669
review pop peer, and we will, we will, uh,

706
00:37:15.679 --> 00:37:18.040
flag papers for all kinds of problems. So if

707
00:37:18.040 --> 00:37:19.840
you do a literature search, you can type in

708
00:37:19.840 --> 00:37:22.500
the DOI, the unique identifier of the paper, and

709
00:37:22.500 --> 00:37:25.405
you can see if somebody else has. Seen something

710
00:37:25.405 --> 00:37:27.485
about the paper. It could be a positive command

711
00:37:27.485 --> 00:37:30.754
or a negative command. Now most comments on Papa

712
00:37:30.754 --> 00:37:34.485
are about image problems because they're visible, right? Like

713
00:37:34.485 --> 00:37:36.725
it's very easy to see that a photo is

714
00:37:36.725 --> 00:37:40.084
duplicated. It's much easier to see than that a

715
00:37:40.084 --> 00:37:42.645
bar graph has been made up. Like that is

716
00:37:42.645 --> 00:37:44.885
very often if you are a good fraudster, you

717
00:37:44.885 --> 00:37:47.655
would, you, you, that would not be visible, but.

718
00:37:48.100 --> 00:37:50.560
Uh, WE want to warn the, the readers of

719
00:37:50.560 --> 00:37:53.399
these papers that there's particular problems. As soon as

720
00:37:53.399 --> 00:37:55.409
I can find it, I will post it on

721
00:37:55.409 --> 00:37:58.879
Pope. So within 5 minutes, it's on Papa, while

722
00:37:58.879 --> 00:38:02.320
retractions take, you know, sometimes 10 years, and, and

723
00:38:02.320 --> 00:38:05.479
that is just too slow. Like, why would it

724
00:38:05.479 --> 00:38:09.429
take so long? And, and so it's frustrating and

725
00:38:09.699 --> 00:38:12.290
all this time, people might think that the paper

726
00:38:12.290 --> 00:38:15.139
is completely fine, they might not realize that there's

727
00:38:15.139 --> 00:38:17.979
potential problems with it. So, um, it's a, it's

728
00:38:17.979 --> 00:38:20.879
a very good point and that is why we're

729
00:38:22.949 --> 00:38:26.040
We science detectives are very often frustrated with the

730
00:38:26.040 --> 00:38:27.000
lack of response.

731
00:38:27.879 --> 00:38:31.010
So, I have one final question slash topic I

732
00:38:31.010 --> 00:38:34.159
would like to explore with you here today. So,

733
00:38:34.290 --> 00:38:37.439
what do you think are the best solutions here?

734
00:38:37.610 --> 00:38:40.209
How can we best combat fraud?

735
00:38:41.090 --> 00:38:45.729
Yeah, it's a big question, right? Um, SO, well,

736
00:38:46.010 --> 00:38:48.520
there's, there's multiple things you can think of. Of

737
00:38:48.520 --> 00:38:52.090
course, we need to educate young people, um, you

738
00:38:52.090 --> 00:38:54.010
know, at the start of their science career, what

739
00:38:54.010 --> 00:38:57.290
is acceptable, what is OK, what are things you

740
00:38:57.290 --> 00:39:00.379
should not be doing, um. That is one thing.

741
00:39:00.479 --> 00:39:02.590
I think education is important, but I think at

742
00:39:02.590 --> 00:39:04.939
the same time, there need to be consequences for

743
00:39:04.939 --> 00:39:08.379
science fraud. There need to be quicker corrections and

744
00:39:08.379 --> 00:39:12.419
retractions. There need to be um a lower threshold

745
00:39:12.419 --> 00:39:15.500
for science fraud. Like if you find a photoshopped

746
00:39:15.500 --> 00:39:18.209
image, you should not allow the authors to send

747
00:39:18.209 --> 00:39:21.139
in a completely new set of figures that would

748
00:39:21.139 --> 00:39:23.659
be, you know, winning the Tour de France, testing

749
00:39:23.659 --> 00:39:27.239
positive for doping. And saying, well, just bring in

750
00:39:27.239 --> 00:39:29.310
a clean sample in 2 weeks and we're all

751
00:39:29.310 --> 00:39:32.399
good. No, that is just, no, that paper should

752
00:39:32.399 --> 00:39:36.600
be retracted. There should be um bigger consequences for

753
00:39:36.600 --> 00:39:39.679
people who have been caught fraud, uh, doing fraud

754
00:39:40.000 --> 00:39:44.649
by their institutions, um, and funders, people who, who,

755
00:39:44.719 --> 00:39:47.479
you know, they should not receive any funding for

756
00:39:47.479 --> 00:39:49.550
the, for the next future. And some of these

757
00:39:49.550 --> 00:39:53.080
things happen occasionally, but it seems. Too little too

758
00:39:53.080 --> 00:39:58.629
late, it's too slow, um, so consequences, education, um,

759
00:39:58.879 --> 00:40:01.790
those I think are the, the most important things

760
00:40:01.790 --> 00:40:04.000
and, and I think we also need to focus

761
00:40:04.000 --> 00:40:08.639
more on open science, um, you know, sharing your

762
00:40:08.639 --> 00:40:13.800
data set, um, sharing, uh, negative results, publishing negative

763
00:40:13.800 --> 00:40:17.870
results. And maybe doing science a little bit more

764
00:40:17.870 --> 00:40:21.820
slowly, uh, being able to show that in, uh,

765
00:40:21.830 --> 00:40:26.110
that experiments are reproducible. And, and sometimes I dream

766
00:40:26.110 --> 00:40:28.510
about a new way of, of science publishing where

767
00:40:28.510 --> 00:40:32.709
we publish smaller experiments, one experiment, one figure, and

768
00:40:32.709 --> 00:40:35.870
then other people can reproduce that, and then say,

769
00:40:35.919 --> 00:40:38.389
yes, we, we were able to exactly get the

770
00:40:38.389 --> 00:40:40.510
same results or no, we could not do it.

771
00:40:40.629 --> 00:40:43.909
Maybe am I missing something, you know, are there

772
00:40:43.909 --> 00:40:48.239
certain Experimental conditions you didn't include or so, um,

773
00:40:48.429 --> 00:40:50.590
as a way maybe to slow down science a

774
00:40:50.590 --> 00:40:52.949
little bit, but make it more reproducible and, and

775
00:40:52.949 --> 00:40:55.709
reliable, and I think we, we should just focus

776
00:40:55.709 --> 00:40:58.110
more on that. But it, it's so easy to

777
00:40:58.110 --> 00:41:01.310
focus on metrics and publications and how are we

778
00:41:01.310 --> 00:41:04.149
gonna change our complete system? It's probably not gonna

779
00:41:04.149 --> 00:41:07.199
happen anytime soon. But I think we need to

780
00:41:07.199 --> 00:41:10.479
think about other ways of publishing our papers and

781
00:41:11.030 --> 00:41:14.489
rewarding reproducibility and negative results publishing.

782
00:41:16.110 --> 00:41:19.090
Great. So, Doctor Bick, just before we go, where

783
00:41:19.090 --> 00:41:21.929
can people find you and your work on the

784
00:41:21.929 --> 00:41:22.250
internet?

785
00:41:23.159 --> 00:41:26.280
Well, I was on Twitter X, but now I'm,

786
00:41:26.439 --> 00:41:29.360
I switched to Blue Sky, so Elizabeth Bick, my

787
00:41:29.360 --> 00:41:32.239
name, um, you would, you should be able to

788
00:41:32.239 --> 00:41:35.629
find me on Blue Sky. Um, I'm on LinkedIn,

789
00:41:35.760 --> 00:41:39.959
I'm on, um, Science Integrity Digest, that is my

790
00:41:39.959 --> 00:41:44.320
blog, um, and usually very responsive. Usually on Blue

791
00:41:44.320 --> 00:41:47.679
Sky, I that that's where I'll, uh. Uh, YOU'LL

792
00:41:47.679 --> 00:41:49.830
have the biggest chance of finding me. And if,

793
00:41:49.959 --> 00:41:51.850
if I don't follow you, just ask me if

794
00:41:51.850 --> 00:41:53.280
I can follow you so you can send me

795
00:41:53.280 --> 00:41:56.399
a direct message and uh then I'll share my

796
00:41:56.399 --> 00:41:58.510
email address and uh we can talk.

797
00:41:59.159 --> 00:42:01.600
Great. So thank you so much for taking the

798
00:42:01.600 --> 00:42:03.879
time to come on the show. It's been really

799
00:42:03.879 --> 00:42:06.149
fun and a real pleasure to talk with you.

800
00:42:06.600 --> 00:42:09.620
Same as, yeah, same here. Thank you, Ricardo and

801
00:42:11.280 --> 00:42:14.510
Uh, yeah, I hope everybody will stay very honest.

802
00:42:16.500 --> 00:42:19.020
Hi guys, thank you for watching this interview until

803
00:42:19.020 --> 00:42:21.169
the end. If you liked it, please share it,

804
00:42:21.340 --> 00:42:24.129
leave a like and hit the subscription button. The

805
00:42:24.129 --> 00:42:26.330
show is brought to you by Nights Learning and

806
00:42:26.330 --> 00:42:30.409
Development done differently, check their website at Nights.com and

807
00:42:30.409 --> 00:42:34.129
also please consider supporting the show on Patreon or

808
00:42:34.129 --> 00:42:36.610
PayPal. I would also like to give a huge

809
00:42:36.610 --> 00:42:39.719
thank you to my main patrons and PayPal supporters

810
00:42:39.719 --> 00:42:43.939
Perergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyches Olaf,

811
00:42:44.050 --> 00:42:47.290
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

812
00:42:47.429 --> 00:42:50.719
Erika Lenny, John Connors, Philip Fors Connolly. Then the

813
00:42:50.719 --> 00:42:54.520
Mari Robert Windegaruyasi Zup Mark Nes called in Holbrookfield

814
00:42:54.520 --> 00:42:59.280
governor Michael Stormir, Samuel Andre, Francis Forti Agnsergoro and

815
00:42:59.280 --> 00:43:03.629
Hal Herzognun Macha Joan Labrant Juan and Samuel Corriere,

816
00:43:03.790 --> 00:43:07.469
Heinz, Mark Smith, Jore, Tom Hummel, Sardus France David

817
00:43:07.469 --> 00:43:11.330
Sloan Wilson, asilla dearraujoro and Roach Diego London Correa.

818
00:43:11.709 --> 00:43:16.830
Yannick Punter Darusmani Charlotte blinikol Barbara Adamhn Pavlostaevsky nale

819
00:43:16.830 --> 00:43:21.350
back medicine, Gary Galman Sam of Zallirianeioltonin John Barboza,

820
00:43:21.389 --> 00:43:26.510
Julian Price, Edward Hall Edin Bronner, Douglas Fre Francaortolotti

821
00:43:26.510 --> 00:43:32.040
Gabrielon Scorteseus Slelitsky, Scott Zacharyishtim Duffyani Smith John Wieman.

822
00:43:32.379 --> 00:43:36.939
Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai Giorgio

823
00:43:36.939 --> 00:43:41.659
Theophanous, Chris Williamson, Peter Wozin, David Williams, Dio Augusta,

824
00:43:41.739 --> 00:43:45.979
Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez, Coralli

825
00:43:45.979 --> 00:43:50.340
Chevalier, bungalow atheists, Larry D. Lee Junior, old Erringbo.

826
00:43:51.100 --> 00:43:55.770
Sterry Michael Bailey, then Sperber, Robert Grassyigoren, Jeff McMann,

827
00:43:55.939 --> 00:44:00.100
Jake Zu, Barnabas radix, Mark Campbell, Thomas Dovner, Luke

828
00:44:00.100 --> 00:44:04.620
Neeson, Chris Stor, Kimberly Johnson, Benjamin Galbert, Jessica Nowicki,

829
00:44:04.669 --> 00:44:10.419
Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George Eoriatis, Valentin

830
00:44:10.419 --> 00:44:16.540
Steinman, Perkrolis, Kate van Goller, Alexander Aubert, Liam Dunaway,

831
00:44:16.649 --> 00:44:22.179
BR Masoud Ali Mohammadi, Perpendicular John Nertner, Ursula Gudinov,

832
00:44:22.330 --> 00:44:26.969
Gregory Hastings, David Pinsoff Sean Nelson, Mike Levine, and

833
00:44:26.969 --> 00:44:30.155
Jos Net. A special thanks to my producers. These

834
00:44:30.155 --> 00:44:33.685
are Webb, Jim, Frank Lucas Steffinik, Tom Venneden, Bernard

835
00:44:33.685 --> 00:44:37.915
Curtis Dixon, Benedic Muller, Thomas Trumbull, Catherine and Patrick

836
00:44:37.915 --> 00:44:41.245
Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick Golden,

837
00:44:41.475 --> 00:44:44.754
and to my executive producers Matthew Levender, Sergio Quadrian,

838
00:44:44.875 --> 00:44:47.504
Bogdan Kanivets, and Rosie. Thank you for all.

