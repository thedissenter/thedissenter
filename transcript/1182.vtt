WEBVTT

1
00:00:00.449 --> 00:00:03.240
Hello everyone. Welcome to a new episode of The

2
00:00:03.240 --> 00:00:06.369
Dissenter. I'm your host, as always, Ricardo Lops, and

3
00:00:06.369 --> 00:00:10.039
today I'm joined by Doctor Dres Boston. He's a

4
00:00:10.039 --> 00:00:14.699
social psychologist, statistician, and philosopher currently doing a BOF

5
00:00:14.699 --> 00:00:19.770
senior postdoctoral fellowship at the Social Psychology lab at

6
00:00:19.770 --> 00:00:24.520
Ghent. University. His primary research focus is the study

7
00:00:24.520 --> 00:00:28.159
of moral psychology, and today we're going to talk

8
00:00:28.159 --> 00:00:33.959
mostly about truly type moral dilemmas, sacrificial dilemmas, moral

9
00:00:33.959 --> 00:00:37.599
cognition, and some other related topics. So Doctor Bostin,

10
00:00:37.680 --> 00:00:39.959
welcome to the show. It's a huge pleasure to

11
00:00:39.959 --> 00:00:40.259
everyone.

12
00:00:40.880 --> 00:00:43.319
Thank you for having me and looking forward to

13
00:00:43.319 --> 00:00:43.880
our chat.

14
00:00:45.459 --> 00:00:48.490
So, uh, just to introduce the topic for people

15
00:00:48.490 --> 00:00:51.090
who might not be familiar with what we're going

16
00:00:51.090 --> 00:00:55.049
to talk about here, what are trolley type moral

17
00:00:55.049 --> 00:00:55.950
dilemmas?

18
00:00:57.720 --> 00:01:00.880
I mean, so there's not one official definition for

19
00:01:00.880 --> 00:01:03.400
the trolley type dilemmas, of course, um, but I

20
00:01:03.400 --> 00:01:04.760
would say there are, there are a class of

21
00:01:04.760 --> 00:01:09.440
moral dilemmas, um, that, um, the most well-known example

22
00:01:09.440 --> 00:01:11.400
of them is the, the trolley dilemma, which is

23
00:01:11.400 --> 00:01:15.010
a well-known, uh, moral dilemma, possibly, maybe even the,

24
00:01:15.150 --> 00:01:19.080
the most well-known dilemma, um, and so I, I,

25
00:01:19.160 --> 00:01:21.750
I usually always start just by giving people the

26
00:01:21.750 --> 00:01:24.559
example even though by now many people are so

27
00:01:24.559 --> 00:01:27.389
familiar with it. Um, BUT just kind of ground

28
00:01:27.389 --> 00:01:30.059
the discussion, uh, what we're talking about, um, so

29
00:01:30.059 --> 00:01:32.750
destroyer dilemma is basically a moral dilemma that, um,

30
00:01:32.910 --> 00:01:37.029
depicts a scene where a runaway trolley train is

31
00:01:37.029 --> 00:01:39.510
about to run over 5 people. Um, SOMETIMES these

32
00:01:39.510 --> 00:01:41.269
are 5 people that are tied to the tracks,

33
00:01:41.510 --> 00:01:44.089
sometimes they're just unsuspecting workmen, but in any case,

34
00:01:44.470 --> 00:01:46.069
um, the train is going to run over 5

35
00:01:46.069 --> 00:01:49.839
people, and the moral dilemma poses a question, um,

36
00:01:50.569 --> 00:01:53.889
imagine that you could divert, avert this disaster, um.

37
00:01:54.459 --> 00:01:56.940
Usually there are multiple ways in which this can

38
00:01:56.940 --> 00:01:59.330
be done, and the most classic way is imagine

39
00:01:59.330 --> 00:02:03.699
you can avert this disaster by diverting the train

40
00:02:03.699 --> 00:02:06.260
to a secondary track, um, which would be great,

41
00:02:06.339 --> 00:02:09.100
but then unfortunately there's one other person on the

42
00:02:09.100 --> 00:02:11.699
track, and so then the moral dilemma emerges. Should

43
00:02:11.699 --> 00:02:15.860
you sacrifice one person to save the 5, and

44
00:02:15.860 --> 00:02:18.339
the core idea behind these dilemmas is just that

45
00:02:18.339 --> 00:02:23.779
they. They contrast um a desire to minimize harm,

46
00:02:23.910 --> 00:02:28.250
to strive towards the greater good, with a, a

47
00:02:28.250 --> 00:02:33.490
sort of refusal, a sort of um. Uh, DISAVOWMENT

48
00:02:33.490 --> 00:02:36.490
of actively harming others. Sometimes that's framed in terms

49
00:02:36.490 --> 00:02:40.800
of norms or rules first and principles versus consequences.

50
00:02:41.419 --> 00:02:44.210
Uh, SOMETIMES people say this is about deontology versus

51
00:02:44.210 --> 00:02:49.050
utilitarianism, um, but I prefer just kind of keeping

52
00:02:49.050 --> 00:02:51.050
it as, as, as low level as possible and

53
00:02:51.050 --> 00:02:53.410
just saying it's about, it's about, um, do you

54
00:02:53.410 --> 00:02:56.050
wanna actively harm, like, are you OK with actively

55
00:02:56.050 --> 00:02:58.919
harming others to pursue a greater good? Um, I

56
00:02:58.919 --> 00:03:00.910
would say that is what trolley tight dilemmas are

57
00:03:00.910 --> 00:03:05.029
all about. Um, WHY trolley type, um, I guess

58
00:03:05.029 --> 00:03:08.699
because trolleys are the, the quintessential example. Um, THERE'S

59
00:03:08.699 --> 00:03:12.690
many variations within the trolley dilemmas, but there's also

60
00:03:13.309 --> 00:03:15.750
many other dilemmas that have like this same basic

61
00:03:15.750 --> 00:03:18.830
shape, the same basic structure that do not involve

62
00:03:18.830 --> 00:03:22.550
trolley trains whatsoever, um, hence why it's a, it's

63
00:03:22.550 --> 00:03:24.190
a class of moral dilemmas, yeah.

64
00:03:25.589 --> 00:03:28.149
So, I mean, please correct me if I'm wrong,

65
00:03:28.229 --> 00:03:31.440
but I, I think that one of the goals

66
00:03:31.440 --> 00:03:36.589
with studying or doing studies where we expose people

67
00:03:36.589 --> 00:03:40.750
to truly type moral dilemmas and see how they

68
00:03:40.750 --> 00:03:44.100
respond to them is to try to understand whether

69
00:03:44.100 --> 00:03:50.229
people prefer uh the ontological or a consequentialist approach

70
00:03:50.229 --> 00:03:53.550
to them. So what do people tend to prefer?

71
00:03:53.679 --> 00:03:54.779
Do we know that?

72
00:03:55.919 --> 00:04:00.190
Um, IT, it, it really depends on what specific

73
00:04:00.190 --> 00:04:03.270
dilemma that you give them. Um, SO there are,

74
00:04:03.350 --> 00:04:06.149
there are some dilemmas, the classic trolley dilemma where

75
00:04:06.149 --> 00:04:08.669
you just have to divert the trolley train to

76
00:04:08.669 --> 00:04:11.789
a secondary track, and they're like 80% of people

77
00:04:11.789 --> 00:04:15.509
will prefer the, uh, the so-called utilitarian response, the

78
00:04:15.509 --> 00:04:19.670
response that corresponds with, um, sacrificing one person for

79
00:04:19.670 --> 00:04:24.028
the greater good, um. There's some debate on whether

80
00:04:24.028 --> 00:04:27.748
we should really label these specific responses as utilitarian

81
00:04:27.748 --> 00:04:30.829
or deontological. Uh YOU'RE certainly correct in that, that

82
00:04:30.829 --> 00:04:33.109
the, the classic traditional way of looking at these

83
00:04:33.109 --> 00:04:36.148
dilemmas and looking at these responses is to sort

84
00:04:36.148 --> 00:04:39.558
of see them as proxies of these underlying philosophical

85
00:04:39.558 --> 00:04:42.919
ideas, and it's about contrasting, um, the ontological versus

86
00:04:42.919 --> 00:04:46.380
utilitarian. Um, BUT it's very easy to switch people's

87
00:04:46.380 --> 00:04:48.420
opinion on these dilemmas like the, the, the, the,

88
00:04:48.540 --> 00:04:51.130
again, the, the, much of the theorizing in this

89
00:04:51.130 --> 00:04:54.220
field started with the, the observation that if you

90
00:04:54.220 --> 00:04:57.519
give people the classic troy dilemma, they prefer deleitarian

91
00:04:57.519 --> 00:04:59.899
response. But if you change it a little bit

92
00:04:59.899 --> 00:05:03.820
around, if you ask people, um, about a variation

93
00:05:03.820 --> 00:05:06.269
of a troy dilemma, the footbridge dilemma, where it's

94
00:05:06.269 --> 00:05:08.730
not about diverting the trolley train to another track,

95
00:05:08.779 --> 00:05:11.230
but where you have to stop the trolley. By

96
00:05:11.230 --> 00:05:14.670
pushing a large stranger in front of the train

97
00:05:14.670 --> 00:05:18.380
and using their considerable mass, then people tend to

98
00:05:18.380 --> 00:05:21.269
refer to, the deontological response. So it really depends

99
00:05:21.269 --> 00:05:24.470
on, um, the type of dilemma that you give

100
00:05:24.470 --> 00:05:28.470
them. But having said, um, there is a, a

101
00:05:28.470 --> 00:05:32.429
consistency across dilemmas though. So people, um, there are

102
00:05:32.429 --> 00:05:36.230
a certain set of personality, uh, differences that sort

103
00:05:36.230 --> 00:05:39.109
of help determine what response that people prefer in

104
00:05:39.109 --> 00:05:41.390
general. And so you could say that there are

105
00:05:41.390 --> 00:05:44.790
some people that are consistently more likely to prefer

106
00:05:44.790 --> 00:05:46.700
the so-called utilitarian choice. There are people that are

107
00:05:46.700 --> 00:05:49.309
consistently more likely to prefer to prefer the so-called

108
00:05:49.309 --> 00:05:49.899
deontological choice, yeah.

109
00:05:52.109 --> 00:05:55.029
Uh, BUT in one of your studies, you found

110
00:05:55.029 --> 00:05:59.029
out that when confronted with a trolley problem, people

111
00:05:59.029 --> 00:06:05.670
conform to the ontological but no consequentialist majorities. Could

112
00:06:05.670 --> 00:06:06.910
you explain that?

113
00:06:08.019 --> 00:06:10.390
Yeah, so what we did in that study was,

114
00:06:10.459 --> 00:06:12.899
um, we, we confronted people with a, with a

115
00:06:12.899 --> 00:06:15.279
series of these, these, these dilemmas. Not all Troy

116
00:06:15.279 --> 00:06:17.679
dilemmas, some of them were the same basic structure

117
00:06:17.679 --> 00:06:21.279
but different, different contents, um, and then we told

118
00:06:21.279 --> 00:06:23.570
people like, hey, the majority of people prefer this

119
00:06:23.570 --> 00:06:25.459
option or the majority of people prefer that option.

120
00:06:25.540 --> 00:06:28.239
And what we found there is that indeed, um,

121
00:06:28.420 --> 00:06:32.890
people, um, were more likely to shift their responses

122
00:06:33.459 --> 00:06:36.440
in the direction of the deontological response of not

123
00:06:36.440 --> 00:06:39.350
harming. Of not committing the sacrificial harm, of not

124
00:06:39.350 --> 00:06:41.750
harming the single person to save the 5 versus

125
00:06:41.750 --> 00:06:44.790
the other way around. Um, THE, the way that

126
00:06:44.790 --> 00:06:48.619
we interpret that effect is that, um, I mean,

127
00:06:48.660 --> 00:06:52.070
moral judgment is to some extent about judging behavior,

128
00:06:52.179 --> 00:06:55.079
about judging actions, but it's also about judging people,

129
00:06:55.540 --> 00:06:58.980
um, and, uh, when we, we make a moral

130
00:06:58.980 --> 00:07:02.700
judgment, obviously we, we, we will usually try and

131
00:07:02.700 --> 00:07:04.700
pursue the, the course of action that we think

132
00:07:04.700 --> 00:07:07.380
is best, but we do take into account what

133
00:07:07.380 --> 00:07:10.869
other people. Might think about that as well, obviously,

134
00:07:11.579 --> 00:07:13.859
um, and the moral judgments that we make, they

135
00:07:13.859 --> 00:07:16.579
are in a way also a sort, they serve

136
00:07:16.579 --> 00:07:20.579
a communicative purpose as well, um, and there's been

137
00:07:20.579 --> 00:07:22.859
some research, and I'm sure we might talk about

138
00:07:22.859 --> 00:07:25.859
this later as well, uh, that, that has suggested

139
00:07:25.859 --> 00:07:30.260
that in general, um, people tend to prefer theontologists

140
00:07:30.260 --> 00:07:32.619
over utilitarians and the way that we interpret it.

141
00:07:33.000 --> 00:07:37.359
Our, um, our results is that people strategically shift

142
00:07:37.359 --> 00:07:39.720
their responses towards the ontological side because it's the,

143
00:07:39.799 --> 00:07:43.540
it's the more socially safe side to, to pursue.

144
00:07:43.959 --> 00:07:46.470
Um, IT'S a little bit more risky, uh, people,

145
00:07:46.519 --> 00:07:49.920
people might distrust you a little bit more if

146
00:07:49.920 --> 00:07:53.320
you make utilitarian judgment, um, and that is why,

147
00:07:53.359 --> 00:07:57.899
uh, we found a, a larger conformity effect, uh,

148
00:07:57.989 --> 00:08:01.519
in the deontological direction versus utilitarian direction, yeah.

149
00:08:02.829 --> 00:08:05.429
I mean, let me ask you this, with these

150
00:08:05.429 --> 00:08:09.510
kinds of studies, are you interested in or is

151
00:08:09.510 --> 00:08:14.989
it possible to tell whether people would be in

152
00:08:14.989 --> 00:08:20.809
a sort of innate way predisposed toward the ontology

153
00:08:20.809 --> 00:08:24.790
or toward consequentialism? I mean, is that something that

154
00:08:24.790 --> 00:08:28.670
you're interested in exploring and is that something that

155
00:08:28.670 --> 00:08:32.109
these studies can tell us or not?

156
00:08:32.597 --> 00:08:34.138
Do you, do you, do you, do you mean,

157
00:08:34.359 --> 00:08:36.438
uh, like, like, like personality wise, are people with

158
00:08:36.438 --> 00:08:38.438
like more of a utilitarian personality, people with,

159
00:08:38.457 --> 00:08:40.638
I mean, I mean, if, if, if, if in

160
00:08:40.638 --> 00:08:43.438
terms, uh, uh, of course these are probably not

161
00:08:43.438 --> 00:08:45.419
the best terms to use, but if in terms

162
00:08:45.419 --> 00:08:48.879
of our human nature we tend to be more

163
00:08:48.879 --> 00:08:54.239
predisposed toward preferring the ontology or consequentialism.

164
00:08:55.349 --> 00:08:59.890
Um, I, um. I think there certainly have been

165
00:08:59.890 --> 00:09:03.250
people that have been trying to, to show one

166
00:09:03.250 --> 00:09:06.809
way or the other. Um, I'm, I'm myself would

167
00:09:06.809 --> 00:09:09.289
be a little bit hesitant to say that the

168
00:09:09.289 --> 00:09:11.929
type of studies that we're doing would be able

169
00:09:11.929 --> 00:09:15.830
to answer that kind of question, um, because mainly

170
00:09:15.830 --> 00:09:18.280
uh the amount of variety that I see in

171
00:09:18.289 --> 00:09:21.049
in how people respond to these dilemmas and how

172
00:09:21.049 --> 00:09:25.109
quickly people shift shift from one. Um, FROM one

173
00:09:25.109 --> 00:09:27.340
option to the other, I find it's very hard

174
00:09:27.340 --> 00:09:29.830
to, to make a kind of general statement like

175
00:09:29.830 --> 00:09:34.409
people overall preferred the intelligence, people overall preferred utilitarians.

176
00:09:34.750 --> 00:09:36.909
I think a lot of the dilemmas that we

177
00:09:36.909 --> 00:09:39.270
have been using in the fields are also a

178
00:09:39.270 --> 00:09:43.500
very specific set of dilemmas, um, and it's, it's,

179
00:09:43.549 --> 00:09:47.580
it's. We haven't explored a sufficiently broad set of

180
00:09:47.580 --> 00:09:50.500
dilemmas just yet um to be able to really

181
00:09:50.500 --> 00:09:53.000
provide a, a good answer to that question.

182
00:09:54.950 --> 00:09:57.349
Yeah, no, that's fair enough. I just wanted to

183
00:09:57.349 --> 00:09:59.909
know if this was something that these kinds of

184
00:09:59.909 --> 00:10:02.950
studies could answer or not. So, uh, you have

185
00:10:02.950 --> 00:10:07.390
done experiments on people where they had to decide

186
00:10:07.390 --> 00:10:11.145
whether to allow two Confederates to receive. A painful

187
00:10:11.145 --> 00:10:15.585
electroshock or shock a third confederate. I mean this

188
00:10:15.585 --> 00:10:20.184
is also a trolley type kind of problem. Which

189
00:10:20.184 --> 00:10:23.744
results did you get and what motivations did people

190
00:10:23.744 --> 00:10:25.364
provide for their choice?

191
00:10:26.140 --> 00:10:29.150
Yeah, so the, the, the, this is, this, this

192
00:10:29.150 --> 00:10:31.109
is somewhat have been my, my, my, my most

193
00:10:31.109 --> 00:10:33.950
recent focus, um, actually that, that, that paper just

194
00:10:33.950 --> 00:10:36.390
got accepted and this will be upcoming, uh, pretty

195
00:10:36.390 --> 00:10:40.469
soon, um, so very timely question, um, but yeah,

196
00:10:40.669 --> 00:10:43.469
so the, the, the main idea of that study

197
00:10:43.469 --> 00:10:45.880
was, um, we're doing all of this research on

198
00:10:46.619 --> 00:10:50.590
rota dilemmas and for like very obvious reasons we

199
00:10:50.590 --> 00:10:54.239
can't go. Um, HIRE trains to run over people

200
00:10:54.239 --> 00:10:56.760
like that would not be a fairly ethical way

201
00:10:56.760 --> 00:11:00.039
to conduct research, um, and so a lot of

202
00:11:00.039 --> 00:11:01.469
the research that we have been doing in moral

203
00:11:01.469 --> 00:11:04.159
psychology is just asking people to reflect on these

204
00:11:04.159 --> 00:11:07.260
moral dilemmas. Um, THERE'S a little bit of a

205
00:11:08.320 --> 00:11:10.630
Asking someone to reflect on a dilemma or having

206
00:11:10.630 --> 00:11:13.979
them actually respond to a dilemma with real consequences,

207
00:11:14.549 --> 00:11:17.770
um. The two things are not fully the same

208
00:11:17.770 --> 00:11:20.260
psychologically speaking, and so, so I have been looking

209
00:11:20.840 --> 00:11:23.950
for ways to create, um, sort of trolley type

210
00:11:23.950 --> 00:11:27.119
dilemmas within the lab and confront people with an

211
00:11:27.119 --> 00:11:29.760
actual consequential real dilemma and see like how do

212
00:11:29.760 --> 00:11:32.039
they behave. And so that was the, the, the

213
00:11:32.039 --> 00:11:33.960
main idea behind that study and the way we

214
00:11:33.960 --> 00:11:36.760
did that is like you described, um, I invited

215
00:11:36.760 --> 00:11:38.780
participants to a lab and then I had 3

216
00:11:38.780 --> 00:11:41.729
confederates there, 3 targets, all of which were hooked

217
00:11:41.729 --> 00:11:43.429
up to electroshock machines and I would tell my

218
00:11:43.429 --> 00:11:46.659
participants, hey, um. We're going to do a moral

219
00:11:46.659 --> 00:11:48.469
dilemma today. Two of these people are going to

220
00:11:48.469 --> 00:11:51.789
get a shock unless you decide. Um, TO shock

221
00:11:51.789 --> 00:11:55.969
the single person instead. Um, THE results that we

222
00:11:55.969 --> 00:11:58.169
found, I thought they were quite interesting, and there's

223
00:11:58.169 --> 00:12:01.450
many different angles that we can, I can look

224
00:12:01.450 --> 00:12:03.640
at, um, but as you pointed out, one of

225
00:12:03.640 --> 00:12:05.450
the things that we also did is we had

226
00:12:05.450 --> 00:12:07.849
people make these decisions and then we asked them

227
00:12:07.849 --> 00:12:11.309
why did you make these decisions and, um, the

228
00:12:11.309 --> 00:12:13.609
motivations that people gave for those, those decisions, I

229
00:12:13.609 --> 00:12:16.520
thought they were quite interesting. Um, AND so just

230
00:12:16.520 --> 00:12:19.489
to give like a sort of general summary of,

231
00:12:19.570 --> 00:12:21.289
of what we found there is that when people

232
00:12:21.289 --> 00:12:25.760
make. Uh, THE so-called utilitarian choice when they decided

233
00:12:25.760 --> 00:12:27.400
to shock the single person to save it to

234
00:12:27.400 --> 00:12:31.380
others, um, in almost all cases, they did motivate

235
00:12:31.380 --> 00:12:34.000
that with a sort of cost-benefit analysis. They just

236
00:12:34.000 --> 00:12:36.559
said that, well, I mean, if it, if, if

237
00:12:36.559 --> 00:12:39.359
people are going to get hurt, I'm gonna pursue

238
00:12:39.359 --> 00:12:41.880
the greater good, and it's better if only one

239
00:12:41.880 --> 00:12:45.799
person gets hurt versus two. WHAT is interesting is

240
00:12:45.799 --> 00:12:50.020
what, uh, how people motivated their choices. When they

241
00:12:50.020 --> 00:12:52.539
didn't decide to do that, uh, there we actually

242
00:12:52.539 --> 00:12:55.830
found what I feel is a great variety of,

243
00:12:55.880 --> 00:12:59.940
of motivations. Um, THERE were some people that, um,

244
00:12:59.950 --> 00:13:03.190
I wasn't really expecting this ahead of time that,

245
00:13:03.200 --> 00:13:05.789
um, sort of said, um, I did not want

246
00:13:05.789 --> 00:13:08.460
to hurt the single person because that person was

247
00:13:08.460 --> 00:13:10.830
sitting alone and at least the two people, they

248
00:13:10.830 --> 00:13:14.109
have each other. Um, SHARED harm is, is better

249
00:13:14.109 --> 00:13:16.469
than having to suffer alone. Now just to be

250
00:13:16.469 --> 00:13:18.229
clear, like, like everyone was sitting in one room,

251
00:13:18.309 --> 00:13:20.590
like it was not like the one person was

252
00:13:20.590 --> 00:13:23.349
separated. Uh, FROM the two waters they were like

253
00:13:23.349 --> 00:13:26.200
everyone was sitting like 1 m apart, um, but

254
00:13:26.200 --> 00:13:27.559
still like a lot of people were like, oh,

255
00:13:27.599 --> 00:13:31.489
I didn't wanna. Uh, THE single person was already

256
00:13:31.710 --> 00:13:35.349
sort of punished because they were sitting alone, um,

257
00:13:35.669 --> 00:13:37.630
so that was a motivation that occurred quite often.

258
00:13:38.030 --> 00:13:40.460
Some participants said, well, I didn't feel like it

259
00:13:40.460 --> 00:13:43.390
was my responsibility to make this decision. Um, THE,

260
00:13:43.590 --> 00:13:46.349
the sort of universe and fate had already decided

261
00:13:46.349 --> 00:13:47.989
that these two people were going to get shocked

262
00:13:47.989 --> 00:13:51.030
and, and who am I to interfere and interject

263
00:13:51.030 --> 00:13:53.650
my own moral opinion in this sort of situation.

264
00:13:54.359 --> 00:13:56.840
Um, DID it have a group of people, um,

265
00:13:57.080 --> 00:13:59.919
which I would say corresponds a little bit more

266
00:13:59.919 --> 00:14:01.799
with like the classic way that we're thinking about

267
00:14:01.799 --> 00:14:03.280
these dilemmas that did actually say like I did

268
00:14:03.280 --> 00:14:05.700
not want to actively harm others. I didn't wanna,

269
00:14:06.159 --> 00:14:08.979
um, I thought it was bad. Um, I thought,

270
00:14:09.039 --> 00:14:12.000
um, everyone had an equal right not to receive

271
00:14:12.000 --> 00:14:14.479
a shock, um, and so, so what right do

272
00:14:14.479 --> 00:14:18.250
I have to like sacrifice an innocent person. Um

273
00:14:19.039 --> 00:14:21.890
And then you also had people that uh just

274
00:14:21.890 --> 00:14:25.130
expressed like a a a and it might might

275
00:14:25.130 --> 00:14:26.369
appear a little bit silly, but I don't think

276
00:14:26.369 --> 00:14:28.450
it's, it's, it's, it's that silly a a preference

277
00:14:28.450 --> 00:14:31.570
for like a specific person like like oh that

278
00:14:31.570 --> 00:14:32.929
person was smiling at me, so I didn't want

279
00:14:32.929 --> 00:14:36.929
to shock them with this, um, obviously from like

280
00:14:36.929 --> 00:14:40.130
a philosophical perspective might not be the best argumentation

281
00:14:40.130 --> 00:14:42.770
for that type of judgment. To me it sort

282
00:14:42.770 --> 00:14:46.820
of corresponds with um. And maybe there's a little

283
00:14:46.820 --> 00:14:49.419
bit farfetched, but, but almost like this moral dessert

284
00:14:49.419 --> 00:14:50.820
type of thinking. I did not, I feel like

285
00:14:50.820 --> 00:14:54.640
that person didn't deserve to be shocked. Um, AND

286
00:14:54.650 --> 00:14:56.559
so one of the takeaways that we have from

287
00:14:56.559 --> 00:14:59.359
that study is that And we tend to see

288
00:14:59.359 --> 00:15:02.440
these, uh, to look at the judgments that people

289
00:15:02.440 --> 00:15:06.359
make, it's just like it's deontological versus utilitarian, um.

290
00:15:07.599 --> 00:15:11.760
But I struggle to, um, you could obviously argue

291
00:15:11.760 --> 00:15:13.960
that what we're seeing in the deontological case is

292
00:15:13.960 --> 00:15:17.229
that, um, the various types of motivations that weren't

293
00:15:17.229 --> 00:15:20.400
governing there, that's sort of a post hoc, post

294
00:15:20.400 --> 00:15:22.469
hoc reasoning that people are doing to sort of,

295
00:15:22.969 --> 00:15:24.760
um, they don't want to commit to the judgment

296
00:15:24.760 --> 00:15:26.919
and so they, they, they grasp onto every possible

297
00:15:26.919 --> 00:15:31.260
explanation that they can, um. And that is like

298
00:15:31.260 --> 00:15:32.780
we can't prove that with the studies that we've

299
00:15:32.780 --> 00:15:35.539
done so far that these are actual things that

300
00:15:35.539 --> 00:15:40.539
have influenced people's moral judgments. Um, BUT if you're

301
00:15:40.539 --> 00:15:43.179
willing to sort of assume that, hey, maybe this

302
00:15:43.179 --> 00:15:46.140
is actually cool, maybe, maybe people are being sincere

303
00:15:46.140 --> 00:15:50.299
and maybe um. There are these different types of

304
00:15:50.299 --> 00:15:52.450
motivations driving these judgments, and all of a sudden

305
00:15:52.789 --> 00:15:55.530
there appears to be um all of these layers

306
00:15:55.950 --> 00:15:58.390
that we've, that I feel had previously gone and

307
00:15:58.390 --> 00:16:02.260
explored. Um, AND I think we can actually make

308
00:16:02.260 --> 00:16:04.820
a good case that that is what is actually

309
00:16:04.820 --> 00:16:06.020
going on because a lot of the things that

310
00:16:06.020 --> 00:16:09.140
we're seeing, um, we are seeing in, in other

311
00:16:09.140 --> 00:16:12.539
types of moral judgments that people make when And

312
00:16:13.700 --> 00:16:16.179
For instance, the, the no responsibility type of judgment,

313
00:16:16.260 --> 00:16:17.580
people that are saying, oh, I don't want to

314
00:16:17.580 --> 00:16:20.469
get engaged. This is not my, my responsibility. We

315
00:16:20.469 --> 00:16:22.739
make those judgments all the time. Like if you're

316
00:16:22.739 --> 00:16:25.049
walking down the street and you see, you, you

317
00:16:25.049 --> 00:16:27.460
see a homeless person, that is a sort of

318
00:16:27.460 --> 00:16:29.820
moral dilemma that you're encountering then. But you, and

319
00:16:29.820 --> 00:16:32.659
in many cases people will decide actively, this is

320
00:16:32.659 --> 00:16:35.729
not my responsibility. This is not my, my moral

321
00:16:35.729 --> 00:16:37.580
responsibility to solve, and I think that some of

322
00:16:37.580 --> 00:16:41.440
what we're seeing. In, in this real life sacrificial

323
00:16:41.440 --> 00:16:45.039
dharma is, is exactly that and um so one

324
00:16:45.039 --> 00:16:46.479
of the things that we certainly want to, want

325
00:16:46.479 --> 00:16:47.650
to try and do in the future is, is,

326
00:16:47.679 --> 00:16:51.119
is, is disentangle that a little bit more and

327
00:16:51.119 --> 00:16:53.840
see if we can really see are there different

328
00:16:53.840 --> 00:16:57.270
types of psychological processes underlying these types of adjustments

329
00:16:57.270 --> 00:16:59.760
versus just the ontological versus utilitarian.

330
00:17:01.000 --> 00:17:04.900
I mean, that no responsibility approach that people might

331
00:17:04.900 --> 00:17:07.800
have uh, I find it very interesting because, I

332
00:17:07.800 --> 00:17:11.880
mean, from the perspective of moral philosophy or ethics

333
00:17:11.880 --> 00:17:17.219
where instead of just trying to describe how people

334
00:17:17.439 --> 00:17:21.444
react or respond to this kind of. Of problems

335
00:17:21.444 --> 00:17:25.405
or moral dilemmas where we're trying to establish what

336
00:17:25.405 --> 00:17:30.005
is the most moral way to conduct ourselves, whether

337
00:17:30.005 --> 00:17:33.285
it is to divert the trolley toward the other

338
00:17:33.285 --> 00:17:36.165
track and kill one person or just let the

339
00:17:36.165 --> 00:17:41.619
other people die, um. I mean, is the, when

340
00:17:41.619 --> 00:17:45.589
people say that they don't want to just interfere

341
00:17:45.589 --> 00:17:48.660
or do anything because they're not responsible for what's

342
00:17:48.660 --> 00:17:54.380
happening, wouldn't that be a philosophically valid approach as

343
00:17:54.380 --> 00:17:57.880
well, because I mean, I, I'm just thinking that

344
00:17:58.209 --> 00:18:02.099
if you find yourself in a situation where a

345
00:18:02.099 --> 00:18:05.619
trolley is coming down the track. I mean, it's

346
00:18:05.619 --> 00:18:10.050
not you causing that, and if you do something,

347
00:18:10.180 --> 00:18:14.660
whatever happens, then it's your fault, right? So, I

348
00:18:14.660 --> 00:18:18.530
mean, uh uh as a philosopher, because you're also

349
00:18:18.530 --> 00:18:21.619
a philosopher, what do you think about that? Do

350
00:18:21.619 --> 00:18:25.780
you think that's a valid uh ethical approach as

351
00:18:25.780 --> 00:18:26.119
well?

352
00:18:27.319 --> 00:18:29.680
Um, um, I mean, I would, I would say

353
00:18:29.680 --> 00:18:32.439
it is, um, I, I, I, I think, um,

354
00:18:32.560 --> 00:18:33.880
I mean, I gave the example of the homeless

355
00:18:33.880 --> 00:18:37.939
person, right? Um, WE have to, we can't decide

356
00:18:37.939 --> 00:18:40.829
with any moral dilemma situ, we can't engage with

357
00:18:40.829 --> 00:18:43.780
any possible moral dilemma situation out there. There's, there's

358
00:18:43.780 --> 00:18:47.280
plenty of moral dilemmas and, and horribly immoral things

359
00:18:47.280 --> 00:18:49.099
going on over the world, all over the world

360
00:18:49.280 --> 00:18:51.670
that we're actively choosing not to engage with. And

361
00:18:51.670 --> 00:18:54.270
so we, we, we have to make those judgments

362
00:18:54.270 --> 00:18:57.729
in, in, in some way, um. And so I

363
00:18:57.729 --> 00:18:59.449
think, I, I think it's a, it's a valid

364
00:18:59.449 --> 00:19:01.359
philosophical position to take. I also think it's just,

365
00:19:01.410 --> 00:19:03.770
it's, it's a psychologically necessary thing that we need

366
00:19:03.770 --> 00:19:07.790
to do. Um, IT'S, it's, it's. But it's also

367
00:19:07.790 --> 00:19:11.819
something that I feel um we haven't fully taken

368
00:19:11.819 --> 00:19:14.930
into account when it comes to studying how people

369
00:19:15.229 --> 00:19:18.189
approach moral dilemmas because if we, if we present

370
00:19:18.189 --> 00:19:20.750
people with a, with a hypothetical moral dilemma, we're

371
00:19:20.750 --> 00:19:23.459
sort of assuming that they've already engaged with the

372
00:19:23.459 --> 00:19:28.270
situation, right? We've, we've sort of, um, you won't

373
00:19:28.270 --> 00:19:31.430
have, I mean, some, some, some, some people will

374
00:19:31.430 --> 00:19:35.560
say like it's, it's, it's not my, um, If

375
00:19:35.560 --> 00:19:36.910
you confront with a troy dilemma, a lot of

376
00:19:36.910 --> 00:19:38.140
people will say, oh, I don't want to get

377
00:19:38.140 --> 00:19:40.339
like sued, or I don't want to get like

378
00:19:40.339 --> 00:19:42.640
a, um, I don't want to murder someone, and

379
00:19:42.640 --> 00:19:45.119
then, you know, um, so there's some of that

380
00:19:45.119 --> 00:19:47.920
there, um, but I, I, I don't think we

381
00:19:47.920 --> 00:19:51.680
fully reckoned with the, the possible difference between people

382
00:19:51.680 --> 00:19:55.160
refusing to engage with a moral dilemma versus people

383
00:19:55.160 --> 00:19:57.719
actively deciding I don't want to harm someone. I

384
00:19:57.719 --> 00:19:59.079
think, I think there, I think there's a, there's

385
00:19:59.079 --> 00:20:01.969
a difference there, um, that we need to explore,

386
00:20:02.119 --> 00:20:03.920
uh, further. Yeah,

387
00:20:03.930 --> 00:20:07.189
mhm. No, yeah, that, that's very interesting. So let

388
00:20:07.189 --> 00:20:10.829
me ask you now about sacrificial dilemmas and then

389
00:20:10.829 --> 00:20:14.099
some of the principles associated with them. So, but

390
00:20:14.099 --> 00:20:17.189
first of all, what is a sacrificial dilemma?

391
00:20:18.189 --> 00:20:19.920
So the, the, the, the, the sacrificial dilemma, it's,

392
00:20:19.989 --> 00:20:22.050
it's, it's, it's sort of, um, I tend to

393
00:20:22.050 --> 00:20:24.040
use it as a similar term as trolley type

394
00:20:24.040 --> 00:20:27.400
dilemma. I think I early in my career, I

395
00:20:27.400 --> 00:20:29.800
think I used to call things troy type dilemmas,

396
00:20:29.810 --> 00:20:32.359
and then eventually it's, I think the fields were

397
00:20:32.359 --> 00:20:34.880
still sort of figuring out how do we, how

398
00:20:34.880 --> 00:20:36.760
do we call this broader class of dilemmas, and

399
00:20:36.760 --> 00:20:40.810
then, um, we eventually settled on sacrificial because it's

400
00:20:41.439 --> 00:20:43.969
Um, that is, in a way that the, the

401
00:20:43.969 --> 00:20:46.170
core problem that is underlying Islam, the problem of

402
00:20:46.170 --> 00:20:51.170
sacrificial harm is appropriate to actively harm, to, um,

403
00:20:51.410 --> 00:20:52.930
pursue it as a sort of greater good. So

404
00:20:52.930 --> 00:20:54.729
that, I would say they're, they're troll type and

405
00:20:54.729 --> 00:20:57.930
sacrificial. You can use those, uh, um, in the

406
00:20:57.930 --> 00:20:58.290
same way.

407
00:20:58.930 --> 00:21:02.250
OK, OK, fine, uh, but, uh, tell us then

408
00:21:02.250 --> 00:21:06.319
about the action principle, the contact principle, and the

409
00:21:06.319 --> 00:21:10.609
intention principle and how they play out in. SACRIFICIAL

410
00:21:10.609 --> 00:21:12.750
or trolley type dilemmas.

411
00:21:13.589 --> 00:21:15.020
I mean, so there's, there, there, there's, there's a

412
00:21:15.020 --> 00:21:17.890
lot of variations that you can make in, in

413
00:21:18.160 --> 00:21:20.839
Troy de lamas and, um, I guess the action,

414
00:21:20.910 --> 00:21:22.630
like these three principles that you're, that you're calling

415
00:21:22.630 --> 00:21:24.660
out, they're, they're big ones. They have been studied

416
00:21:24.660 --> 00:21:27.160
quite a lot, um, and the, the core idea

417
00:21:27.160 --> 00:21:29.270
of the action principle, for instance, is that people

418
00:21:29.270 --> 00:21:32.469
tend to, uh, if you, uh, tend to think

419
00:21:32.469 --> 00:21:35.229
it's, it's, it's worse to actively cause harm than

420
00:21:35.229 --> 00:21:38.949
to cause harm by omitting an action. Um, IT,

421
00:21:39.069 --> 00:21:40.349
it's, it's, it's, it's maybe a little bit of

422
00:21:40.349 --> 00:21:42.670
an, an, an over the top example, but it's,

423
00:21:42.750 --> 00:21:45.069
it's sort of, it's worse to push someone into

424
00:21:45.069 --> 00:21:47.270
a river causing them to drown than to, if

425
00:21:47.270 --> 00:21:50.770
you see someone drowning, not save them. Um, SO

426
00:21:50.869 --> 00:21:53.339
committing a harm by action is worse than committing

427
00:21:53.339 --> 00:21:56.829
a harm by inaction. Um, SIMILARLY, if you, if

428
00:21:56.829 --> 00:22:01.599
you intend, uh, a harmful outcome. Um, THAT is

429
00:22:01.599 --> 00:22:04.160
considered to be worse than if the, the harmful

430
00:22:04.160 --> 00:22:06.880
outcome is the, is the result of a, a

431
00:22:06.880 --> 00:22:08.959
sort of a by effect like it's collateral damage.

432
00:22:09.040 --> 00:22:11.959
You do not necessarily intend to the harm, but

433
00:22:11.959 --> 00:22:13.640
it is a, is a, is a, is a

434
00:22:13.640 --> 00:22:16.819
side effect, um, and the contact principle is just

435
00:22:17.030 --> 00:22:19.660
that people tend to think harm is worse when

436
00:22:19.660 --> 00:22:22.760
uh the harm that you commit is, is, uh,

437
00:22:22.839 --> 00:22:25.989
a close and personal, is, is direct. Uh, IF

438
00:22:25.989 --> 00:22:28.910
it requires physical contact versus if it's a little

439
00:22:28.910 --> 00:22:33.209
bit more indirect, um. And those, those, those three

440
00:22:33.489 --> 00:22:36.959
differences, um, so those three principles have been studied

441
00:22:36.959 --> 00:22:39.810
lots through sacrificial dilemmas and depending on like how

442
00:22:39.810 --> 00:22:41.890
you, how you, which side of the principle that

443
00:22:41.890 --> 00:22:45.250
you're on, um, you can push people's preferred response

444
00:22:45.250 --> 00:22:45.939
around a little bit.

445
00:22:46.719 --> 00:22:52.550
Mhm. And what are iterative sacrificial dilemmas, uh, and

446
00:22:52.930 --> 00:22:55.890
how do they work and what kinds of results

447
00:22:55.890 --> 00:22:57.189
do you get there?

448
00:22:57.609 --> 00:23:00.689
Yeah, so the, the, the core idea behind iterative

449
00:23:00.689 --> 00:23:03.170
dilemmas is that, um, when we, when we give

450
00:23:03.170 --> 00:23:07.739
people moral dilemmas in a typical, uh, experiment. Um,

451
00:23:08.359 --> 00:23:10.280
THOSE moral dilemmas, they're also, they're always sort of

452
00:23:10.280 --> 00:23:13.439
isolated. They're always sort of within their own little

453
00:23:13.439 --> 00:23:17.800
universe. Um, PEOPLE give one response and then, um,

454
00:23:18.000 --> 00:23:20.040
there's nothing that happens after, there's nothing that comes

455
00:23:20.040 --> 00:23:23.239
before, they're abstracted from time and space, and, and,

456
00:23:23.349 --> 00:23:25.680
and if you think about that, that's actually very

457
00:23:25.680 --> 00:23:30.520
different from how moral dilemma situations are in real

458
00:23:30.520 --> 00:23:33.239
life in, in, in real life, um, there is

459
00:23:33.239 --> 00:23:35.760
a prior history, there is a future history. And

460
00:23:35.760 --> 00:23:38.469
there's never only one thing that you can do.

461
00:23:38.839 --> 00:23:41.439
Um, YOU can react and then you can, you

462
00:23:41.439 --> 00:23:43.680
know, see the outcome and you can possibly react

463
00:23:43.680 --> 00:23:47.280
again and react again and react again, and um.

464
00:23:49.060 --> 00:23:52.020
The, the, the, the, what we've done is, is

465
00:23:52.020 --> 00:23:56.619
essentially we, we, um, confronted participants with a series

466
00:23:56.619 --> 00:23:59.219
of sacrificial dilemma and essentially it was always the

467
00:23:59.219 --> 00:24:02.209
same dilemma with the same targets, the same victims,

468
00:24:02.300 --> 00:24:05.060
the same people involved, um, and we were just

469
00:24:05.060 --> 00:24:07.819
interested like is that gonna change how people respond

470
00:24:07.819 --> 00:24:10.739
to these dilemmas and, um, just like as, as

471
00:24:10.739 --> 00:24:12.510
a, as a, as a classic example or less

472
00:24:12.510 --> 00:24:15.969
as an easy example. In that uh that electroshock

473
00:24:15.969 --> 00:24:18.619
study that we did, and that also included an

474
00:24:18.619 --> 00:24:21.849
iterative element. So participants entered the room, they were

475
00:24:21.849 --> 00:24:23.979
asked, OK, so the two people are going to

476
00:24:23.979 --> 00:24:25.579
get shocked. Do you want to shock the single

477
00:24:25.579 --> 00:24:28.900
person? And then, and they decided, some people got

478
00:24:28.900 --> 00:24:31.329
shocked, and then, and they did not notice ahead

479
00:24:31.329 --> 00:24:33.780
of time, we asked them, we basically said, OK,

480
00:24:33.859 --> 00:24:36.619
we're going to do that same dilemma again. The

481
00:24:36.619 --> 00:24:39.099
same two people are going to get shocked, and,

482
00:24:39.310 --> 00:24:41.500
and you can decide once again to shock the

483
00:24:41.500 --> 00:24:45.290
third person instead. And now, even though that is

484
00:24:45.290 --> 00:24:47.849
in a way exactly the same dilemma situation, right,

485
00:24:47.890 --> 00:24:51.030
it's the same people sitting there. Um, IT'S the

486
00:24:51.030 --> 00:24:52.829
same type of shock. It's the same type of

487
00:24:52.829 --> 00:24:55.310
do you want to harm one person to save

488
00:24:55.310 --> 00:24:57.849
to others to pursue the greater, a greater good,

489
00:24:59.530 --> 00:25:01.670
um, the, the, because we've repeated the dilemma, that

490
00:25:01.670 --> 00:25:05.430
the underlying moral conflict has shifted and all of

491
00:25:05.430 --> 00:25:08.030
a sudden it's not only about do you want

492
00:25:08.030 --> 00:25:10.349
to minimize harm, it's also about how do you

493
00:25:10.349 --> 00:25:11.939
want to balance harm, how do you want to,

494
00:25:12.189 --> 00:25:14.069
do you want to pursue a fair outcome or

495
00:25:14.069 --> 00:25:16.949
do you want to pursue. Uh, AN outcome, um,

496
00:25:17.079 --> 00:25:21.079
that, um, leads to the greatest good. And, and

497
00:25:21.079 --> 00:25:23.280
so what we, what we see is that, um,

498
00:25:24.010 --> 00:25:26.219
And maybe this isn't too surprising, but in a

499
00:25:26.219 --> 00:25:28.959
way it is quite surprising is that people do

500
00:25:29.380 --> 00:25:33.500
really shift their responses quite heavily, um, and I

501
00:25:33.500 --> 00:25:36.819
think the, the, the electroshock study, and we did

502
00:25:36.819 --> 00:25:39.020
it twice and in the first, the first time

503
00:25:39.020 --> 00:25:41.660
we did it, I think 35% of people switched

504
00:25:41.660 --> 00:25:43.540
their response. I think in the second time we

505
00:25:43.540 --> 00:25:45.930
did it, it was a little bit lower, um,

506
00:25:46.140 --> 00:25:49.510
maybe 25% of people shifted their response in the

507
00:25:49.510 --> 00:25:54.520
direction of um. Just just just just switch their

508
00:25:54.520 --> 00:25:57.280
response, um, and if you then ask people like

509
00:25:57.280 --> 00:25:59.520
why did you do it, um, all of a

510
00:25:59.520 --> 00:26:02.229
sudden you see this new type of motivation emerge,

511
00:26:02.530 --> 00:26:06.079
a motivation that wasn't present, um, when you ask

512
00:26:06.079 --> 00:26:09.550
them about their motivations for the first decision, and

513
00:26:09.550 --> 00:26:11.439
that is this, this, this idea of, oh, I

514
00:26:11.439 --> 00:26:14.579
actually wanted to pursue a, a fair outcome, um,

515
00:26:14.760 --> 00:26:17.119
I wanted to give everyone one single shock and

516
00:26:17.119 --> 00:26:20.770
so that's, that's why I switched. And I think

517
00:26:20.770 --> 00:26:23.010
it's important because it's, it's, it's something that we've

518
00:26:23.010 --> 00:26:25.369
also been neglecting in the way that we've been

519
00:26:25.369 --> 00:26:29.489
studying moral dilemmas. Um, WE'VE sort of been thinking

520
00:26:29.489 --> 00:26:34.050
about, um, moral dilemmas as though they're every decision

521
00:26:34.050 --> 00:26:37.359
is independent from another decision, and that's not how

522
00:26:37.359 --> 00:26:41.150
morality works works at all. Um, EVERY moral decision

523
00:26:41.150 --> 00:26:43.770
is dependent on decisions that come before it and

524
00:26:43.770 --> 00:26:46.530
decisions that come after it and so. And if

525
00:26:46.530 --> 00:26:49.050
we truly want to understand moral cognition, then we

526
00:26:49.050 --> 00:26:54.430
have to also start to investigate that dependency across

527
00:26:55.010 --> 00:26:56.890
decisions and so that's kind of the idea that

528
00:26:56.890 --> 00:26:59.540
we've, uh, about these, these iterative dilemmas.

529
00:27:00.359 --> 00:27:04.329
Mhm. Uh, WHAT is the dual process model for

530
00:27:04.329 --> 00:27:07.540
moral cognition and how does it work?

531
00:27:08.209 --> 00:27:10.050
Uh, SO the, the dual process model, uh, for

532
00:27:10.050 --> 00:27:13.380
moral cognition, um. I, I think we're, we're, we're

533
00:27:13.380 --> 00:27:16.280
almost approaching an anniversary for the model right now.

534
00:27:16.619 --> 00:27:18.619
Um, I think that it was originally formulated in

535
00:27:18.619 --> 00:27:22.599
2001, uh, by Joshua Green and, and his colleagues,

536
00:27:22.819 --> 00:27:27.280
um, and, uh, essentially, um, the dual process model

537
00:27:27.280 --> 00:27:31.119
posits that, um. It's sort of a classic dual

538
00:27:31.119 --> 00:27:34.599
process model. It's, it's intuition versus deliberation, um, and

539
00:27:34.599 --> 00:27:37.880
it links intuition and deliberation to two types of

540
00:27:37.880 --> 00:27:41.439
responses when it comes to sacrificial dilemmas. Essentially it

541
00:27:41.439 --> 00:27:44.209
says when you confront people with a sacrificial dilemma,

542
00:27:44.640 --> 00:27:46.760
if you give them some time to deliberate, um,

543
00:27:47.040 --> 00:27:49.439
if they are, they're motivated, if they have a

544
00:27:49.439 --> 00:27:53.369
high motivation. Um, AND if they're not too emotionally

545
00:27:53.369 --> 00:27:57.390
impacted, that will lead to a higher likelihood of,

546
00:27:57.449 --> 00:28:00.790
um, a utilitarian decision of a, a decision that

547
00:28:01.199 --> 00:28:03.489
weighs the costs and benefits. That is sort of

548
00:28:03.489 --> 00:28:05.770
what the deliberative process is doing. It's weighing costs

549
00:28:05.770 --> 00:28:10.390
and benefits. It's looking at outcomes, whereas, um. It

550
00:28:10.390 --> 00:28:11.989
could also be the case, for instance, in the

551
00:28:11.989 --> 00:28:14.829
foot bridge dilemma, um, you have to push someone

552
00:28:14.829 --> 00:28:16.869
in front of the train, and that is a

553
00:28:16.869 --> 00:28:21.030
very, um, well, emotionally impactful thing to have to

554
00:28:21.030 --> 00:28:23.550
do, and so people like all of these, these

555
00:28:23.550 --> 00:28:26.030
emotional alarm bells are ringing and then people are

556
00:28:26.030 --> 00:28:28.550
like, oh, I don't even care about the outcomes

557
00:28:28.550 --> 00:28:32.030
anymore, um, and that is what causes, um, a

558
00:28:32.030 --> 00:28:34.349
deontological decision. So it's, it's, it's, it's sort of

559
00:28:34.349 --> 00:28:37.290
like this, this idea that cognition will lead to

560
00:28:37.290 --> 00:28:43.290
utilitarian decisions and, um. Emotion, intuition drives this sort

561
00:28:43.290 --> 00:28:48.010
of um evaluation of the action itself, um, this,

562
00:28:48.040 --> 00:28:49.829
this avowal of harm, yeah.

563
00:28:51.310 --> 00:28:56.109
I mean, does, is there a relationship between cognitive

564
00:28:56.109 --> 00:29:01.390
ability and consequentialist judgment? I mean, is it that

565
00:29:01.390 --> 00:29:07.030
people with higher cognitive ability tend to favor consequentialism

566
00:29:07.030 --> 00:29:09.939
or is there no relationship at all there?

567
00:29:11.010 --> 00:29:15.650
Um, SO there, there, there are studies suggesting both

568
00:29:15.650 --> 00:29:18.510
that it is and, and some studies that suggest,

569
00:29:18.530 --> 00:29:22.589
um, That it's not the case. Um, I think

570
00:29:22.589 --> 00:29:24.030
in general, I think, I think most people in

571
00:29:24.030 --> 00:29:27.670
the field would say that um there is an

572
00:29:27.670 --> 00:29:30.270
association, it might not be a super strong association

573
00:29:30.270 --> 00:29:35.349
between cognitive ability and, um, respond, condoning the sacrificial

574
00:29:35.349 --> 00:29:39.030
harm, responding in an egalitarian way and sacrificial dynammas,

575
00:29:39.510 --> 00:29:43.520
um. We don't always tend to find it. I

576
00:29:43.520 --> 00:29:46.680
myself have been very unlucky. It seems uh in

577
00:29:46.680 --> 00:29:48.829
most of the things that I've done, um, I

578
00:29:48.829 --> 00:29:52.359
don't really find this effect of, of either motivation

579
00:29:52.359 --> 00:29:55.959
or ability on the types of judgments, um, that

580
00:29:55.959 --> 00:30:00.130
people make, um, so my My personal opinion is

581
00:30:00.130 --> 00:30:02.810
that I do think there's, there's something there, um,

582
00:30:03.099 --> 00:30:05.290
I think it's mostly going to be there in

583
00:30:05.290 --> 00:30:07.770
these types of dilemmas where there's a strong emotional

584
00:30:07.770 --> 00:30:10.369
response that has to be overcome. Um, I'm a

585
00:30:10.369 --> 00:30:12.770
little bit more hesitant to say that if you

586
00:30:12.770 --> 00:30:16.160
were to look at all possible sacrificial dharmas, um,

587
00:30:16.209 --> 00:30:18.209
you would still find that association. I, I think

588
00:30:18.209 --> 00:30:21.170
it's likely true for a subset of dlamas. Um,

589
00:30:21.290 --> 00:30:23.270
I don't think it's true for all the lamas,

590
00:30:23.729 --> 00:30:26.349
um, for what it's worth in the, in the.

591
00:30:27.119 --> 00:30:30.410
Electroshock studies that we've been doing, um, I tend

592
00:30:30.410 --> 00:30:34.380
to find no effect whatsoever of, of, um. Short

593
00:30:34.380 --> 00:30:38.739
measures of ability or measures of motivation on the

594
00:30:38.739 --> 00:30:41.339
types of judgments that people prefer. I, I see

595
00:30:41.339 --> 00:30:43.510
no. No difference.

596
00:30:44.199 --> 00:30:46.500
Mhm. And do we know if there are any

597
00:30:46.500 --> 00:30:51.949
other types of psychological traits that might predispose people

598
00:30:51.949 --> 00:30:57.900
toward preferring the ontology or consequentialism slash utilitarianism?

599
00:30:59.300 --> 00:31:03.089
Yeah, um, uh, so the, the. Actually, the, the

600
00:31:03.089 --> 00:31:07.060
one, personality measure that I always seem to find

601
00:31:07.060 --> 00:31:10.290
has an effect, um, is, is on the emotional

602
00:31:10.290 --> 00:31:12.010
side of things. It's, it's, it's, it's for instance

603
00:31:12.010 --> 00:31:16.359
like something like primary psychopathy, measures of, of anti-social

604
00:31:16.359 --> 00:31:21.930
behavior, empathic concern, um, how easily are you affected

605
00:31:21.930 --> 00:31:23.489
by these horrible things that you might have to

606
00:31:23.489 --> 00:31:26.989
do in a dilemma, um, the, the, the, the.

607
00:31:27.719 --> 00:31:29.599
The less that you care about those, the more

608
00:31:29.599 --> 00:31:32.119
likely that you will be to condone the sacrificial

609
00:31:32.119 --> 00:31:33.839
harm, and I tend to find much stronger and

610
00:31:33.839 --> 00:31:39.099
consistent effects of those versus um Versus the the

611
00:31:39.109 --> 00:31:40.239
the deliberation measures.

612
00:31:41.459 --> 00:31:45.439
Right, so in that case, people who have the

613
00:31:45.569 --> 00:31:49.369
the that kind of trait would prefer consequentialism,

614
00:31:50.000 --> 00:31:51.849
yeah, yeah, yeah, um, yeah, so the, the, the,

615
00:31:51.900 --> 00:31:54.290
the, the, the, if you score higher on like,

616
00:31:54.300 --> 00:31:58.859
um, measures of anti-social thinking, um, you will tend

617
00:31:58.859 --> 00:32:01.140
to prefer, uh, utilitarian side of things which has

618
00:32:01.140 --> 00:32:03.209
also caused some debate in the field like are

619
00:32:03.209 --> 00:32:07.420
we actually measuring utilitarianism which is supposed to be

620
00:32:07.420 --> 00:32:12.660
this. Good thing worth pursuing if we're finding that

621
00:32:12.959 --> 00:32:16.119
um psychopaths tend to tend to favor those types

622
00:32:16.119 --> 00:32:19.839
of judgments um and and there's been some work

623
00:32:19.839 --> 00:32:23.270
on that um. Um, SOME interesting work that, that,

624
00:32:23.339 --> 00:32:25.280
that, that also suggests that, that we should think

625
00:32:25.280 --> 00:32:29.319
about utilitarianism as having multiple facets. Um, THERE'S a

626
00:32:29.319 --> 00:32:32.439
dimension of this instrumental harm dimension. Do you wanna,

627
00:32:32.640 --> 00:32:35.520
are you OK with using others to pursue a

628
00:32:35.520 --> 00:32:39.310
greater good, um, but also this dimension of, um,

629
00:32:39.479 --> 00:32:43.359
impartial beneficence. Um, ARE you OK with, do, do

630
00:32:43.359 --> 00:32:46.699
you, do you, do you want to spread well-being,

631
00:32:46.959 --> 00:32:53.130
um. To everyone, basically. Um, AND but the the

632
00:32:53.140 --> 00:32:55.069
the people that make that decision, that that split

633
00:32:55.069 --> 00:32:56.310
tend to say like, oh, it's the, it's the

634
00:32:56.310 --> 00:33:00.359
instrumental harm part that corresponds with antisocial thinking versus

635
00:33:00.359 --> 00:33:03.599
the, the beneficial part. Mhm.

636
00:33:04.400 --> 00:33:07.400
Yeah, OK. So, uh let me ask you now

637
00:33:07.400 --> 00:33:10.199
because I know this is something that moral philosophers

638
00:33:10.199 --> 00:33:17.420
and experimental philosophers debate sometimes. Are truly problems realistic?

639
00:33:17.640 --> 00:33:20.560
I mean, do they really tell us how people

640
00:33:20.560 --> 00:33:23.680
would behave in a real life situation?

641
00:33:25.550 --> 00:33:28.650
Um, I think I think there's a lot of

642
00:33:28.650 --> 00:33:35.839
layers to that question. Um, SO, so, um, One

643
00:33:35.839 --> 00:33:37.239
of, one of the, so one of the reasons

644
00:33:37.239 --> 00:33:39.069
why I'm, why I'm trying to study these, these

645
00:33:39.069 --> 00:33:41.640
things in the lab is because I, I do

646
00:33:41.640 --> 00:33:44.839
genuinely think that there's a difference between being actually

647
00:33:44.839 --> 00:33:48.760
confronted with a dilemma and um thinking about a

648
00:33:48.760 --> 00:33:53.699
dilemma in the abstract. Um, IF, if, if If

649
00:33:53.699 --> 00:33:55.619
the question is like, are these dilemmas realistic, they're

650
00:33:55.619 --> 00:33:57.699
obviously not realistic. Like, like a lot of them

651
00:33:57.699 --> 00:34:00.579
are very unrealistic. The Troy dilemma itself is, is

652
00:34:00.579 --> 00:34:04.060
horribly unrealistic. And that having said, it's not because

653
00:34:04.060 --> 00:34:07.359
it's unrealistic that it might not, it might still

654
00:34:07.859 --> 00:34:12.699
trigger psychological processes within our minds that corresponds to

655
00:34:12.699 --> 00:34:15.020
the processes that would be triggered in a real

656
00:34:15.020 --> 00:34:19.618
life similar situation. Um, SO it's, it's, I. I

657
00:34:19.618 --> 00:34:21.447
would say that the realism is, is something that

658
00:34:21.447 --> 00:34:23.898
we should worry about and, and there's some very

659
00:34:23.898 --> 00:34:27.498
interesting research showing that um if you confront people

660
00:34:27.498 --> 00:34:30.299
with these trolley dilemmas, um, they, they do tend

661
00:34:30.299 --> 00:34:31.849
to be engaged, they do tend to like them,

662
00:34:31.898 --> 00:34:35.467
they like the psychological intellectual exercise that they are,

663
00:34:35.819 --> 00:34:38.279
um, some people find them very fun to do,

664
00:34:38.658 --> 00:34:40.728
um, and then you could wonder, well, is that

665
00:34:40.728 --> 00:34:42.658
actually like, like, like if you would encounter such

666
00:34:42.658 --> 00:34:44.178
a situation in real life, it would not be

667
00:34:44.178 --> 00:34:48.469
very fun. Um, AT all, um, and so it

668
00:34:48.469 --> 00:34:50.659
is, it is something that we, I think do

669
00:34:50.659 --> 00:34:52.280
need to take into account. I think it's something

670
00:34:52.280 --> 00:34:54.020
that we have been neglecting a little bit too

671
00:34:54.020 --> 00:34:59.020
much as a field, um. We like these dilemmas.

672
00:34:59.340 --> 00:35:00.739
They're, they're, they're fun to think about, and I

673
00:35:00.739 --> 00:35:02.179
think that's why we've been, been using them so

674
00:35:02.179 --> 00:35:05.659
much, and they allow for, um, for a lot

675
00:35:05.659 --> 00:35:07.699
of types of experimentation as well, right? Like it's,

676
00:35:07.780 --> 00:35:10.379
it's, it's, it's very easy to shift vignettes around

677
00:35:10.379 --> 00:35:13.540
a little bit and see, oh, what change, what,

678
00:35:13.780 --> 00:35:16.379
what this change, what causes, what resulting change that

679
00:35:16.379 --> 00:35:18.780
we see in our, in our moral cognition, and

680
00:35:18.780 --> 00:35:21.399
but still, um, we should do more to try

681
00:35:21.399 --> 00:35:25.600
and ground, um, our research a little bit more

682
00:35:25.600 --> 00:35:30.030
in reality. Um If you're actually going to compare

683
00:35:30.030 --> 00:35:31.550
like how do people think about it and what

684
00:35:31.550 --> 00:35:33.830
do they do in the lab, um, we've, we've

685
00:35:33.830 --> 00:35:37.889
done some studies on this, um. A good while

686
00:35:37.889 --> 00:35:41.649
ago, um, we did a study where, uh, sort

687
00:35:41.649 --> 00:35:43.810
of similar to the electroshock dilemma that I explained,

688
00:35:44.409 --> 00:35:47.070
um, but where I invited people to the lab

689
00:35:47.070 --> 00:35:49.409
and I did not have people hooked up to

690
00:35:49.409 --> 00:35:53.209
electroshocks. I had cages of mice, so I had

691
00:35:53.209 --> 00:35:54.929
one cage of 5 mice and I had another

692
00:35:54.929 --> 00:35:57.479
cage of 1 mouse, and I said, You know,

693
00:35:57.570 --> 00:36:00.310
um, very similar setup. The 5 mice are going

694
00:36:00.310 --> 00:36:03.149
to get an electric shock, um, but you can

695
00:36:03.330 --> 00:36:05.530
decide to shock the single mouse instead. And then

696
00:36:05.530 --> 00:36:08.199
we had, um, some participants think about that dilemma.

697
00:36:08.489 --> 00:36:11.929
Just we just read text-based vignettes, and others were

698
00:36:11.929 --> 00:36:13.489
invited to the lab. And what we did find

699
00:36:13.489 --> 00:36:16.889
there was that people were actually more likely to

700
00:36:16.889 --> 00:36:19.189
commit the sacrificial harm, to shock the single mouse

701
00:36:19.370 --> 00:36:24.040
in the real-life case versus the hypothetical case, um.

702
00:36:25.030 --> 00:36:26.669
So there does seem to be like a shift

703
00:36:26.669 --> 00:36:28.969
that happens like if you, if you, if you're

704
00:36:29.310 --> 00:36:33.510
comparing the two, then I would say there's some

705
00:36:33.510 --> 00:36:36.590
evidence, um, there's also similar evidence when you look

706
00:36:36.590 --> 00:36:39.290
at VR virtual reality studies, people tend to prefer

707
00:36:39.610 --> 00:36:43.600
sacrificial harm more in a VR virtual reality version

708
00:36:43.600 --> 00:36:45.830
of a dilemma versus when they're just reading the

709
00:36:45.830 --> 00:36:48.229
text and thinking about it. Um, AND so there

710
00:36:48.229 --> 00:36:50.909
does seem to be something about making the dilemma

711
00:36:50.909 --> 00:36:55.189
more real that shifts responses towards, uh, the sacrificial

712
00:36:55.189 --> 00:36:57.830
harm side, utilitarian side, and so that could be

713
00:36:57.830 --> 00:37:00.449
an argument to say that there's a difference here.

714
00:37:00.830 --> 00:37:03.870
Um, ON the other side of things, um, we

715
00:37:03.870 --> 00:37:07.540
have also looked at. If you ask people to

716
00:37:07.540 --> 00:37:09.540
respond to a bunch of trolley dilemmas and then

717
00:37:09.540 --> 00:37:12.580
you, you, you, you see, do their responses predict

718
00:37:12.580 --> 00:37:14.419
what they actually do in real life, we do

719
00:37:14.419 --> 00:37:17.139
also find a little bit of a, an association

720
00:37:17.139 --> 00:37:19.100
there. So it's not like, like thinking about these

721
00:37:19.100 --> 00:37:22.179
dilemmas is completely different than being exposed to them

722
00:37:22.179 --> 00:37:24.300
in real life. The way that we think about

723
00:37:24.300 --> 00:37:27.929
them does predict what they do in real life

724
00:37:27.929 --> 00:37:31.889
as well. Um, IT'S not a super strong correlation.

725
00:37:31.989 --> 00:37:33.689
Um, I think across studies we have something like

726
00:37:33.689 --> 00:37:38.159
a correlation of, no, we have about um. 66%

727
00:37:38.159 --> 00:37:42.000
of variation explained, um, which is not a tremendous

728
00:37:42.000 --> 00:37:43.830
amount, but then again, like it's, it's, it's, it's

729
00:37:43.830 --> 00:37:47.879
psychology, right? So with 6% of variation explained, maybe

730
00:37:47.879 --> 00:37:49.959
we're already happy, I don't know, it depends on

731
00:37:49.959 --> 00:37:54.600
your perspective. So my, my personal opinion on the

732
00:37:54.600 --> 00:37:59.649
matter is that um. What we're doing in, in,

733
00:37:59.770 --> 00:38:02.100
in these hypothetical dilemmas like, like, obviously that is

734
00:38:02.100 --> 00:38:08.199
not completely um It's, it's, it's, it's, it relates

735
00:38:08.199 --> 00:38:11.520
to how people behave in real life. Um, THAT

736
00:38:11.520 --> 00:38:14.429
having said, if you put people in a real-life

737
00:38:14.429 --> 00:38:17.760
dilemma situation, all these sorts of other elements start

738
00:38:17.760 --> 00:38:20.409
to, start to sort of get pulled in situations,

739
00:38:20.419 --> 00:38:22.919
and you can't fully replicate that in, in a

740
00:38:22.919 --> 00:38:26.560
hypothetical vignette study. Um, ONE of the problems that

741
00:38:26.560 --> 00:38:30.100
we've So I did those, those mouse studies and,

742
00:38:30.120 --> 00:38:33.739
and um we, we tried to replicate those, those

743
00:38:33.739 --> 00:38:36.060
initial results and actually the the the that paper

744
00:38:36.060 --> 00:38:39.719
is also upcoming pretty soon. Um, AND just we

745
00:38:39.719 --> 00:38:41.679
made some minor changes in the way that we

746
00:38:41.679 --> 00:38:45.060
were administrating the dilemmas and that already led to

747
00:38:45.239 --> 00:38:48.479
quite a shift in how people behaved, um, and

748
00:38:48.600 --> 00:38:51.000
it is really hard to sort of, if you

749
00:38:51.000 --> 00:38:54.060
have a hypothetical dilemma, just a text-based dilemma, to

750
00:38:54.060 --> 00:38:56.639
create a situation in real life that is fully

751
00:38:56.639 --> 00:39:01.320
comparable to that hypothetical dilemma. Just imagine, um, in

752
00:39:01.320 --> 00:39:03.199
the, in the real-life case, for instance, we typically

753
00:39:03.199 --> 00:39:05.439
work with a time limits. Let's say you have

754
00:39:05.439 --> 00:39:09.570
20 seconds to react. And you could implement a

755
00:39:09.570 --> 00:39:12.169
20-second time limit when you confront participants with a

756
00:39:12.169 --> 00:39:15.489
hypothetical dilemma as well, but 20 seconds in the

757
00:39:15.489 --> 00:39:17.969
real life situation might not be comparable to 20

758
00:39:17.969 --> 00:39:21.050
seconds in a hypothetical situation. In a hypothetical situation,

759
00:39:21.120 --> 00:39:23.370
there's all of these other sort of impulses coming

760
00:39:23.370 --> 00:39:26.030
at people. It's a much more emotionally impactful situation

761
00:39:26.169 --> 00:39:28.949
and just thinking about it. And so, Then all

762
00:39:28.949 --> 00:39:30.270
of a sudden you have to wonder, OK, so

763
00:39:30.270 --> 00:39:32.669
what would be a good comparison here? Like, like

764
00:39:32.669 --> 00:39:34.719
how many seconds do we need to use in,

765
00:39:34.899 --> 00:39:37.270
in one instance versus the other to, to, to

766
00:39:37.270 --> 00:39:41.340
be able to even compare these two. Dilemmas that

767
00:39:41.340 --> 00:39:43.340
are at the core they're the same, but they're

768
00:39:43.340 --> 00:39:47.179
still very different experiences, right? And so if, if

769
00:39:47.179 --> 00:39:49.060
the, if the answer is, can we, can we

770
00:39:49.060 --> 00:39:51.739
use these hypothetical dilemmas to study real-life cognition, I

771
00:39:51.739 --> 00:39:54.939
think we can, uh, but there's also real limits

772
00:39:54.939 --> 00:39:56.979
to what we can learn from these hypothetical dilemmas

773
00:39:56.979 --> 00:39:59.580
and at some point we as a field will

774
00:39:59.580 --> 00:40:02.139
need to actually put people in the lab and

775
00:40:02.139 --> 00:40:06.540
confront them consistently and systematically with, uh, with real

776
00:40:06.540 --> 00:40:08.189
life dilemmas to see how people behave.

777
00:40:09.030 --> 00:40:13.669
Mhm. So another kind of aspect of sacrificial moral

778
00:40:13.669 --> 00:40:17.709
dilemmas you study or have studied is how people

779
00:40:17.709 --> 00:40:21.350
infer moral character the moral character of others based

780
00:40:21.350 --> 00:40:25.419
on how they resolve these moral dilemmas. So, uh,

781
00:40:25.550 --> 00:40:28.590
what results do you get there? I mean, how

782
00:40:28.590 --> 00:40:33.229
do people tend to evaluate the moral character depending

783
00:40:33.229 --> 00:40:35.790
on how others respond to this dilemmas.

784
00:40:36.649 --> 00:40:38.090
Yeah, we, we, we talked about it a little

785
00:40:38.090 --> 00:40:41.389
bit previously already, um, and indeed like, like, like

786
00:40:41.570 --> 00:40:43.129
every moral judgment is also like it has a

787
00:40:43.129 --> 00:40:45.209
communicative purpose. Like if you make, if, if you,

788
00:40:45.320 --> 00:40:49.010
if you, if you hate the, if, if you

789
00:40:49.010 --> 00:40:51.149
disagree with the things that I disagree with, with,

790
00:40:51.169 --> 00:40:52.449
or if you agree with the things that I

791
00:40:52.449 --> 00:40:55.330
agree with, um, that will usually form a bond

792
00:40:55.330 --> 00:40:57.840
between people. And what we tend to see in,

793
00:40:57.939 --> 00:41:00.159
in these sacrificial lemmas or what we used to

794
00:41:00.159 --> 00:41:02.770
see, um, I'll, I'll, I'll add a little bit

795
00:41:02.770 --> 00:41:04.679
of a nuance at the end there. Is that

796
00:41:04.679 --> 00:41:06.939
if you confront people with a sacrificial dilemma, um,

797
00:41:07.760 --> 00:41:11.699
People will usually prefer others that make deontological judgments

798
00:41:12.179 --> 00:41:15.679
versus those that make utilitarian judgments. Um, THAT basic

799
00:41:15.679 --> 00:41:19.830
effect has been, um, I think, um, Professor Jim

800
00:41:19.830 --> 00:41:21.320
Everett was the first one who wrote the first

801
00:41:21.320 --> 00:41:22.909
paper about it, and it has been replicated a

802
00:41:22.909 --> 00:41:25.449
bunch of times. We've replicated it as well. I've

803
00:41:25.760 --> 00:41:30.219
found that results quite often, people tend to. Uh,

804
00:41:30.550 --> 00:41:35.149
DISLIKE utilitarians more than they like the intelligence, than

805
00:41:35.149 --> 00:41:40.929
to trust utilitarians less than they trust theontologists. Um,

806
00:41:41.679 --> 00:41:44.429
That has a lot of downstream consequences. There's even

807
00:41:44.429 --> 00:41:47.949
some studies showing that, um, obviously in the abstract,

808
00:41:47.989 --> 00:41:49.790
but if, if, if you, if you ask people

809
00:41:49.790 --> 00:41:51.580
who would you prefer as a mate, for instance,

810
00:41:51.899 --> 00:41:54.870
uh, people will tend to favor the ontologists versus

811
00:41:54.870 --> 00:41:58.510
utilitarians. There's a few wrinkles to that. In some

812
00:41:58.510 --> 00:42:02.129
cases, people do actually prefer utilitarians. Um, IF you,

813
00:42:02.790 --> 00:42:05.979
uh, meet a sort of like a general, like

814
00:42:05.979 --> 00:42:08.850
a cold ruthless leader. Um, THEN you might actually

815
00:42:08.850 --> 00:42:11.370
prefer someone that is a bit more utilitarian and

816
00:42:11.370 --> 00:42:13.850
a bit more pragmatic in the types of moral

817
00:42:13.850 --> 00:42:18.149
judgments that they make, um. Than than theontologist, I,

818
00:42:18.239 --> 00:42:20.600
I think some of the, the reasons why we

819
00:42:20.600 --> 00:42:23.399
tend to find the judgments, I think are related

820
00:42:23.399 --> 00:42:25.080
to what we've talked about previously as well is

821
00:42:25.080 --> 00:42:27.959
that um one of the reasons why someone might

822
00:42:27.959 --> 00:42:31.199
prefer utilitarian judgment is because they might just not

823
00:42:31.199 --> 00:42:34.040
care about the harm um if someone scores high

824
00:42:34.040 --> 00:42:39.389
on our anti-social measures, um. They prefer utilitarian judgments

825
00:42:39.389 --> 00:42:41.479
and maybe some of what we're seeing is that

826
00:42:41.479 --> 00:42:44.919
where people are making these utitarian judgments and think,

827
00:42:44.959 --> 00:42:48.080
oh, this might be a genuine concern for the

828
00:42:48.080 --> 00:42:49.399
greater good, but it might also just be a

829
00:42:49.399 --> 00:42:52.360
psychopath, um, and that is why we, we sort

830
00:42:52.360 --> 00:42:55.959
of see this distrust emerge, um, but also be

831
00:42:55.959 --> 00:42:59.399
related to, um, there's some research suggesting that it's

832
00:42:59.399 --> 00:43:03.399
related to, to, um, predictability. Um, SOMEONE that's a

833
00:43:03.399 --> 00:43:05.310
dontologist, some of that will always go like, oh,

834
00:43:05.320 --> 00:43:07.439
I don't want to harm anyone else. Um, IT'S

835
00:43:07.439 --> 00:43:10.340
a bit more predictable and might because of that,

836
00:43:10.479 --> 00:43:12.840
be a little bit more preferable as a, as

837
00:43:12.840 --> 00:43:15.959
a partner, as a cooperation partner than, you know,

838
00:43:16.040 --> 00:43:19.040
a utilitarian, which is great when you're on the,

839
00:43:19.100 --> 00:43:20.239
the right side of the track, but if you're

840
00:43:20.239 --> 00:43:22.320
on the wrong side of the track, you, they

841
00:43:22.320 --> 00:43:26.330
might decide against you. Um, But I will say

842
00:43:26.330 --> 00:43:27.870
um some of the more recent research that we've

843
00:43:27.870 --> 00:43:33.860
been doing is um Cause when we A few

844
00:43:33.860 --> 00:43:36.620
years ago, I started doing this type of who

845
00:43:36.620 --> 00:43:40.070
do people prefer uh with iterative dilemas, um, like

846
00:43:40.070 --> 00:43:43.989
in these sequences of decisions, and who do people

847
00:43:43.989 --> 00:43:46.310
prefer there and there we consistently were finding that

848
00:43:46.310 --> 00:43:49.340
people actually liked utilitarians more than they liked the

849
00:43:49.520 --> 00:43:53.620
theonhologists, which was Um, very strange and which went

850
00:43:53.620 --> 00:43:57.419
against an entire literature and initially confused us very

851
00:43:57.419 --> 00:44:00.179
much, um, and so what we hadn't been taken

852
00:44:00.179 --> 00:44:02.300
into account, I think hasn't been taken into account

853
00:44:02.300 --> 00:44:05.629
in a lot of this research is that I've

854
00:44:05.629 --> 00:44:07.550
already hinted at it. People tend to prefer similar

855
00:44:07.550 --> 00:44:10.830
odors, and within the fields, um, a lot of

856
00:44:10.830 --> 00:44:12.669
the dilemmas that we have been using have been

857
00:44:12.669 --> 00:44:18.100
dilemmas that most people will prefer a, um, uh,

858
00:44:18.110 --> 00:44:21.290
a deontological response to. And so some of the

859
00:44:21.290 --> 00:44:24.989
preference that we're seeing for Utilitarians does not appear

860
00:44:24.989 --> 00:44:27.669
to be like a, uh, on every type of

861
00:44:27.669 --> 00:44:32.709
sacrificial dilemma. People prefer theontologists over utilitarians. It really

862
00:44:32.709 --> 00:44:34.649
depends on the dilemma, um, and so we've, we've,

863
00:44:34.770 --> 00:44:36.909
we, we've done, we've done one study where in

864
00:44:36.909 --> 00:44:39.389
study one we found, oh, people prefer, prefer utilitarians,

865
00:44:39.399 --> 00:44:42.870
and in study 20, people prefer deontologists, um, and

866
00:44:42.870 --> 00:44:44.149
what seems to be going on is that a

867
00:44:44.149 --> 00:44:46.870
lot of the difference is just explained by similarity.

868
00:44:46.949 --> 00:44:50.229
People prefer. The ones that have the same judgments

869
00:44:50.229 --> 00:44:52.350
as they do, the anthologists prefer the anthologists and

870
00:44:52.350 --> 00:44:54.909
utilitarians prefer utilitarians, and once you start controlling for

871
00:44:54.909 --> 00:44:59.830
that, um, the, the difference between Um, trust for

872
00:44:59.830 --> 00:45:01.830
citytarians and trust for city nithologists, it, it sort

873
00:45:01.830 --> 00:45:04.419
of gets, gets diminished a lot. I think in

874
00:45:04.419 --> 00:45:08.409
general, um. Based on, because there is this huge

875
00:45:08.409 --> 00:45:11.770
literature that people prefer the anthologists, I would still

876
00:45:11.770 --> 00:45:14.469
be tempted to say there likely is something there,

877
00:45:14.850 --> 00:45:17.830
um, but it is, the effect is much more

878
00:45:18.090 --> 00:45:21.649
variable and much more vulnerable than, um, if you

879
00:45:21.649 --> 00:45:23.689
had asked me this question five years ago, I

880
00:45:23.689 --> 00:45:26.379
would have been like, oh yeah, certainly people, uh,

881
00:45:26.489 --> 00:45:29.330
people hate utilitarians. Now I'm like it really depends

882
00:45:29.330 --> 00:45:31.449
on the type of moral dilemma that we're dealing

883
00:45:31.449 --> 00:45:33.550
with. Um, YEAH.

884
00:45:34.379 --> 00:45:37.580
Yeah, no, that's very interesting. So I have one

885
00:45:37.580 --> 00:45:41.080
last question then. uh, IN the real life situation,

886
00:45:41.219 --> 00:45:44.379
people can also choose, and we've already touched a

887
00:45:44.379 --> 00:45:46.139
little bit on this, but I want to ask

888
00:45:46.139 --> 00:45:49.379
you directly, so people, people can also choose to

889
00:45:49.379 --> 00:45:53.919
just exit the social dilemma. Do people do so

890
00:45:54.020 --> 00:45:55.540
and if so, why?

891
00:45:56.820 --> 00:46:00.469
It's, it's, it's. It's also something that we haven't

892
00:46:00.469 --> 00:46:03.510
really been, been, been studying a lot, um, and,

893
00:46:03.590 --> 00:46:04.949
and you're right, like, like, like, like, like when,

894
00:46:05.020 --> 00:46:07.709
when people are confronted with a hypothetical dilemma, again,

895
00:46:07.750 --> 00:46:09.830
like engagement is assumed, right? And so, so we

896
00:46:09.830 --> 00:46:13.020
don't really allow people the possibility to, to exit

897
00:46:13.020 --> 00:46:14.550
the dilemma. I think it happens in real life.

898
00:46:14.629 --> 00:46:19.399
I think people, um, decide to leave moral dilemma

899
00:46:19.399 --> 00:46:22.110
situations all the time, um, but it is hard

900
00:46:22.110 --> 00:46:24.909
to study. Um, I, I, I guess one way

901
00:46:24.909 --> 00:46:28.580
of Sting it would be in like sort of

902
00:46:28.580 --> 00:46:31.100
the lab studies that we have been doing and

903
00:46:31.260 --> 00:46:33.820
in those lab studies we have encountered a few

904
00:46:33.820 --> 00:46:35.780
times, not a lot. I think by now I've

905
00:46:35.780 --> 00:46:41.080
done. I think I've exposed over 1800 participants like

906
00:46:41.080 --> 00:46:44.219
this, these, these types of real-life lab dilemmas. So

907
00:46:44.219 --> 00:46:49.270
far, um, and across the 1800, I think. Maybe

908
00:46:50.169 --> 00:46:53.810
1011 or 12 when they encountered the dilemma we're

909
00:46:53.810 --> 00:46:55.449
like, oh, I would rather not participate in this.

910
00:46:55.530 --> 00:46:59.010
I would like to exit the situation. Um, IT

911
00:46:59.010 --> 00:47:01.889
happened a bit more in the mouse dilemmas than

912
00:47:01.889 --> 00:47:05.090
it happens when in the, in the electoral dilemmas

913
00:47:05.090 --> 00:47:08.090
with people, um, and the, the main driver of

914
00:47:08.090 --> 00:47:11.409
that was always, um, people that have this, this

915
00:47:11.409 --> 00:47:15.320
very high concern for, for animal welfare, um. There

916
00:47:15.320 --> 00:47:17.280
were a couple of people there that had patronized

917
00:47:17.280 --> 00:47:22.919
themselves and that just, just, um. Really could not

918
00:47:22.919 --> 00:47:24.669
participate. They were like, they were like like this,

919
00:47:24.739 --> 00:47:26.479
this, this, this, this, this is, this is potential

920
00:47:26.479 --> 00:47:28.600
animal abuse. Um, I don't want to partake in

921
00:47:28.600 --> 00:47:30.870
this at all. And so that's, it's, it's, it's

922
00:47:30.870 --> 00:47:33.219
in those cases, it was like a really strong

923
00:47:33.899 --> 00:47:40.100
moral concern, um, about the animals, um. Maybe I

924
00:47:40.100 --> 00:47:41.739
need to add though, like, like in those experiments

925
00:47:41.739 --> 00:47:43.580
we didn't actually harm any animals like we, we,

926
00:47:43.689 --> 00:47:45.959
we just pretended that we're going to harm animals,

927
00:47:46.379 --> 00:47:48.570
um, and then as soon as people made a

928
00:47:48.570 --> 00:47:51.219
decision, we just stopped the experiments and so no

929
00:47:51.219 --> 00:47:54.370
mice were harmed, um, over the course of that

930
00:47:54.370 --> 00:47:57.179
experiment, but that was the main, the main, the

931
00:47:57.179 --> 00:47:59.899
main driver of people deciding to engage, to disengage

932
00:47:59.899 --> 00:48:02.659
from the situation. It's just a concern for, for

933
00:48:02.659 --> 00:48:06.459
animal welfare welfare, um. I, I, I think in

934
00:48:06.459 --> 00:48:10.179
general, I, I think people might exit social dilemma

935
00:48:10.179 --> 00:48:12.699
situations for multiple reasons. Um, I think one of

936
00:48:12.699 --> 00:48:15.139
the reasons might be they're just emotionally overwhelmed and

937
00:48:15.139 --> 00:48:17.939
so they're just shutting down. Um, I think another

938
00:48:17.939 --> 00:48:22.219
reason might also just be they, they, um, They're

939
00:48:22.219 --> 00:48:25.100
confronted with, with a bunch of options and they

940
00:48:25.100 --> 00:48:27.939
don't like any of them. And so, um, as

941
00:48:27.939 --> 00:48:32.379
a sort of result of that, they, they rather

942
00:48:32.379 --> 00:48:34.739
than pick one of the, the options that they

943
00:48:34.739 --> 00:48:36.060
really don't want to pick, they just go, I

944
00:48:36.060 --> 00:48:37.300
don't wanna, I don't want to deal with this

945
00:48:37.300 --> 00:48:40.770
at all, um. Or it might also be just,

946
00:48:40.879 --> 00:48:44.310
just, just. People were never fully engaged with the

947
00:48:44.310 --> 00:48:45.979
situation at all. Um, SORT of what we talked

948
00:48:45.979 --> 00:48:49.290
about previously with this, this, there's no responsibility, uh,

949
00:48:49.669 --> 00:48:51.739
idea is that, is that people were, are like.

950
00:48:53.489 --> 00:48:55.209
It's not my, it's not, it's not my place

951
00:48:55.209 --> 00:48:57.239
to make a decision here, um, and that that

952
00:48:57.239 --> 00:49:00.110
might cause people to sort of step back from

953
00:49:00.110 --> 00:49:03.250
the loan situation. Um, I think it definitely happens.

954
00:49:03.330 --> 00:49:04.969
I also think we, I also don't think that

955
00:49:04.969 --> 00:49:07.719
we have sufficient good data on it just yet,

956
00:49:07.729 --> 00:49:10.560
and I, um, everyone would definitely be interested in,

957
00:49:11.010 --> 00:49:13.129
in doing some more research on in, in that

958
00:49:13.129 --> 00:49:13.629
direction, yeah.

959
00:49:14.260 --> 00:49:17.379
Mhm. OK, so would you like to tell us

960
00:49:17.379 --> 00:49:20.459
what are the kinds of things you will be

961
00:49:20.459 --> 00:49:23.379
working on in the near future and are they

962
00:49:23.379 --> 00:49:28.379
also related to these kinds of uh psychological and

963
00:49:28.379 --> 00:49:32.739
philosophical exploration of truly type moral dilemmas or something

964
00:49:32.739 --> 00:49:33.159
else?

965
00:49:34.149 --> 00:49:38.080
Yeah, uh, so, so, um, My main focus for

966
00:49:38.080 --> 00:49:40.239
the future, um, though I, I, I will need

967
00:49:40.239 --> 00:49:41.679
to get some more grant money to actually get

968
00:49:41.679 --> 00:49:44.679
it going, um, well, like, like I want to

969
00:49:44.679 --> 00:49:46.560
put people in laps and confront them with, with,

970
00:49:46.639 --> 00:49:49.760
with real-life moral dilemmas, and, um, I think the

971
00:49:49.760 --> 00:49:51.899
basic setup that we have right now with the,

972
00:49:52.399 --> 00:49:55.320
the electroshocks and, and shocking people is a, it's

973
00:49:55.320 --> 00:49:57.840
a very rich setup already. Um, THERE'S a lot

974
00:49:57.840 --> 00:49:59.520
of things that you can tweak about that setup

975
00:49:59.520 --> 00:50:02.520
to see how it impacts people's moral judgments. Um,

976
00:50:02.760 --> 00:50:05.239
SO like just to give like one example, um.

977
00:50:05.800 --> 00:50:08.760
In, um, in the studies that we've conducted so

978
00:50:08.760 --> 00:50:11.909
far, like the basic, basic version is just, um,

979
00:50:11.919 --> 00:50:13.639
you have 3 people sitting there and they are

980
00:50:13.639 --> 00:50:16.600
all men or all women. Um, ONE manipulation that

981
00:50:16.600 --> 00:50:18.419
you could do is you could do, oh, it's,

982
00:50:18.469 --> 00:50:21.199
it's 2 men versus 1 woman, 1 woman versus

983
00:50:21.199 --> 00:50:23.649
2 men, and see to what extent does the,

984
00:50:24.040 --> 00:50:26.790
the, the general characteristics of the victims, to what

985
00:50:26.790 --> 00:50:28.919
extent does that drive the types of judgments that

986
00:50:28.919 --> 00:50:31.919
people prefer, or you could, you could manipulate the

987
00:50:31.919 --> 00:50:33.500
way in which that harm needs to be done

988
00:50:33.500 --> 00:50:37.189
in one version of the experiments. Um, WE separated

989
00:50:37.189 --> 00:50:40.419
the participants and the, the targets, uh, with a

990
00:50:40.419 --> 00:50:44.070
one-way, one-way mirror, um, so participants could see the

991
00:50:44.070 --> 00:50:47.189
targets, the Confederates, um, but not vice versa, and

992
00:50:47.189 --> 00:50:49.550
another way, in another version of the experiments, and

993
00:50:49.550 --> 00:50:50.909
they were all located in the same room and

994
00:50:50.909 --> 00:50:54.780
then the participants had to. Commit the sacrificial harm

995
00:50:54.780 --> 00:50:56.760
in the one way, the mirror version, they could

996
00:50:56.760 --> 00:50:57.959
do it in, in their room, they just have

997
00:50:57.959 --> 00:51:00.159
to press a button there and in the, in

998
00:51:00.159 --> 00:51:02.639
the one room version, and participants had to walk

999
00:51:02.639 --> 00:51:04.199
up to the victim and actually touch them on

1000
00:51:04.199 --> 00:51:07.399
the arm and so it's much more emotionally direct

1001
00:51:07.399 --> 00:51:11.439
experience, um, and there's, there's also, there's all these

1002
00:51:11.439 --> 00:51:12.840
sorts of parameters that can play, but you could

1003
00:51:12.840 --> 00:51:14.959
give participants the option to shock themselves, for instance,

1004
00:51:15.000 --> 00:51:18.040
and see, um, when will people decide in favor

1005
00:51:18.040 --> 00:51:22.010
of, of self-harm. Um, YOU could, you could tell

1006
00:51:22.010 --> 00:51:24.169
participants something about the people that are sitting there.

1007
00:51:24.250 --> 00:51:26.290
Like maybe someone is like a volunteer for a

1008
00:51:26.290 --> 00:51:28.699
charity. To what extent are people going to weigh

1009
00:51:28.699 --> 00:51:31.350
that? And I think it's a very rich setup

1010
00:51:31.409 --> 00:51:34.689
that we can use to explore, um, not just

1011
00:51:34.689 --> 00:51:37.379
how people respond to sacrificial dilemmas, but how they

1012
00:51:37.379 --> 00:51:39.250
respond to, to moral dilemmas in general because I

1013
00:51:39.250 --> 00:51:42.629
feel. And we have, we talked about the dual

1014
00:51:42.629 --> 00:51:45.639
process model, which is a, um, which has been

1015
00:51:45.639 --> 00:51:47.830
a, a very influential and, and, and important model

1016
00:51:47.830 --> 00:51:50.070
to sort of explain how do people respond to

1017
00:51:50.070 --> 00:51:54.750
sacrificial dilemmas, um, but there's many different types of

1018
00:51:54.750 --> 00:51:59.770
moral dilemmas out there, right? Um, IF you, um,

1019
00:52:00.149 --> 00:52:01.830
I don't know, if you, if your co-worker did

1020
00:52:01.830 --> 00:52:03.590
something wrong and you have to like wait, do

1021
00:52:03.590 --> 00:52:06.090
I want to tell my boss yes or no.

1022
00:52:06.550 --> 00:52:09.239
Um, THAT is also a moral dilemma, but it's

1023
00:52:09.239 --> 00:52:11.159
not really a sacrificial dilemma, and so we don't

1024
00:52:11.159 --> 00:52:15.959
really have a, a good, um, process theory for

1025
00:52:15.959 --> 00:52:17.639
what goes on in the mind when people are

1026
00:52:17.639 --> 00:52:19.860
confronted with a dilemma like that, and I think,

1027
00:52:20.320 --> 00:52:23.800
um. By confronting people in the lab with real

1028
00:52:23.800 --> 00:52:27.520
dilemmas, um, we can work towards a more general

1029
00:52:27.520 --> 00:52:30.270
sort of process model for how people cope with

1030
00:52:30.270 --> 00:52:31.679
moral dilemmas, and I think we can start with

1031
00:52:31.679 --> 00:52:34.479
the dual process model, um, but what some of

1032
00:52:34.479 --> 00:52:37.219
our studies with these real real dilemmas have already

1033
00:52:38.010 --> 00:52:41.580
shown is that the dual process model cannot explain

1034
00:52:42.080 --> 00:52:44.669
everything that's out there. The dual process model does

1035
00:52:44.669 --> 00:52:47.360
not really explain why people, when you confront them

1036
00:52:47.360 --> 00:52:50.209
with these types of iterative dilemmas, why they switch.

1037
00:52:50.899 --> 00:52:53.060
Um, IT talks about not wanting to harm others.

1038
00:52:53.100 --> 00:52:55.989
It talks about utitarianism. It doesn't talk about when

1039
00:52:55.989 --> 00:52:58.429
people go, actually, I don't want to just minimize

1040
00:52:58.429 --> 00:53:00.669
harm. I want to spread harm fairly. That's sort

1041
00:53:00.669 --> 00:53:02.899
of new moral concern that's emerging, and we don't

1042
00:53:02.899 --> 00:53:06.310
know what drives people to stay with the utilitarian

1043
00:53:06.310 --> 00:53:08.750
choice or what drives them towards the fairness choice.

1044
00:53:08.790 --> 00:53:11.389
And so I think we, we have a, a

1045
00:53:11.389 --> 00:53:13.270
core set up here that will allow for a

1046
00:53:13.270 --> 00:53:17.199
lot of more, um. Variation in moral judgments to

1047
00:53:17.199 --> 00:53:20.370
explore in a real-life lab set setting and just

1048
00:53:20.370 --> 00:53:23.629
by just systematic manipulations, we will be able to

1049
00:53:24.090 --> 00:53:28.050
uh create better models. Um, EVENTUALLY, I would also

1050
00:53:28.050 --> 00:53:30.929
like to just. Design out of real life world

1051
00:53:30.929 --> 00:53:32.439
delamos that we can set in the lab um.

1052
00:53:33.669 --> 00:53:35.989
The, the, I very much like this electroshock paradigm.

1053
00:53:36.110 --> 00:53:37.870
I think it's, there's a lot of things to

1054
00:53:37.870 --> 00:53:40.590
be done there, um, but just in the way

1055
00:53:40.590 --> 00:53:45.989
that, um, roda lamas are not realistic, um, like

1056
00:53:45.989 --> 00:53:47.629
going into a lab and having to make decisions

1057
00:53:47.629 --> 00:53:50.469
about who you want to shock is arguably also

1058
00:53:50.469 --> 00:53:52.790
not really realistic. Like it's not a type of

1059
00:53:52.790 --> 00:53:55.310
choice that people encounter a lot in, in, in,

1060
00:53:55.389 --> 00:53:58.610
in their, in their day to day, um. Again,

1061
00:53:58.729 --> 00:54:00.000
I do think that we can use it to

1062
00:54:00.000 --> 00:54:02.649
study some of the psychological processes that work in

1063
00:54:02.939 --> 00:54:05.090
real life cognition, but it's still a very artificial

1064
00:54:05.090 --> 00:54:07.889
situation, and I would like to explore a wider

1065
00:54:07.889 --> 00:54:11.169
variety of moral dilemma situations in the lab. And

1066
00:54:11.169 --> 00:54:13.709
so that's, that's essentially gonna be my, my focus

1067
00:54:13.709 --> 00:54:16.889
for the, the next couple of years, um, try

1068
00:54:16.889 --> 00:54:19.350
and get some money to do this, to, to

1069
00:54:19.350 --> 00:54:23.149
do more of these studies and, um, really explore

1070
00:54:23.149 --> 00:54:27.110
the, the difference between. What can thinking about these

1071
00:54:27.110 --> 00:54:29.510
lamas, what can that teach us versus if you

1072
00:54:29.510 --> 00:54:31.469
put people in these situations, how do they react

1073
00:54:31.469 --> 00:54:34.050
and, and that tension and the similarities and differences

1074
00:54:35.449 --> 00:54:36.689
that emerge that is really going to be my

1075
00:54:37.270 --> 00:54:38.229
main research interest.

1076
00:54:39.280 --> 00:54:42.239
And where can people find your work on the

1077
00:54:42.239 --> 00:54:42.580
internet?

1078
00:54:43.669 --> 00:54:46.429
Um, SO I am on Twitter, uh, still, or

1079
00:54:46.429 --> 00:54:49.590
X or whatever we call it nowadays. Um, I

1080
00:54:49.590 --> 00:54:51.510
should actually, I don't even know my username username.

1081
00:54:51.550 --> 00:54:54.510
I think DH Boston, um, will probably, if you

1082
00:54:54.510 --> 00:54:57.350
just Google this Boston, um, I'm, I'm sure I

1083
00:54:57.350 --> 00:54:59.969
have, I have a personal website, it will emerge.

1084
00:55:00.219 --> 00:55:05.429
Um, I'm very easily findable on scholar, um. People

1085
00:55:05.429 --> 00:55:08.750
should not hesitate to reach out. Um, I'm always

1086
00:55:08.750 --> 00:55:11.070
happy to talk. I'm always happy to have a

1087
00:55:11.070 --> 00:55:14.169
chat, especially about these sorts of things, um, yeah.

1088
00:55:15.340 --> 00:55:17.790
OK, thank you so much for taking the time

1089
00:55:17.790 --> 00:55:19.389
to come on the show. It's been a real

1090
00:55:19.389 --> 00:55:20.540
pleasure to talk with you.

1091
00:55:21.110 --> 00:55:23.270
No, thank you very much for having me and

1092
00:55:23.969 --> 00:55:25.590
for dealing with the, the, the little bit of

1093
00:55:25.590 --> 00:55:28.790
the situation we had halfway through. Um, NO, it's

1094
00:55:28.790 --> 00:55:30.389
been a, it's been, it's been a very enjoyable

1095
00:55:30.389 --> 00:55:31.030
conversation, yeah.

1096
00:55:32.080 --> 00:55:34.600
Hi guys, thank you for watching this interview until

1097
00:55:34.600 --> 00:55:36.719
the end. If you liked it, please share it,

1098
00:55:36.919 --> 00:55:39.679
leave a like and hit the subscription button. The

1099
00:55:39.679 --> 00:55:41.939
show is brought to you by Enlights Learning and

1100
00:55:41.939 --> 00:55:45.979
Development done differently. Check their website at enlights.com and

1101
00:55:45.979 --> 00:55:49.709
also please consider supporting the show on Patreon or

1102
00:55:49.709 --> 00:55:52.169
PayPal. I would also like to give a huge

1103
00:55:52.169 --> 00:55:55.290
thank you to my main patrons and PayPal supporters,

1104
00:55:55.709 --> 00:55:59.550
Perergo Larsson, Jerry Muller, Frederick Sundo, Bernard Seyaz Olaf,

1105
00:55:59.590 --> 00:56:02.860
Alex, Adam Cassel, Matthew Whittingberrd, Arnaud Wolff, Tim Hollis,

1106
00:56:03.020 --> 00:56:06.580
Eric Elena, John Connors, Philip Forst Connolly. Then Dmitri

1107
00:56:06.580 --> 00:56:10.610
Robert Windegerru Inai Zu Mark Nevs, Colin Holbrookfield, Governor,

1108
00:56:11.090 --> 00:56:14.879
Michel Stormir, Samuel Andrea, Francis Forti Agnun, Svergoo, and

1109
00:56:14.879 --> 00:56:18.649
Hal Herzognun, Machael Jonathan Labran, John Yardston, and Samuel

1110
00:56:18.649 --> 00:56:22.580
Curric Hines, Mark Smith, John Ware, Tom Hammel, Sardusran,

1111
00:56:22.739 --> 00:56:26.429
David Sloan Wilson, Yasilla Dezaraujo Romain Roach, Diego Londono

1112
00:56:26.429 --> 00:56:30.810
Correa. Yannik Punteran Ruzmani, Charlotte Blis Nico Barbaro, Adam

1113
00:56:30.810 --> 00:56:34.760
Hunt, Pavlostazevski, Alekbaka Madison, Gary G. Alman, Semov, Zal

1114
00:56:34.760 --> 00:56:38.770
Adrian Yei Poltontin, John Barboza, Julian Price, Edward Hall,

1115
00:56:38.850 --> 00:56:43.379
Edin Bronner, Douglas Fry, Franco Bartolati, Gabriel Pancortez or

1116
00:56:43.379 --> 00:56:47.129
Suliliski, Scott Zachary Fish, Tim Duffy, anny Smith, and

1117
00:56:47.129 --> 00:56:51.560
Wisman. Daniel Friedman, William Buckner, Paul Georg Jarno, Luke

1118
00:56:51.560 --> 00:56:56.300
Lovai, Georgios Theophanous, Chris Williamson, Peter Wolozin, David Williams,

1119
00:56:56.439 --> 00:57:00.550
Dio Costa, Anton Ericsson, Charles Murray, Alex Shaw, Marie

1120
00:57:00.550 --> 00:57:04.800
Martinez, Coralli Chevalier, Bangalore atheists, Larry D. Lee Junior.

1121
00:57:05.030 --> 00:57:09.629
Old Eringbon. Esterri, Michael Bailey, then Spurber, Robert Grassy,

1122
00:57:09.739 --> 00:57:14.310
Zigoren, Jeff McMahon, Jake Zul, Barnabas Raddix, Mark Kempel,

1123
00:57:14.419 --> 00:57:18.830
Thomas Dovner, Luke Neeson, Chris Story, Kimberly Johnson, Benjamin

1124
00:57:18.830 --> 00:57:23.590
Gelbert, Jessica Nowicki, Linda Brendan, Nicholas Carlson, Ismael Bensleyman.

1125
00:57:24.229 --> 00:57:28.189
George Ekoriati, Valentine Steinmann, Per Crawley, Kate Van Goler,

1126
00:57:28.540 --> 00:57:35.590
Alexander Obert, Liam Dunaway, BR, Massoud Ali Mohammadi, Perpendicular,

1127
00:57:35.750 --> 00:57:40.389
Jannes Hetner, Ursula Guinov, Gregory Hastings, David Pinsov, Sean

1128
00:57:40.659 --> 00:57:44.729
Nelson, Mike Levin, and Jos Necht. A special thanks

1129
00:57:44.729 --> 00:57:47.689
to my producers Iar Webb, Jim Frank Lucas Stink,

1130
00:57:47.800 --> 00:57:52.209
Tom Vanneden, Bernardine Curtis Dixon, Benedict Mueller, Thomas Trumbull,

1131
00:57:52.530 --> 00:57:55.469
Catherine and Patrick Tobin, John Carlomon Negro, Al Nick

1132
00:57:55.469 --> 00:57:58.679
Cortiz, and Nick Golden, and to my executive producers,

1133
00:57:58.689 --> 00:58:02.409
Matthew Lavender, Sergio Quadrian, Bogdan Kanis, and Rosie. Thank

1134
00:58:02.409 --> 00:58:03.070
you for all.

