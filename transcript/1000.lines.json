[
	{
		"start_time": "00:00",
		"speaker": "Ricardo Lopes",
		"words": "Hello, everyone. Welcome to a new episode of the Center. I'm your host, Ricardo Lobs. And today I'm here. I have a very special guest, Doctor Carl Freston. He is Professor of Imaging Neuroscience and welcome principal research fellow of Imaging Neuroscience at University College London. He is a theoretical neuroscientist and his main contribution to theoretical neurobiology has been a free energy principle for action and perception. And today we're going to focus a lot on the free energy principle. And let's say what point we can get to because there are many topics that Doctor Freston researchers that, that I would be interested in talking with him about about today. But let's see where we can get. So, Doctor Frisson, welcome to the show. It's a pleasure to have everyone."
	},
	{
		"start_time": "00:51",
		"speaker": "Karl Friston",
		"words": "Thank you very much for inviting me. It's a great pleasure to be here."
	},
	{
		"start_time": "00:55",
		"speaker": "Ricardo Lopes",
		"words": "OK. So the free energy principle, first of all, could you tell us in the simplest terms possible? What is, what the free energy principle is about? And could you tell us perhaps also a little bit about uh the history behind its development?"
	},
	{
		"start_time": "01:14",
		"speaker": "Karl Friston",
		"words": "Yes, the free energy principle is in effect the physics backstory to um a move in cognitive neuroscience towards understanding brain function in terms of predictive predictive processing and specifically the way in which we actively get information to process and to predict. Um AND that's sometimes in my world referred to as active inference. So the free energy principle is the physicists or the mathematicians foundation from which you can build a story about how we make sense of our world. And I repeat how we actively engage with the world to get the right kinds of sensations to make sense of our world."
	},
	{
		"start_time": "02:09",
		"speaker": "Ricardo Lopes",
		"words": "So when applied to, for example, perception and cognition, you call it active inference is that it"
	},
	{
		"start_time": "02:19",
		"speaker": "Karl Friston",
		"words": "Yeah, absolutely. So active inference which um I read as synonymous with predictive processing provided there's an active and embodied aspect to uh predictive processing. Um IS an application of the free energy principle. Um SOMETIMES in explaining the free energy principle, um one can take a low road or a high road and by the low road and the high road, I mean a an intuitive um theological account of what, what it means to engage in active inference and predictive processing um where the the high road will be the physicists way of telling exactly the same story. So um you, you asked for a simple explanation, so perhaps I'll go for the uh the low road explanation um which I think he could probably um start with the ideas of Plato and and his students in terms of um the Allegorical Cave. Um BUT perhaps best developed by um Herman Helmholtz, predicated on his understanding of not only physics, but of course, um perception in the brain as an anatomist and physiologist, possibly predicated on the ideas of Kant that were prevalent at that time. So this um if we start with um Helmholtz and his notion of unconscious inference, the idea here is that we make sense of the world by internally generating a hypothesis, an explanation in our head that could best account for the sensory impressions on our ears, on, on our eyes and all our sensory epithelia. So as Andy Clark puts it where turning the usual conception of perception on its head. So in the 20th century, people thought that perception was basically an outside in process. So sensory information impressed itself upon the sensory organs that was registered by the brain. And somehow this sensory information provided was successively refined and then provided the basis for perception. The predictive processing or active inference story is exactly the other way around. The perception rests upon an inside out process that you are actively and constructively generating hypotheses. Explanations. Sometimes you can think of these as hallucinations or fantasies that are fit for purpose in terms of explaining what I am actually encountering and perhaps the best way of understanding that would be through the lens of predictive coding. So, predictive coding is a scheme in fact, originally invented to compress sound files in the 19 fifties. But more recently, over the past few decades, it has become a very popular scheme for understanding how the brain works in terms of message passing and technically belief, updating. And it's very simple. All that happens is that if I had an idea or a hypothesis about the cause of this pattern of visual input, say it was you and your face that I would be able to predict what I would see if I was right. And I can compare my prediction to what I'm actually sensing. So if I am correct, then there should be no difference between my prediction and my sensations. And that difference is scored by a prediction error. So if I can eliminate or resolve prediction errors, I've got the right explanation for states of affairs in the world out there. And when I say out there, what I, what I'm implying is that the brain can't, doesn't have direct access to the world. It can only sample it all it has access to are very sparse sensory signals afforded by our eyes, our ears, our skin and our body, all our all our sense organs. So this picture puts prediction error at the center of driving changes in our beliefs. So if we have a large prediction error, that basically means our perception, our belief, I should qualify this. When I say belief, I mean a mathematical belief, a representation. If you like an encoding of the cause of a sensation by some neuronal uh activity. So if I have a belief that is not able to predict what I'm currently seeing, I can now use that prediction error to change my mind to. So using the prediction error, the prediction error will drive belief updating until it eliminates itself. And this can be um described in many ways but has a very nice description in relation to the wiring in the brain. And in particular, the sort of hierarchical structure of our brains where there is recurrent message passing or um or influences between successive levels of a hierarchical brain, very much like say an onion. If you imagine the brain being an onion and the surface is in receipt of all the sensory impressions. But there are layers upon layers and layers, literally giving you a very deep structure and deep is used exactly in the sense of deep learning. For example, in machine learning, it's the hierarchical depth that is implied by this steepness. And in the brain, there are connections between one level and a deeper level, which suggests that we can understand these reciprocal or recurrent connections in terms of prediction errors being passed from a low level to a deeper level or a higher level, changing the beliefs. So that the recurrent or the inside out connections now convey the predictions that are the result of my belief updating and the predictions come back down again. They are compared with the input from the lower levels get the prediction error and then that was sent forward. So you have this view of recurrent message passing in the brain where at each level you are trying to minimize the prediction error. And if you can do that in a hierarchical context, then you've got an apt description of the state of the world at multiple levels of abstraction, all driven by these ascending messages ascending deep into the hierarchy that can be construed as prediction errors. So that's if you like the sort of I I mentioned sort of a teen explanation. So the one concern, the objective of the brain is to make sense of the world by finding an appropriate explanation for everything that it is currently sensing. Um And if you take that view and generalize it to um the consequences of action, you can now tell a story whereby some of the things that you're predicting some of the sensations you're predicting are basically reporting the state of your, your body. And this can be in two flavors. It can either be um in terms of the state of your muscles, you know, physically where your limbs are and how the degree to which various muscles are stretched, or it could be um more autonomic, you know. So we can only really affect the world in one of two ways. We can only act upon the world in one or two ways. We can either move a muscle, um striated muscle would be those muscles that we can move, volitionally. Um OR smooth muscle means that we can secrete things or the kind of muscles we find in the heart or the gut. But they're both in essence, providing the brain with signals about the state of the motor system, sometimes known as the motor plant. So notice if I've now got a good prediction of what's happening in the world that includes a prediction of what I should feel my muscles and my autonomic nervous system doing. I can then use that prediction to basically realize or make that prediction come true. And that's um effectively equipping a predictive coding scheme with reflexes. So this this perspective on active sensing, active prediction, um active inference um can be told in many different ways. For example, um in robotics, it would be model predictive control. If you were a motor physiologist, you might describe it in terms of the equilibrium point hypothesis that some people like perceptual control theory. The idea is that you specify you predict the state of your body and then you reflexively fulfill those predictions. And that now takes you beyond this perception, but now subsumes action under this generic imperative to minimize prediction error. And that is one way of understanding um perception and action within the same frame. And then you can sort of add cognition on that and that all this perception has to be contextualized. You have to attend to various things, you have to have the history um of things that have just happened and also crucially roll out into the future. It's certainly if you want to make predictions about your um your behavior that unfolds in terms of policies and plans and narratives of your, what's going to happen to me, what should I do? Um Given I have some predictions about the consequences of acting in this way, uh act or acting in that way. And the link to the free energy principle is that roughly speaking, the total amount of prediction error can be regarded as free energy from the point of view of a theorist um um an information theorist or a physicist. So the free energy of the free energy principle is effectively scoring the prediction er the overall prediction error or the implausibility that if I was right, if my model of the world as it is at the moment was correct, then I'm going to want to um that if it was correct, then the probability of getting these sensory data will be very high and the free energy scores a negative log probability of, of um of, of these data given a particular model of those data um sometimes known as surprise. So you can see immediately that sort of prediction error scores also or reflects the surprise. Technically, uh surprisal. Again, if you're doing information theory, the mathematical description of this is called self information. And that's important because from the point of view of the physicist to thinking about self organizing systems, like you and me, but also like um macro molecules and viruses possibly even to biospheres. Then the minimization of prediction error means the minimization of surprise, which means the minimization of self information. And the key thing here is that the average of the self information is also known as entropy, which means that self-organization of this inactive sort where the system is actually acting upon the world to realize its predictions, or basically means you're resisting um the natural tendency of systems to increase in their entropy, to decay, to dissipate, to die, um to become more disordered. So things that self organize according to the free energy principle resist this entropic erosion. If you are a physiologist, it's just a statement of homeostasis. It just means that I've got certain predictions that I should be say at a particular temperature. And if I sense that my temperature is deviating from these predictions, then that is surprising in a non propositional sense. Um It entails a prediction error because the, you know, my prediction and my sensation are now not in accord. And if I now act in a way to eliminate that prediction error, I will act in a way that brings my temperature back towards my preferred, my preferred homeostatic set point. So there are lots of ways of telling this story depending upon whether you're a physiologist or whether you're an economist or, you know, a mythologist. But they all basically speak to the importance of having a model of the world that prescribes certain behaviors so that you can sample the world so that everything is unsurprising from your point of view, given the kind of thing that you are. Uh I'll just conclude with a couple of um a couple of key points. I'm talking about a model uh very um deliberately because at the heart of this is the notion of a generative model that can generate the predictions. Um So this um this perspective um places center stage, the notion of an agent or anything that has some kind of sentient behavior. Um It means that you can always describe the behavior of this kind of thing in terms of the agent or the system, having a generative model that can generate predictions which it actively pursues and and tries to uh and tries to fulfill. So, you know, the notion of a generative model brings us exactly back to the low road discussion um in the sense that I have models hypotheses, explanations in my mind that I'm actively revising through this process of basing belief, updating another way of describing this. Um um IN order to realize my predictions, given my model of the world that effectively I am, you know, everything that defines my characteristics, sensations and exchange with the world can be described as a generative model. The final point just to explain why people might, terms of people might come across if they read this literature, particularly the philos philosophical literature. It so happens that that's surprise that self information um which mathematically is a negative logarithm of the probability of my sensory data given my model or given me um is also known as the negative log evidence. And the evidence is the evidence for your model of the world, which means you can describe active inference and this active sampling of the data to minimize free energy or prediction error as exactly the same as gathering evidence for your model of the world sometimes refer to as self evidencing. So that's something that Jacob Hai has promoted, which I think is a very neat summary of this kind of explanation. We're just in the game of gathering evidence for our model of the lived or possibly sensed world."
	},
	{
		"start_time": "19:29",
		"speaker": "Ricardo Lopes",
		"words": "Mhm No, it is a great summary. And before we delve deeper into the cognitive science and the neuroscience of it, because there are also a few concepts, there are terms that you use that I would like you to uh explain a little bit better. Like for example, generative model, autonomous states, hierarchical organization. Before we get into all of that, let's go step by step here. So one concept that I would like you to explain to people who are not familiar with the free energy principle is what are Markov blankets?"
	},
	{
		"start_time": "20:08",
		"speaker": "Karl Friston",
		"words": "That was the one thing that I didn't include in my long, long, well spotted, right? So the physics um um story, the free energy principle has to derive the self evidencing. I've just described the predictive pressing I have just described from first principles. So how does, how does one do that? Um It's relatively straightforward, although it does involve um um some esoteric maths if you're not, if you're not uh a physicist. Uh BUT the arguments are quite straightforward. THE, you f you basically start by asking the question, if you want to talk about self-organization, what is a self? And that's quite a deep question. So the first thing you realize is that you have to be able to mathematically in a, in a principled way, individuate the states of the self from everything else that is not the self. How would you do that? Well, as a physicist, you can't assume that the self is closed. So you can't assume the kind of physics that you would have been taught at school where there is an equilibrium in a closed system. So for example, the second law of thermodynamics only applies to closed systems where there is no openness of the self, to the environment, to the eco niche um to other selves. So we are now talking about the physics of open systems. And in so doing, we have to now distinguish the states of a self or anything, anything basically from everything else or nothing. And that's where the Markov blanket comes into play. So the Markov blanket is a set of states that intervene between the states that are internal to let's call it an agent. It could be anything, it could be any self, let's call it, call it an agent, but it could be a particle, it could be a person, it could be a population, but we'll call it an agent. So the Markov blanket is a way of drawing a boundary around the states that are internal to the agent that separates the internal states from the external states. And the external states are now the environment and the Markov blanket very much in the same way that we talked about before that the brain is in the darkness of the skull and only has access to the environment through the sensory impressions. So the Markov blanket contains all the sensory impressions that the external states provide on the blanket. These are sensations and then those sensory states that are part of the boundary of the blanket then affect the intern states in the neuronal activity. But the Markov blanket also contains those states that mediate the influence of the inside on the outside and they refer to as active states. So you've got this notion that there's a set of states, the internal states say my brain, I've got my external states, the rest of the universe. And then there are the boundary or the blanket states. This is a Markov blanket and they are split into two and the sensory states mediate the effects of the outside on the inside and the active states mediate the effects of the inside on the outside. So this is the physics and the Markov blanket has a very particular definition. It simply means that the the um the sensory states cannot be influenced by the internal states and the active states cannot be influenced by the external states. So let me try and make that even simpler. Um If you were a systems theorist, you would define any given system. So now our agent is now a system um in terms of its inputs and outputs and the active and sensory states are just the outputs and the inputs of a particular system. So it's a way of defining a system or an agent in terms of its relationship to everything else through its inputs and outputs. And the Markov blanket condition requires that the inputs to a system cannot be affected by the system that is being into which the inputs are being supplied. Similarly, the outputs of a system can't be influenced by the, by the external states that are receiving that output. And with that very, very simple rule or definition of some thing, an agent or a system. Um IN terms of Markov blankets, you can then show that there is always a description of the observable aspects of the system which are just the uh the blanket state. So you can never see inside you observing me means that all you can see are my blanket states, my surface, my boundary. Uh And usually you can only see the consequences of my, of my action. Um So given those, um given those um uh those conditions and the existence of a Markov blanket, um it is then fairly straightforward to show mathematically that the internal states and the active states have to minimize free energy if this system has some characteristic states and it has to have characteristic states, otherwise it wouldn't have a markoff blanket. So what you're saying is that anything that exists in the sense of possessing a Markov blanket or a boundary that persists over a certain amount of time can always be described as self evidencing, can always be described as if it is engaging in some kind of active infant sense making and acting on the basis of, of that sense making. So the mark of black is absolutely crucial because that's where the story starts really. And it also it takes you beyond um physics as we know and love it. Um IN the sense that um this description of the dynamics of something or an agent is in terms of the, the internal states holding mathematical or probabilistic beliefs about external states. This is quite unique to the free energy principle. Normally, when people talk about probability distributions, they are talking about a probability distribution over some states. When you're, when you're talking about um the free energy principle and what some people refer to as the basing mechanics that ensues you're talking now truly about um beliefs encoded by the internal states about something. So it's information beyond Shannon, it's, you know, it's truly representational beliefs. And I introduced the notion of basing mechanics because um just to reassure you that the maths that underwrites the basic mechanics um is exactly the same maths that you will find in quantum mechanics in statistical mechanics. And in classical mechanics, it's all the same maths. The only difference um or the only special thing required by the basing mechanics is the existence of, of this separation. Uh The Markov blanket that separates the states of something from the states that are not part of, of the thing in question."
	},
	{
		"start_time": "28:34",
		"speaker": "Ricardo Lopes",
		"words": "And is it possible to determine precisely where the boundary or the mark of blanket is located and how we separate internal states from external states?"
	},
	{
		"start_time": "28:48",
		"speaker": "Karl Friston",
		"words": "Yes. Uh YOU know, with, with a formal and um um mathematically crisp definition. Yes, it is. So um um you and there are if you like numerical um studies illustrating, how you can identify markoff blankets, given some data. Um I have colleagues um um in academia and in industry working on Markov blanket discovery algorithms. Um So it certainly is possible and what you're basically looking for is um an absence of, of influence or coupling that it's this sparsity um that emerges from, oh sorry, that underpins Marko blankets. And interestingly in um some work by, by my friend Dalton shows that for any um technically any random uh random dynamical system with sparse coupling, there will always be with probability 11 or more markoff blankets in any system. So, you know, these are not remarkable things, you know, it can be no, the way for any random, you know, any random dynamical system that has um that persists it doesn't sort of diverge exponentially into nothing as long as it persists and has technically what's called a pull back a tractor, then you will have markoff blankets having said that um you have to pick the scale and the internal states to define the Markov blanket. So there's not one Markov blanket, there are possibly uncountable, countably large number of Markov blankets and you can have Markov blankets within Markov blankets. You can have Markov blankets of Markov blankets. So you, you have to, if you actually wanted to identify a Markov blanket and there are lots of good reasons why you might want to do that. You really have to specify the scale at which you are doing it. So if you were a molecular biologist, the Markov blanket or a cellular biologist, you might draw a Markov blanket around a particular cell of interest. If you were a cognitive neuroscientist, you might draw it around the brain. If you were a theoretical biologist interested in evolution, you might draw it around a species. If you were doing climate change, you might draw it around certain aspects of the biosphere. So uh and each one of these Markov blankets would, would contextualize all the, all the smaller Markov blankets within it and at different scales. So I I'm trying to emphasize there is no wrong Markov blanket. But as soon as you identify some thing, then it must, by definition have a markoff blanket. So if it is a natural kind, then it must have a Markov blanket just to describe it. So a cell has a markoff blanket. Um A organ has a markoff blanket, a set of organs, a phenotype has a markoff blanket. A family has a markoff blanket, a community, a population, they all have their inputs and outputs. And I repeat because you've got a very precise specification of what it means how you define the Markov blanket. All you have to do is to identify the sparse coupling. You just have to identify what is not influenced by what. And once you've done that, there are easy mathematical recipes for identifying the Markov blanket just for your entertainment. Um The the Marko blanket of any given state or set of states is effectively the parents of by which I mean, those things that influence the parents of your internal states, the Children, which are your active states, the things that you influence in the things that the internal states influence and the parents of the Children. So, if you think about the the two rules, I said that the sensory states cannot be influenced by the internal states and the active states cannot be influenced by the external states. Then the complete set of parents and Children and parents of the Children are just the blanket state. It is sensitive based on the active states. So you can identify that using matrix algebra and dynamical systems theory. Um OR you can infer that by using Markov blanket discovery algorithms."
	},
	{
		"start_time": "33:26",
		"speaker": "Ricardo Lopes",
		"words": "Mhm But there's a bidirectional interaction between the internal states and the external states."
	},
	{
		"start_time": "33:33",
		"speaker": "Karl Friston",
		"words": "Correct. Absolutely. And, and that, I mean that that is a key construct uh that the Markov blanket uh brings to the table, you've got this sort of um boundary that allows you to say no, these are the states of the agent or the system in question. And these are the state of the environment um or the external states, but they are uh bidirectionally coupled. So the system is vicariously open to everything else, but only through this two way street, which is the Markov blanket. And you know, and that reciprocal aspect that you you foregrounded there is really important. And that's just why I made such a big point about saying there are sensory and active parts of the Markov blanket sometimes referred to as sectors. Um And I say that because the, you know, you can think of the Markov blanket in terms of inputs and outputs. Um But you can also um apply quantum information theory. Um And people like chris fields have done this and then the Markov blanket becomes something known as a holographic screen. And the holographic principle says that all the information that is accessible um from an observer from the outside about the internal states on the other side of the holographic screen is available on the holographic screen. So that's all, that's what I was intimating before. When I, you know, when I'm looking at you, I'm basically looking at your Markov blanket or your holographic screen at the scale that you know that we are actually communicating."
	},
	{
		"start_time": "35:15",
		"speaker": "Ricardo Lopes",
		"words": "So if we want to talk about causality here, would there be a circular causality between the internal states and the external states? Is that it?"
	},
	{
		"start_time": "35:26",
		"speaker": "Karl Friston",
		"words": "Yes. Yeah, that, that, that, that is another really important point. Yes. Um So, first of all, just to be very clear and precise about um causal causality in this framework. Um So this framework, the three principle always stays very close to the physics. So causality here is very, very clear. It is basically the influence of one state on another state that is expressed in time, usually expressed in terms of a stochastic differential equation, which would be one way of writing down a random dynamical system such as I say a long random system. So we're using causality in a mathematical sense. Um um THAT could also be read as in a controlled theoretic sense I cause changes in you simply by having an influence on you. So is that causal structure that defines the Markov blanket? Now notice there is another whole universe of causality which does not rest upon control theory or dynamical systems theory or random dynamical systems. Um You know, the French principle is not interested in that kind of causality. It's just interested in exactly the kind of causality that underwrites the uh the market blanket. And simply because we've got these bidirectional influences causal inferences, you're absolutely right. There's c causality and that circular causality just as the action perception cycle. And that becomes really important when you think about two agents or an ensemble of agents all communicating with each other or coupled through their own bound markoff boundaries or markoff blankets with each other, no one system causes the others, they all cause each other. Uh And you even see that uh within the o let's take, let's come back to this notion of the brain as a hierarchical generative model. Then the hierarchies are defined by their Markov blankets. So you've got nested Markov blankets in the brain. And we were saying before that one functional architecture that is um necessary, not that is necessary in terms of the Markov banker, but also necessary to understand the predicted coding is the sort of they are sending and descending messages, predictions and prediction errors. So again, you know, you've got circular causality. Does the prediction cause a prediction error or does the prediction revise your prediction. So circular circular causality wherever you look."
	},
	{
		"start_time": "38:20",
		"speaker": "Ricardo Lopes",
		"words": "Mhm Perhaps this is a good point to introduce. Then another concept, what is an autonomous state,"
	},
	{
		"start_time": "38:28",
		"speaker": "Karl Friston",
		"words": "an autonomous state in the context of a Markov blanket partition is just a description of those states that can be described as minimizing free energy and very simply they are just the internal states and the active states. So mathematically, um it's just those states, those subset, those states of a particular agent um that can be described as if they are minimizing prediction error or free energy. So the nice thing about having that split of autonomous states into internal states and active states, then you can say that it must be the case that the internal states are minimizing free energy. So it looks as if they are minimizing their prediction. So it looks as if they are doing perception while the active states which are the other part of the autonomous states um are also minimizing exactly the same quantity which is the prediction error. But in this instance, in the productions of the felt body, technically the proprioceptive or inceptive um uh predictions. And of course, that's just action. So again, you've got a nice if you like a common framework or a first principle account that ties together action and perception through a common application of the free energy principle to, to those states to which the free energy principle applies, which are just the internal states and the active states. So notice the sensory states don't do that. The sensory states don't minimize prediction error because they are influenced by the external states. But the active states by construction are only influenced, they are not influenced by the external states and the internal states are not influenced directly by the external states because they are hidden behind or sequestered by the Markov blanket. So the autonomous states, the active states and the internal states are unique in the sense that they are not directly influenced or caused by external states. And that's, you know, that then um leads you to this interpretation in terms of inference, in terms of, you know, the internal states holding beliefs about the external states and using those beliefs to predict how they're going to act, act upon the world."
	},
	{
		"start_time": "40:54",
		"speaker": "Ricardo Lopes",
		"words": "Mhm You've already explained here more or less how this framework might apply to different kinds of systems. But how do we get to living systems? I mean, what is the difference between a living system and the nonliving system?"
	},
	{
		"start_time": "41:13",
		"speaker": "Karl Friston",
		"words": "Um Yeah, that's an excellent question. Um I think probably best tackled from the point of view, the free energy principle. Um So you've got this um this description of any kind of particle, any kind of thing um in terms of its markoff blanket partition. Now that means that there are different kinds of things. And your question I think is at what point would you call this particle living or biotic? And at which point would it um take on the um the attribute that would associate with agency and possibly sentience? Um So we just work through it again, it's relatively straightforward. So we can have things where the mark where the active sector of the markoff blanket is empty. So we're talking about things that cannot act upon the world, they can still sense the world, they can still be affected by the universe, but they can't act. So, you know, one might imagine a rock on the moon, for example, or something that, that, that does not have the um does not have a full complement of autonomous states in the sense that if the autonomous states are the internal states and the active states, if they don't have the active states, they can't move. So you wouldn't call these kinds of inert systems or inert particles. Um um Living, would you call them uh living if they were able to move basically um active states that express themselves by acting upon the environment? We were just talking about things that move effectively in a, in a classical uh context. Um AND things that move in a way that depends on and only on the internal states and not the external states. So there will be another definition of autonomous dynamics. Then I think you're probably getting closer to biological self-organization or biotic self-organization. Uh Is it living um at this point, um probably not there, there's an extra uh if you like condition that you would probably want to apply before you would say that this is biotic or biological uh self organization. And that is the precision or the reproducibility of the movement. So, if something is very, very small, it usually comes along with lots of random fluctuations. So usually things fluctuate very quickly. They have a higher temperature from the point of view of cis mechanics. When they are very, very small. And in the limit of quantum uh physics, it's all uh it's all random effectively um or occurring so quickly, you can't measure it. So you can only describe it in terms of probability distributions. Um But living things have characteristic states that they keep on revisiting that you can recognize um you know, when um in, you know, in a life form simply because um there is some systematic revisiting of certain states. And that tells you two things about self-organization of biological, of a biological sort or a living sort. It tells you that they have to be realized in systems that are sufficiently large for the random fluctuations to be averaged away. So you won't find living things at the quantum level. Um But you may find it at the level of um say a virus and you'll certainly find it at the level of a cell as you get to sort of, you know, hundreds of microns or you know, even millimeters. Uh AND when you get to something, our size, then if something, our size moves around of its own accord because it, it has active states that are driven by turn. So you'll say yes, that that is certainly living. Um So size really does matter. The scale really does matter, I think. Um And the reason that scale is so important is as things get bigger, all the random fluctuations disappear and things get much more precise in terms of their dynamics that flowed. But the other interesting aspect of this and interestingly um this inherits from the physics work of Helmholtz when he was trying to understand the flow of systems, say for example, turbulence in water. Um YOU know, is there any way that you can sort of classify the dynamics of the flow or the movement of states in some state in some abstract state space? And the Helmholtz decomposition says yes, you can divide things into those flows that try to minimize their energy. And for our invasive mechanics, this is the variation of free energy in classical mechanics. It would be things like gravity in thermodynamics. It would be things like a potential potential energy. So systems can be described in terms of their flows, you know, so I'm using flows and dynamics interchangeably here. So and the flow can be split into two parts. The first part is known as a dissipative flow. And it's that which can be described as flowing downhill, like literally water flowing um flowing down a valley or down a potential gradient. And that dissipative um aspect underwrites. Um A lot of cis mechanics in terms of fluctuation dissipation theorems. Uh um um uh AND a lot of the rhetoric surrounding c mechanics like heat and work all speaks to the way in which heat is dissipated or work with work due to this kind of flow. There's another kind of flow. So if you imagine a valley and you've got flows down the valley and these flows are equipped with random fluctuation, so you never settle at the bottom of the valley. You, you, you um uh because as soon as you get there, there will be some random fluctuation that pushes you away. So there'll be a little um a little regime at the bottom of the valley that every you have flown down into and then you're perturbed by the random fluctuations that's thermodynamic equilibrium. And that will be the dissipative physics that you know, the statistical mechanics is based on. But there's another really important kind of flow which emerges in non equilibrium systems. Um And that's flow around the iso contours around the rim or any given level of the bowl. So this conserves the energy or the height of the bowl. So it's called conservative uh mechanics. Um And um this is really important. So in the limit that things get very, very, very, very big, the random fluctuations are almost zero. And so what do you see, you just see this conservative flow, like for example, the motion of a heavenly body like the moon orbits. Now, these things have a certain aspect which brings us back to um what we're talking about for in terms of biological self-organization, which is revisiting states that you were once in characteristic states that you sometimes wander away from um with an itinerant trajectory, but you always come back again. Um So in the limit of being very, very big, much bigger than you and me, uh then, then we now conform to conservative mechanics and we have this c this circular flow. This is sometimes a solenoid flow as well. And you can read about that if you look at you know basic descriptions of things like fluid, fluid mechanics. But that decomposition or variance of it holds for any kind of state including the states that comprise the Markov blanket partition into internal blanket states and external states. And this is so this is to my mind is so important because what it says is if the random fluctuations are sufficiently small that I can see the steroidal flow, what that means is things of a certain scale will always revisit states that they were once in. And then you just think about this at different scales. What you are talking about is either very, very fast oscillations, for example, in your brain cells or they could be the cardiac cycle going around these um itinerant orbits just like the move but much more complicated now and uh slightly fuzzy because you're sufficiently small um to actually have random fluctuations there. Um It could be walking or it could be by sleep weight cycle. It could in fact be reproduction or replication at an evolutionary scale. At every scale, you've got this characteristic solenoid dynamics, this conservative aspect that I think is definitive of biological self-organization. So essentially things that have biorhythms and life cycles. Um MEAN that um well put it another way, if um biological, if living things have evinced spy RMS and life cycles, then that means that they must belong to a certain class of um things at a particular scale, they have to be bigger than the quantum scale, but to be sufficiently itinerant, um they have to be smaller than the, you know, the, the cosmological scale you we can go on if you wanted to, you know, there are, there are, there's one final um kind of thing that has this um this um conservative aspect, um the solenoid um um aspect to it. And that's when the dynamics become very, very precise. Um WHICH means that you can effectively roll out into the future. And if you can run out into the future, that means not only can you predict the state at the moment that um um this um uh this system is in but the long term future And if the uh if you can predict the long term future, it will look as though certain kinds of things like you may plan into the future we have intentions. And I think now we're now getting very close to um life forms with agency. Um So, you know, to uh cut a long story short. Um um So the answer to your question is yes, there are certainly different kinds of physical systems uh at different scales with different degrees of random fluctuations um and different kinds of dynamics. Um And I would posit that life is basically um uh a special kind of thing. Um THAT would subsume things that are um properly a gent in the sense that they plan and some things that are not, I don't think viruses plan. Um Whereas I do think, say b is a new, you and I plan into the future. Um And, but we are quite special. We're in a goldilocks regime between being very, very small and being very, very big."
	},
	{
		"start_time": "53:17",
		"speaker": "Ricardo Lopes",
		"words": "So would this framework help us deal with the, with questions surrounding what separates exactly the organic from the inorganic and particular kinds of entities like viruses? Because viruses in particularly are very people debate a lot about whether they are living systems or nonliving systems,"
	},
	{
		"start_time": "53:39",
		"speaker": "Karl Friston",
		"words": "right? Yeah. Um I mean, you could certainly use this framework um to happily accommodate the vagueness and I'm using vagueness now in a philosophical sense. Yeah. How many grains of sand, make a pile of sand. So at what point would a virus be called living when it was sufficiently big or complicated? You can certainly locate a vague notion of um, living versus nonliving. Um And even to, uh I think to a certain extent agents versus non agents or sentient, but versus non agent on, on this space afforded by, um, the, the Markov blanket and implicitly the free Engine principle. Absolutely. Uh You know, could you draw bright lines um between, um you know, is, I think you possibly could draw some bright lines. Um um YOU know, in, for example, you could say, well, in the limit that the random fluctuations disappear on the inside, that might be a bright line between things that are biotic and not biotic. Um That's not to say that, you know, so a virus would certainly be biotic. Um It would have, it would have recognizable um states and trajectories and character and it would have this solo aspect. It will, it will have a life cycle, it will, you know, survive in a particular eco niche. So usually intracellular one. BUT you wouldn't necessarily say it was living. So you ask, well, what does it have to, what does it mean to, to, to, to, to live? Um um AND in particular, what does it mean to, I, I think be um to be an agent in, in, in a sort of, in the sense that um there is that kind of anthropomorphic agency and autonomy, autonomy. And I think that the bright line there is that the um the system can be described as if it has a garr model of the consequences of its own active states. I think that is a, that possibly is a bright line that, that helps basically give you an ontology of different kinds of things. Um And, you know, if you've got, if so, I'm now appealing to you, if you recall at the beginning, I was setting up the notion of a model as foundational. Um So the jar model is just basically a description of a pro description of the characteristic states of the thing in question. Um And if you, if your generative model is such that it includes uh paths into the future and because the future is caused because of the circular, circular causality you mentioned before, which is acting upon the world, then implicitly, you've got a generative model or at least your internal dynamics can be described as having an internal model of the consequences of your action in the future. And at that point, I think you are starting to talk about minimal agency, not minimal self hood. This kind of system doesn't know it's an agent, but it now has a minimal kind of agency in the sense it represents the consequences of its actions and can be understood or read as selecting among a number of different actions. And choosing the one that is most probable for that kind of thing, that kind of thing that it is. So, yeah, I think I'm not quite sure what a virus would, um, the virus would not plan. Um So it's certainly biotic. Is it an agent? Is a thermostat agent? Um You know, probably not."
	},
	{
		"start_time": "57:42",
		"speaker": "Ricardo Lopes",
		"words": "So earlier, we've talked about the need to reduce prediction errors. And in your work, you talk about existential imperative. So what is an existential imperative in this particular context? What does that mean?"
	},
	{
		"start_time": "58:00",
		"speaker": "Karl Friston",
		"words": "Um It's just another way of um putting a teleology, a theological understanding or description on the physics backstory. So um in the same way that you can describe the behavior of anything in terms of self evidencing which we rehearsed in terms of gathering evidence from my model of the world, um existential imperatives are what it would look like to be something. So the free energy principle actually turns this on its head and says, if you exist in the sense of possessing a markoff blanket which itself implies or entails a set of attracting or characteristic states, that is the gerit model that is the pullback attractor when described um oh sorry, that is the pullback attract tractor that when described probabilistically is the gerit model. That means if you exist, you will um look as if you are conforming to certain imperatives, namely minimizing your prediction error or your free energy. So this the minimization of free energy is a consequence of existing I exist. Therefore, I can be described as thinking and inferring and perceiving and acting. Um But you can sort of turn that on its head and say if you wanted to, you are licensed to tell a much more theological function story now to exist. I must infer I must perceive and act. I must have a generative model of my lived world in order to act sensibly in that world. So that's the um the genesis of the existential imperative that if I do not uh minimize my free energy, then I cannot exist. It's a slight of hand, it's, it's, it's slightly cheeky because the physics. So the other way around the physics says that if you exist, um then you can always be described as minimizing uh of conforming to the looking as if you are minimizing your free energy. So that means you can be described as if you like optimizing something extremis something complying with a principle of least action which means that you can, it looks as if you are complying with it to minimize um you know, to minimize your free energy. Uh JU just for um people who are not familiar with the principle of least action action here um as in um is read as time times free energy. Um So it's, it's, it's uh technically, it's the area under the curve as you move forward in time. But the ba mechanics just is a principle of least action in exactly the same way that Hamilton's principle of least action just describes the motion of massive bodies. All we're doing here is describing the motion or the dynamics of internal states in terms of them holding beliefs about external states and then acting upon. It's the exactly the same kind of maths. Mhm."
	},
	{
		"start_time": "1:01:23",
		"speaker": "Ricardo Lopes",
		"words": "So I would like to get now a little bit more into nervous systems and the brain. But there's another concept that you touched on briefly earlier that I think it's important for you to explain a bit better just for people to understand what we're going to talk about here. So what is self-organization and specifically biological self-organization?"
	},
	{
		"start_time": "1:01:47",
		"speaker": "Karl Friston",
		"words": "Well, self organization um is a term U usually used um by um physicists to describe um uh how systems, how the dynamics of systems bring about particular states. Um And it's effectively what we have just been talking about. I mean, I've slipped in some technical words like pullback tractors. Um um So you want to understand the physics or more specifically the mechanics um behind um the uh propensity or the existence of certain systems that seem to be drawn to, you know, to certain attracting states. So the simplest example of that is uh equilibrium physics, you know, why is it that things cool down, you know, why is it that you can um why do we have the second law? Um YOU know, what, why do things settle to a free energy minimum when isolated from the world, when, when they, when they're closed systems. So the physics of closed systems is just a way of writing down the physics or the laws, literally, you know, the equations of motion of states that describe the behavior of things that are isolated and they just settle down to the, the lowest energy point. Um uh One when one moves um to systems that are now open, that also now induces the circular causality you're picking up on before. Then one now moves to self-organization the context of non equilibrium. So non equilibrium physics because there are no equilibria anymore because, you know, because of this circular causality and then one, what, what one is aspiring to understand is the um the ability or the existence of certain systems that have um what can be described as a non equilibrium steady state, which is just a way of saying that, you know, if I wait long enough, um my system will confine itself to a set of characteristic states. Um And it will never leave those states. So do so just, you know, probabilistically or um um very sporadically, that's the attracting set, that's the pullback attractor. So you want to understand how do things converge upon their, their attracting uh states, their characteristic states and characteristic states here is just a euphemism for um being a member of this attracting set that is characteristic of me. And you can actually score it with a characteristic function. The characteristic function is one if, if uh I'm in one of my characteristic states and zero, if I'm not, if I'm, you know, long temperature or um in two pieces or um or anything else that would be inconsistent um and violate those existential imperatives. So, self-organization, you know, it is an umbrella term to, that starts to get really um interesting certainly in physics, when you're talking about non uh non equilibria and open systems,"
	},
	{
		"start_time": "1:05:19",
		"speaker": "Ricardo Lopes",
		"words": "and how does it apply specifically in the case of nervous systems? What does it mean for a nervous system to be self organizing?"
	},
	{
		"start_time": "1:05:30",
		"speaker": "Karl Friston",
		"words": "Well, self-organization, strictly speaking, um can be applied to any system that possesses this characteristic based. But of course, the most interesting for people like you and me is, is is the nervous system. Um BUT you know, the principles should be exactly the same. Um So the self-organization um um usually um is manifest at a number of different scales. Um And at each scale, you're basically looking for an explanation which um from the point of view of the Free engine principle is a minimization of free energy, but at different scales. So at a very fast scale, well, actually, now let's start off with a very large uh large scales and then sort of winnow down. So at the, you know, at an evolutionary timescale, we are now looking for processes, self-organizing processes uh where the where speciation is you know, uh a self organizing uh phenomena at the level of evolution itself. Um AND you know, things like adaptive fitness and indeed, the definition of the species entails a persistence over time with characteristic states, characteristic pheno states. So what kind of basing mechanics would you be looking at if you were applying the free energy principle? Well, you'd be looking at those free energy minimizing processes that have this uh notion of selecting the good phenotypes that are a good match minimum prediction error with their environment. Um In statistics, this is known as Bayesian model selection. So selecting a number of different generative models, which we can now read as the phenotype. So, before I am my genitive model, I am seeking evidence from my own existence because I am seeking evidence from my model and I am my model. So now I can label each phenotype each uh each baby um uh uh um as a model and I can score the adaptive fitness of that model in terms of um that baby's um well, that over the lifetime, um we can take the uh the free energy over the lifetime or the pathogen or the free action um and that would be adaptive fitness. So now we've got be model selection now is just natural selection. So evolution is nature's way of doing be model selection to select the free engine minimizing generative model with her Markov blanket. That is the phenotype. What does that mean from the point of view of neurodevelopment at a slightly sort of faster time scale. Well, it means that the model that I've inherited from my genotype of my parents and this basic model selection of, you know, over millennia means that I can now um learn my environment quickly and efficiently in an adaptive way in a free and minimizing way. Also, it means that my environment is actually built to be learnable. So the self-organization that you'd expect to see during um uh neurodevelopment um say development of the brain in um in utron then crucially during early developmental periods is basically a um a selection of the neuronal elements and systems and connectivity. The connectome um that is fit for purpose to um to provide a good generative model of this phenotype of this infant's world where this world is carefully crafted by the mother by school, you know, by carers um by teachers to make it as learnable as possible. And then you'd be asking uh if you're a neurobiologist, what are the mechanisms of this? Well, there are all sorts of um um um levels on which you can study this sort of epigen specification and of neurodevelopment right through to experience dependent plasticity where neurons or brain cells that wi together fire together. This itself is a free energy minimizing process. It's also known as heavy plasticity and effectively the the connectivity as it grows. Well, interestingly, in fact, we're usually born with too many connections. It's a bit like being a large language model that we have to actually prove. So it's actually a question of the right level of sparsity. So we actually lose redundant connections as we grow up and become old and wise and sparse in our uh our much simpler and more efficient uh generative model. But um irrespective of that, um all of this can be written down as uh effectively learning in exactly the same way that machine learning uses recurrent neural networks to um um identify the the best weights that the best explain the content to which this um um recurrent ne network was exposed. Uh For example, for, you know, for example, a large language model. So this will be plasticity of a developmental sort but also experience dependent plasticity um that we still enjoy today, we, we can still learn to drive a car when we're quite old or you can now go off and learn a new sport or you could run away from fires in a, in a way that you've never, never done before. Um So we, you know, we still have this capacity to learn. Um And we also have the capacity to revise the very structure of our models of the kind that is um prescribed epigenetic at birth. Uh You know, when we go to sleep at night, for example, you know, we are eliminating certain synaptic connections that are redundant that have accumulated through experience dependent during the day. So this is self organ organization over a timescale of years, developmentally um days, in terms of the sleep wake cycle and hours, in terms of learning a new skill, and then we go to um a faster um um uh level of um self-organization um which um would be um framed now, not in terms of hours, but in terms of milliseconds and seconds. And this would be the kind of self-organization that would be appropriate for describing the dynamics, not neuroplasticity, not the efficacy of the strength of neural connections, but the actual activity, the neural activity, the dynamics themselves, the amount to which brain cells are spiking or shouting or trying to influence each other through message passing and depolarization events. And this is the inference part, this is the perception and the active part. So what is this kind of self organization? Well, we've just said it's neuronal dynamics that have, that can be described in terms of space mechanics that can be read as perception of action, changing your mind in the moment to predict accommodate and to realize predictions about the sensorium given the way that you are sampling your sensorium. And then you get into the world of cold in neuroscience, even rated potentials and things that change very, very quickly. So we've moved away from learning and ethology um into the world of self-organization over milliseconds. There's an interesting intermediate scale which I just slip in. Um WHICH I think makes us um intelligent and, and cognitive, which is uh being able to recognize the context um in further context you are in and make short term adjustments to the connectivity that it has been learned. Um And this is often framed in terms of representation of uncertainty or confidence or precision ascribed to different sources of evidence in psychology. It would be attention. Um um um So this is not actually recognizing or inferring the particular content at the moment or in the future, but the context in which the ability that your brain has to recognize you're in the dark and therefore make appropriate adjustments in terms of pupillary dilatation. Um um um SORT of, you know, adapt your photoreceptors uh in a way that knows that there's a different dynamic range range at hand. So being in the dark provides a context that outlives in time, the very fast sense making they get from say visually avert responses. I mentioned that because that's quite important, attention is quite important when it comes to talking about things like sentience and phenomenal experience and possibly mental action. Because if you've got the ability to recognize the context in which you are making sense, it may be that a higher level of the brain in this deep architecture is able to now act upon lower levels to say it's in this context, you're making you're doing your predictive coding or it's in that context. And now you've got the notion of action on the inside, not just through moving your bodies. And that mental action, which I repeat, it could be just optimizing or seen as optimizing your intentional set or in the active domain, your intentional set may be an important if you like facility that thermostats don't have and possibly insects don't have it. Certainly, you and I have this sort of, um, fast but not very fast. Um, SCALE of self organization, coordinating things in, you know, 300 milliseconds through to several seconds, for example."
	},
	{
		"start_time": "1:15:50",
		"speaker": "Ricardo Lopes",
		"words": "Mhm And still on the topic of nervous systems and the brain more specifically, uh earlier, you mentioned another concept that I said, I would need to ask you more about what is hierarchy in this context. What does it mean for the brain to be hierarchical?"
	},
	{
		"start_time": "1:16:10",
		"speaker": "Karl Friston",
		"words": "Right? So again, that's an excellent question. So um hierarchy um simply means um that there are separable levels or scales of some thing and implicitly lots of things because there's things at every scale. Um So, from the point of view of a physicist, uh there's another kind of circular causality which you'll find in um non equilibrium physics. And in particular, um the uh synergetics of, of the sort that Herman Hagen is famous for was, was, was famous for um he unfortunately died very recently. Um So this circular causality is a circular causality between scales. And you think of this as a sort of a hierarchical ladder So the hierarchy very much as in an organization or in a caste system means that there is a certain ordering. So you can be low in the hierarchy or high in the hierarchy. Um And this hierarchical structure always exists in the universe. Um SIMPLY because you can take small fast fluctuations at any given scale and take the averages of these. And then what happens is that the fast stuff gets averaged away, you're left with slightly slower, larger scale things. So you've got a representation of the fast stuff at the lowest, at any given level. And then at the higher level there, um things slow down. There are um they are if you like um caused by or subtended by the fasting simply because the fast things are um uh added together to create the slow things. So um um but at the same time, the slow things provide a context for the fast things. And this is the kind of circular causality um celebrated in synergetics. And that providing the context means that you can write down mathematically akin to things like the center manifold theorem. Um A notion that the slow things at the higher scale enslave the fast fluctuations to particular manifolds on which the slow things operate. Um And, and you repeat that you can rinse swash and repeat. Now you've got the slow things that can be averaged to make even slower things. And if it is a case as it probably is. Um AND certainly from the point of view that the energy it is relatively easy to show it is the case uh if it is the case that the dynamics, uh the functional form of the dynamics expressed as a Lagrangian or in terms of the equations of motion are conserved over scales with different scaling properties. But you know, the functional forms the same you have what they call the renormalization group. So what that means is that the world out there has this hierarchical structure where slow things contextualize and enslave fast things and the fast things effectively um are coarse grained in order to summarize and create the slow things. So this is a circular causality that um that emerges from the slaving principle in physics. Why is that interesting? Well, remember before, the free energy principle says that um an existential imperative is to have a good uh um narrative model of your lived world. Um And by good, it just basically means that the probability of the data being provided by of the sensory input being provided by this world is very high on your model, which means that you have to have a good model of the world because I'm using that expression um with a nod to an acknowledgment of things like the good regulator theorem and the law of requisite variety that was established in the early days of cybernetics. And what this basically says is that if I am in an open exchange with my environment. And if I could control and measure, if I can act upon and sense my environment um in the language of the good regulator there, if I can regulate this environment, then I have to be a good model of that environment. So this is just another statement of the free energy principle. In the sense that um um to be a good model means that my model evidence is very high. And if I maximize my model evidence, I'm imply minimizing my surprise or, or my free energy. Um But we've said that the world has to have a hierarchical structure, which means that my brain as a good model of this world has to have a hierarchical structure. So, what would that look like? Well, it would look like um effectively bits of the brain sitting very close to the sensory input, responding very, very quickly um to what's going on. And then there would be a level above it, a level above another part of the brain, a brain system that's looking at the level below as if it was the Sensorium as if it was a sensory interface, as if it was the Markov blanket. And indeed, and then you've got a third level or scale that's looking at the second level. Um And it's enacting in this um in the spirit of the renormalization group, it's doing exactly the same thing. In fact, it's doing predicted coding, which is minimizing free energy that that is the, the Lagrange in this instance. Um uh And so that gives you a view of the, of the brain as basically layer upon layer, very much like that onion analogy. Uh We had before where each layer is trying to make sense of the layer towards the outside, the lower layer, the layer or the level that is closer to the fast sensory fluctuations that are provided by vision or hearing or interception and making sense of means um being able to predict um um vertically and simply um um in the, in the spirit of minimizing or maximizing model evidence or minimizing um free energy. Um And that basically can always be read as providing a simple account that is accurate of what's going on at the level below. Uh And I use that language because um the simplicity is important here um in the sense that the model evidence, the, well, let's put it the other way, the free energy can always be expressed as complexity minus accuracy. So if I want to minimize my free energy or maximize my model evidence or minimize my prediction error or minimize my surprise, I'm effectively trying to find the minimally complex explanation that retains an accurate account. So I'm always trying to simplify, I'm always trying to compress, I'm always trying to dimension reduce, I'm always trying to summarize coarse grain without losing accuracy. So it's lossy compression, but the least lossy in terms of the amount of information. So this notion of compression is really important because this is where um a lot of people think about um uh the the existential imperatives can also be seen not as minimizing prediction, but just by compressing. Uh JUST arac in coarse brain away what really matters in describing what's really going on and increasingly longer and larger spatial scales. So as you send the hierarchy in the brain, as you move deeper into the brain, in terms of this anion analogy, then you see more of the surface. Um So, you know, after a certain uh uh after a certain depth, there will be brain systems that are multimodal, they will be receiving say prediction errors from the visual brain, from the hearing brain from the intercepted brain from the, you know, the proceed brain, they will be multimodal, they will be a modal, they will be domain general. And furthermore, because this scaling as um as we were emphasizing when describing the slaving principle, um the the the compression will be into slower and slower narratives. So receptive fields inside the visual cortex in the back of the brain at the very the very lowest hierarchical level, um the one primary visual cortex, they just see a very small part of the visual scene and they, you know, and they respond to very little transient changes, little bars and you know, very, very quickly. Um And if you go right down to the brain stem. You know, you're talking about very fast processing in less than less than 100 100 milliseconds. But as you get out, say to the prefrontal cortex or intermediately parietal cortex, then things you start to get notions of things moving visual motion, which means that something has to move for a period of time before you can actually estimate, you know how fast this thing is moving. Is it a bird or is it an airplane? Meaning that the spatial temporal receptive fields increase in size crucially in time as well. So you might argue that when you get to very high levels, say hippocampus, you've got whole narratives lasting encoding, you know where I've been and where I'm going, uh you know, seconds into the future. So this would be um yeah. So that would be the implication of having a world which has this renormalized structure, which it always has to, it has had different levels of description. Um HOW that hierarchical structure gets installed into the neur anatomy of a brain that has this increasingly deep, slow compressed representation of a narrative self that transcends any given moment. So there is no present. We are just representing things that are unfolding very, very, very slowly. So perhaps language is a good example here, the primary auditory cortex will be perfectly attuned and respond to a particular frequency glide. Then there will be sort of per Aitor cortices that may respond to phones the the the higher levels that may start responding to the words and the lexical structure of words, there will be higher levels that actually look over increasing numbers uh periods, extended periods of time and start to look at the, the, the put semantics in the con of syntax. And then you have higher levels that have, you know, representing the narrative. And then the genre of the book say that you're reading. So each level you're moving up same principles that play out at different different scales."
	},
	{
		"start_time": "1:28:03",
		"speaker": "Ricardo Lopes",
		"words": "So when it comes to studying the brain, we have different methodologies, different ways we can do it. Like for example, neuroimaging lesion studies, brain stimulation studies, animal models and so on. Uh I would like to start by talking a little bit about neuroimaging because that's an area where you've done lots of work. So what can we learn about the brain by studying it through imaging? What are the limitations of neuroimaging?"
	},
	{
		"start_time": "1:28:36",
		"speaker": "Karl Friston",
		"words": "Um Well, neuroimaging read as brain mapping. Um I think is a way of testing hypotheses about functional brain architectures. And we've just been, we've spent, you know, 20 minutes talking about some really important aspects of, of, of, of functional brain architectures. Um But as we said, at the beginning, uh I can't peer beneath your Markov blanket without breaking you. So if your Markov blanket goes away, you're no longer you. Um So that, that, that, that presents an interesting problem if I want to understand how I work. I can't use my own brain because if I use my own brain, it, um I will disturb all the perception and action that underwrites me, trying to understand and experiment my own. But I could study your brain. But can I? Well, no, I can't. Well, perhaps I could study um things like your brain. Uh Perhaps I could study um the uh the brain um of uh your um great grandparents after they died and I could do some neuron after and dissect dead dead brains. Um So that would be, you know, if you just look at the history of neuroscience, it all starts with dissecting dead ones where the thing doesn't exist anymore. So its Marko blanket has disappeared and you can intervene as you look at the structure and get a lot of insights into the function simply by looking at the structure. So just discovering hierarchical organizations, just discovering the preponderance of connections in the brain. Um So if you just think about that, that's interesting from the point of view of that good regulator theorem. I was saying before, if you were able to dissect the brain, you would be amazed that nearly all of it is wire, there are, there's only a very small part of it mostly on the surface like a cauliflower uh covered um with the um you know, the cortex, which literally means that, you know, that which is on the on the surface which is just a few millimeters deep. That's where all your brain cells are. All the rest of it to a certain approximation uh is just wiring. Um And that wiring is uh um very sparse uh mediated by axons that conduct um or have influences over um a whole range of distances. Um YOU know, uh you know, up up to several centimeters um in some instances with a, with a certain fasula and they are able to conduct with high fidelity and influence other neurons by being wrapped in myelin, which is a fatty insulator like a cable. And that fatty substance looks white. So it's called white matter. So the cortex is gray matter, the interior, most of the interior of the collectivity is white matter. That is interesting because what it means is if you are AJ model of your world, then your world basically must be of the sort where there is action at a distance and it is quite sparse, which is interesting, isn't it? Because by,"
	},
	{
		"start_time": "1:31:53",
		"speaker": "Ricardo Lopes",
		"words": "by the way, sorry for interrupting you, what does the term sparse or sparsity mean in the context of neuroscience?"
	},
	{
		"start_time": "1:32:03",
		"speaker": "Karl Friston",
		"words": "Um Well, it's used, I think in the same context everywhere. It just means that it's, it's it's um can be used um us refers to um um distributions. So a very sparse distribution um would for example be where there are a very, very small number of big things and a large number, a very large number of, of, of small things when used in the context of connectivity, which is a sense in which we've been using it throughout this conversation. Um uh Either in terms of coupling in random dynamic systems and causal inferences or in terms of brain connections that are if you like the physical instantiation of those causal causal influences. Um Then we're talking about uh we can envisage a matrix, a big square with lots of elements where there's where there's an entry when one thing is connected to another thing. So if everything was connected to everything that matrix would be populated with nonzero elements everywhere and it would be dense. So it would be a non sparse matrix. On the other hand, if everything is just connected to one other thing, then for every column in that matrix, there would just be one entry. So although you have N squared possible connections, there will only be a small number in that particular simple example, just N connections. So that's a sparse matrix. So what people are looking for when they look at connectomes of connectivity or adjacency matrix in graph theory they're looking for um or they expect to see um sparse sparse connections. So of all the possible connectivity, only a very small number of connections are actually evident or or present. So the brain um has, you know, billions of neurons. Um So you can imagine an enormous big connectivity matrix sometimes calls them called an adjacency matrix that I'm next to you. If I'm connected to you, a massive billion times of billions, billions times billions of potential entries. But the actual number of connections is a tiny, tiny fraction of all the possible connections. And that's what people mean when they say spars, people who are experts in this field nowadays dubbed connectome say from, in terms of the connectivity, the brain is almost empty of all the possible ways it could be, you know, wires you could have, there's hardly anything there. And yet, of course, it takes up most of the uh the wiring at most of the space. So that's what sparsity means. It's an important point to pick up on because um you know, again, this theme that the brain is or entails the model of the lived world, it means that your world has very sparse causal structure. And of course, that's exactly where we started with the definition of things in terms of Markov blankets. So if I live in a universe that is constituted by things that just means I live in a universe with lots of Markov blankets at a particular scale, if I live in a universe with lots of markup blankets at a particular scale, that means it has to be sparsely coupled. And if my self-organization in terms of synaptic plasticity is a way of learning that sparse, you know, where those sparse connections or couplings are, that means that my brain should have a sparse connectivity and it does. So that, that, you know, sorry, we wander away from your question about brain imaging. But I'm just saying you can learn a lot about the anatomy. Yeah, just knowing, just being able to dissect a dead brain, it tells me a lot about the world that this brain used to be in and the principles, you know, and you know, and the scale at which, you know, this brain was operating and it also tells you about the different modalities. Then he then, then he moved to neuropsychology. So, and they, you know, af after you had all the famous um natural scientists um doing that section of dead bodies, then came along the ability to um do um uh lesion experiments on, you know, on animals, for example, on electrical stimulation experiments. So there's a lot of work in the 17th and 18th century um asking a really simple question is the brain plural potential in the sense that every capacity, perceptual or cognitive capacity that we enjoy involves or entails every neuron in a distributed fashion or are certain parts of the brain specialized for particular functions. So, you know, there were sort of uh and this, this debate raged even into the 20th century in terms of sort of lash's law of mass action, for example, you know, if I take a little mouse brain and I just scoop out a thumbnail or less, does it matter where I do it in terms of what function I have now compromised in this little animal. Um And one could argue no uh that, that it was a bad argument because it does matter. But yeah, that was, that was what people were trying to understand was there. And this uh you know, became known subsequently as um the principle of functional specialization or segregation that different parts of our brain are specialized for modeling different modalities in different parts of the world. Um AND different aspects of the world at different temporal scales, for example. So most of the uh the compelling work in terms of um lesioning studies, usually um use certainly with humans. The only lesions that you could use were those that were um occur naturally as in terms of strokes, for example, um or if you were um involved as you know, as a war medic, you, you had the opportunity to discover lesions caused by bullets, for example, in say World War One that became known as neuropsychology. So, neuropsychology just is uh understanding the deficits that follow from a lesion to the brain. Um YOU know, you can then say, well, if I've lost this function, I have this, you know, by appeal to this lesion deficit model, if I've lost this function when I've had a stroke here, whereas I've lost this function. When I've had a stroke here, you've got a double dissociation. That's one of the main workhorses of neuropsychology and that sort of means that that part of the brain might be specialized for this kind of function, whereas that might be specialized for another one. So you are starting to segregate the functional specialization anatomically. So it's also known as functional segregation. So functional segregation would be the anatomical segregation of functionally specialized brain areas. And we're talking about brain areas that at different scales cover the entire lobe load that say doing right down to little thumbnail sized patches of the brain, say B five also known in animal research as NT have been specialized for visual motion. So this was uh this was if you like um the, the whole set of um really important questions um pertaining to is the functional specialization is a functional segregation in the brain. We think there is because you look at all these beautiful neuropsychology experiments, all these very fine neur anatomy and tracing studies. Um EVERYTHING points to functional specialization, but no one knew, no one knew until brain imaging came along. It's strange. But before really before the sort of the late eighties, early nineties, this was just a hypothesis. It was only when you could actually measure the entire brain doing one thing that you could say yes, this part of the brain was involved in visual motion and it didn't activate anything else. And this part of the brain was involved in processing visual color and it's not the visual motion area and it didn't activate anything else? So it's only when you were able to do imaging proper by which I, I'm assuming that your question is about being able to form pictures of the entire brain natural. Yeah. Uh So at that point, all of these, um I mean, unsurprising but, but strictly speaking, all up until that, up until that time, functional specialization and segregation and all the theories about uh that intend in terms of uh functional brain architectures were just hypotheses. No one actually knew until you could actually measure the brain responding or executing a particular function or engage in a particular kind of perception. Uh um um Until you can actually see that and see the anatomical segregation in the images that we knew that was the case. And that having established the principle of functional specialization or segregation, the next stage was to turn to the second principle which was um which is in my word referred to as the principle of functional integration. So now you've got this notion, you've got lots of different sort of centers and anatomically segregated specialized processing brain regions. Now, you want to know how they talk to each other, how are the messages passed around? How does one system influence another or contextualize another? What is the nature of the distributed processing? And that really requires you to understand the connectivity. So we we're back now to the adjacent sea matrices and the collective and the causal influences of one region on another region that is going to recapitulate or install that causal architecture out there. You know that it matters. You know, when you're speaking that I can either infer what you are saying through the movement of your lips, through the visual modality or through my ears. In terms of the sound waves that you you produce, there is a common cause and therefore there has to be part of the brain in which there is audio visual integration is mechanistically evinced through connections between or through convergence into these a modal domain general readers through these hierarchical structures. So this then became the focus of um decades of research ongoing uh in brain imaging to understand the connectivity and the uh and how the connections um underwrite patterns of activation and systems uh you know a different scales so that um understanding the integration of these functionally specialized regions was very important for me. Um And many people like me. Um ONE important aspect of functional integration in the brain was this hierarchical structure. So the very first paper in cerebral cortex and the first issue was a very famous Fellow of Vanessen paper just looking at the connectivity studies usually in primates of post mortem studies, looking at great detail about the nature and the lambda specificity of different connections. And they came up with a wiring diagram that was the visual hierarchy. And a lot of work since that time has now been trying to understand the hierarchical relationship in terms established by the connections by using brain imaging to measure not just which brain areas are activated, but can you infer the influence that this brain area has on this brain area?"
	},
	{
		"start_time": "1:44:53",
		"speaker": "Ricardo Lopes",
		"words": "Right. And so uh we've been talking a lot about brain imaging, but are there ways by which we can integrate the more coarse grained approaches to the brain with the more fine grained approaches? And for example, integrates the smaller sort of microscopic aspects of brain physiology with the more abstract understanding of the mind, like what we have in psychiatry or psychology."
	},
	{
		"start_time": "1:45:25",
		"speaker": "Karl Friston",
		"words": "Yeah. No, that's an excellent question. Um um And in essence, yes, you know, there is, you know that that would be the holy grail of um um um certainly translational neuroscience in the context of translating into a clinical context, neurology or or psychiatry to do that usefully as you intimate, you really need to link the scales. So sometimes um um framed in terms of from um from molecules to behavior. Uh I like from sin, the sentience. Um So how, how do you bridge that gap? Um I if you want, if you want my honest and personal answer, you, you, you need to have a generative model of the brain. So we've been talking about the brain as a generative model but a lot of the maths um behind the the energy principle that kind that, that description of self organizing self-organization in the brain inherited from the problem of actually analyzing brain imaging data and integrating data at different scales and different modalities. And the secret behind it is to have a model of how you think the brain works that can generate predictions of the data that you would get if you measured it at this scale versus that scale. So what normally happens is that people um specialize in a particular scale. So you can be a molecular biologist and work with um um um all the the the delicate and, and sometimes overwhelming plurality of um of, of different in intercellular mechanisms and biochemical pathways and or even the genetics and genomics and all this. Uh SO the whole universe just looking at not just one cell but even mitochondria within one cell. You know, there are people who know more than I could possibly know about the entire brain, you know, just about the dynamics and the structure of organelles and, and things like, you know, I repeat mitochondria in the cell right through to eho you know, who just look at um uh stigma, look, look at ant colonies, for example, uh sort of multiple individuals or psychologists uh um of a certain b who are, you know, interested in the social neuroscience. Um So all of these people work at different scale, but in principle, you should be able to link the scales. Um And that means you have to have um you have to have again, another instance of this sort of hierarchical scale free modeling, which means you need to have the right principles. So, you know, just to come back to where we started, you know, the rates on debt for the free principles to identify these scale free principles that you should find operating at every level so that you can start to link the levels. So just experimentally, what one is looking for is at a very, very fine scale to try and estimate certain dynamics rate constants connectivity structures. Um IN a way that allows you now to constrain um estimates um in more elaborate higher scale models of what's going on. Uh So just to give you a concrete example, say take psychiatry um and you're uh predicating your understanding of psychiatry, say psychosis in terms of a predisposition to hallucinations. And you can start to look for models psychological models of hallucinations and do brain scanning when people are hallucinating versus not hallucinating at a very large scale level. And then start to say does this kind of paradigm and the emergent brain activations and effective connectivity um um matter in terms of uh identifying people with and without say schizophrenia or some kind of volcanic psycho syndrome or um um uh snap. Um"
	},
	{
		"start_time": "1:49:50",
		"speaker": "Ricardo Lopes",
		"words": "WHAT do you sign up top? Sorry,"
	},
	{
		"start_time": "1:49:53",
		"speaker": "Karl Friston",
		"words": "a pathology of synapses? So, um um so the connection, the connectivity we've been talking about is uh connections between nerve cells or neurons. Those connections come in many different flavors. And how people argue that, you know, they are also um non neuron cells are important in in in message passing and self-organization. But the primary way in which one nerve cell or population of nerve cells, neurons talk to another uh population is through this, these axons, the wires and these wires basically um impinge upon receptors, dendritic trees uh of the of the recipient neuron. And the actual connection is very much like a little plug and socket. It's called a synapse. So, so this is a very, very small um um uh plug and socket um that has an amazing physiology and, and it can, it can be tuned much like a transistor or, or uh you know, modern day implementations of chips and but much more complicated than you'll find in the computer. Um So that's the synapse, a pathology of synaptic communications transmission. Um um IS a synaptophysin transmission. It is the understanding of the biochemistry, the Nero and the genomics and the physiology of how one neuron nerve cell talks to another through these synapses and it can go wrong in all sorts of ways. So, synaptophysin communication, if you are machine learning, it would be basically breaking the connection weights. Um There are lots and lots of different ways in which it can be changed. Um USUALLY associated with neurodegenerative conditions. But also you can generalize the notion that because this synaptic communication involves, involves neurotransmitters and the neurotransmitters cause their effects in the post synaptic cell by latching onto receptors. Those receptors are the targets of drugs. So nearly all the fact that all the drugs that we use. Nearly all the drugs that we use in neurology and psychiatry are those drugs that affect synaptic transmission. Interestingly not to usually directly cause a response but to attenuate or augment the impact of an existing input. So it's like switching again on the synaptic connections much in the way that we could imagine increasing the gain on connectivity in the brain by deploying attention attending over here. As opposed to over there means that I can use certain mechanisms, usually neuromodulatory mechanisms to modulate the gain of the synaptic transmission, sometimes known as post synaptic gain or sensitivity. Um So, you know, if that those systems go wrong, then you've got now um a very small scale and you understand the mechanics because you've done all the um the neuropharmacology and the molecular biology. Um um I studied these cells in context in microcircuits. If you understand that you've now got a link between the the macro scale belief, updating that underlies a hallucination and what could possibly cause the difference in the propensity for illusory or hallucinatory perception in this group of people versus that group of people predicated on an understanding of the mear biology or the drug status of these people because he started to understand the biophysics of the message passing that ultimately, that just is the neural dynamics of the internal states. But because you've now got a principled account of how the internal dynamics can manifest in terms of perception. You've now got an explanation at the level of perception and hallucinations and belief, updating, and failures of belief, updating like delusions. Or in fact, if you think about it, nearly every psychiatric syndrome can be thought of as some kind of false inference whether it's anorexia, nervosa and I have I falsely infer that I am much fatter than I am. So I just morph a phobia. So it's quite natural. If I, if I infer and believe that I'm grossly obese, then it's quite understandable. I'm going to be very resistant to overeating um through to um you know, um delusional systems of paranoia. For example, if I, you know, falsely infer that you have some antagonistic view or I falsely infer that the TV presenter is actually talking to me personally, you know, of, you know, a failure of attribution of uh a failure of attribution of kind of false inference right through to um you know, um the kind of the, you know, the influences that need new learning for if I, if we lose a loved one in our lives, you know, the grief reaction requires a lot of new learning, which um you know, um means that for a certain amount of time, um you will be making false influence, I expect, you know, my daily life to be like this and the loved one is not there. You know, you have to completely relearn a new world model or a new challenge of model where that person is no longer there that can be very painful. Um AND involve lots of mistakes and high free energy which you try to repair by, by learning a new way. I think, slowly coming to terms with what with what has happened."
	},
	{
		"start_time": "1:56:21",
		"speaker": "Ricardo Lopes",
		"words": "Could that also apply to depression? And in this case, depression, what would it be like? Would it be sort of having pessimistic generative models of the world or something like that?"
	},
	{
		"start_time": "1:56:36",
		"speaker": "Karl Friston",
		"words": "Yeah, there are lots of interesting counts of depression under the free energy principle, more particularly sort of active influence focusing on different aspect of it. You know, there is an argument that depression is not false inference in the sense that it is actually veridical and the very fact that we have depression after millennia of natural selection means that there must be something good about, about the ability to be depressed. Um And of course, you know, if you read depression in a very simple way as avoidance behavior that restricts that precludes um putting yourself in situations where you will be surprised or hurt or damaged or embarrassed or upset, um in some way, um then it can be very protective. Um So it's quite good to be de depressed and withdrawn in situations where if you didn't withdraw from this different if you didn't preclude or um um preclude that interaction, you could actually end up being more surprised and, and, and um agitated or upset than if you did the problem with depression. Um IS when you, when that base optimal um response, active inference response, um um outlives the circumstances that caused it. So say I have a bad experience, but, you know, you can see how the argument could be extended to all sorts of phobias and obsession compulsive disorder. But let's just simplicity, say I have a very bad experience. That means that if I leave my house, I am going to be very surprised. I, I um by something awful happening, whether it's a social, uh socially awful thing or emotionally aw awful thing or physically awful thing. Um, YOU know, I'm going to be in excruciating pain or, um, or feel terrified, uh have a panic attack if I leave the house. Um This will happen to me because it happened to me once. Therefore, this is the kind of thing I don't do because I don't have panic attacks. And I don't, you know, I don't, uh um I don't get upset by uh by, by other people. Um You know, I have um I have this kind of self model, therefore, it's highly unlikely that something like me will leave the house. So I end up socially withdrawn. Uh I avoid social media, for example, you know, um and that can be very functional if you have, if it's actually quite dangerous out there, but now say the world changes and all your friends are missing you and they don't know where you are, but you don't have the opportunity to revise your prior beliefs that if I go outside, I'm going to get hurt simply because your behavior has prevented you from self evidencing from gathering evidence to revise your self model or your model of the consequences of your behavior. So there are some certain pernicious responses, active um inference that mean you can never get out of that behavior because you're not exploring, you're not always seeking evidence for this way of being and that way of being. So this is sometimes framed in terms of literally getting stuck in a rut because you can't get out of the rut, you can't get out of the valley because you've got no evidence that's going to cause prediction errors to drive you to change your mind. So what you need to do in these situations such as depression is to basically um make the sides of the valley shallower to allow people to escape again and to, to, to allow the prediction errors to uh you know, drive to, to seek out information um to um um resolve uncertainty to where uncertainty is expected, surprise and expected free energy. And to do that, you can do, you can do it in one of two ways. You can either create an environment where the patient feels safe so that they can explore different ways of interacting and you'd be thinking about. So di um um either cognitive behavioral therapy or, or psychotherapy, talking therapy and psychoanalysis um with uh or you can think about doing it chemically if you understand the men, if you understand um how we encode the precision, the conviction, the depth of our, you know, I'm stuck in this flat and I've got no latitude to move. So you've got very, very precise beliefs that are holding you in place. Do you understand what neurochemically and biophysical is representing that particular belief? Which is, this is about the, about the precision of your conviction that this is the way I behave. And then you can actually give drugs to try and relax that to try and uh um relax the constraints. So people can start to explore other ways. Uh You know, so you can understand a lot of uh pharmacotherapy uh in terms of the uh uh retuning um the encoding of uncertainty or precision, which is if you remember that sort of slow but not very fast sort of thing between inference and learning. So this is the contextual inference. This is the attention, this is the, you know, the way that we encode uncertainty in the brain. Um And if, if surprise expected, surprise is uncertainty, which it is self information is expected. Self information, average self information is entropy. Um PUT in layman's language or the vernacular would basically the surprise I expect is uh uh is uncertainty then I'm in the, you know, I am always in the game of reducing uncertainty under the free energy principle, reducing expected surprise. But to do that, I've also got to recognize my level of uncertainty, my level of surprise or average surprise, which brings us back to this sort of crucial um um ability to recognize the context in terms of how confident I am in my beliefs. And that relates to microscopic on the molecular level, to synaptic fish p the same post synaptic gain and all the neuromodulation that we've been talking about."
	},
	{
		"start_time": "2:03:32",
		"speaker": "Ricardo Lopes",
		"words": "Mhm So, Doctor Frieden, I would really love to get into other topics like for example, the application of active influence to explaining or understanding certain social phenomena like social conformity. And then I would like to ask you about culture, but I, I'm just seeing that we've already done two hours. So do you think it would be better for us to uh do it? Uh Again, another time,"
	},
	{
		"start_time": "2:04:01",
		"speaker": "Karl Friston",
		"words": "I would be very happy to come back and do all those, we have to come back a few times to do to do all that."
	},
	{
		"start_time": "2:04:09",
		"speaker": "Ricardo Lopes",
		"words": "Yeah, because if we get, if we get into the topic of social expectations, culture and all of that, we will be here for another two hours."
	},
	{
		"start_time": "2:04:20",
		"speaker": "Karl Friston",
		"words": "Probably. Yes, if not longer. Now, I I now I now need to go neuromodulation and have a nice cigarette and a cup of coffee to think about what we've been talking about."
	},
	{
		"start_time": "2:04:31",
		"speaker": "Ricardo Lopes",
		"words": "Great. So, just before we finish this interview, are there any places on the internet where people can find your work?"
	},
	{
		"start_time": "2:04:41",
		"speaker": "Karl Friston",
		"words": "Uh Yes, there are. Uh I mean, it's not really my work. I mean, the, the active influence is now, you know, growing well beyond, um, even that, which I know about. Um, SO there, there's a, a large literature in all of the fields that we've, we've spoke about. So I think you just, you know, Google, active infants and um but specifically that there um there's something called the Active Inference Institute. Um The um um and it's based in California and it's, it, it, it, it provides an excellent resource of teaching materials and possible live streams and mentorship uh for people who are interested in sharing ideas or just learning about how, you know, um um active influence in its various um various fields of application. There's also international workshops where the community gets together and um tells each other, tell each other what what's happening and what's exciting in the field. Uh So uh active Inference Institute, but otherwise you're just looking at the theoretical neuro literature in the field in the field of your, your inquiry don't necessarily look for my name. As I say, it's, it's go, it's gone, it's gone past me. I'm now an admiring spectator of uh on the applications."
	},
	{
		"start_time": "2:05:59",
		"speaker": "Ricardo Lopes",
		"words": "Great. So look this has been a really fascinating conversation. We've covered a lot of ground when it comes to the free energy principle nervous systems, active influence. So I'm very much looking forward to our"
	},
	{
		"start_time": "2:06:13",
		"speaker": "Karl Friston",
		"words": "next talk. Excellent. I look forward to that."
	},
	{
		"start_time": "2:06:18",
		"speaker": "Ricardo Lopes",
		"words": "Hi guys. Thank you for watching this interview. Until the end. If you liked it, please share it. Leave a like and hit the subscription button. The show is brought to you by N Lights learning and development. Then differently check the website at N lights.com and also please consider supporting the show on Patreon or paypal. I would also like to give a huge thank you to my main patrons and paypal supporters, Perego Larson, Jerry Muller and Frederick Suno Bernard Seche O of Alex Adam, Castle Matthew Whitting Bear, no wolf, Tim Ho Erica LJ Connors, Philip Forrest Connelly. Then the Met Robert Wine in Nai Z Mar Nevs calling Hobel Governor Mikel Stormer Samuel Andre Francis for Agns Ferger Ken Hall, her ma J and Lain Jung Y and the Samuel K Hes Mark Smith J Tom Hummel S friends, David Sloan Wilson, Yaar, Roman Roach Diego, Jan Punter, Romani Charlotte Bli Nicole Barba, Adam Hunt Pavla Stassi Na me, Gary G Almansa Zal Ari and YPJ Barboza Julian Price Edward Hall, Eden Broner Douglas Fry Franca, Bela Gil Cortez Solis Scott Zachary ftdw Daniel Friedman, William Buckner, Paul Giorgio, Luke Loki, Georgio Theophano Chris Williams and Peter Wo David Williams Di A Costa Anton Erickson. Charles Murray, Alex Shaw, Marie Martinez, Coralie Chevalier, Bangalore Fists Larry Dey Junior, Old Ebon, Starry Michael Bailey. Then Spur by Robert Grassy Zorn. Jeff mcmahon, Jake Zul Barnabas, Radis Mark Temple, Thomas Dvor Luke Neeson, Chris Tory Kimberly Johnson, Benjamin Gilbert Jessica Week, Linda Brendan, Nicholas Carlson, Ismael Bensley Man George Katis Valentine Steinman Perros, Kate Van Goler Alexander, Abert Liam Dan Biar Masoud Ali Mohammadi Perpendicular J Ner Urla. Good enough Gregory Hastings David Pins of Sean Nelson, Mike Levin and Jos Net. A special thanks to my producers, these our web, Jim Frank Luca Stina, Tom Vig and Bernard N Cortes Dixon, Bendik, Muller Thomas Trumble, Catherine and Patrick Tobin, John Carlman, Negro, Nick Ortiz and Nick Golden. And to my executive producers, Matthew Lavender, Sergi, Adrian Bogdan Knits and Rosie. Thank you for all."
	}
]