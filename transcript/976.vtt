WEBVTT

1
00:00:00.219 --> 00:00:02.880
Hello, everybody. Welcome to a new episode of the

2
00:00:02.940 --> 00:00:05.420
Center. I'm your host as always Ricardo Lob. And

3
00:00:05.429 --> 00:00:07.349
today I'm joined for a second time by Dr

4
00:00:07.480 --> 00:00:11.189
Azim Sharif. He's professor and Canada research chair at

5
00:00:11.199 --> 00:00:14.789
the University of British Columbia in Canada. And today

6
00:00:14.800 --> 00:00:18.420
we're talking about cultural differences in how people react

7
00:00:18.430 --> 00:00:22.079
to robots A I and so on robot preachers,

8
00:00:22.120 --> 00:00:25.739
people's belief in free will and the political ramifications

9
00:00:25.750 --> 00:00:29.750
of that. Why people love rags to riches stories,

10
00:00:29.760 --> 00:00:34.430
causal attributions to poverty, the moralization of effort and

11
00:00:34.439 --> 00:00:37.310
some other related topics. So, Doctor Sharif, welcome back

12
00:00:37.319 --> 00:00:39.090
to the show. It's always a pleasure to talk

13
00:00:39.099 --> 00:00:39.349
to you.

14
00:00:39.740 --> 00:00:41.700
Oh, it's great to be here, Ricardo. I, you

15
00:00:41.709 --> 00:00:44.000
know, I was looking at the all the interviews

16
00:00:44.009 --> 00:00:46.360
that you've done in apparently the five years uh

17
00:00:46.369 --> 00:00:49.810
since we last talked and you have built such

18
00:00:49.819 --> 00:00:53.880
an incredible corpus of interviews. I think you have

19
00:00:53.889 --> 00:00:56.759
probably had in depth conversations with a wider range

20
00:00:57.029 --> 00:01:01.360
of interesting psychologists and social scientists than anyone on

21
00:01:01.369 --> 00:01:04.239
the planet, which is a thing, right? I mean,

22
00:01:04.250 --> 00:01:05.970
I think I can't think of somebody who's done

23
00:01:05.980 --> 00:01:07.000
more than that.

24
00:01:08.250 --> 00:01:10.319
Well, I, I mean, I don't know, but I

25
00:01:10.330 --> 00:01:13.389
thank you very much for that kind of words.

26
00:01:14.059 --> 00:01:15.949
Well, what makes me think here, here's what it

27
00:01:15.959 --> 00:01:19.910
makes me wonder. Um, YOU have these conversations on

28
00:01:19.919 --> 00:01:23.339
a weekly basis you're talking to really interesting people.

29
00:01:23.519 --> 00:01:27.790
Uh You must synthesize a lot of really interesting

30
00:01:27.800 --> 00:01:30.610
points and I'd be curious about what the kind

31
00:01:30.620 --> 00:01:34.279
of uh synthesis you're making. What kind of summaries,

32
00:01:34.290 --> 00:01:35.959
what kind of connections you're making between all of

33
00:01:35.970 --> 00:01:40.129
these? Have you thought about doing some of you,

34
00:01:40.139 --> 00:01:43.239
like producing something of your own? Like maybe uh

35
00:01:43.389 --> 00:01:45.650
so a couple ways you, you could do this

36
00:01:45.660 --> 00:01:48.680
and, and we'll get into talking about um my

37
00:01:48.690 --> 00:01:52.910
research eventually. But uh for one, you could cut

38
00:01:52.919 --> 00:01:56.699
interviews topically. So you bring, say you had a

39
00:01:56.709 --> 00:01:59.440
um a topic of cultural evolution and you, you

40
00:01:59.449 --> 00:02:01.680
bring all the different cultural evolution people you talk

41
00:02:01.690 --> 00:02:04.000
to and like do, do little clips. Alternatively, you

42
00:02:04.010 --> 00:02:08.679
could produce some sort of like book or, or

43
00:02:08.690 --> 00:02:11.860
your own lectures that were based on bringing these

44
00:02:11.869 --> 00:02:13.919
insights together and you can have little clips of,

45
00:02:13.929 --> 00:02:16.139
of many of the interviews. I mean, I think

46
00:02:16.149 --> 00:02:19.309
you probably have in your head right now. Uh

47
00:02:19.320 --> 00:02:23.919
JUST a, an unrivaled amount of psychological knowledge.

48
00:02:24.979 --> 00:02:28.320
Well, uh le let's see if uh some A

49
00:02:28.330 --> 00:02:31.119
I tools in the near future will help me

50
00:02:31.130 --> 00:02:35.179
with that because with the enormous amount of content

51
00:02:35.190 --> 00:02:38.639
I already have just thinking about going to the

52
00:02:38.649 --> 00:02:43.039
hundreds of interviews and clipping them and then putting

53
00:02:43.050 --> 00:02:46.220
it together. I mean, it's, it's just a nightmare

54
00:02:46.229 --> 00:02:49.800
in my head. So hopefully a I will help

55
00:02:49.809 --> 00:02:52.740
here. So, uh in the near future. So, but,

56
00:02:52.750 --> 00:02:55.059
but thank you so much for the suggestions, by

57
00:02:55.070 --> 00:02:57.949
the way. Uh AND so to get into the

58
00:02:57.960 --> 00:03:01.520
topics here today. Uh Yeah, I know. II, I

59
00:03:01.529 --> 00:03:05.619
mean, you study a broad, broad range of topics,

60
00:03:05.630 --> 00:03:08.309
but let's get into some of the work you've

61
00:03:08.320 --> 00:03:13.559
done on how people uh react and deal with

62
00:03:13.770 --> 00:03:17.990
robots, algorithms, artificial intelligence because it seems that there

63
00:03:18.000 --> 00:03:20.179
are cultural differences here,

64
00:03:20.190 --> 00:03:25.179
right? Yeah. So one of the things that we

65
00:03:25.190 --> 00:03:28.460
see is that uh most of this research tends

66
00:03:28.470 --> 00:03:30.009
to be put out and most of the discussion

67
00:03:30.020 --> 00:03:31.869
of this tends to be put out as most

68
00:03:31.880 --> 00:03:33.399
things tend to be put up by the United

69
00:03:33.410 --> 00:03:36.529
States, the United States as well as Canada where

70
00:03:36.539 --> 00:03:39.880
I live. Um TEND to be outliers in terms

71
00:03:39.889 --> 00:03:43.649
of their fear of A I. Uh If you

72
00:03:43.660 --> 00:03:45.779
look across the world, other countries tend to be

73
00:03:45.789 --> 00:03:49.789
much more embracing of it. Um And so, uh

74
00:03:49.800 --> 00:03:51.679
I think a lot of the discourse is probably

75
00:03:51.690 --> 00:03:56.710
dominated by an unrepresented, un unrepresentative fear of uh

76
00:03:56.720 --> 00:03:58.970
uh a global fear of A I that probably

77
00:03:58.979 --> 00:03:59.929
doesn't exist everywhere.

78
00:04:02.600 --> 00:04:04.699
Uh And so, but what are some of the

79
00:04:04.710 --> 00:04:08.940
cultural differences that you found? And what cultures did

80
00:04:08.949 --> 00:04:10.899
you study particularly?

81
00:04:11.380 --> 00:04:12.789
Well, we've, we've looked at it in a few

82
00:04:12.800 --> 00:04:15.580
different ways. Um, I think the biggest differences and

83
00:04:15.589 --> 00:04:19.440
maybe the most interesting differences emerged between, uh, the

84
00:04:19.450 --> 00:04:22.519
West and East Asia. Um, AND when I say

85
00:04:22.529 --> 00:04:24.209
the west, I mean, I guess I could say

86
00:04:24.220 --> 00:04:27.640
even more, more specifically North America and East Asia.

87
00:04:28.000 --> 00:04:31.730
Um, YOU see, as I mentioned, the most fear

88
00:04:31.739 --> 00:04:35.640
and worry in, uh, in the US and Canada

89
00:04:35.859 --> 00:04:38.200
and you see much, much less in East Asia,

90
00:04:38.209 --> 00:04:41.070
you see much more, uh, willingness to engage with

91
00:04:41.079 --> 00:04:44.119
social robots, you see much more willingness to engage

92
00:04:44.130 --> 00:04:48.950
with uh uh chatbot buddies. Um uh So a

93
00:04:48.959 --> 00:04:51.450
lot of the things which freak people out in

94
00:04:51.459 --> 00:04:54.769
the US seem to be just fine in, in

95
00:04:54.779 --> 00:04:56.929
places like Japan, China, Korea.

96
00:04:59.299 --> 00:05:02.929
Uh And why is it that uh we find

97
00:05:02.940 --> 00:05:05.200
these differences? I mean, where is it in the

98
00:05:05.209 --> 00:05:09.149
culture of people from East Asia that make them

99
00:05:09.209 --> 00:05:13.640
more receptive to these kinds of technologies than people

100
00:05:13.649 --> 00:05:15.350
in North America, for example.

101
00:05:15.459 --> 00:05:19.089
Yeah. So we've, this is work that was led

102
00:05:19.100 --> 00:05:21.869
by um Yan Kai Chi. Uh HE goes by

103
00:05:21.880 --> 00:05:25.269
Sam. So com Sam. Um WE, we can only

104
00:05:25.279 --> 00:05:28.529
speculate about various reasons for this. Um But I

105
00:05:28.540 --> 00:05:31.869
think there are some interesting kind of foundational cultural

106
00:05:31.880 --> 00:05:36.239
differences which then get exaggerated through a sort of

107
00:05:36.250 --> 00:05:38.700
positive feedback loop, right? So you have a what

108
00:05:38.709 --> 00:05:41.839
could be a, a potentially pretty small cultural difference

109
00:05:42.079 --> 00:05:45.220
which then leads to different levels of exposure, right?

110
00:05:45.230 --> 00:05:50.510
So historically, the, well, let's OK. So there's there's

111
00:05:50.519 --> 00:05:54.190
some religious differences, right? So, in, in uh Buddhism

112
00:05:54.200 --> 00:05:57.640
and Shintoism, there's more of an embracing of animism.

113
00:05:57.649 --> 00:06:02.209
Whereas in, say the Abrahamic religions, there's, it's very

114
00:06:02.220 --> 00:06:05.309
centered on human exceptionalism, right? It's, it's much more

115
00:06:05.320 --> 00:06:08.470
about separating humans from all the other creatures of

116
00:06:08.480 --> 00:06:11.350
the world and putting humans on this, this interesting

117
00:06:11.359 --> 00:06:16.000
pedestal. So that can lead to a bit of

118
00:06:16.040 --> 00:06:19.309
a uh a competition between humans and any other

119
00:06:19.320 --> 00:06:22.299
types of creatures where other types of creatures including

120
00:06:22.309 --> 00:06:26.070
like, you know, we here maybe in, in um

121
00:06:26.079 --> 00:06:30.519
North America, think about the terminator as a anthropomorphic

122
00:06:30.529 --> 00:06:35.079
uh quintessentially terrible uh robot. Whereas what you tend

123
00:06:35.089 --> 00:06:36.820
to see in terms of these social robots in

124
00:06:36.829 --> 00:06:39.149
East Asia are kind of like cuddly robot, like

125
00:06:39.160 --> 00:06:42.440
like ro robotic seals and, and things like that,

126
00:06:42.679 --> 00:06:46.070
which um they seem much more willing to embrace

127
00:06:46.079 --> 00:06:48.920
potentially because of this connection, this, this willingness to

128
00:06:48.929 --> 00:06:51.929
embrace animism than a more animated world to see

129
00:06:51.940 --> 00:06:57.260
uh agency among animals. Um Now, that could be

130
00:06:57.269 --> 00:06:59.869
a very small difference which then leads to, as

131
00:06:59.880 --> 00:07:03.059
I mentioned, uh a few changes in terms of

132
00:07:03.130 --> 00:07:06.630
how fiction portrays these robots, you know, the terminator

133
00:07:06.640 --> 00:07:09.679
versus Astro boy and that could lead to then

134
00:07:09.750 --> 00:07:12.179
uh a difference in the exposure. So, so in,

135
00:07:12.190 --> 00:07:15.399
in Japan, you had social robots quite a bit

136
00:07:15.410 --> 00:07:18.649
earlier. Uh PEOPLE have been exposed to them, people

137
00:07:18.660 --> 00:07:20.899
who are, you know, my age or your age

138
00:07:20.910 --> 00:07:23.059
now have grown up within their whole lives, uh

139
00:07:23.070 --> 00:07:26.380
which means that, that they're much more comfortable with

140
00:07:26.390 --> 00:07:28.309
them entirely. And so that can kind of lead

141
00:07:28.320 --> 00:07:30.660
to these positive feedback loops which draws the cultures

142
00:07:30.670 --> 00:07:31.859
in, in different directions.

143
00:07:32.809 --> 00:07:35.790
That that's actually very interesting because I'm a big

144
00:07:35.799 --> 00:07:39.309
anime and manga fan and follower. And I, I

145
00:07:39.320 --> 00:07:42.980
mean, I've been following it for 10 plus years

146
00:07:42.989 --> 00:07:46.049
and you see, even in this sort of artistic

147
00:07:46.059 --> 00:07:49.190
mediums out, people in Japan tend to be much

148
00:07:49.200 --> 00:07:54.375
more receptive to technological innovations and how even sometimes

149
00:07:54.385 --> 00:07:57.144
they tend to think about them as just part,

150
00:07:57.154 --> 00:08:00.875
even of the natural environment. I mean, it's something

151
00:08:00.885 --> 00:08:03.795
that humans create but they look at them more

152
00:08:03.975 --> 00:08:06.454
in a more natural way than people in the

153
00:08:06.464 --> 00:08:07.755
west, I guess.

154
00:08:07.845 --> 00:08:11.304
Yeah. Yeah. Um, THERE'S a, there's a few other

155
00:08:11.315 --> 00:08:14.515
explanations, right? So, in another project, um, unpublished uh

156
00:08:14.524 --> 00:08:18.339
project right now, we've been looking at how fear

157
00:08:18.350 --> 00:08:22.369
of A I uh relates to people's belief about

158
00:08:22.380 --> 00:08:25.880
how well, uh about the capacities for A I

159
00:08:25.890 --> 00:08:29.220
for placing particular jobs, right. So you can imagine

160
00:08:29.230 --> 00:08:32.280
that one of the jobs that you probably, uh,

161
00:08:32.289 --> 00:08:34.159
that I probably wouldn't want an A I to

162
00:08:34.169 --> 00:08:37.169
do though there's some, maybe exceptions to this is,

163
00:08:37.179 --> 00:08:39.308
is being a judge, having the, the moral decision

164
00:08:39.320 --> 00:08:42.398
making. Um, AND, and most people around the world

165
00:08:42.597 --> 00:08:45.499
have some hesitancy about, about a, is taking over

166
00:08:45.508 --> 00:08:48.189
the, the role of judges or of religious leaders

167
00:08:48.198 --> 00:08:51.390
or whatever. Um, BUT it does vary between countries.

168
00:08:51.400 --> 00:08:54.909
And the, the variation between countries seems to be

169
00:08:54.919 --> 00:08:58.820
explained by a belief that, well, actually the requirements

170
00:08:58.830 --> 00:09:01.469
of such a job uh are not outside of

171
00:09:01.479 --> 00:09:03.549
the requirement of the abilities of a, of a

172
00:09:03.559 --> 00:09:06.669
particular uh A I or robot. And the more

173
00:09:06.679 --> 00:09:08.440
you see the A I able to fulfill those

174
00:09:08.450 --> 00:09:10.250
positions, the less you tend to fear A I

175
00:09:10.260 --> 00:09:12.625
and so big a big difference for this is,

176
00:09:12.674 --> 00:09:15.174
is in Japan versus in, say the United States

177
00:09:15.294 --> 00:09:19.325
care workers. Japan sees a is or robots to

178
00:09:19.335 --> 00:09:23.525
be very uh capable of, of breaching the requirements

179
00:09:23.534 --> 00:09:25.724
of being a care worker. Whereas in the United

180
00:09:25.734 --> 00:09:28.604
States, that's not the case. Uh And that seems

181
00:09:28.614 --> 00:09:31.244
to correspond to the country's uh uh fear of

182
00:09:31.255 --> 00:09:34.924
A I as well. And there's another um interesting

183
00:09:34.934 --> 00:09:37.320
uh this is not my paper, this is Noah

184
00:09:37.330 --> 00:09:40.280
Costello I think, did this really interesting study where

185
00:09:40.289 --> 00:09:46.659
he found that uh people from more corrupt countries,

186
00:09:46.669 --> 00:09:50.710
countries with higher levels of corruption uh have uh

187
00:09:50.719 --> 00:09:54.590
less algorithm aversion. So they're less averse to algorithms,

188
00:09:54.599 --> 00:09:57.380
making decisions for them rather than the humans themselves.

189
00:09:57.390 --> 00:10:00.409
Um And that makes sense if you think that,

190
00:10:00.510 --> 00:10:03.679
well, I trust humans because I trust them to

191
00:10:03.690 --> 00:10:06.010
be able to make complex moral decisions. But if

192
00:10:06.020 --> 00:10:08.299
I'm coming from a place where, well those moral

193
00:10:08.309 --> 00:10:10.309
decisions tend to be made in very corrupt ways,

194
00:10:10.320 --> 00:10:13.039
then I'm po potentially going to trust the algorithm

195
00:10:13.049 --> 00:10:15.989
that it uh it is more fair minded or

196
00:10:16.000 --> 00:10:18.580
more objective in terms of its decisions, less willing

197
00:10:18.590 --> 00:10:20.799
to give into favoritism. Humans are flawed in that

198
00:10:20.809 --> 00:10:23.020
way. Maybe the algorithms wouldn't be. Uh, SO you

199
00:10:23.030 --> 00:10:24.640
do see that cultural variation as well.

200
00:10:25.500 --> 00:10:27.650
Well, I, I guess that in that case people

201
00:10:27.659 --> 00:10:32.359
would prefer algorithms if they are not corrupt themselves,

202
00:10:32.390 --> 00:10:35.679
otherwise they probably would prefer to deal with other

203
00:10:35.690 --> 00:10:36.580
people. Right.

204
00:10:37.289 --> 00:10:39.650
That's right. So, and, and, and furthermore, the, the

205
00:10:39.659 --> 00:10:42.719
beneficiaries of corruption are probably gonna prefer the humans

206
00:10:42.729 --> 00:10:45.659
or the corrupt algorithms above more fair minded ones

207
00:10:45.669 --> 00:10:46.039
as well.

208
00:10:47.059 --> 00:10:49.960
A and so you mentioned something there that uh

209
00:10:49.969 --> 00:10:54.479
in regards to people who occupy certain positions and

210
00:10:54.719 --> 00:10:59.119
something that you've done work on is about robot

211
00:10:59.130 --> 00:11:03.700
preachers. So, what results did you get there? And

212
00:11:03.710 --> 00:11:08.500
what implications would it have to cultural evolutionary theories

213
00:11:08.510 --> 00:11:09.400
of religion?

214
00:11:10.369 --> 00:11:13.460
Yeah. So this was um a another study that

215
00:11:13.469 --> 00:11:17.010
involved Sam and, and, and Josh Jackson. Um YOU

216
00:11:17.020 --> 00:11:18.409
should speak to Josh if you haven't already. I

217
00:11:18.419 --> 00:11:21.539
don't know if you have um where we did

218
00:11:21.549 --> 00:11:26.549
a couple of experiments um field experiments. First one

219
00:11:26.559 --> 00:11:30.570
was in a Buddhist temple in Japan uh where

220
00:11:30.580 --> 00:11:33.429
we looked at people who had visited. They have

221
00:11:33.440 --> 00:11:37.679
this interesting robot preacher called Minar um that exists

222
00:11:37.690 --> 00:11:40.469
in a, in a pretty well known old Buddhist

223
00:11:40.479 --> 00:11:44.340
temple uh which creates an interesting juxtaposition. And you

224
00:11:44.349 --> 00:11:46.799
have um we looked at people who had, who

225
00:11:46.809 --> 00:11:50.419
had heard a sermon delivered by mind versus a

226
00:11:50.429 --> 00:11:53.929
human preacher. And then we looked at um how

227
00:11:53.940 --> 00:11:56.809
willing they were to donate to the temple. Uh

228
00:11:56.820 --> 00:12:01.190
AND, and um other measures of support for the

229
00:12:01.200 --> 00:12:04.169
religion or the temple. And then another study and

230
00:12:04.179 --> 00:12:07.520
this was a more controlled experiment. This was in

231
00:12:07.530 --> 00:12:13.190
a temple in Singapore. We uh uh randomly assigned

232
00:12:13.200 --> 00:12:15.150
on, well, not randomly assigned. We had, we had

233
00:12:15.159 --> 00:12:18.320
on different days, we had a robot preacher appear

234
00:12:18.330 --> 00:12:21.830
or not. Um So that was a, a more

235
00:12:21.840 --> 00:12:24.099
controlled experiment in terms of who got the humans

236
00:12:24.109 --> 00:12:27.030
and who got, who got the uh preachers. And

237
00:12:27.039 --> 00:12:31.270
again, measured things like willingness to donate, willingness to

238
00:12:31.280 --> 00:12:34.169
share flyers about their religion to spread the word

239
00:12:34.179 --> 00:12:37.320
of the religion, uh et cetera. Uh And what

240
00:12:37.330 --> 00:12:39.570
we found there, I probably won't surprise many, which

241
00:12:39.580 --> 00:12:42.450
is that when people engaged with a uh a

242
00:12:42.460 --> 00:12:47.169
robot preacher, they showed less support. Um They, they

243
00:12:47.179 --> 00:12:49.119
gave, they were willing to give less to their,

244
00:12:49.130 --> 00:12:52.549
to their uh temple. Um They were less willing

245
00:12:52.559 --> 00:12:54.419
to spread the word of the religion. It, it

246
00:12:54.429 --> 00:12:59.130
decreased their commitment to the religion. And the, the

247
00:12:59.140 --> 00:13:01.340
result I, I don't think is super surprising, but

248
00:13:01.349 --> 00:13:03.700
I think what's interesting is the theory that surrounds

249
00:13:03.710 --> 00:13:05.969
it, right? Why, why is it that we would

250
00:13:05.979 --> 00:13:10.909
react differently in the context of religious leaders to

251
00:13:10.919 --> 00:13:16.359
uh robot preachers versus human preachers? And as we're

252
00:13:16.369 --> 00:13:18.559
thinking about this, one of the things that we,

253
00:13:18.570 --> 00:13:21.510
we, you know, talked about was that a lot

254
00:13:21.520 --> 00:13:25.159
of the discussion about what jobs, uh, robots and

255
00:13:25.169 --> 00:13:27.619
A I can take over and which ones they

256
00:13:27.630 --> 00:13:30.099
can't, uh, seem to be focused on the current,

257
00:13:30.179 --> 00:13:33.619
uh, capabilities of the robot. And what's interesting about

258
00:13:33.630 --> 00:13:35.580
that is that what we've seen over the last,

259
00:13:35.590 --> 00:13:39.390
I guess, 15 months, is that the capabilities of

260
00:13:39.400 --> 00:13:44.460
A I are just expanding so rapidly that it's,

261
00:13:44.750 --> 00:13:48.330
that's an ever shrinking, uh, a space of things,

262
00:13:48.340 --> 00:13:50.340
right? Um Even when you see the the modern

263
00:13:50.349 --> 00:13:55.340
robots that they're able to engage in, in surprisingly

264
00:13:55.349 --> 00:14:01.200
nimble fine motor patterns. Um So, so in terms

265
00:14:01.210 --> 00:14:04.630
of capabilities that the jobs that uh A I

266
00:14:04.640 --> 00:14:07.179
don't have the capability to do are are shrinking,

267
00:14:07.419 --> 00:14:09.200
but we were looking at a separate thing instead

268
00:14:09.210 --> 00:14:11.000
of looking at capability, we were looking at at

269
00:14:11.109 --> 00:14:14.289
credibility. And I think this capability, credibility distinction is

270
00:14:14.299 --> 00:14:19.070
interesting because credibility depends on more than just your

271
00:14:19.080 --> 00:14:23.440
ability to improve the technology of um uh of

272
00:14:23.450 --> 00:14:27.059
A I for credibility. It seems to rely on

273
00:14:27.419 --> 00:14:31.739
um something about what the uh uh the the

274
00:14:31.750 --> 00:14:34.630
person who's passing on the information to you uh

275
00:14:34.729 --> 00:14:38.059
has experienced or is as a person. And so

276
00:14:38.070 --> 00:14:42.710
this, this uh digs into cultural evolutionary theories, dual

277
00:14:42.719 --> 00:14:45.900
inheritance theories about the fact that when we get

278
00:14:45.909 --> 00:14:50.059
ideas or when we uh absorb rituals or um

279
00:14:50.599 --> 00:14:55.669
different norms and practices, values, beliefs, morals. Um WE

280
00:14:55.679 --> 00:14:58.739
do so not solely based on the content, it's

281
00:14:58.750 --> 00:15:01.440
not simply what the content of the ritual is

282
00:15:01.450 --> 00:15:03.900
that, that determines whether we accept it to what

283
00:15:03.909 --> 00:15:07.539
degree. Um, IT also depends greatly on the, the

284
00:15:07.549 --> 00:15:10.909
context and in particular, uh, the person who's delivering

285
00:15:10.919 --> 00:15:12.309
it to us, the person who's telling us about

286
00:15:12.320 --> 00:15:13.559
the ritual or trying to teach it to us

287
00:15:13.570 --> 00:15:16.760
or trying to pass on the, the, the moral,

288
00:15:16.770 --> 00:15:20.719
uh, pass on the, um, sermon or whatever. And,

289
00:15:21.210 --> 00:15:25.119
uh, there are various things which increase the credibility

290
00:15:25.130 --> 00:15:29.200
of the, uh, the person passing it on. Um,

291
00:15:29.330 --> 00:15:31.559
uh Joe Henri has talked about these in the

292
00:15:31.570 --> 00:15:35.450
context of uh credibility enhancing displays. There's various things

293
00:15:35.460 --> 00:15:39.789
which the credibility, um, and the canonical example of

294
00:15:39.799 --> 00:15:42.700
what a credibility enhancing display is, is, is something

295
00:15:42.710 --> 00:15:46.280
that it would be very, um, it would be

296
00:15:46.289 --> 00:15:50.380
costly to the, uh uh to the model, the

297
00:15:50.390 --> 00:15:54.460
cultural model if they didn't actually believe it. Um

298
00:15:54.469 --> 00:15:57.669
And so a, if a, if a cultural model,

299
00:15:57.679 --> 00:16:00.559
like a, an elder or something is, uh, uh

300
00:16:00.570 --> 00:16:02.900
trying to get you to eat a blue mushroom

301
00:16:03.330 --> 00:16:06.119
and they eat the blue mushroom themselves, that's a

302
00:16:06.130 --> 00:16:10.409
very credibility enhancing display. Uh Because if that, if

303
00:16:10.419 --> 00:16:14.299
they didn't believe that that mushroom uh was safe,

304
00:16:14.570 --> 00:16:19.010
they wouldn't eat it themselves. Uh Now, in order

305
00:16:19.020 --> 00:16:22.320
to have this credibility, you have to have some

306
00:16:22.330 --> 00:16:25.710
sort of level of mind uh to actually believe

307
00:16:25.719 --> 00:16:28.830
in what you're doing, uh which a is don't

308
00:16:28.840 --> 00:16:31.849
have as far as we know. Um And, or

309
00:16:31.859 --> 00:16:35.989
uh some ability to suffer for the consequences. And

310
00:16:36.400 --> 00:16:38.190
again, as far as we know, a, I don't,

311
00:16:38.200 --> 00:16:41.489
don't yet suffer. And so without those abilities, without

312
00:16:41.500 --> 00:16:44.280
things that you see for, um, uh, among religious

313
00:16:44.289 --> 00:16:47.150
leaders, like, say, uh, a priest who, um, uh,

314
00:16:47.159 --> 00:16:50.369
are celibate for life, that's a pretty credibility enhancing

315
00:16:50.380 --> 00:16:52.760
display. They're going to a lot of expense to

316
00:16:52.770 --> 00:16:55.140
show that they are somebody who firmly believes in,

317
00:16:55.150 --> 00:16:58.549
in the religion. Uh, Those things aren't gonna affect

318
00:16:58.559 --> 00:17:00.460
it. Uh, AN A I, they're not gonna be,

319
00:17:00.469 --> 00:17:02.239
they're, they're not gonna be credibility enhancing if you,

320
00:17:02.250 --> 00:17:04.348
if you say, oh, well, this preacher never had

321
00:17:04.358 --> 00:17:06.030
sex in their life. You know, it's like, no,

322
00:17:06.560 --> 00:17:08.439
that doesn't tell me much about them.

323
00:17:09.400 --> 00:17:11.348
That, that, that's very interesting. And, and by the

324
00:17:11.358 --> 00:17:15.400
way, just out of curiosity, these robot preachers, I

325
00:17:15.410 --> 00:17:18.618
mean, can they perform a mess?

326
00:17:19.819 --> 00:17:21.989
Well, yeah, so they, they can, now it's not,

327
00:17:22.280 --> 00:17:27.348
it's not as charismatic and elegant and even fluid,

328
00:17:27.358 --> 00:17:30.489
uh as, as a human would, would do. But

329
00:17:30.829 --> 00:17:32.969
the point about the capabilities is, well, that's a,

330
00:17:32.979 --> 00:17:35.750
that's a gap that can be easily closed. Uh

331
00:17:35.900 --> 00:17:39.150
So at some point, possibly soon they will be

332
00:17:39.160 --> 00:17:41.430
able to do this charismatic and, and uh uh

333
00:17:41.439 --> 00:17:44.829
fluidly as a human, but that credibility gap probably

334
00:17:44.839 --> 00:17:46.430
won't be closing. And you can see that not

335
00:17:46.439 --> 00:17:48.300
just for Rob robot preachers, but you can see

336
00:17:48.310 --> 00:17:53.510
that in other, um, and other uh uh professions

337
00:17:53.520 --> 00:17:58.650
where credibility matters as well, and so, um, one

338
00:17:58.660 --> 00:18:00.209
that I think about a lot and I find

339
00:18:00.219 --> 00:18:04.150
really interesting. My wife is a psychiatrist. Um, AND

340
00:18:04.160 --> 00:18:07.079
so in the, in the realm of therapy, uh,

341
00:18:07.189 --> 00:18:10.160
there has been some, uh, uh, a lot of

342
00:18:10.170 --> 00:18:14.160
progress with, uh, therapy chat bots, uh, because they

343
00:18:14.170 --> 00:18:18.640
can rapidly, uh, or, or massively expand the, they,

344
00:18:18.650 --> 00:18:20.589
they reduce the cost and massively expand the number

345
00:18:20.599 --> 00:18:22.339
of people who, who can have access to therapy

346
00:18:22.349 --> 00:18:26.069
and for some reasons, uh, for some, yeah, for

347
00:18:26.079 --> 00:18:28.420
some functions, they've been shown to be very effective.

348
00:18:28.430 --> 00:18:30.310
But what they don't have, what they don't have

349
00:18:30.319 --> 00:18:31.910
is is the credibility and that might be a

350
00:18:31.920 --> 00:18:35.849
limiting, uh, uh, factor for how effective they can

351
00:18:35.859 --> 00:18:39.209
actually be. Uh, Ricardo. Have you seen Good Will

352
00:18:39.219 --> 00:18:42.520
hunting? Yes. Yes. I think most people have seen

353
00:18:42.530 --> 00:18:44.329
good will hunting most. Hopefully most of your audience

354
00:18:44.339 --> 00:18:46.000
has seen Good Will hunting. But one of the

355
00:18:46.010 --> 00:18:48.130
interesting things about that is you have the Matt

356
00:18:48.140 --> 00:18:52.400
Damon character who's incredibly capable, right? This disturbed genius.

357
00:18:52.880 --> 00:18:56.949
And then you have the, um, Robin Williams character,

358
00:18:56.959 --> 00:19:00.910
the, the therapist who is not as smart as

359
00:19:00.920 --> 00:19:03.319
Matt Damon, right? Like Matt Damon goes through all

360
00:19:03.329 --> 00:19:05.420
these therapists because he's just, he can outsmart them

361
00:19:05.430 --> 00:19:07.469
and they're, they're, he's much more capable than they

362
00:19:07.479 --> 00:19:10.430
are at any intellectual feat. And eventually what you

363
00:19:10.439 --> 00:19:13.479
see is that, well, the Robin Williams character has

364
00:19:13.489 --> 00:19:15.719
this credibility. He has this credibility because he has

365
00:19:15.729 --> 00:19:17.719
this depth of feeling he has this experience of,

366
00:19:17.729 --> 00:19:19.859
of, of being in the war and then having

367
00:19:19.869 --> 00:19:22.640
his, his wife die of cancer. And he's this

368
00:19:22.650 --> 00:19:26.300
deep feeler and that credibility is what ultimately gets

369
00:19:26.310 --> 00:19:28.280
to Matt Damon and ends up helping him.

370
00:19:29.359 --> 00:19:31.540
Yeah. No, no, that's very interesting. And I guess

371
00:19:31.550 --> 00:19:33.939
that in the case of chat bots to help

372
00:19:33.949 --> 00:19:39.030
with mental health, perhaps to, uh, benefits that they

373
00:19:39.040 --> 00:19:43.380
would have in comparison to human psychotherapists or someone

374
00:19:43.390 --> 00:19:46.699
like that is that they would have more readily

375
00:19:46.709 --> 00:19:50.300
access to uh a bigger amount of data from

376
00:19:50.310 --> 00:19:54.160
the patient and also better memory, I guess in

377
00:19:54.170 --> 00:19:56.189
terms of what the patients have and all of

378
00:19:56.199 --> 00:19:56.430
that.

379
00:19:56.439 --> 00:19:58.420
That's right. That's right. And there's a third benefit

380
00:19:58.430 --> 00:20:02.540
which is that um they're not human, uh which

381
00:20:02.550 --> 00:20:04.699
means that they don't come with all the judgment

382
00:20:04.709 --> 00:20:08.500
or the uh uh uh a perceived judgment that

383
00:20:08.510 --> 00:20:10.380
a human comes with. So people might be more

384
00:20:10.390 --> 00:20:12.839
willing to admit things or talk about things with

385
00:20:12.849 --> 00:20:15.619
a nonhuman chat bot than they would with an

386
00:20:15.630 --> 00:20:19.619
actual human therapist as, as empathetic and, and um

387
00:20:19.630 --> 00:20:21.469
a good listener as they are.

388
00:20:22.439 --> 00:20:26.140
So, let's get into another topic. Now, where do

389
00:20:26.150 --> 00:20:30.699
people's belief or lack thereof in free will comes

390
00:20:30.709 --> 00:20:31.060
from?

391
00:20:32.199 --> 00:20:36.219
Yeah, good question. So, I mean, 101 answer to

392
00:20:36.229 --> 00:20:41.760
that question would be uh uh uh philosophy, right?

393
00:20:41.770 --> 00:20:43.349
So it could, you know, there's the saying that

394
00:20:43.359 --> 00:20:46.530
behind every living person, there's a dead philosopher and

395
00:20:46.550 --> 00:20:47.739
and what they mean by that is that, you

396
00:20:47.750 --> 00:20:51.040
know, it's not that, you know, everybody's read David

397
00:20:51.050 --> 00:20:54.020
Hume or whatever, but those ideas do penetrate the

398
00:20:54.030 --> 00:20:56.959
culture and affect us. And so I ideas that

399
00:20:56.969 --> 00:21:00.359
philosophers from days of Yore have bestowed upon our

400
00:21:00.369 --> 00:21:03.670
culture have have affected us growing up. So that's

401
00:21:03.680 --> 00:21:07.300
one explanation. Another explanation is that there's something um

402
00:21:07.420 --> 00:21:11.270
uh just uh intuitive about it that our brains

403
00:21:11.280 --> 00:21:13.339
are constructed in a way which has us believe

404
00:21:13.349 --> 00:21:16.680
if we will. Um But another explanation is that

405
00:21:16.689 --> 00:21:20.489
there are situational factors that, that, that determine when

406
00:21:20.689 --> 00:21:23.290
and in what context and for whom we believe

407
00:21:23.300 --> 00:21:24.459
in free will. And this can be kind of

408
00:21:24.469 --> 00:21:27.750
flexibly allocated. And iii I take from your question

409
00:21:27.760 --> 00:21:29.969
that you're, you're pushing me towards the research that

410
00:21:29.979 --> 00:21:32.890
we've done on these flexible uh free will beliefs.

411
00:21:33.290 --> 00:21:36.109
Um WHICH I, I find, I, I think it's

412
00:21:36.119 --> 00:21:38.420
a really interesting body of research. This is led

413
00:21:38.430 --> 00:21:40.890
by uh Corey Clark who I think you have

414
00:21:40.900 --> 00:21:46.130
spoken with. Uh AND, and for her. So behind

415
00:21:46.140 --> 00:21:49.119
that living person, the dead philosopher is uh Nietzsche

416
00:21:49.439 --> 00:21:53.540
uh who said, um who, who pointed out, you

417
00:21:53.550 --> 00:21:56.420
know, among the various various forms of morality that

418
00:21:56.430 --> 00:21:58.530
he didn't, he didn't like he said this, this,

419
00:21:58.540 --> 00:22:02.849
this reprehensible idea of free will uh that people

420
00:22:02.859 --> 00:22:06.339
flexibly uh uh deploy when they want to punish

421
00:22:06.349 --> 00:22:09.209
somebody uh that we use it as a justification.

422
00:22:09.219 --> 00:22:12.520
We say that that person has free will because

423
00:22:12.530 --> 00:22:14.250
I want to punish them for what they did.

424
00:22:14.260 --> 00:22:15.829
And I want, and, and in order to do

425
00:22:15.839 --> 00:22:18.140
so, I need to be able to hold them

426
00:22:18.150 --> 00:22:19.959
responsible. I need it to be their fault and

427
00:22:19.969 --> 00:22:21.290
for it to be their fault, they have to

428
00:22:21.300 --> 00:22:23.270
have been able to have done otherwise, which means

429
00:22:23.280 --> 00:22:26.810
they have to have free will. And so in,

430
00:22:26.920 --> 00:22:30.020
in the work that I've done with Corey and

431
00:22:30.030 --> 00:22:33.699
uh others, uh we, we find that in all

432
00:22:33.709 --> 00:22:36.859
these different circumstances when, when we make it such

433
00:22:36.869 --> 00:22:39.050
that people really want to punish that other people,

434
00:22:39.060 --> 00:22:41.630
that other person, they raise their level of free

435
00:22:41.640 --> 00:22:43.829
will belief. So one of the best studies of

436
00:22:43.839 --> 00:22:45.560
this in the, in the first package we did

437
00:22:45.569 --> 00:22:49.250
on this was um Corey was teaching two classes,

438
00:22:49.260 --> 00:22:51.040
two sections of a class. I don't know, it

439
00:22:51.099 --> 00:22:53.020
was Corey was teaching. But anyway, somebody was teaching

440
00:22:53.030 --> 00:22:55.969
two sections of the class. Um And in one

441
00:22:55.979 --> 00:23:00.510
of them, uh she said that there was a

442
00:23:00.520 --> 00:23:04.569
cheater on the last midterm. Uh And so riled

443
00:23:04.579 --> 00:23:07.800
up everybody's, you know, oh no, that person screwing

444
00:23:07.810 --> 00:23:09.920
us all over. And in the other condition, the

445
00:23:09.930 --> 00:23:12.699
other section, she didn't say that. Um And then

446
00:23:12.709 --> 00:23:14.660
she had all of them do a free will

447
00:23:14.670 --> 00:23:17.439
uh scale. And in the case where people were

448
00:23:17.449 --> 00:23:19.489
told that there was a cheater where people had

449
00:23:19.500 --> 00:23:22.709
this sense, this desire to punish somebody to see,

450
00:23:22.719 --> 00:23:26.680
to enforce the uh the retribution for moral wrongness.

451
00:23:26.729 --> 00:23:28.709
Well, in that case, they believe in free world

452
00:23:28.719 --> 00:23:30.880
more and there's a bunch of more controlled vignette

453
00:23:30.890 --> 00:23:32.400
studies which, which share that.

454
00:23:33.260 --> 00:23:38.180
But apart from these more situational aspects of whether

455
00:23:38.189 --> 00:23:41.319
you want to punish someone or not, are there

456
00:23:41.329 --> 00:23:44.939
individual differences in the degree to which people believe

457
00:23:44.949 --> 00:23:45.800
in free will?

458
00:23:46.839 --> 00:23:51.959
Um, THERE, there's some, uh, there's some personality correlates.

459
00:23:52.199 --> 00:23:56.500
Uh, SO people hire and I think openness and

460
00:23:56.510 --> 00:24:00.540
extroversion believe in free will more. Uh I think

461
00:24:00.550 --> 00:24:03.849
there's no relationship with IQ or anything like that.

462
00:24:04.109 --> 00:24:08.329
Um You do find in, in this potentially this,

463
00:24:08.339 --> 00:24:10.540
this causal direction goes in the, in the opposite

464
00:24:10.550 --> 00:24:13.420
direction. Uh People who are happier who have higher

465
00:24:13.430 --> 00:24:15.609
life satisfaction tend to believe in free will more.

466
00:24:15.750 --> 00:24:17.449
Um It might be that people who believe in

467
00:24:17.459 --> 00:24:20.739
free will more uh as a consequence of are

468
00:24:20.750 --> 00:24:26.520
happier. Um One of the more interesting and very

469
00:24:26.530 --> 00:24:30.540
reliable group differences that we find is that conservatives

470
00:24:30.550 --> 00:24:33.020
tend to believe in free will more than do

471
00:24:33.030 --> 00:24:36.010
liberals and, and that sort of fits with some

472
00:24:36.020 --> 00:24:39.569
of our stereotypes about uh what liberals and conservatives

473
00:24:39.579 --> 00:24:43.550
believe, right? Conservatives tend to all be all about

474
00:24:43.560 --> 00:24:46.670
the individual picking themselves up by their own bootstraps,

475
00:24:46.680 --> 00:24:50.000
everybody's accountable, responsible, you know, go clean your room,

476
00:24:50.010 --> 00:24:53.050
et cetera. Whereas liberals are like, oh no, everything

477
00:24:53.060 --> 00:24:56.819
was systemic. Nothing. Is anybody any individuals doing. It's

478
00:24:56.829 --> 00:24:59.069
all because of the structures that, that surround us.

479
00:24:59.079 --> 00:25:01.339
So liberals tend to believe in free will less

480
00:25:01.349 --> 00:25:03.670
and that comes up over and over again. Um

481
00:25:03.680 --> 00:25:06.000
When, when those studies tend to be done, they

482
00:25:06.010 --> 00:25:10.119
tend to use this. Um uh Well, you, you,

483
00:25:10.130 --> 00:25:11.410
you find it with a variety of free will

484
00:25:11.420 --> 00:25:14.890
measures, it tends to be in these very philosophical,

485
00:25:14.900 --> 00:25:17.640
free will measures that are measuring people's philosophical commitments

486
00:25:17.650 --> 00:25:21.079
to free will. But oftentimes when people read those,

487
00:25:21.089 --> 00:25:23.160
they think about it in the political sphere. So

488
00:25:23.170 --> 00:25:25.300
they think about it on constraints of freedom. So

489
00:25:25.310 --> 00:25:29.239
that kind of what that liberal conservative difference that

490
00:25:29.250 --> 00:25:31.959
we were talking about about the systemic versus individual

491
00:25:31.969 --> 00:25:32.630
accountability.

492
00:25:33.719 --> 00:25:36.739
And when it comes to that more political side

493
00:25:36.750 --> 00:25:39.520
of things do people who believe in free will.

494
00:25:39.530 --> 00:25:41.359
And I guess that in this particular case, it

495
00:25:41.369 --> 00:25:47.020
would be more conservatives than liberals support economic inequality,

496
00:25:47.030 --> 00:25:49.310
more. Is there any relationship there?

497
00:25:49.319 --> 00:25:55.180
Yeah. So um conservatives tend to have uh more

498
00:25:55.189 --> 00:25:58.709
tolerance for economic inequality than, than liberals do. Uh

499
00:25:58.729 --> 00:26:02.560
AS you know, um interestingly though we have a,

500
00:26:02.569 --> 00:26:04.729
an interesting paper that go that, that looks at

501
00:26:04.739 --> 00:26:08.400
again unpublished. Uh THAT shows that um when you

502
00:26:08.410 --> 00:26:12.500
look at uh liberals and conservatives, economic preferences, if

503
00:26:12.510 --> 00:26:15.060
you look at uh economic preferences for different ways

504
00:26:15.069 --> 00:26:16.579
that you could slice up the pie or grow

505
00:26:16.589 --> 00:26:18.969
the pie or different goals that you have alleviating

506
00:26:18.979 --> 00:26:22.949
poverty versus say reducing inequality versus just growing the

507
00:26:22.959 --> 00:26:26.300
amount of wealth for everybody. Uh Both liberals and

508
00:26:26.310 --> 00:26:30.430
conservatives by far their primary goal is to alleviate

509
00:26:30.439 --> 00:26:32.819
poverty, but they don't think that about each other.

510
00:26:33.260 --> 00:26:37.119
So liberals think that what conservatives are really after

511
00:26:37.130 --> 00:26:39.859
is just growing the pie and increasing the amount

512
00:26:39.869 --> 00:26:43.010
of money for the rich. And uh conservatives think

513
00:26:43.020 --> 00:26:45.119
that liberals just want to reduce inequality when in

514
00:26:45.130 --> 00:26:47.739
reality, both groups really just want to alleviate poverty.

515
00:26:47.750 --> 00:26:49.689
But um where you were going with that was

516
00:26:49.699 --> 00:26:53.099
that uh do we find that people with different,

517
00:26:53.109 --> 00:26:55.780
that, that free will beliefs actually seem to affect

518
00:26:55.790 --> 00:26:58.670
um a support for inequality? We tested that. Uh

519
00:26:58.680 --> 00:27:01.170
WE tried to test that in a few different

520
00:27:01.180 --> 00:27:04.560
ways and we never found uh overwhelming support for

521
00:27:04.569 --> 00:27:08.189
that. Um So we, we did publish a paper

522
00:27:08.199 --> 00:27:10.609
reporting our no results for that. So I, I,

523
00:27:11.349 --> 00:27:13.770
I have no evidence to, to, to support that.

524
00:27:13.780 --> 00:27:14.849
That's, that's the case.

525
00:27:15.630 --> 00:27:18.130
By the way, do you think that studying a

526
00:27:18.140 --> 00:27:20.939
free will or belief in free will through a

527
00:27:20.949 --> 00:27:26.250
psychological perspective and really having this understanding that uh

528
00:27:26.430 --> 00:27:28.859
the degree to which people believe in free will

529
00:27:28.869 --> 00:27:32.849
varies according to the situation and their own, perhaps

530
00:27:32.859 --> 00:27:37.050
individual characteristics? Do you think it has any bearing

531
00:27:37.060 --> 00:27:40.170
on the free will debate and whether free will

532
00:27:40.209 --> 00:27:43.569
itself is real or not or not at all?

533
00:27:45.359 --> 00:27:49.219
No, not really. Uh Free will either exists or

534
00:27:49.229 --> 00:27:51.170
it doesn't. And it's, I mean, ii, I don't

535
00:27:51.180 --> 00:27:56.550
think the beliefs that all these like psychological consequences

536
00:27:56.560 --> 00:27:58.810
of beliefs uh reflect on in the same way

537
00:27:58.819 --> 00:28:02.300
that I think it, it, for religion. Right. You

538
00:28:02.310 --> 00:28:05.660
know, religion and morality is a big, uh, topic

539
00:28:05.670 --> 00:28:09.000
of research for mind, whether religion causes people to

540
00:28:09.010 --> 00:28:10.689
be more moral or not, doesn't really have any

541
00:28:10.699 --> 00:28:12.560
bearing on whether God exists.

542
00:28:13.290 --> 00:28:15.770
No, no. But that's an interesting question because there

543
00:28:15.780 --> 00:28:18.780
are many people who pick on these studies and

544
00:28:18.790 --> 00:28:20.959
this kind of knowledge to say, oh, you see,

545
00:28:20.969 --> 00:28:25.390
it's just people, uh, people just attribute free will

546
00:28:25.400 --> 00:28:28.349
to other people when they want and when they,

547
00:28:28.359 --> 00:28:30.709
when they don't, they don't do it. And so

548
00:28:30.719 --> 00:28:33.410
free will doesn't exist at all. I mean, just

549
00:28:33.420 --> 00:28:38.219
to support uh deterministic uh stance, let's say,

550
00:28:38.849 --> 00:28:42.260
yeah, I think they're reasonably independent uh your belief

551
00:28:42.270 --> 00:28:44.020
about free will and the actual existence of free

552
00:28:44.030 --> 00:28:47.050
will where I guess you could have some, OK,

553
00:28:47.060 --> 00:28:49.349
let's think about this. So, in I, I'm, I'm

554
00:28:49.359 --> 00:28:52.540
extrapolating now from the example of religion. So one

555
00:28:52.550 --> 00:28:55.430
of the things where the research, the psychological research

556
00:28:55.439 --> 00:28:57.910
on religion would probably have a bearing on whether

557
00:28:58.400 --> 00:29:01.709
uh God exists is that it can produce alternative

558
00:29:01.719 --> 00:29:04.750
explanations for why we believe in God. Uh So,

559
00:29:05.160 --> 00:29:09.920
um in that sense, it gives you uh the

560
00:29:09.930 --> 00:29:12.050
belief in God could be over determined and I

561
00:29:12.060 --> 00:29:14.849
guess free will belief could also be over determined,

562
00:29:14.859 --> 00:29:16.489
it could exist and we could believe it, we

563
00:29:16.500 --> 00:29:18.650
could believe it for reasons that are unrelated to

564
00:29:18.660 --> 00:29:21.689
why it exists. I don't know. My sense is

565
00:29:21.699 --> 00:29:24.369
that it, they're pretty, um, independent. Yeah.

566
00:29:25.469 --> 00:29:28.130
So, and since we were just, uh, on the

567
00:29:28.140 --> 00:29:32.290
topic of politics, uh, another thing that you've studied

568
00:29:32.300 --> 00:29:34.550
and I wanted to ask you about is when

569
00:29:34.560 --> 00:29:39.050
it comes to voting behavior and supporting specific political

570
00:29:39.060 --> 00:29:43.530
candidates or leaders, do people always follow their own

571
00:29:43.540 --> 00:29:47.250
self-interest? Because that's something that we hear a lot.

572
00:29:47.260 --> 00:29:49.880
That, that's what people do. So,

573
00:29:49.930 --> 00:29:53.680
yeah. Um, SO I guess it depends what you

574
00:29:53.689 --> 00:29:59.000
mean by the self. Um People can expand, you

575
00:29:59.010 --> 00:30:00.300
know what, before we tackle that, I just want

576
00:30:00.310 --> 00:30:01.859
to mention one more thing about the, the free

577
00:30:01.869 --> 00:30:06.319
will and politics thing because we did this very

578
00:30:06.329 --> 00:30:10.469
big paper uh with, with Jim Everett. Um LOOKING

579
00:30:10.479 --> 00:30:13.660
at uh those the two things about free will

580
00:30:13.670 --> 00:30:15.400
that we talked about, right? So we talked about

581
00:30:15.410 --> 00:30:20.500
this, this flexible um uh deployment of, of free

582
00:30:20.510 --> 00:30:23.719
will attributions uh when people want to punish somebody.

583
00:30:23.729 --> 00:30:25.810
And, and we talked then about the, the politics.

584
00:30:26.000 --> 00:30:28.239
We combined both of those into that paper to

585
00:30:28.250 --> 00:30:32.160
show that one of them, one of the a,

586
00:30:32.170 --> 00:30:34.959
a driving factor, uh which is, I think what

587
00:30:34.969 --> 00:30:37.760
we say in, in the paper. But what the

588
00:30:37.770 --> 00:30:40.400
paper actually showed is one of the primary factors

589
00:30:40.410 --> 00:30:43.079
about why those differences in free will beliefs exist

590
00:30:43.089 --> 00:30:47.359
between liberals and conservatives is because uh in general,

591
00:30:47.430 --> 00:30:51.589
conservatives tend to moralize a broader array of things

592
00:30:51.599 --> 00:30:53.770
and thus want to punish more things. And as

593
00:30:53.780 --> 00:30:55.670
a result, simply because they want to punish more

594
00:30:55.680 --> 00:30:58.180
things, they want to hold people more accountable for

595
00:30:58.189 --> 00:31:02.119
all these different moral transgressions. They ramp up their,

596
00:31:02.130 --> 00:31:04.760
their belief in free will in order to, to

597
00:31:04.770 --> 00:31:08.329
justify doing that. And, and we demonstrate this in

598
00:31:08.339 --> 00:31:09.560
a few different ways. But one, I think the

599
00:31:09.569 --> 00:31:12.949
most interesting is that when you look at topics

600
00:31:12.959 --> 00:31:16.540
that, uh, liberals and conservatives, uh, uh, moralized to

601
00:31:16.550 --> 00:31:19.550
the same degree, uh, they don't differ in terms

602
00:31:19.560 --> 00:31:21.930
of their free will beliefs about that. And four

603
00:31:21.939 --> 00:31:25.599
topics that, that liberals actually moralize to a greater

604
00:31:25.609 --> 00:31:29.219
degree, uh then liberals will now believe in free

605
00:31:29.229 --> 00:31:32.040
will uh more for those types of things. And

606
00:31:32.050 --> 00:31:35.189
so again, it gets to that, that flexibility that

607
00:31:35.199 --> 00:31:37.430
um yes, it's the case that by and large

608
00:31:37.439 --> 00:31:40.069
conservatives believe in and believe in free will more.

609
00:31:40.079 --> 00:31:42.550
But when it's inconvenient for them to do so,

610
00:31:42.560 --> 00:31:44.430
and when it's convenient for liberals to do so,

611
00:31:44.439 --> 00:31:45.069
they will flip.

612
00:31:45.680 --> 00:31:48.739
OK. No, that's, that's very interesting. And, and so

613
00:31:48.750 --> 00:31:54.150
just going back to the self-interest, self-interest uh aspect

614
00:31:54.160 --> 00:31:57.469
here. So, uh I mean, do people actually in

615
00:31:57.479 --> 00:32:01.219
their voting behavior and support for political candidates for

616
00:32:01.229 --> 00:32:04.479
always follow their self-interest or not?

617
00:32:05.000 --> 00:32:09.670
Uh N no. So um it seems that people

618
00:32:09.680 --> 00:32:13.599
have people's uh group identities matter a lot to

619
00:32:13.609 --> 00:32:17.390
them. And um oftentimes they will vote against their

620
00:32:17.400 --> 00:32:23.349
obvious uh individualistic economic self interest, say uh in

621
00:32:23.359 --> 00:32:27.449
order to uh uh maintain their loyalty to the

622
00:32:27.459 --> 00:32:30.680
group or to make sure that their particular uh

623
00:32:30.689 --> 00:32:34.859
cultural interests, uh broader cultural interests uh win out

624
00:32:34.869 --> 00:32:41.890
over a competing group. Um So, um uh when,

625
00:32:42.689 --> 00:32:45.530
when, so I guess this would argue against that,

626
00:32:45.540 --> 00:32:48.630
that famous James Carville, um a point that it's

627
00:32:48.640 --> 00:32:50.869
the economy stupid that people always vote with their,

628
00:32:50.880 --> 00:32:52.729
with their pocketbooks and to some degree they do.

629
00:32:52.739 --> 00:32:56.839
But we, we ran a study um Steve Rathje

630
00:32:57.130 --> 00:33:03.020
and uh Simone uh Chanel um on, uh on

631
00:33:03.030 --> 00:33:08.390
Trump voters, uh who had disappointing tax returns. And

632
00:33:08.400 --> 00:33:10.949
we, we ran a longitudinal study which said, uh

633
00:33:10.959 --> 00:33:12.880
what do you expect for your to get for

634
00:33:12.890 --> 00:33:16.250
your tax return? Um And because this was when

635
00:33:16.260 --> 00:33:18.979
there were some uh changes in the tax record,

636
00:33:18.989 --> 00:33:20.250
I'm trying to remember what happened. There are all

637
00:33:20.260 --> 00:33:23.140
these crazy things that happened during that, that those

638
00:33:23.150 --> 00:33:25.180
years and, and one of the things was like

639
00:33:25.719 --> 00:33:29.719
a rejiggering of the state taxes and the federal

640
00:33:29.729 --> 00:33:32.560
taxes such that uh people's tax returns were gonna

641
00:33:32.569 --> 00:33:34.420
change and, and their predictions were gonna be off.

642
00:33:34.829 --> 00:33:37.219
And so we looked at Trump voters and asked,

643
00:33:37.229 --> 00:33:41.589
uh, how important is, are your taxes to whom

644
00:33:41.599 --> 00:33:43.800
you vote for? Um And what do you expect

645
00:33:43.810 --> 00:33:46.880
for your taxes? Uh And then we followed them

646
00:33:46.890 --> 00:33:50.619
up and for those people who had um disappointing

647
00:33:50.630 --> 00:33:54.290
tax uh uh records, we asked, will you still

648
00:33:54.300 --> 00:33:56.349
vote for, for Trump for re election? They said

649
00:33:56.359 --> 00:33:57.780
yes. And then we asked, well, how much do

650
00:33:57.790 --> 00:33:59.989
your taxes matter to your voting. And they said,

651
00:34:00.000 --> 00:34:03.400
well, well, now much less. Uh, SO they, they

652
00:34:03.410 --> 00:34:07.109
downgraded how important that factor was because that factor

653
00:34:07.119 --> 00:34:09.540
wasn't really what was driving their voting intentions. They

654
00:34:09.550 --> 00:34:12.179
just, they just kind of thought that maybe it

655
00:34:12.188 --> 00:34:14.188
is, maybe I'm, I'm gonna care about things which

656
00:34:14.199 --> 00:34:17.040
affect me, but really there, there are larger group

657
00:34:17.050 --> 00:34:20.850
political commitments. Uh, uh I'm trying not to use

658
00:34:20.860 --> 00:34:24.379
the word trumped their, their other concerns. So overrode

659
00:34:24.389 --> 00:34:26.290
their other concerns. And when we published that it

660
00:34:26.300 --> 00:34:28.570
took us a little while to publish it. Um

661
00:34:28.580 --> 00:34:30.070
And by the time we did Trump was out

662
00:34:30.080 --> 00:34:32.530
of office and Biden was in, we were like,

663
00:34:32.540 --> 00:34:33.958
ok, well, I guess this will be a nice

664
00:34:33.969 --> 00:34:37.389
historical record. It's not gonna be that relevant anymore,

665
00:34:37.688 --> 00:34:40.370
but of course, now it seems like there's a

666
00:34:40.379 --> 00:34:42.659
really competitive election going on and it might be

667
00:34:42.668 --> 00:34:43.510
relevant all over.

668
00:34:45.000 --> 00:34:47.580
So, uh I, I mean, there are many different

669
00:34:47.590 --> 00:34:50.570
follow up questions that I uh interesting follow up

670
00:34:50.580 --> 00:34:53.060
questions that I could ask you. But with our

671
00:34:53.070 --> 00:34:55.739
time limit in mind, let me just ask you

672
00:34:55.750 --> 00:34:59.219
one. So, uh does it make any sense to

673
00:34:59.229 --> 00:35:04.409
think that perhaps since a political candidate usually proposes

674
00:35:04.419 --> 00:35:07.939
several different kinds of policies and not just one

675
00:35:08.340 --> 00:35:12.550
that let's say, even if people are voting against

676
00:35:12.560 --> 00:35:16.189
their own self-interest when it comes to, for example,

677
00:35:16.199 --> 00:35:21.010
uh taxation, they would be voting uh for their

678
00:35:21.020 --> 00:35:23.669
own self-interest when it comes to, for example, it,

679
00:35:24.840 --> 00:35:27.520
if that's something that they care more about, I

680
00:35:27.669 --> 00:35:27.820
mean, that,

681
00:35:29.399 --> 00:35:32.110
that's, that, yeah, that does make sense though. I

682
00:35:32.120 --> 00:35:35.330
suspect there you also have uh the group commitment.

683
00:35:35.340 --> 00:35:38.610
So for instance, if an individual was going to

684
00:35:38.620 --> 00:35:41.169
be negatively, let's say an individual was going to

685
00:35:41.179 --> 00:35:43.709
be positively affected by immigration or they were, they

686
00:35:43.719 --> 00:35:47.879
were personally going to be positively affected by uh

687
00:35:48.090 --> 00:35:50.280
a restriction and I don't know how that's gonna

688
00:35:50.300 --> 00:35:53.149
work. So let's say a liberalizing of abortion laws.

689
00:35:53.399 --> 00:35:59.750
Um uh I think that if their group uh

690
00:35:59.760 --> 00:36:03.860
was going against that, so if it differentially affected

691
00:36:03.870 --> 00:36:06.580
them versus their group and they were very uh

692
00:36:06.590 --> 00:36:08.790
fused with their group, they had a lot of

693
00:36:08.800 --> 00:36:10.899
uh uh uh uh their identity tied in with

694
00:36:10.909 --> 00:36:14.909
the group. They would still go against the immigration

695
00:36:14.919 --> 00:36:17.820
policy or the abortion policy that favored themselves the

696
00:36:17.830 --> 00:36:19.780
most in order to, to stand by their group.

697
00:36:21.370 --> 00:36:25.229
So sort of related to economics here. Why do

698
00:36:25.239 --> 00:36:29.689
people tend to like rags to riches stories? And

699
00:36:29.820 --> 00:36:33.239
I mean, do people tend to have more positive

700
00:36:33.250 --> 00:36:36.739
views of those who become rich than those who

701
00:36:36.750 --> 00:36:38.879
are born rich or not?

702
00:36:39.479 --> 00:36:45.189
Yeah. So I'm interested and I'm interested in whether,

703
00:36:45.290 --> 00:36:48.060
where you are Poland, right. Poland,

704
00:36:48.870 --> 00:36:49.800
Portugal, Portugal,

705
00:36:50.570 --> 00:36:52.750
you're in Portugal. I haven't thought this whole time.

706
00:36:52.770 --> 00:36:55.229
Well, I'd be more interested in what, what people

707
00:36:55.239 --> 00:36:56.580
in Poland would say, but I'm also interested in

708
00:36:56.590 --> 00:36:59.050
what, what people in Portugal think in the US,

709
00:36:59.060 --> 00:37:00.760
this is a really big thing. The rags to

710
00:37:00.919 --> 00:37:04.280
stories are part of the American mythos. And, and

711
00:37:04.290 --> 00:37:07.959
I, I've been marinated in the American Canadian culture

712
00:37:07.969 --> 00:37:11.520
since I was born. Um, AND so to me

713
00:37:11.610 --> 00:37:16.260
it seems like, well, people, uh, people love these

714
00:37:16.270 --> 00:37:19.429
stories because they show that it's possible, uh, and

715
00:37:19.439 --> 00:37:21.729
it shows that it's earned. Uh, AND so people

716
00:37:21.739 --> 00:37:23.800
really want to think that both of those things

717
00:37:23.810 --> 00:37:27.010
are true. It's system justifying to, to believe that

718
00:37:27.020 --> 00:37:30.689
we live in a place where, uh, uh, we're,

719
00:37:30.699 --> 00:37:35.320
we're not restricted by our accidents of birth. Um

720
00:37:35.629 --> 00:37:38.610
And, uh, and so what you tend to see,

721
00:37:38.620 --> 00:37:41.530
uh, in North America, we measured it in the

722
00:37:41.540 --> 00:37:45.560
United States that yes, people, people much prefer, uh,

723
00:37:45.570 --> 00:37:49.250
those who've become rich, uh, to those who are

724
00:37:49.260 --> 00:37:52.729
born rich. Um, AND, and we have many, many

725
00:37:52.739 --> 00:37:55.330
examples in the us of these great stories of

726
00:37:55.340 --> 00:37:59.070
the Oprah Winfrey and, um, all these people, um,

727
00:37:59.080 --> 00:38:02.159
Bill Clinton is another good example and when these

728
00:38:02.169 --> 00:38:04.649
happen, which they occasionally do because it's a big

729
00:38:04.659 --> 00:38:07.590
country. Uh, PEOPLE will like shout it from the

730
00:38:07.600 --> 00:38:10.219
rooftops because they really wanna wanna believe that's possible.

731
00:38:10.330 --> 00:38:14.310
Um, I was, uh, I'll tell you a story.

732
00:38:14.320 --> 00:38:19.520
So I was, uh, uh, Thanksgiving dinner. No, fourth

733
00:38:19.530 --> 00:38:21.199
of July. Uh, I was living in the U

734
00:38:21.209 --> 00:38:25.409
SI was actually, um, in on vacation with, uh,

735
00:38:25.419 --> 00:38:27.110
my girlfriend at the time with a bunch of

736
00:38:27.120 --> 00:38:30.790
her friends. Um, Fourth of July. Uh, uh, WE

737
00:38:30.800 --> 00:38:35.040
were in Mexico and we were having dinner and

738
00:38:35.050 --> 00:38:37.030
I, I, there's this really cheesy thing that they

739
00:38:37.040 --> 00:38:38.840
did, I guess these people really liked America and

740
00:38:38.850 --> 00:38:41.179
they said, let's go around the table and everybody

741
00:38:41.189 --> 00:38:43.649
say what they love about America. And there were

742
00:38:43.659 --> 00:38:46.159
these two guys who were there, uh, both of

743
00:38:46.169 --> 00:38:48.600
whom were like, they were, they were finance bros,

744
00:38:49.000 --> 00:38:52.840
but they had very different backgrounds. So one had,

745
00:38:52.850 --> 00:38:57.520
one came from money, uh, and um, had done

746
00:38:57.530 --> 00:38:59.479
everything he could to lose it. He'd been like

747
00:38:59.489 --> 00:39:00.959
a beach bum and whatever and he could not,

748
00:39:00.969 --> 00:39:03.699
he just, he was just destined to stay rich.

749
00:39:03.989 --> 00:39:07.149
Um And the other guy was the exact opposite.

750
00:39:07.159 --> 00:39:09.889
So he was, uh he, he clawed his way

751
00:39:09.899 --> 00:39:13.360
up from, from poverty. He was a poor student,

752
00:39:13.370 --> 00:39:16.179
remedial student and he just like worked his ass

753
00:39:16.189 --> 00:39:19.570
off to become rich. And uh we were going

754
00:39:19.580 --> 00:39:21.080
around the table and it first came to that

755
00:39:21.090 --> 00:39:25.330
guy, the guy who had become rich. Um And

756
00:39:25.340 --> 00:39:27.699
he said, well, the great thing about America is

757
00:39:27.709 --> 00:39:30.469
that anybody can make it here if you just

758
00:39:30.479 --> 00:39:32.919
work hard enough. If you just work hard enough,

759
00:39:32.929 --> 00:39:35.350
uh anybody can become rich, they can succeed like

760
00:39:35.360 --> 00:39:36.949
I have, I don't know if he said like

761
00:39:36.959 --> 00:39:39.219
I have. Um, AND then it went to the

762
00:39:39.229 --> 00:39:41.419
next guy and he's just the, the rich guy

763
00:39:41.429 --> 00:39:42.800
who's always been rich and he was just staring

764
00:39:42.810 --> 00:39:45.260
at him and he goes, no, they can't. I've

765
00:39:45.270 --> 00:39:47.790
looked at the data, people who are born rich,

766
00:39:47.800 --> 00:39:50.419
stay rich and people are born poor, uh stay

767
00:39:50.429 --> 00:39:54.719
poor on average. And his experiences suggested that and

768
00:39:55.370 --> 00:39:58.659
um and so I tho those two different perspectives

769
00:39:58.669 --> 00:40:02.709
are, are very interesting. Um What, what you, what

770
00:40:02.719 --> 00:40:04.889
we find is that like I said, people tend

771
00:40:04.899 --> 00:40:09.629
to like the became rich people better. What's interesting

772
00:40:09.639 --> 00:40:12.560
is they also think that they became rich people.

773
00:40:12.570 --> 00:40:14.360
The people who had clawed their way up from

774
00:40:14.370 --> 00:40:19.169
poverty will be more sympathetic to and supportive of

775
00:40:19.179 --> 00:40:24.000
the plight of the poor. Uh BECAUSE uh they

776
00:40:24.340 --> 00:40:26.739
come, they, they have the uh a totally reasonable

777
00:40:26.750 --> 00:40:30.780
conclusion that those people have been there. Um They

778
00:40:30.790 --> 00:40:33.850
have more sympathy, the, the rich people, the, the

779
00:40:33.860 --> 00:40:37.100
Rockefellers, whatever. They, they're out of touch, they don't

780
00:40:37.110 --> 00:40:38.419
know what it's like to be poor. Why, why

781
00:40:38.429 --> 00:40:40.459
would they want to help the poor? And what

782
00:40:40.469 --> 00:40:43.580
we find in reality is the opposite. Um Even

783
00:40:43.590 --> 00:40:45.189
though people perceive that things are gonna be like

784
00:40:45.199 --> 00:40:48.270
that way. In reality, the people who were born

785
00:40:48.280 --> 00:40:50.760
rich tend to be more supportive of the poor,

786
00:40:50.770 --> 00:40:53.159
more sympathetic to the plight of the poor, more

787
00:40:53.169 --> 00:40:56.300
willing to engage in redistribution than the people who

788
00:40:56.320 --> 00:40:59.379
uh became rich. And the explanation we find is

789
00:40:59.389 --> 00:41:02.899
that for those people who've clawed their way up,

790
00:41:03.439 --> 00:41:06.389
they believe that it is possible for everybody because

791
00:41:06.399 --> 00:41:07.879
they've done it themselves. And if they believe that

792
00:41:07.889 --> 00:41:11.379
it's possible for everybody. They hold other people accountable

793
00:41:11.389 --> 00:41:13.879
for it. Not having happened for them. Well, if

794
00:41:13.889 --> 00:41:15.479
it hasn't happened for them, it's because they didn't

795
00:41:15.489 --> 00:41:17.379
try hard enough. I tried hard enough and it

796
00:41:17.389 --> 00:41:20.439
happened for me. Whereas the people like the, the

797
00:41:20.449 --> 00:41:24.699
rich surfer guy, he's like, you know what? I,

798
00:41:24.709 --> 00:41:26.800
I tried to lose money and I couldn't, and

799
00:41:26.810 --> 00:41:29.000
people down there probably tried to make money and

800
00:41:29.010 --> 00:41:32.100
they couldn't. So, it's not because of the, their

801
00:41:32.110 --> 00:41:36.770
individual abilities or efforts or merit. It's because these

802
00:41:36.780 --> 00:41:37.620
are structural things.

803
00:41:38.600 --> 00:41:41.300
You know, where you see that a lot where

804
00:41:41.310 --> 00:41:45.770
people who become rich are really, uh, against. II,

805
00:41:45.780 --> 00:41:49.469
I don't know, poverty alleviation or more taxation or

806
00:41:49.479 --> 00:41:54.250
something like that among, uh, immigrants who succeeded. I

807
00:41:54.260 --> 00:41:58.300
mean, immigrants who succeeded. No, no one from their,

808
00:41:58.310 --> 00:42:02.429
uh, original country likes them because they are just

809
00:42:02.439 --> 00:42:05.830
annoying. They, they think that anyone can do it

810
00:42:05.840 --> 00:42:07.739
and if you don't have it, it's because you

811
00:42:07.750 --> 00:42:11.070
don't work hard enough or something like that. So,

812
00:42:11.080 --> 00:42:11.379
yeah.

813
00:42:12.719 --> 00:42:17.020
Yeah. Yeah. It's, uh, it's very important to pay

814
00:42:17.030 --> 00:42:19.699
attention to the headwinds and the tailwinds that affect

815
00:42:19.709 --> 00:42:21.010
people and how they go where they are.

816
00:42:21.800 --> 00:42:25.800
Yeah. A and so does this relate in any

817
00:42:25.810 --> 00:42:29.870
way to support for social welfare? I mean, is

818
00:42:29.879 --> 00:42:33.760
it that people who became rich also tend to

819
00:42:33.770 --> 00:42:37.239
support social welfare less than the ones who were

820
00:42:37.250 --> 00:42:38.449
born rich?

821
00:42:39.419 --> 00:42:40.989
You know what I'm not sure about that. I

822
00:42:41.000 --> 00:42:44.570
think I think we saw some evidence for that

823
00:42:44.580 --> 00:42:46.600
and we mentioned it in the paper that we

824
00:42:46.610 --> 00:42:49.030
wrote. Certainly you can think of examples, but of

825
00:42:49.040 --> 00:42:51.080
course, these are cherry picked examples, right? You think

826
00:42:51.090 --> 00:42:54.139
of uh the Roosevelts, right? The Roosevelts came from

827
00:42:54.149 --> 00:42:57.189
a very, very wealthy family, landowning New York family.

828
00:42:57.479 --> 00:42:59.679
Uh AND they were some of the more progressive

829
00:42:59.689 --> 00:43:03.219
presidents. Um But I think you could probably come

830
00:43:03.229 --> 00:43:05.669
up with counterexamples of that pretty easily. So uh

831
00:43:05.679 --> 00:43:07.919
let's not quote me on any of that. Um

832
00:43:08.250 --> 00:43:10.000
The data out there, I just can't remember what

833
00:43:10.010 --> 00:43:10.399
they say.

834
00:43:12.219 --> 00:43:16.989
And so um when it comes to causal attributions

835
00:43:17.000 --> 00:43:22.389
for poverty, do they relate to attitudes towards inequality?

836
00:43:22.399 --> 00:43:24.000
And by and by the way, what tend to

837
00:43:24.010 --> 00:43:27.639
be the, the more common causal attributions here

838
00:43:28.689 --> 00:43:30.770
for, for poverty?

839
00:43:31.060 --> 00:43:31.479
Yeah. No,

840
00:43:31.600 --> 00:43:36.290
for poverty. Yeah. Yeah. So um in America, uh

841
00:43:36.350 --> 00:43:42.260
people tend to believe um more so than I

842
00:43:42.270 --> 00:43:45.500
would have suspected. They tend to believe that that

843
00:43:45.510 --> 00:43:49.280
the poor are responsible for being poor. Um WHICH

844
00:43:49.290 --> 00:43:52.479
is, um which is interesting because they're, they're often

845
00:43:52.489 --> 00:43:55.159
given a lot of evidence to the contrary. Um

846
00:43:55.169 --> 00:43:57.510
MAYBE that's in the media that I'm exposed to.

847
00:43:57.520 --> 00:44:00.520
But uh uh they believe that that more than

848
00:44:00.530 --> 00:44:02.169
other places and they believe that more than I

849
00:44:02.179 --> 00:44:06.199
would have suspected in the US, by the way,

850
00:44:06.209 --> 00:44:08.580
in, in Korea. This is just some new data

851
00:44:08.590 --> 00:44:11.060
that um the lead author on that uh former

852
00:44:11.070 --> 00:44:13.780
student of mine, Hyun Jin Koo uh collected. Uh

853
00:44:13.790 --> 00:44:18.409
SHE found that in South Korea, uh people also

854
00:44:18.419 --> 00:44:24.719
have this preference uh for the no people also

855
00:44:24.729 --> 00:44:27.899
believe that they became rich uh would be more

856
00:44:27.909 --> 00:44:31.159
sympathetic to the poor. But unlike in America, where

857
00:44:31.169 --> 00:44:35.530
people prefer to be the became rich in South

858
00:44:35.540 --> 00:44:39.540
Korea, people prefer to be the born rich and

859
00:44:39.550 --> 00:44:41.929
she suspects and this is what she's measuring currently

860
00:44:41.939 --> 00:44:44.280
is that that's due to a sense of uh

861
00:44:44.290 --> 00:44:47.469
social class fatalism. So in Korea, they have this

862
00:44:47.479 --> 00:44:50.260
metaphor of being born with either a golden spoon

863
00:44:50.270 --> 00:44:52.500
or a clay spoon. And so you're a gold

864
00:44:52.510 --> 00:44:55.649
spoon or a clay spoon type of person. And

865
00:44:55.659 --> 00:44:57.229
people want to be the people who are born

866
00:44:57.239 --> 00:44:59.250
with the gold spoon because that, that stays with

867
00:44:59.260 --> 00:45:01.580
you, regardless of whether your economics changes, you're a

868
00:45:01.590 --> 00:45:04.129
gold spoon person or, or a clay spoon person.

869
00:45:04.139 --> 00:45:06.320
And I think one of the things that uh

870
00:45:06.340 --> 00:45:09.229
the American ethos is about is that, well, nobody

871
00:45:09.239 --> 00:45:13.149
is essentially their class. Uh THAT America is a

872
00:45:13.159 --> 00:45:16.570
place which, which doesn't have those essential characteristics and,

873
00:45:16.580 --> 00:45:18.120
and, and you don't have to be a clay

874
00:45:18.129 --> 00:45:19.360
spoon your whole life if you were born.

875
00:45:20.659 --> 00:45:24.229
So, I mean, if people tend to believe that

876
00:45:24.239 --> 00:45:27.949
uh people who are poor, it's because of their

877
00:45:27.959 --> 00:45:30.790
own, it's their own fault. It's because of the

878
00:45:30.800 --> 00:45:34.669
bad decisions they made. They also tend to be

879
00:45:34.679 --> 00:45:39.439
more uh accepting of inequality in their society,

880
00:45:39.639 --> 00:45:43.399
right? That's Right. Yeah, that's right. So um uh

881
00:45:43.679 --> 00:45:47.850
yeah, so um we, we see that across countries.

882
00:45:47.860 --> 00:45:52.840
So places where there's a greater uh uh belief

883
00:45:52.850 --> 00:45:57.620
in um uh where there's a greater belief in

884
00:45:57.629 --> 00:46:03.340
social mobility, there's a greater tolerance for inequality. And

885
00:46:03.350 --> 00:46:05.800
in fact, that's a greater predictor of the level

886
00:46:05.810 --> 00:46:08.310
of inequality of, of the support for inequality than

887
00:46:08.320 --> 00:46:11.330
the level of inequality in that country itself. Um

888
00:46:11.790 --> 00:46:14.419
So when people believe that you're able to get

889
00:46:14.429 --> 00:46:17.989
out of your position, uh they're gonna be less

890
00:46:18.000 --> 00:46:21.189
willing to help you for not having done it.

891
00:46:22.840 --> 00:46:26.500
So to get into the last topic, what is

892
00:46:26.510 --> 00:46:30.620
the moralization of effort? I mean, what, what is

893
00:46:30.629 --> 00:46:34.360
it really about? And what do these plays of

894
00:46:34.370 --> 00:46:37.139
effort signal socially to other people?

895
00:46:37.709 --> 00:46:40.000
Yeah. So this is uh work I want to

896
00:46:40.010 --> 00:46:41.969
keep giving credit to, to all the, the first

897
00:46:41.979 --> 00:46:43.399
authors and students on this. So this is a

898
00:46:43.409 --> 00:46:45.699
work by, um it was led by Jared Sneer.

899
00:46:46.530 --> 00:46:49.629
Um It's based on some, some other work um

900
00:46:49.639 --> 00:46:53.429
as well by, by various researchers. But basically what

901
00:46:53.439 --> 00:46:56.540
we found there is that, um even for people

902
00:46:56.550 --> 00:47:01.429
who don't produce, uh um more than somebody else,

903
00:47:01.439 --> 00:47:03.610
somebody who's putting a lot of effort into their

904
00:47:03.620 --> 00:47:06.969
uh behaviors, whether it's at work, whether it's in

905
00:47:06.979 --> 00:47:10.189
their hobbies, uh they're seen as a more moral

906
00:47:10.199 --> 00:47:13.669
person. Um So if you see the, the actual,

907
00:47:13.679 --> 00:47:18.449
the um the uh anecdote I give about this

908
00:47:18.649 --> 00:47:20.199
is that one of the co authors on the

909
00:47:20.209 --> 00:47:21.909
paper. Paul Piff, he's a good friend of mine.

910
00:47:22.199 --> 00:47:25.260
Um, AND he, he's a runner. He, he runs

911
00:47:25.270 --> 00:47:27.040
in the movie. He's a jogger. He's a jogger.

912
00:47:27.050 --> 00:47:31.149
Let's be, let's be real. Um, AND he, uh,

913
00:47:32.330 --> 00:47:34.330
when I, when I met him II, I moved,

914
00:47:34.340 --> 00:47:36.280
I moved to this when I was UC Irvine,

915
00:47:36.290 --> 00:47:37.750
he was a UC Irvine and I kept hearing

916
00:47:37.760 --> 00:47:39.129
he would go on these jogs and I find

917
00:47:39.139 --> 00:47:40.889
people who jog kind of annoying because they're all

918
00:47:40.899 --> 00:47:45.050
like holier than now and whatever. Um But then

919
00:47:45.850 --> 00:47:49.320
one day I saw him jogging and I expected

920
00:47:49.330 --> 00:47:51.060
when he was jogging, he just be like, you

921
00:47:51.070 --> 00:47:54.350
know, like smiling at the beautiful Californian sun and

922
00:47:54.360 --> 00:47:55.979
just like taking it easy and just being in

923
00:47:55.989 --> 00:47:58.300
like, you know, this Mr perfect kind of guy,

924
00:47:58.659 --> 00:48:02.010
but he was like hobbling over, he was like

925
00:48:02.020 --> 00:48:04.260
one leg wasn't working and it was dripping in

926
00:48:04.270 --> 00:48:05.959
sweat and he had this look of just agony

927
00:48:05.969 --> 00:48:08.280
on his face and it made me have that

928
00:48:08.290 --> 00:48:10.280
much more respect for him because he was putting

929
00:48:10.290 --> 00:48:14.260
in that much more effort uh to, you know,

930
00:48:14.270 --> 00:48:17.139
a task which, which didn't really help anybody. Um

931
00:48:17.389 --> 00:48:19.550
It was just for him. Uh IT has no

932
00:48:19.560 --> 00:48:23.459
bearing on, on uh harming or helping anybody else,

933
00:48:23.469 --> 00:48:25.290
but it did make him seem like a, a

934
00:48:25.600 --> 00:48:30.320
more moral person for some reason. Um And so

935
00:48:30.780 --> 00:48:34.590
the theory behind it is that we are constantly

936
00:48:34.770 --> 00:48:38.120
uh trying to figure out amongst the market for

937
00:48:38.129 --> 00:48:40.909
people, uh, who is going to be a good

938
00:48:40.919 --> 00:48:43.209
cooperator, who's going to be a good moral person.

939
00:48:43.219 --> 00:48:44.939
We want to surround ourselves with people who are

940
00:48:44.949 --> 00:48:49.360
dependable, trustworthy, uh, moral people. And so we're looking

941
00:48:49.370 --> 00:48:52.790
for any cue that actually would, would suggest that

942
00:48:52.800 --> 00:48:54.709
somebody is like that somebody would be a good

943
00:48:54.719 --> 00:48:57.850
cooper partner. And effort is a really good one.

944
00:48:58.600 --> 00:49:04.149
SO effort, uh, is, um, tells you whether if

945
00:49:04.159 --> 00:49:07.449
you needed help, if you needed somebody to actually

946
00:49:07.479 --> 00:49:10.110
do something for you, that that's somebody who's willing

947
00:49:10.120 --> 00:49:13.189
to invest effort into things. And in a way,

948
00:49:13.199 --> 00:49:15.060
potentially, we don't have evidence for this. But in

949
00:49:15.070 --> 00:49:17.100
a way, you could imagine that the people who

950
00:49:17.110 --> 00:49:19.879
are willing to invest effort into the most meaningless

951
00:49:19.889 --> 00:49:22.669
things are the people who are even that much

952
00:49:22.679 --> 00:49:26.090
more trustworthy because they would put in effort to,

953
00:49:26.100 --> 00:49:28.750
uh, to, to whatever whatever you needed them, uh

954
00:49:28.760 --> 00:49:32.310
for not just things which have economic uh benefits.

955
00:49:33.260 --> 00:49:34.909
And so we showed in a number of different,

956
00:49:35.149 --> 00:49:38.020
um studies that, yeah, so people who uh are

957
00:49:38.030 --> 00:49:40.419
shown. So like if you have two widget makers

958
00:49:40.560 --> 00:49:42.600
and they produce the same amount of widgets, the

959
00:49:42.610 --> 00:49:44.550
one for whom it's hard works really hard. That

960
00:49:44.560 --> 00:49:46.280
person is seen as more moral and you'd want

961
00:49:46.290 --> 00:49:48.679
them as a, as a partner, you want them

962
00:49:48.689 --> 00:49:51.580
as a trust partner. Uh More than the person

963
00:49:51.590 --> 00:49:54.090
who, who, who it comes easily to

964
00:49:55.239 --> 00:49:57.379
do, do you like soccer?

965
00:49:58.469 --> 00:50:00.459
I don't like soccer. I mean, I, I have

966
00:50:00.469 --> 00:50:01.689
nothing against soccer but

967
00:50:01.699 --> 00:50:03.719
no, no, no, no, no. But, but I, I'm

968
00:50:03.729 --> 00:50:08.260
mentioning that because one very famous example of how

969
00:50:08.270 --> 00:50:11.169
people look at effort and how they moralize it

970
00:50:11.179 --> 00:50:14.510
is the debate between who's the best soccer player

971
00:50:14.520 --> 00:50:18.870
ever. Cristiano Ronaldo or Messi because since Cristiano Ronaldo

972
00:50:18.879 --> 00:50:22.929
is always on social media, advertising his work ethic

973
00:50:22.939 --> 00:50:26.030
people and many, many people out there tend to

974
00:50:26.040 --> 00:50:29.750
think that he's the best soccer player ever because

975
00:50:29.989 --> 00:50:33.989
he puts in more effort than Messi, which by

976
00:50:34.000 --> 00:50:36.449
the way is wrong because Messi has also improved

977
00:50:36.459 --> 00:50:39.030
his play through the years. So he trains a

978
00:50:39.040 --> 00:50:41.739
lot. But anyway, that's just an example.

979
00:50:41.949 --> 00:50:44.830
I mean, another good example of this uh is

980
00:50:44.840 --> 00:50:48.290
in basketball with uh Kobe Bryant and, and Shaquille

981
00:50:48.330 --> 00:50:52.000
o'neal because Shaq was born with these amazing natural

982
00:50:52.010 --> 00:50:54.439
gifts and Kobe was like the hardest working person

983
00:50:54.449 --> 00:50:56.399
in the NBA. And, and you really, if you,

984
00:50:56.719 --> 00:50:59.060
uh if I were to try to pick somebody

985
00:50:59.070 --> 00:51:01.979
who's, I, I mean, I guess Kobe had those

986
00:51:02.320 --> 00:51:07.629
sexual assault allegations, but, you know, he seemed, I

987
00:51:07.639 --> 00:51:09.719
guess the, the work ethic does make him seem

988
00:51:09.729 --> 00:51:13.929
seem like a more moral uh person. Um Yeah,

989
00:51:13.939 --> 00:51:16.090
and, and I think there's probably many examples of

990
00:51:16.100 --> 00:51:18.010
that and I think we, we do venerate people

991
00:51:18.020 --> 00:51:19.489
who are seen as hard working. And I was

992
00:51:19.500 --> 00:51:22.050
thinking even, you know, even some of these, these

993
00:51:22.060 --> 00:51:24.570
memes or these trends that take off on social

994
00:51:24.580 --> 00:51:27.370
media. Uh I was thinking about this cold plunge

995
00:51:27.379 --> 00:51:29.550
thing. You know, these people who like wake up

996
00:51:29.560 --> 00:51:32.100
in the morning, they jump in a, like a

997
00:51:32.110 --> 00:51:35.439
bunch of ice cubes. Um I don't know if

998
00:51:35.449 --> 00:51:39.330
that helps people, but I imagine that people, people

999
00:51:39.340 --> 00:51:41.919
venerate the people who go through that simply because

1000
00:51:41.929 --> 00:51:44.010
it's like a grit your teeth and bear, it's

1001
00:51:44.020 --> 00:51:47.830
hard to do And it seems to just have

1002
00:51:47.840 --> 00:51:50.469
this automatic, I respect that person. I think that

1003
00:51:50.479 --> 00:51:53.209
person is good because they're doing this really hard

1004
00:51:53.219 --> 00:51:55.370
thing. Uh, I think we tend to do that.

1005
00:51:55.379 --> 00:51:56.810
I think we tend to, to think of people

1006
00:51:56.820 --> 00:51:59.530
who are, um, just willing to put in a

1007
00:51:59.540 --> 00:52:00.969
lot of effort for things. We see them as,

1008
00:52:00.979 --> 00:52:04.810
as very, as very moral. There's an interesting paper,

1009
00:52:05.270 --> 00:52:08.600
uh, in behavioral brain science, Leo Fushi was the,

1010
00:52:08.610 --> 00:52:09.889
I don't know how to pronounce his last name.

1011
00:52:09.899 --> 00:52:12.550
I hope that's right. Uh, WAS the lead author

1012
00:52:12.560 --> 00:52:14.500
on it, which was looking at this idea of

1013
00:52:14.510 --> 00:52:19.629
a Puritan, a puritan moralizing that their argument was

1014
00:52:19.639 --> 00:52:22.709
that the reason why people moralize these things, which

1015
00:52:22.719 --> 00:52:27.889
tend not to have, uh, uh, victims or beneficiaries,

1016
00:52:27.899 --> 00:52:33.070
um, like, uh, eating too much or, uh, drinking,

1017
00:52:33.080 --> 00:52:36.110
gambling, all these kinds of things. Their argument was

1018
00:52:36.120 --> 00:52:38.500
that, well, those things lead people to have less

1019
00:52:38.510 --> 00:52:42.209
self control. But I think, I think that's insightful.

1020
00:52:42.500 --> 00:52:44.000
Uh, BUT I think a small tweak on that

1021
00:52:44.010 --> 00:52:46.770
is that those things signal who has high self

1022
00:52:46.780 --> 00:52:48.820
control and who doesn't. And so if you see

1023
00:52:48.830 --> 00:52:52.820
somebody who's engaging in, uh, eating too much or,

1024
00:52:52.830 --> 00:52:55.760
uh, is, is giving into gambling too much or

1025
00:52:55.770 --> 00:52:58.120
whatever, that's a signal to you that, well, if

1026
00:52:58.129 --> 00:53:00.070
you really needed them, you don't know that they'll

1027
00:53:00.080 --> 00:53:03.699
be able to come through because they, they indulge,

1028
00:53:03.709 --> 00:53:06.729
they can't, they don't have the self control to,

1029
00:53:06.739 --> 00:53:13.659
um, uh to, to um uh to, to, to,

1030
00:53:13.669 --> 00:53:15.659
to, to rein in their behavior when, when they

1031
00:53:15.669 --> 00:53:18.340
really need to. And um, so I gave a,

1032
00:53:18.350 --> 00:53:21.419
a talk, I give a uh a talk on

1033
00:53:21.429 --> 00:53:24.500
this effort, moralization thing was up on youtube. And

1034
00:53:24.510 --> 00:53:26.780
um, of course, I, I made the mistake of

1035
00:53:26.790 --> 00:53:29.800
reading the comments, um which were generally positive, but

1036
00:53:29.810 --> 00:53:33.840
then 111 comment which I can't forget was like

1037
00:53:34.250 --> 00:53:38.300
uh talking about. Um, YEAH, uh all this uh

1038
00:53:38.310 --> 00:53:41.280
effort important for self control. Uh You should know

1039
00:53:41.290 --> 00:53:44.090
that self control is also relating to uh hair

1040
00:53:44.100 --> 00:53:48.090
grooming, bro. It was like accusing me of being

1041
00:53:48.100 --> 00:53:51.919
unkempt. Uh WHICH I guess I am. Um And

1042
00:53:51.929 --> 00:53:53.370
it made me realize that even those types of

1043
00:53:53.379 --> 00:53:56.590
cues, right? Like being clean, shaven, well kept a

1044
00:53:56.600 --> 00:54:00.340
well groomed people. That's a, that's a another demonstration

1045
00:54:00.350 --> 00:54:05.139
of uh fastidiousness, discipline, uh self control, I guess.

1046
00:54:05.149 --> 00:54:07.209
Uh HARD work, I guess it's hard to, to

1047
00:54:07.219 --> 00:54:11.850
stay well groomed. Um All these cues, we constantly

1048
00:54:11.860 --> 00:54:14.520
surveying the social landscape to find these cues of

1049
00:54:14.530 --> 00:54:16.100
who's gonna be a dependable partner and then we

1050
00:54:16.110 --> 00:54:19.830
moralize everything. Uh, THAT, that is, is such a

1051
00:54:19.840 --> 00:54:20.260
queue.

1052
00:54:20.729 --> 00:54:23.199
We, well, perhaps that would be another reason why

1053
00:54:23.209 --> 00:54:27.449
so many people nowadays advertise on social media, they're

1054
00:54:27.459 --> 00:54:29.770
going to the gym, I guess.

1055
00:54:29.780 --> 00:54:33.050
Yes. Exactly. Yeah. That's right. It's a, it's a,

1056
00:54:33.060 --> 00:54:35.739
it's a great example of effort, moralization. It doesn't

1057
00:54:35.750 --> 00:54:39.389
help anybody. I guess it may be eases some

1058
00:54:39.399 --> 00:54:43.310
pressure on the health care system. But, uh, yeah,

1059
00:54:43.320 --> 00:54:45.449
I don't know, I think people respect people who

1060
00:54:45.459 --> 00:54:47.459
do that more, uh, for better or worse.

1061
00:54:48.379 --> 00:54:50.780
But, but when it comes to the more rational

1062
00:54:50.790 --> 00:54:54.659
aspects of this, I mean, is it actually rational

1063
00:54:54.669 --> 00:54:58.399
to value effort for efforts sake, even if it

1064
00:54:58.409 --> 00:55:02.629
is sometimes unproductive or, or at least doesn't reach,

1065
00:55:02.639 --> 00:55:05.580
doesn't help reaching the goals that we have in

1066
00:55:05.590 --> 00:55:06.000
mind.

1067
00:55:06.620 --> 00:55:08.260
Well, it depends on what you mean by those

1068
00:55:08.270 --> 00:55:13.419
goals. Right. So, so I think that if I

1069
00:55:13.429 --> 00:55:16.729
think that we are constantly, uh, whether we like

1070
00:55:16.739 --> 00:55:18.830
to think of it or, or, or, or not

1071
00:55:18.840 --> 00:55:22.580
because it seems inauthentic but we are constantly trying

1072
00:55:22.590 --> 00:55:25.260
to send out signals to other people, uh, to

1073
00:55:25.270 --> 00:55:27.139
show that we're the type of people that we

1074
00:55:28.219 --> 00:55:30.280
want to be wanted or something like that or,

1075
00:55:30.290 --> 00:55:33.419
or, or desired as, as good cooper partners, even

1076
00:55:33.429 --> 00:55:35.669
to our own friends and partners, we want to

1077
00:55:35.679 --> 00:55:39.439
show, uh, commitment, trustworthiness, all these kinds of things.

1078
00:55:39.770 --> 00:55:44.090
And so when we engage in this seemingly at

1079
00:55:44.100 --> 00:55:47.639
least economically unproductive effort. It is productive in this

1080
00:55:47.649 --> 00:55:49.919
other sphere. It is productive for signaling to other

1081
00:55:49.929 --> 00:55:53.189
people that we are uh reliable partners. Uh WHICH

1082
00:55:53.199 --> 00:55:55.110
I think is an important thing to do. I

1083
00:55:55.120 --> 00:55:57.860
think. So, in that sense, it does have whether

1084
00:55:57.870 --> 00:55:59.570
this is the reason that we do it uh

1085
00:55:59.679 --> 00:56:01.639
consciously or not, it does have that sort of

1086
00:56:01.649 --> 00:56:05.909
deep rationality to it. Um Yeah. Uh I think

1087
00:56:05.919 --> 00:56:11.129
one of the problems comes when it pops up

1088
00:56:11.139 --> 00:56:14.929
in uh contexts which are then removed from that

1089
00:56:14.939 --> 00:56:17.459
signal, right? So, because we venerate hard work so

1090
00:56:17.469 --> 00:56:25.050
much, we now construct an entire ll labor system

1091
00:56:25.340 --> 00:56:30.100
that rewards hard work uh Even when it's economically

1092
00:56:30.110 --> 00:56:35.120
unproductive. Um And it's not serving that interpersonal process.

1093
00:56:35.429 --> 00:56:38.370
So if a boss is, if we're trying to

1094
00:56:38.379 --> 00:56:40.310
show that we are the hardest working person in

1095
00:56:40.320 --> 00:56:44.790
the office, uh when that doesn't accomplish the interpersonal

1096
00:56:44.800 --> 00:56:47.260
benefits and it doesn't accomplish the economic goals of

1097
00:56:47.270 --> 00:56:50.800
the company. Uh Then it's just draining people's time

1098
00:56:50.810 --> 00:56:52.239
that could be spent doing other things.

1099
00:56:53.360 --> 00:56:56.899
A and so just one last question, does this

1100
00:56:56.909 --> 00:57:00.659
phenomenon of moralization of effort connect in any way

1101
00:57:00.669 --> 00:57:03.129
to support for social welfare?

1102
00:57:04.429 --> 00:57:07.320
So that's a good question. That's uh uh a

1103
00:57:07.330 --> 00:57:10.810
project that uh Jared, the, the lead author has

1104
00:57:10.820 --> 00:57:13.000
wanted to do. Uh We haven't gotten to it

1105
00:57:13.010 --> 00:57:18.159
yet. Um Right now, you, you see examples of

1106
00:57:18.169 --> 00:57:21.300
that all the time that people uh uh are

1107
00:57:21.310 --> 00:57:24.020
reticent to give social welfare when it's not connected

1108
00:57:24.030 --> 00:57:26.979
to work. Even if the work doesn't produce anything,

1109
00:57:26.989 --> 00:57:29.270
people want to see people work. They don't, they

1110
00:57:29.280 --> 00:57:32.500
get turned off by, uh, people not working because

1111
00:57:32.510 --> 00:57:34.590
they perceive it as laziness, which makes you not

1112
00:57:34.600 --> 00:57:36.530
respect the person, not like the person, not think

1113
00:57:36.540 --> 00:57:38.034
of the person, moral, not think of the person

1114
00:57:38.044 --> 00:57:41.104
is deserving. And as a result, probably you're less

1115
00:57:41.114 --> 00:57:44.375
willing to extend uh social welfare to them. So

1116
00:57:44.385 --> 00:57:47.135
again, back to those attributions that we talked about

1117
00:57:47.145 --> 00:57:51.094
earlier, those really matter because those determine uh what

1118
00:57:51.104 --> 00:57:54.254
you feel about the people uh to whom you

1119
00:57:54.264 --> 00:57:57.495
would be um either donating money to or, or

1120
00:57:57.504 --> 00:57:58.554
giving taxes for.

1121
00:57:59.300 --> 00:58:01.479
And I would imagine, I mean, this just came

1122
00:58:01.489 --> 00:58:03.810
to my mind that in the case, for example,

1123
00:58:03.820 --> 00:58:08.520
of homeless people, then there would be many social

1124
00:58:08.530 --> 00:58:11.939
signals in their case that would signal to people

1125
00:58:11.949 --> 00:58:14.360
that I they are not putting in the effort

1126
00:58:14.370 --> 00:58:17.110
because they are on the street. They are probably

1127
00:58:17.120 --> 00:58:21.590
not working, they are not groomed again that bit

1128
00:58:21.600 --> 00:58:23.659
from a few minutes ago. So

1129
00:58:24.070 --> 00:58:27.429
all of that. Yeah. And, and similarly, um you

1130
00:58:27.439 --> 00:58:29.979
know, they might have uh here in Vancouver a

1131
00:58:29.989 --> 00:58:33.120
lot of uh drug addiction problems. Uh So they

1132
00:58:33.129 --> 00:58:35.810
have that cue as well. Uh So all those

1133
00:58:35.820 --> 00:58:39.500
things which people might attribute to just being low

1134
00:58:39.510 --> 00:58:43.540
quality uh social partners, even if that person is

1135
00:58:43.550 --> 00:58:44.860
not going to be a social partner to you.

1136
00:58:44.870 --> 00:58:46.590
So that's a good example of a misfiring, right?

1137
00:58:46.600 --> 00:58:49.429
Because you then make all these attributions, the, the

1138
00:58:49.439 --> 00:58:51.030
person you don't like them, you see them as

1139
00:58:51.040 --> 00:58:55.629
immoral, uh, because they would be a bad social

1140
00:58:55.639 --> 00:58:57.959
partner, but you aren't evaluating them to be a

1141
00:58:57.969 --> 00:59:00.280
social partner. You're evaluating them as to whether they're

1142
00:59:00.290 --> 00:59:01.689
gonna be the recipient of help.

1143
00:59:02.429 --> 00:59:05.850
Mhm. Per, perhaps that's also why we evolved psychology

1144
00:59:05.860 --> 00:59:11.270
misfiring in our huge societies where we no longer

1145
00:59:11.280 --> 00:59:16.239
interact just with 150 people where we know everyone

1146
00:59:16.250 --> 00:59:16.770
around.

1147
00:59:16.969 --> 00:59:20.620
Right. Yeah, that's right. And, and um uh you

1148
00:59:20.629 --> 00:59:22.149
know, I've, I've written a couple of papers on

1149
00:59:22.159 --> 00:59:27.949
this idea that um the environmental change that we've

1150
00:59:27.959 --> 00:59:31.239
undergone in the last 20 years, moving from the

1151
00:59:31.250 --> 00:59:33.449
real world to a world that's more like this

1152
00:59:33.459 --> 00:59:35.729
uh where we look at screens, we interact with

1153
00:59:35.989 --> 00:59:40.409
many, many strangers, millions of people that environmental change

1154
00:59:40.419 --> 00:59:41.850
from the real world to a virtual world to

1155
00:59:41.860 --> 00:59:45.239
an online world is probably bigger than the environmental

1156
00:59:45.250 --> 00:59:48.610
change that we went through 10, 12,000 years ago

1157
00:59:48.620 --> 00:59:51.270
when we moved from being uh nomadic people to

1158
00:59:51.280 --> 00:59:54.189
sedentary city dwellers. That was a big change. It

1159
00:59:54.199 --> 00:59:57.629
had all these uh uh cultural evolutionary consequences. This

1160
00:59:57.639 --> 01:00:00.570
one might be actually bigger and happening way faster.

1161
01:00:01.919 --> 01:00:05.780
Great. So let's wrap up the interview here, Doctor

1162
01:00:05.959 --> 01:00:08.060
Chri. And uh do, would you like to tell

1163
01:00:08.070 --> 01:00:10.550
people where they can find you when you work

1164
01:00:10.560 --> 01:00:11.459
on the internet.

1165
01:00:11.979 --> 01:00:15.979
Oh, good question. Uh uh Google scholar. You can

1166
01:00:15.989 --> 01:00:18.570
find my work there. Uh Google scholar. Uh I'm

1167
01:00:18.580 --> 01:00:23.330
also on Twitter um at Azim Sharif. Uh Yeah.

1168
01:00:23.899 --> 01:00:26.379
Yeah. Oh, and I have a website but I

1169
01:00:26.389 --> 01:00:28.889
don't really update it very much. That's Shari lab.com.

1170
01:00:29.080 --> 01:00:32.580
Really Google scholar, research, Gator. Probably the places you

1171
01:00:32.590 --> 01:00:33.270
can get papers.

1172
01:00:33.600 --> 01:00:35.489
Yeah. And of course, I will also leave the

1173
01:00:35.500 --> 01:00:38.879
link to our first interview in the description of

1174
01:00:38.889 --> 01:00:41.540
this one. So thank you so much again for

1175
01:00:41.550 --> 01:00:43.090
taking the time to come on the show. It's

1176
01:00:43.100 --> 01:00:44.709
always a pleasure to talk with you.

1177
01:00:44.719 --> 01:00:46.260
Great to see you again. Take care.

1178
01:00:47.479 --> 01:00:50.209
Hi guys. Thank you for watching this interview. Until

1179
01:00:50.219 --> 01:00:52.379
the end. If you liked it, please share it.

1180
01:00:52.389 --> 01:00:55.189
Leave a like and hit the subscription button. The

1181
01:00:55.199 --> 01:00:56.850
show is brought to you by the N Lights

1182
01:00:56.860 --> 01:01:00.070
learning and development. Then differently check the website at

1183
01:01:00.080 --> 01:01:03.879
N lights.com and also please consider supporting the show

1184
01:01:03.919 --> 01:01:07.040
on Patreon or paypal. I would also like to

1185
01:01:07.050 --> 01:01:09.489
give a huge thank you to my main patrons

1186
01:01:09.500 --> 01:01:13.679
and paypal supporters, Perego Larson, Jerry Muller and Frederick

1187
01:01:13.760 --> 01:01:16.610
Suno Bernard Seche O of Alex Adam, Castle Matthew

1188
01:01:16.620 --> 01:01:19.879
Whitten bear. No wolf, Tim Ho Erica LJ Condors

1189
01:01:19.899 --> 01:01:22.669
Philip Forrest Connolly. Then the Met Robert Wine in

1190
01:01:22.679 --> 01:01:26.229
NAI Z Mark Nevs calling in Holbrook Field, Governor

1191
01:01:26.520 --> 01:01:30.389
Mikel Stormer, Samuel Andre Francis for Agns Ferus and

1192
01:01:30.719 --> 01:01:33.760
H her meal and Lain Jung Y and the

1193
01:01:33.840 --> 01:01:37.659
Samuel K. Hes Mark Smith J Tom Hummel s

1194
01:01:37.969 --> 01:01:41.389
friends, David Sloan Wilson. Ya dear, Roman Roach Diego

1195
01:01:41.879 --> 01:01:46.429
and Jan Punter Romani Charlotte, Bli Nico Barba, Adam

1196
01:01:46.439 --> 01:01:49.939
Hunt, Pavla Stassi na Me, Gary G Alman, Sam

1197
01:01:50.040 --> 01:01:54.679
Ofri and YPJ Barboa, Julian Price Edward Hall, Eden

1198
01:01:54.689 --> 01:01:58.899
Broner Douglas Fry Franca, Beto Lati Cortez or Solis

1199
01:01:59.919 --> 01:02:05.530
Scott Zachary FTD and W Daniel Friedman, William Buckner,

1200
01:02:05.540 --> 01:02:09.580
Paul Giorgio, Luke Loki, Georgio Theophano Chris Williams and

1201
01:02:09.590 --> 01:02:14.209
Peter Wo David Williams, the Ausa Anton Erickson Charles

1202
01:02:14.219 --> 01:02:19.520
Murray, Alex Shaw, Marie Martinez, Coralie Chevalier, Bangalore Larry

1203
01:02:19.530 --> 01:02:24.310
Dey junior, Old Ebon, Starry Michael Bailey. Then Spur

1204
01:02:24.320 --> 01:02:28.610
by Robert Grassy Zorn, Jeff mcmahon, Jake Zul Barnabas

1205
01:02:28.629 --> 01:02:32.330
Radick, Mark Kempel Thomas Dvor Luke Neeson, Chris Tory

1206
01:02:32.340 --> 01:02:36.560
Kimberley Johnson, Benjamin Gilbert Jessica. No week, Linda Brendan

1207
01:02:36.570 --> 01:02:42.050
Nicholas Carlson Ismael Bensley Man, George Katis, Valentine Steinman,

1208
01:02:42.060 --> 01:02:47.879
Perras, Kate Van Goler, Alexander, Abert Liam Dan Biar,

1209
01:02:48.209 --> 01:02:53.850
Masoud Ali Mohammadi Perpendicular Jer Urla. Good enough, Gregory

1210
01:02:53.860 --> 01:02:58.159
Hastings David Pins of Sean Nelson, Mike Levin and

1211
01:02:58.189 --> 01:03:01.435
Jos Net. A special thanks to my producers is

1212
01:03:01.445 --> 01:03:04.084
our web, Jim Frank Luca Stein, Tom Veg and

1213
01:03:04.094 --> 01:03:08.544
Bernard N Corti Dixon Bendik Muller Thomas Trumble Catherine

1214
01:03:08.554 --> 01:03:11.715
and Patrick Tobin John Carlman, Negro, Nick Ortiz and

1215
01:03:11.725 --> 01:03:15.004
Nick Golden. And to my executive producers, Matthew Lavender,

1216
01:03:15.135 --> 01:03:18.314
Si, Adrian Bogdan Knits and Rosie. Thank you for

1217
01:03:18.324 --> 01:03:18.635
all.

