WEBVTT

1
00:00:00.389 --> 00:00:03.259
Hello, everyone. Welcome to a new episode of the

2
00:00:03.259 --> 00:00:06.019
Center. I'm your host, Ricardo Lopes, and today I'm

3
00:00:06.019 --> 00:00:09.010
joined by Doctor Mona Simeon. She is professor of

4
00:00:09.010 --> 00:00:13.050
philosophy and director of the Cognitive epistemology Research Center

5
00:00:13.050 --> 00:00:16.340
at the University of Glasgow. And today we're talking

6
00:00:16.340 --> 00:00:20.709
about her book, Resistance to Evidence. So, Doctor Simeon,

7
00:00:20.819 --> 00:00:22.750
welcome to the show. It's a pleasure to everyone.

8
00:00:23.469 --> 00:00:24.750
Thanks so much for having me.

9
00:00:25.879 --> 00:00:29.670
So, let's start perhaps with a basic question here.

10
00:00:29.829 --> 00:00:33.590
So, could you start by telling us what resistance

11
00:00:33.590 --> 00:00:37.430
to evidence is, what it means and uh as

12
00:00:37.430 --> 00:00:40.319
a philosopher, why do you think we need the

13
00:00:40.319 --> 00:00:44.110
philosophical account of this sort of phenomenon?

14
00:00:45.159 --> 00:00:48.229
Yeah, so I mean, I think that what's important

15
00:00:48.229 --> 00:00:51.700
to note when you look in the landscape of

16
00:00:51.700 --> 00:00:56.669
people resisting scientific evidence is that epistemologically it's a

17
00:00:56.669 --> 00:01:00.110
very complicated landscape. So it's not the case that

18
00:01:00.110 --> 00:01:03.310
everybody who resists scientific evidence is crazy, I guess

19
00:01:03.310 --> 00:01:06.620
is what I'm saying. Um, SOME people, of course,

20
00:01:06.660 --> 00:01:09.080
are doing it for, for no good reason. Um,

21
00:01:09.419 --> 00:01:12.370
SOME people are doing it for, for bad reasons,

22
00:01:12.379 --> 00:01:15.120
like, you know, from motivated reasoning. They just don't

23
00:01:15.120 --> 00:01:17.239
like to believe that stuff, so then they don't.

24
00:01:17.739 --> 00:01:19.540
Um, BUT some people, as a matter of fact,

25
00:01:19.660 --> 00:01:22.699
are in a much more complicated evidential situation where

26
00:01:22.699 --> 00:01:25.559
they find themselves not knowing exactly what to believe

27
00:01:25.559 --> 00:01:28.019
because they get, they get contrary evidence from different

28
00:01:28.019 --> 00:01:31.224
sources. So depending on what on your kind of

29
00:01:31.224 --> 00:01:35.665
evidential environment, what, what evidence you have available to

30
00:01:35.665 --> 00:01:38.824
you, what you know, your environment tells you is

31
00:01:38.824 --> 00:01:41.175
the case, and, and so on, it may well

32
00:01:41.175 --> 00:01:46.264
be that uh your rejection of scientific evidence is

33
00:01:46.264 --> 00:01:49.904
not irrational evidence resistance, but this is a distinction

34
00:01:49.904 --> 00:01:53.169
that is very often not made in. Uh, YOU

35
00:01:53.169 --> 00:01:56.050
know, in mass media, uh, for instance, but also

36
00:01:56.050 --> 00:01:58.680
not in research. So for instance, in social psychology,

37
00:01:59.050 --> 00:02:01.569
the assumption has always been that there's gonna be

38
00:02:01.569 --> 00:02:05.559
some, um, source of irrationality that explains this phenomenon.

39
00:02:05.730 --> 00:02:09.440
So that's why I think a careful epistemological investigation

40
00:02:09.440 --> 00:02:11.970
that is able to draw the relevant distinctions between

41
00:02:11.970 --> 00:02:16.460
rational and irrational rejection of. Evidence is important. The

42
00:02:16.460 --> 00:02:19.339
way, the way I use this terminology in the

43
00:02:19.339 --> 00:02:21.860
book in order to, uh, you know, to, to

44
00:02:21.860 --> 00:02:25.740
make the distinction clear, I call evidence resistance, uh,

45
00:02:25.779 --> 00:02:30.059
irrational resistance to evidence, and I call cases in

46
00:02:30.059 --> 00:02:33.009
which you reject evidence, uh, for good reasons, just

47
00:02:33.009 --> 00:02:35.070
evidence rejections in order to signal that these two

48
00:02:35.070 --> 00:02:36.289
are very different phenomena.

49
00:02:37.119 --> 00:02:40.139
Mhm. Uh, BUT, uh, again, uh, the second part

50
00:02:40.139 --> 00:02:42.229
of my question that I don't think you would

51
00:02:42.229 --> 00:02:45.350
rest directly, why do you, do we need the

52
00:02:45.350 --> 00:02:48.309
philosophical account of it? I mean, we could just

53
00:02:48.309 --> 00:02:51.860
go with, uh, for example, a psychological account or

54
00:02:52.190 --> 00:02:53.119
something like that.

55
00:02:53.570 --> 00:02:57.710
Psychological, social psychological accounts are descriptive accounts. So these

56
00:02:57.710 --> 00:03:01.350
are, this kind of important epistemological distinctions are normative

57
00:03:01.350 --> 00:03:05.029
distinctions, right? So. Psychology doesn't know how to do

58
00:03:05.029 --> 00:03:07.289
normativity. That's the kind of thing that philosophers know

59
00:03:07.289 --> 00:03:09.289
how to do. So what they know how to

60
00:03:09.289 --> 00:03:11.490
do is to describe what they see on the

61
00:03:11.490 --> 00:03:14.559
ground. Uh, WHAT they, what they struggle to do

62
00:03:14.559 --> 00:03:17.610
is to see the normative distinctions between two phenomena

63
00:03:17.610 --> 00:03:20.410
that look exactly the same descriptively, but are normatively

64
00:03:20.410 --> 00:03:22.970
different. So the example that I was giving you

65
00:03:22.970 --> 00:03:28.160
was with, um, evidence, uh, irrational versus rational evidence,

66
00:03:29.009 --> 00:03:32.509
right? So that is a. THAT is normative. RATIONALITY

67
00:03:32.509 --> 00:03:35.110
is a normative phenomenon. If you just look at

68
00:03:35.110 --> 00:03:37.110
what people do on the ground, you're not going

69
00:03:37.110 --> 00:03:39.669
to be able as a psychologist to know whether

70
00:03:39.669 --> 00:03:43.100
what they're doing is uh rational or irrational, um,

71
00:03:43.179 --> 00:03:45.270
and this is exactly what we see when we

72
00:03:45.270 --> 00:03:47.949
look at social psychological studies of this phenomenon. So

73
00:03:47.949 --> 00:03:51.309
just to give you one example, um, the, the

74
00:03:51.309 --> 00:03:55.509
most kind of popular or at least hot hypothesis

75
00:03:55.509 --> 00:03:58.850
in social psychology is that. Uh, THIS phenomenon is

76
00:03:58.850 --> 00:04:03.289
vastly generated by politically motivated reasoning that people reject

77
00:04:03.289 --> 00:04:07.050
scientific evidence when it suits them because, uh, to

78
00:04:07.050 --> 00:04:09.449
do so because it doesn't fit nicely with their

79
00:04:09.449 --> 00:04:12.089
political identity. So you know, if you're a far

80
00:04:12.089 --> 00:04:15.250
right person and part of your political identity is

81
00:04:15.250 --> 00:04:17.608
to reject that climate change is happening, but to

82
00:04:17.608 --> 00:04:20.260
accept that guns are safe, uh, no matter what

83
00:04:20.260 --> 00:04:22.290
evidence you bring, the thought is these people are

84
00:04:22.290 --> 00:04:26.049
going to reject it because. Of motivated reasoning. But

85
00:04:26.049 --> 00:04:28.329
if you look carefully at the experimental setup, you

86
00:04:28.329 --> 00:04:31.089
know, you notice that actually the claim is not

87
00:04:31.089 --> 00:04:34.010
supported by, by the data on the ground in

88
00:04:34.010 --> 00:04:37.660
that uh in these people that are tested, they

89
00:04:37.660 --> 00:04:41.179
uh. They do have the relevant political identity, but

90
00:04:41.179 --> 00:04:45.500
that is also correlated with, uh, reasons with, with

91
00:04:45.500 --> 00:04:50.940
a vast amount of evidence that background evidence that

92
00:04:50.940 --> 00:04:54.940
supports uh the relevant uh claim, right? So one

93
00:04:54.940 --> 00:04:57.779
example is if you grew up in, um, you

94
00:04:57.779 --> 00:05:00.500
know, in an American state where you've been told

95
00:05:00.500 --> 00:05:03.299
since you were small that all kinds of reasons

96
00:05:03.299 --> 00:05:05.619
to believe that guns are important for your safety,

97
00:05:05.630 --> 00:05:07.989
you're gonna have. A lot of testimonial evidence that

98
00:05:07.989 --> 00:05:09.700
guns are safe in a way in which me

99
00:05:09.700 --> 00:05:11.290
in Glasgow, I'm not gonna have any of that,

100
00:05:11.459 --> 00:05:14.059
right? So basically the way in which we're gonna

101
00:05:14.059 --> 00:05:16.320
deal with evidence having to do with gun safety

102
00:05:16.320 --> 00:05:19.299
is going to be rationally so very different because

103
00:05:19.299 --> 00:05:21.899
we have different background, uh, evidence. And this is

104
00:05:21.899 --> 00:05:24.410
the kind of stuff that social psychologists cannot, uh,

105
00:05:24.420 --> 00:05:27.859
capture in their experimental setting without collaborating with an

106
00:05:27.859 --> 00:05:31.579
epistemologist who understands the difference, um, between the two

107
00:05:31.779 --> 00:05:33.779
types of evidential settings, basically.

108
00:05:35.049 --> 00:05:37.589
So in regards to this type of phenomenon, I've

109
00:05:37.589 --> 00:05:40.910
had on the show people like Hugo Mercier and

110
00:05:40.910 --> 00:05:44.350
then Sperber, and they do work on the phenomenon

111
00:05:44.350 --> 00:05:48.299
that they call epistemic vigilance. Does that relate in

112
00:05:48.299 --> 00:05:51.420
any way to resistance to evidence or not?

113
00:05:51.730 --> 00:05:54.880
Yeah, so in the book, I consider whether, uh,

114
00:05:55.119 --> 00:05:57.850
that might be one way to explain what's going

115
00:05:57.850 --> 00:05:59.959
on. So here is how that might, uh, that

116
00:05:59.959 --> 00:06:03.079
might explain it. So what, what, uh, um, Mercier

117
00:06:03.079 --> 00:06:08.100
calls, um, epistemic, uh, and Sperberg called epistemic vigilance,

118
00:06:08.480 --> 00:06:11.799
um, they is basically this property that they stipulate

119
00:06:11.799 --> 00:06:14.880
we have, um, and they, they think that. The

120
00:06:14.880 --> 00:06:17.850
fact that, you know, that the stipulation is plausible

121
00:06:17.850 --> 00:06:21.519
is supported by all kinds of phenomena, uh, whereby

122
00:06:21.519 --> 00:06:25.880
we are good at filtering, they think, uh, you

123
00:06:25.880 --> 00:06:28.600
know, bad from good testifiers, for instance, so that

124
00:06:28.600 --> 00:06:31.230
we have, we have this kind of mechanism that

125
00:06:31.230 --> 00:06:34.410
kind of filter, um, for defeaters as it were.

126
00:06:34.440 --> 00:06:36.899
So for evidence that our testifier is not trustworthy,

127
00:06:37.040 --> 00:06:40.220
right? And they think that that's what explains in

128
00:06:40.220 --> 00:06:42.339
a way our success as a species because of

129
00:06:42.339 --> 00:06:44.339
course we rely a lot on each other on

130
00:06:44.339 --> 00:06:46.980
each other's testimony for the knowledge that we have.

131
00:06:47.059 --> 00:06:49.350
So if we, their hypothesis is that, you know,

132
00:06:49.459 --> 00:06:51.899
if we didn't have this vigilance mechanism, it is

133
00:06:51.899 --> 00:06:54.339
a mystery how we would have succeeded as a

134
00:06:54.339 --> 00:06:55.940
species given how much we need to rely on

135
00:06:55.940 --> 00:06:59.690
each other. Uh, SO the, if you, if you

136
00:06:59.690 --> 00:07:03.119
think this, uh, you know, faculty of epistemic vigilance

137
00:07:03.119 --> 00:07:05.570
exists, one way that you might try to explain

138
00:07:05.570 --> 00:07:08.140
what's going on is to say, well, look, uh,

139
00:07:08.149 --> 00:07:11.730
our, uh, cognitive capacities have evolved in a very

140
00:07:11.730 --> 00:07:14.769
different information environment than the one that we inhabit

141
00:07:14.769 --> 00:07:18.890
right now, right? So they, they aren't evolved to

142
00:07:18.890 --> 00:07:22.399
deal with this kind of high density, high choice

143
00:07:22.399 --> 00:07:24.730
information environment that we are now inhabiting because of

144
00:07:24.730 --> 00:07:29.230
the internet, right? Um, BECAUSE of that, they might

145
00:07:29.230 --> 00:07:33.149
be, uh, malfunctioning, which is to be expected, uh,

146
00:07:33.190 --> 00:07:35.510
you know, for any functional item that is moved

147
00:07:35.510 --> 00:07:38.790
out of its normal conditions into abnormal conditions. So

148
00:07:38.790 --> 00:07:41.429
you might think, well, we have this mechanisms for

149
00:07:41.429 --> 00:07:46.029
epistemic vigilance, but those are working quite well in

150
00:07:46.029 --> 00:07:48.829
a normal environment where our testifiers are like, you

151
00:07:48.829 --> 00:07:51.109
know, people that we meet in person and we

152
00:07:51.109 --> 00:07:54.260
can read maybe signs of this. ON their face,

153
00:07:54.600 --> 00:07:56.750
uh, and we know whom to trust and who

154
00:07:56.750 --> 00:07:59.279
not to trust because they have a particular social

155
00:07:59.279 --> 00:08:02.040
profile that we're aware of. Uh, SO it's much

156
00:08:02.040 --> 00:08:04.809
easier, as it were, to be vigilant, um, if,

157
00:08:04.850 --> 00:08:07.600
uh, your testifiers are all kind of flesh and

158
00:08:07.600 --> 00:08:09.279
blood people that you meet in the street that

159
00:08:09.279 --> 00:08:11.390
you know about from your neighbors and so on.

160
00:08:11.600 --> 00:08:15.160
It's much harder to be vigilant online where everything

161
00:08:15.160 --> 00:08:18.350
is more or less anonymous, right? And the amount

162
00:08:18.350 --> 00:08:20.350
of information coming at you is also hard to

163
00:08:20.350 --> 00:08:23.390
process, right? So it's much easier to process one

164
00:08:23.390 --> 00:08:26.549
or two testifiers uh every minute as you walk

165
00:08:26.549 --> 00:08:28.510
down the street than the amount of stuff that

166
00:08:28.510 --> 00:08:31.109
comes at you on the internet. So maybe what's

167
00:08:31.109 --> 00:08:33.429
going on is that people don't know what to

168
00:08:33.429 --> 00:08:36.020
believe anymore. So this kind of epistemic vigilance faculties

169
00:08:36.020 --> 00:08:39.830
are misfiring, right? So they're, they're being, they're making

170
00:08:39.830 --> 00:08:42.479
you skeptic when you shouldn't be because they just

171
00:08:42.479 --> 00:08:45.429
don't know how to fire well anymore since they

172
00:08:45.429 --> 00:08:47.640
haven't evolved to deal with this kind of craziness.

173
00:08:47.909 --> 00:08:50.469
So that's what, that's one explanation, uh, that you

174
00:08:50.469 --> 00:08:53.590
can give. Um, THE worry that I, that I

175
00:08:53.590 --> 00:08:55.179
have, I mean, I have two worries for this

176
00:08:55.179 --> 00:08:59.830
explanation. One is that again, it stipulates that um.

177
00:09:00.479 --> 00:09:03.830
That everybody who does it is malfunctioning. Everybody who

178
00:09:03.830 --> 00:09:09.059
rejects scientific evidence is malfunctioning, right? And that, uh,

179
00:09:09.309 --> 00:09:13.150
that implies some widespread irrationality assumption which is very

180
00:09:13.150 --> 00:09:16.390
implausible. Uh, WE are a very successful species, so

181
00:09:16.390 --> 00:09:18.869
to think. I think given how well we're doing

182
00:09:18.869 --> 00:09:22.270
in the world, to think that our cognitive mechanisms

183
00:09:22.270 --> 00:09:24.229
are failing in this way on such a large

184
00:09:24.229 --> 00:09:28.030
scale would be very surprising, right? Given how, how

185
00:09:28.030 --> 00:09:31.190
much epistemic, uh, you know, resource we need in

186
00:09:31.190 --> 00:09:33.950
order to be so successful, right? And but the

187
00:09:33.950 --> 00:09:37.270
more important problem is that um a lot of

188
00:09:37.270 --> 00:09:40.340
psychological studies suggest that actually we're very bad at

189
00:09:40.340 --> 00:09:44.409
what they call vigilance. That even in normal settings

190
00:09:44.409 --> 00:09:47.330
with our, you know, friends and neighbors, we are

191
00:09:47.330 --> 00:09:52.520
extremely bad at detecting deception. Indeed we are basically

192
00:09:52.809 --> 00:09:54.729
just a bit above average, so it's a, it's

193
00:09:54.729 --> 00:09:57.710
a coin toss whether we detect deception or not.

194
00:09:58.010 --> 00:10:00.400
So this is, we are, we think of ourselves

195
00:10:00.400 --> 00:10:02.809
as being much better at detecting when someone is

196
00:10:02.809 --> 00:10:05.330
lying or not and so on than we actually

197
00:10:05.330 --> 00:10:08.770
are, as it turns out. Do is that always

198
00:10:08.770 --> 00:10:11.250
true? No, we are much better at detecting deception

199
00:10:11.250 --> 00:10:13.619
in contexts where we have a lot of information.

200
00:10:13.770 --> 00:10:16.369
So when we know this person really well, maybe

201
00:10:16.369 --> 00:10:18.489
finally we start getting used to the ways in

202
00:10:18.489 --> 00:10:22.400
which they behave when, when they lie, or when

203
00:10:22.400 --> 00:10:25.919
we have all kinds of other contextual contextual information

204
00:10:25.919 --> 00:10:28.130
like, you know, we know that there's something fishy

205
00:10:28.130 --> 00:10:31.330
about this person, we know that whatever they say

206
00:10:31.330 --> 00:10:33.760
doesn't fit with whatever this other person says. Uh,

207
00:10:34.140 --> 00:10:36.419
SO then we, we are good at detecting deception,

208
00:10:36.460 --> 00:10:38.179
but that is to be expected, of course, because

209
00:10:38.179 --> 00:10:41.099
that again is explained by the background evidence that

210
00:10:41.099 --> 00:10:44.179
we have. So basically, given these two worries, I'm

211
00:10:44.179 --> 00:10:46.979
thinking that the explanation in terms of our vigilance

212
00:10:46.979 --> 00:10:50.419
mechanisms failing, uh, I think is not likely to

213
00:10:50.419 --> 00:10:51.780
be the best explanation of the data.

214
00:10:52.609 --> 00:10:54.890
So just to ask you briefly about the kind

215
00:10:54.890 --> 00:10:57.570
of approach that you bring into your book when

216
00:10:57.570 --> 00:11:00.440
it comes to resistance to evidence, at a certain

217
00:11:00.440 --> 00:11:04.760
point, you say that you approach it through positive

218
00:11:04.760 --> 00:11:09.380
epistemology instead of negative epistemology. What does that mean

219
00:11:09.380 --> 00:11:13.650
exactly? So it's, it's a term that I borrow

220
00:11:13.650 --> 00:11:17.320
and I, I'm grateful for from um uh Jonathan

221
00:11:17.320 --> 00:11:22.380
Jenkins Ichikawa. So, so that. The tradition in epistemology

222
00:11:22.380 --> 00:11:25.150
for 2000 years is very surprising to me when

223
00:11:25.150 --> 00:11:28.469
you think about it, um, has been very much

224
00:11:28.469 --> 00:11:33.150
focused on epistemic permissions rather than epistemic obligations, which

225
00:11:33.150 --> 00:11:35.299
is not the case in other normative fields, right?

226
00:11:35.309 --> 00:11:37.150
In, in morality, for instance, we care a lot

227
00:11:37.150 --> 00:11:40.500
about obligations, right? So that's very strange that there

228
00:11:40.500 --> 00:11:42.950
we would have a normative field where it, where

229
00:11:42.950 --> 00:11:45.429
we don't discuss obligation, we only discuss permission for

230
00:11:45.429 --> 00:11:47.309
2000 years and you might wonder why that's the

231
00:11:47.309 --> 00:11:51.609
case. So, so here's what happened. People throughout history

232
00:11:51.609 --> 00:11:57.690
of epistemology, very clever people, assumed that believing is

233
00:11:57.690 --> 00:12:01.090
risky. So, you know, forming a belief is like

234
00:12:01.090 --> 00:12:04.369
jumping, right? So like you're taking a risk, and

235
00:12:04.369 --> 00:12:08.109
then they kept asking the question, when is it

236
00:12:08.109 --> 00:12:09.770
safe enough to take the jump, as it were,

237
00:12:09.849 --> 00:12:12.169
when do you have enough evidence? When are you

238
00:12:12.169 --> 00:12:16.049
justified enough to take this jump into believing? And

239
00:12:16.049 --> 00:12:17.969
you can see why, why people might think in

240
00:12:17.969 --> 00:12:20.140
this way, prima facie, because you might think, well,

241
00:12:20.229 --> 00:12:21.789
as soon as you believe you do a lot

242
00:12:21.789 --> 00:12:24.510
of stuff with that belief, right? You start asserting

243
00:12:24.510 --> 00:12:26.909
it, you start acting on it. So that's why

244
00:12:26.909 --> 00:12:30.580
you should be really careful before you jump, uh,

245
00:12:30.590 --> 00:12:33.309
the before for the belief. So this, this kind

246
00:12:33.309 --> 00:12:37.900
of, uh, assumption led people to focus on, uh,

247
00:12:37.909 --> 00:12:40.669
when belief is permissible, but not on when belief

248
00:12:40.669 --> 00:12:44.890
might be obligatory given your evidential, uh, situation. Um,

249
00:12:45.099 --> 00:12:47.369
SO for the, so in that sense it's a,

250
00:12:47.659 --> 00:12:50.299
it's been a, you know, 2000 year history of

251
00:12:50.299 --> 00:12:55.260
negative epistemology in that people have debated what restrictions

252
00:12:55.260 --> 00:12:58.059
we should put on our believing, right, on our

253
00:12:58.059 --> 00:13:00.809
forming beliefs. How should we restricted better such that

254
00:13:01.020 --> 00:13:04.650
when we finally, uh, jump, it's safe to jump,

255
00:13:04.780 --> 00:13:08.340
as it were. Um, I think that the, the

256
00:13:08.340 --> 00:13:11.380
most recent phenomena having to do with evidence rejection

257
00:13:11.380 --> 00:13:14.090
pointed out to us that we were wrong, that,

258
00:13:14.289 --> 00:13:17.659
uh, the, you know, to only focus on problems

259
00:13:17.659 --> 00:13:20.500
arising from jumping too quick, as it were. There

260
00:13:20.500 --> 00:13:23.179
also problems arising for not jumping when you can

261
00:13:23.179 --> 00:13:26.580
jump, uh, right? And that is, that is exactly

262
00:13:26.580 --> 00:13:30.729
what, you know, phenomena like vaccine. Um, SKEPTICISM and

263
00:13:30.729 --> 00:13:33.840
climate change skepticism has shown to us that sometimes

264
00:13:33.840 --> 00:13:36.489
you should form your belief and go ahead and

265
00:13:36.489 --> 00:13:38.969
act, go ahead and and get the vaccine because

266
00:13:38.969 --> 00:13:41.890
you might die if you don't. Uh, SO, so

267
00:13:41.890 --> 00:13:44.530
not forming the belief is hardly the safe option.

268
00:13:44.580 --> 00:13:47.729
It's not safe, safer epistemically, it is not safer

269
00:13:47.729 --> 00:13:50.489
practically, uh, your, your health will not benefit for

270
00:13:50.489 --> 00:13:53.409
it. Uh, AND the same with, with, with climate

271
00:13:53.409 --> 00:13:56.260
change denialism, right? So I think that very recent

272
00:13:56.260 --> 00:14:00.090
phenomena in the landscape have kind of shown us

273
00:14:00.090 --> 00:14:02.940
that that it was a mistake all along to

274
00:14:02.940 --> 00:14:06.760
only focus on epistemic permissions rather than um obligations.

275
00:14:06.979 --> 00:14:08.780
I also think that we have, you know, that

276
00:14:08.780 --> 00:14:13.859
what this phenomena have helped us, theoreticians see is

277
00:14:13.859 --> 00:14:17.460
that this was, this was completely unwarranted, this, this

278
00:14:17.460 --> 00:14:20.523
focus on. Negative epistemology to begin with, because, uh,

279
00:14:20.713 --> 00:14:22.232
when you don't form a belief, you form a

280
00:14:22.232 --> 00:14:25.112
suspension, normally. That's what you do. You suspend belief,

281
00:14:25.192 --> 00:14:28.353
right? A suspension is also a doxastic attitude, just

282
00:14:28.353 --> 00:14:31.333
like belief. It's a kind of doxtastic attitude, but

283
00:14:31.333 --> 00:14:34.143
it's still a doxastic attitude. So the assumption that

284
00:14:34.632 --> 00:14:38.033
one particular doxasic attitude comes for free, that you,

285
00:14:38.302 --> 00:14:40.752
that you, you don't need any evidence for it,

286
00:14:40.793 --> 00:14:44.085
that Doesn't need to be properly justified. That was,

287
00:14:44.245 --> 00:14:46.666
if you think about it, completely crazy all along.

288
00:14:46.806 --> 00:14:48.765
Of course, a suspension is going to be a

289
00:14:48.765 --> 00:14:51.406
good suspension, a permissible suspension, only if it fits

290
00:14:51.406 --> 00:14:54.556
your evidence properly, just like belief. So I guess,

291
00:14:54.846 --> 00:14:57.445
you know, it's one of those very interesting cases

292
00:14:57.445 --> 00:15:01.315
in which the real world has informed the theory,

293
00:15:01.526 --> 00:15:03.885
um, and has made it better for it, I

294
00:15:03.885 --> 00:15:04.205
think.

295
00:15:05.210 --> 00:15:07.429
So, we're also going to talk a little bit

296
00:15:07.429 --> 00:15:12.590
about uh suspending uh judgments or suspending beliefs later

297
00:15:12.590 --> 00:15:15.349
on in our conversation. But just before that, I

298
00:15:15.349 --> 00:15:18.109
guess that there's also another important question to address

299
00:15:18.109 --> 00:15:22.700
here. So, what is evidence? Particularly from a philosophical

300
00:15:22.700 --> 00:15:25.590
perspective, what are some of the most common accounts

301
00:15:25.590 --> 00:15:29.179
of evidence out there and how do you suggest

302
00:15:29.179 --> 00:15:30.349
we should approach it?

303
00:15:31.190 --> 00:15:33.940
Yeah, thanks for the question. So in this, this

304
00:15:33.940 --> 00:15:36.739
project, this book was initially, it's an old project,

305
00:15:36.780 --> 00:15:40.190
it hasn't started quite yesterday. The, the project started

306
00:15:40.190 --> 00:15:43.809
in a way before the phenomenon of evidence resistance

307
00:15:43.809 --> 00:15:46.789
became such a, you know, well known problem, uh,

308
00:15:46.799 --> 00:15:50.099
publicly. Um, I was just trying to develop a

309
00:15:50.099 --> 00:15:53.210
better account of evidence. This was a project about

310
00:15:53.210 --> 00:15:55.780
evidence. That's all there was to it. And the

311
00:15:55.780 --> 00:15:57.239
reason why I was trying to develop a better

312
00:15:57.239 --> 00:15:59.400
account was because I wasn't satisfied with the accounts

313
00:15:59.400 --> 00:16:02.119
that were available on the market. So of course

314
00:16:02.119 --> 00:16:04.469
people in philosophy disagree about everything, so there's not,

315
00:16:04.679 --> 00:16:06.280
there's not a lot of stuff that people agree

316
00:16:06.280 --> 00:16:08.469
on. But here is something that you find with

317
00:16:08.469 --> 00:16:12.400
very few exceptions throughout history of epistemology. Um, WHEN

318
00:16:12.400 --> 00:16:14.960
it comes to what, what it is for someone

319
00:16:14.960 --> 00:16:18.440
to have evidence, people tend to think that that

320
00:16:18.440 --> 00:16:21.479
having of evidence relation refers to, in one way

321
00:16:21.479 --> 00:16:25.059
or another, the evidence being in your head. So

322
00:16:25.059 --> 00:16:27.580
that's what people believe in epistemology if you look

323
00:16:27.580 --> 00:16:30.330
throughout history, that it's either you have the evidence

324
00:16:30.330 --> 00:16:33.099
if, for instance, you believe it. So you, I

325
00:16:33.099 --> 00:16:34.659
have evidence that there's a computer in front of

326
00:16:34.659 --> 00:16:36.179
me because I believe that there's a computer in

327
00:16:36.179 --> 00:16:39.289
front of me, or because it's, I'm justified in

328
00:16:39.299 --> 00:16:41.210
in my, I have a belief and it's justified,

329
00:16:41.219 --> 00:16:43.419
or because I know it. I have evidence that

330
00:16:43.419 --> 00:16:44.900
there's a computer in front of me because I

331
00:16:44.900 --> 00:16:46.299
know that there's a computer in front of me

332
00:16:46.299 --> 00:16:49.940
and so on. Um. Or, uh, so these are,

333
00:16:50.289 --> 00:16:52.530
you know, some doxastic accounts, but you can also

334
00:16:52.530 --> 00:16:55.080
have an account that's not doxastic, doesn't imply belief,

335
00:16:55.250 --> 00:16:58.929
where your evidence are your seemings. So whatever hits

336
00:16:58.929 --> 00:17:01.330
my eye and generates a seeming as of a

337
00:17:01.330 --> 00:17:03.330
laptop in me, that's my evidence that there's a

338
00:17:03.330 --> 00:17:06.300
laptop on the table. Um, THIS is, you know,

339
00:17:06.459 --> 00:17:08.500
it's not about having beliefs, but it's still in

340
00:17:08.500 --> 00:17:10.540
the head, right? So the, the assumption is always

341
00:17:10.540 --> 00:17:13.060
that as soon as you have the evidence, it's

342
00:17:13.060 --> 00:17:15.500
got to be that it lies somewhere within your

343
00:17:15.500 --> 00:17:18.959
skull, as it were, right? And that's an assumptions,

344
00:17:19.380 --> 00:17:22.140
interestingly, that's shared by, by camps that are quite

345
00:17:22.140 --> 00:17:26.020
opposite on all other fronts in epistemology. Internalism, externalists

346
00:17:26.020 --> 00:17:29.369
share this assumption for the most part. Um, NOW,

347
00:17:29.430 --> 00:17:32.630
here's what I found very strange about this assumption.

348
00:17:32.650 --> 00:17:35.500
It doesn't fit at all with our folk conception

349
00:17:35.500 --> 00:17:38.229
of having evidence. So if I talk to my

350
00:17:38.229 --> 00:17:42.670
grandmother tomorrow. And she said, well, she said something

351
00:17:42.670 --> 00:17:45.310
like, Well, why didn't you buy any carrots at

352
00:17:45.310 --> 00:17:46.910
the market? AS I asked you to. And I

353
00:17:46.910 --> 00:17:49.469
said, Well, I didn't have any evidence that there

354
00:17:49.469 --> 00:17:51.510
were carrots at the market. And she would go

355
00:17:51.510 --> 00:17:53.670
like, Didn't you see the carrots on the table

356
00:17:53.670 --> 00:17:57.349
at the market? Well, no, because, uh, you know,

357
00:17:57.589 --> 00:17:59.630
I, I just couldn't believe my eyes that there

358
00:17:59.630 --> 00:18:01.910
were carrots at the market. So since evidence is

359
00:18:01.910 --> 00:18:03.729
belief, I didn't have any evidence. That there are

360
00:18:03.729 --> 00:18:05.609
carrots at the market. So I don't think my

361
00:18:05.609 --> 00:18:08.369
grandma would be very, very, you know, satisfied with

362
00:18:08.369 --> 00:18:11.329
this explanation. And now living my grandma's side, it

363
00:18:11.329 --> 00:18:13.729
looks like even in legal context, we don't use

364
00:18:13.729 --> 00:18:16.010
evidence in that way, right? So now imagine a

365
00:18:16.010 --> 00:18:19.760
detective who's in the, who's taking testimony on the

366
00:18:19.760 --> 00:18:22.400
stand and the judge asked them, Did you have

367
00:18:22.400 --> 00:18:25.010
evidence that the butler killed the victim? And the

368
00:18:25.010 --> 00:18:27.900
detective said, No, I didn't have any. Well, how

369
00:18:27.900 --> 00:18:29.800
do you mean? Didn't you go to the, to

370
00:18:29.800 --> 00:18:33.119
the, you know, crime scene? Didn't you see all

371
00:18:33.119 --> 00:18:35.599
this stuff that suggested that the, the butler did

372
00:18:35.599 --> 00:18:38.319
it? Uh, WELL, I, I saw it, but I

373
00:18:38.319 --> 00:18:40.079
couldn't believe my eyes because the butler is such

374
00:18:40.079 --> 00:18:41.560
a good friend of mine, and I couldn't believe

375
00:18:41.560 --> 00:18:42.760
my eyes that he would do such a thing.

376
00:18:42.839 --> 00:18:44.380
Again, I don't think that that's. When it would

377
00:18:44.380 --> 00:18:47.060
be acceptable in court, so it does look as

378
00:18:47.060 --> 00:18:51.020
though, you know, for instance, accounts that um that

379
00:18:51.020 --> 00:18:53.979
take uh having evidence to having have to have

380
00:18:53.979 --> 00:18:57.260
to do something with believing it, um, depart quite

381
00:18:57.260 --> 00:19:02.130
abruptly from the folk, uh, conception, right? Um, AND

382
00:19:02.130 --> 00:19:04.569
when it comes to seemings, the problem doesn't disappear

383
00:19:04.569 --> 00:19:07.599
either. So we know for instance that our seemings

384
00:19:07.599 --> 00:19:10.530
tend to be penetrated by all kinds of biases.

385
00:19:10.650 --> 00:19:13.010
So you know, we perceive black faces as being

386
00:19:13.010 --> 00:19:15.369
more dangerous than white faces, for instance. So we

387
00:19:15.369 --> 00:19:17.849
get those kinds of seemings that are only sourced

388
00:19:17.849 --> 00:19:20.089
in our bias. They're not justified in any way.

389
00:19:20.569 --> 00:19:23.130
Um, SO one result that we don't want is

390
00:19:23.130 --> 00:19:25.109
to say that as soon as you're biased, your

391
00:19:25.109 --> 00:19:27.489
evidence, you have evidence that black people are, are

392
00:19:27.489 --> 00:19:29.890
more dangerous than white people because that's clearly not

393
00:19:29.890 --> 00:19:31.569
the result that you want, right? You need to

394
00:19:31.569 --> 00:19:33.369
go back to the drawing board if that's the

395
00:19:33.369 --> 00:19:35.800
result that your theory gets. So because of this

396
00:19:35.800 --> 00:19:39.250
dissatisfaction that I had, uh, with extant extant accounts

397
00:19:39.250 --> 00:19:41.569
of evidence, I thought, look, we need an account

398
00:19:41.569 --> 00:19:44.300
that that doesn't place having evidence in your head.

399
00:19:44.380 --> 00:19:47.910
Clearly having the evidence doesn't have to. Uh, THAT

400
00:19:47.910 --> 00:19:49.790
you have the evidence in your head is just

401
00:19:49.790 --> 00:19:52.349
something having to do with availability, right? If it's

402
00:19:52.349 --> 00:19:54.910
very readily available to you, you have it. If

403
00:19:54.910 --> 00:19:56.579
it's lying on the table in front of you,

404
00:19:56.699 --> 00:19:59.949
you have evidence, right? Um, WHETHER you choose to

405
00:19:59.949 --> 00:20:03.420
disbelieve it or not, it's, it's completely, um, uh,

406
00:20:03.430 --> 00:20:06.349
irrelevant. So that's those are kind of the accounts

407
00:20:06.349 --> 00:20:08.469
that exist in the literature very roughly, and that's

408
00:20:08.469 --> 00:20:11.300
what motivated me to put together a new account

409
00:20:11.510 --> 00:20:13.109
that is based on this notion of being in

410
00:20:13.109 --> 00:20:15.229
a position to know. You don't need to know

411
00:20:15.229 --> 00:20:17.109
these facts in order for them to count as

412
00:20:17.109 --> 00:20:18.589
part of your evidence. You just need to be

413
00:20:18.589 --> 00:20:20.510
in a position to know them, in a position

414
00:20:20.510 --> 00:20:23.709
to basically take them up in your cognitive system.

415
00:20:24.770 --> 00:20:27.520
And earlier in my very first question, and I,

416
00:20:27.599 --> 00:20:30.650
I asked you why do we need the philosophical

417
00:20:30.650 --> 00:20:34.390
account of resistance to evidence. You mentioned that uh

418
00:20:34.390 --> 00:20:39.089
science, psychology, more specifically doesn't deal at least directly

419
00:20:39.089 --> 00:20:42.369
with the normative aspects of this sort of phenomenon.

420
00:20:42.680 --> 00:20:46.859
But what normative aspects of resistance to evidence do

421
00:20:46.859 --> 00:20:51.660
you consider and what sort of normativity are we

422
00:20:51.660 --> 00:20:56.020
talking about here exactly? Is it social normativity, moral

423
00:20:56.020 --> 00:21:00.699
normativity normativity or some other sort of normativity?

424
00:21:00.979 --> 00:21:04.180
Yeah, good. So, so one thing that people uh

425
00:21:04.180 --> 00:21:07.300
have thought for a long time is something along

426
00:21:07.300 --> 00:21:10.739
these lines. Look. When it comes to forming beliefs,

427
00:21:10.859 --> 00:21:13.459
we don't have obligations to form them because we

428
00:21:13.459 --> 00:21:15.819
don't have control over our belief formation. They just

429
00:21:15.819 --> 00:21:18.060
happen automatically to us. I, as soon as I

430
00:21:18.060 --> 00:21:19.780
see this laptop, I believe it's there, whether I

431
00:21:19.780 --> 00:21:22.099
want it or not. And people for the longest

432
00:21:22.099 --> 00:21:26.530
time, mistakenly in philosophy thought that an unrestricted version

433
00:21:26.530 --> 00:21:28.979
of the following principle is true. What it implies

434
00:21:28.979 --> 00:21:31.380
can. You cannot have an obligation if you can't

435
00:21:31.380 --> 00:21:33.550
do it. And this sounds plausible. At first it's

436
00:21:33.550 --> 00:21:36.979
a principle, notably put forth by Kant. But recent

437
00:21:36.979 --> 00:21:39.660
results in ethics suggests strongly that it's the false

438
00:21:39.660 --> 00:21:41.780
principle. And if we want the, I mean, there's

439
00:21:41.780 --> 00:21:43.380
something true to it, but it needs to be

440
00:21:43.380 --> 00:21:47.040
restricted, uh, right? For instance, just because I can't

441
00:21:47.040 --> 00:21:50.739
treat, uh, women and black people well because I'm

442
00:21:50.739 --> 00:21:52.890
a racist or, and a sexist, it doesn't follow

443
00:21:52.890 --> 00:21:54.339
that I, it's not the case that I ought

444
00:21:54.339 --> 00:21:56.939
to treat them well, right? So, so the, clearly

445
00:21:56.939 --> 00:21:59.099
the principle is false. But because people believe this

446
00:21:59.099 --> 00:22:01.209
principle and assumed it, they thought there was no

447
00:22:01.209 --> 00:22:03.780
way there can be odds to believe something. So

448
00:22:03.780 --> 00:22:06.040
then how do we Explain this resistance to evidence

449
00:22:06.040 --> 00:22:07.959
and what's wrong with it. People thought, well, it

450
00:22:07.959 --> 00:22:09.599
has to do with the breach of some other

451
00:22:09.599 --> 00:22:12.199
type of normativity. Maybe it's a breach of social

452
00:22:12.199 --> 00:22:15.030
normativity, so it can't be epistemic normativity that's breached.

453
00:22:15.189 --> 00:22:18.189
It's got to be social or moral normativity. So

454
00:22:18.760 --> 00:22:21.859
accounts in the literature, for instance. Uh, TRY to

455
00:22:21.859 --> 00:22:24.380
suggest that what is going on is that we

456
00:22:24.380 --> 00:22:28.260
inhabit a particular social role and because we inhabit

457
00:22:28.260 --> 00:22:30.619
the social roles, the social roles come with social

458
00:22:30.619 --> 00:22:34.699
obligations that sometimes are social epistemic obligations because you

459
00:22:34.699 --> 00:22:37.180
know we all depend on each other for information

460
00:22:37.180 --> 00:22:39.964
and. We can't really cooperate with each other unless

461
00:22:39.964 --> 00:22:42.814
we do this kind of information exchange well, but

462
00:22:42.814 --> 00:22:44.694
in order to do this information exchange, well, it

463
00:22:44.694 --> 00:22:47.375
better be the case that we, you know, privately

464
00:22:47.375 --> 00:22:50.244
as it were also do our information uptake well

465
00:22:50.244 --> 00:22:52.214
because otherwise we're not going to participate in this

466
00:22:52.214 --> 00:22:55.650
exchange, um, in a valuable fashion. So the thought

467
00:22:55.650 --> 00:22:58.290
was, well, look, this resistance to evidence is not

468
00:22:58.290 --> 00:23:01.609
epistemically problematic, but it is socially problematic because you're

469
00:23:01.609 --> 00:23:03.569
going to, as it were, mess up with the

470
00:23:03.569 --> 00:23:07.530
entire epistemic landscape when you don't take up the

471
00:23:07.530 --> 00:23:09.849
evidence that you can and put them in the

472
00:23:09.849 --> 00:23:13.839
common, um, as it were, basket of epistemic resource.

473
00:23:14.250 --> 00:23:16.689
Um, SO that was one explanation that was offered.

474
00:23:16.729 --> 00:23:19.209
Another explanation that was offered, look, it's just a

475
00:23:19.209 --> 00:23:22.540
moral problem. And in this cases, so say the

476
00:23:22.540 --> 00:23:25.380
case that I gave earlier where, uh, you know,

477
00:23:25.500 --> 00:23:28.540
you have all the evidence in the world that

478
00:23:28.540 --> 00:23:30.380
the, the, this black person in front of you

479
00:23:30.380 --> 00:23:33.229
is smiling and is being very kind. But because

480
00:23:33.229 --> 00:23:35.989
you're a racist, you have this this impression that

481
00:23:35.989 --> 00:23:40.829
they look angry. uh, SO some, some epistemologists thought,

482
00:23:40.910 --> 00:23:43.550
look, as soon as you seeming is in that

483
00:23:43.550 --> 00:23:46.819
way, you're justified to believe that they're angry, but

484
00:23:47.109 --> 00:23:49.930
so epistemically that you're doing nothing wrong, but morally,

485
00:23:50.010 --> 00:23:52.030
of course it's bad because the reason why you

486
00:23:52.030 --> 00:23:54.430
have this seeming is because you're a biased racist

487
00:23:54.430 --> 00:23:57.489
person and being a racist is bad. Uh, SO

488
00:23:57.489 --> 00:24:00.030
why you might think, why don't these two explanations

489
00:24:00.030 --> 00:24:02.540
suffice? Why do we need to make it epistemic,

490
00:24:02.709 --> 00:24:06.030
the problem, uh, like my account does it. Uh,

491
00:24:06.130 --> 00:24:08.810
VERY quickly about the social explanation, the reason why

492
00:24:08.810 --> 00:24:12.089
it doesn't work is because if you, if you

493
00:24:12.089 --> 00:24:14.609
want to say that the only thing that's wrong

494
00:24:14.609 --> 00:24:17.459
with this is social. The problem is that so

495
00:24:17.459 --> 00:24:19.729
sometimes social norms are bad. Not all social norms

496
00:24:19.729 --> 00:24:21.650
are good. To the contrary, most of them are

497
00:24:21.650 --> 00:24:24.010
bad. So one thing that you don't want to

498
00:24:24.010 --> 00:24:26.010
say is that as soon as the social norm

499
00:24:26.010 --> 00:24:29.219
says something like, don't believe women. That makes it

500
00:24:29.219 --> 00:24:33.250
OK not to believe women, right? So resistance to

501
00:24:33.250 --> 00:24:36.900
evidence from women is socially OK. If it's also

502
00:24:36.900 --> 00:24:39.660
epistemic, you're OK, OK, what's the problem? You know

503
00:24:39.660 --> 00:24:42.810
what I mean? Um, SO that's, that's the social

504
00:24:42.810 --> 00:24:45.569
problem, uh, as it were. The, the problem with

505
00:24:45.569 --> 00:24:50.119
the moral explanation is, is even worse. So first

506
00:24:50.119 --> 00:24:52.530
of all, because you can easily cook up cases

507
00:24:52.530 --> 00:24:57.199
where it's morally, uh, actually good or even required

508
00:24:57.199 --> 00:24:59.319
to resist this evidence. So I have a, I

509
00:24:59.319 --> 00:25:02.540
have a case in my book. Uh, OF basically

510
00:25:02.540 --> 00:25:05.250
partiality and friendship. So many people in ethics think

511
00:25:05.250 --> 00:25:08.060
that we owe to our friends to be a

512
00:25:08.060 --> 00:25:12.150
bit more skeptical about accepting, um, uh, evidence for

513
00:25:12.150 --> 00:25:15.619
their wrongdoing as a person. Someone comes and tells

514
00:25:15.619 --> 00:25:17.300
me that my friend just killed someone in the

515
00:25:17.300 --> 00:25:19.540
street, I should be skeptical, right? No, my friend

516
00:25:19.540 --> 00:25:21.339
doesn't do such a thing. Well, if they just

517
00:25:21.339 --> 00:25:22.890
come and say the same thing about the stranger,

518
00:25:22.979 --> 00:25:26.430
I should just say, OK. Um, SO, so if,

519
00:25:26.510 --> 00:25:28.500
if that's true that we have a moral obligation

520
00:25:28.709 --> 00:25:30.630
to our friends to be a bit more skeptical

521
00:25:30.630 --> 00:25:32.750
when it comes to this kind of stuff, um,

522
00:25:32.869 --> 00:25:34.790
then it would look as though there are cases

523
00:25:34.790 --> 00:25:36.949
when resistance to evidence is morally good, but you

524
00:25:36.949 --> 00:25:39.750
still want to say that it's epistemically problematic, right?

525
00:25:39.829 --> 00:25:41.589
So remember the case of the detective that we

526
00:25:41.589 --> 00:25:43.670
were talking about earlier. If the detective comes and

527
00:25:43.670 --> 00:25:46.390
says, I didn't believe any evidence that the butler

528
00:25:46.390 --> 00:25:47.869
did it because the butler is a friend of

529
00:25:47.869 --> 00:25:49.630
mine, and I can't believe these things about my

530
00:25:49.630 --> 00:25:52.650
friends, that's not a good thing epistemically, right? So

531
00:25:52.650 --> 00:25:55.719
that's one very serious problem. Another problem is that

532
00:25:55.890 --> 00:25:58.050
we know from a long history of ethics that

533
00:25:58.050 --> 00:26:01.540
moral responsibility has an epistemic condition on it. So

534
00:26:01.540 --> 00:26:04.500
if you are to be, for instance, blameworthy, you,

535
00:26:04.579 --> 00:26:07.099
it better be the case that, you know, you

536
00:26:07.099 --> 00:26:09.459
meet some sort of epistemic condition, otherwise you're going

537
00:26:09.459 --> 00:26:12.219
to be blamelessly ignorant, right? So if you know,

538
00:26:12.300 --> 00:26:15.050
if you did everything in your power to investigate

539
00:26:15.050 --> 00:26:17.520
a particular topic and you still ended up with

540
00:26:17.520 --> 00:26:19.819
a false belief, you're not going to be morally

541
00:26:19.819 --> 00:26:22.535
responsible either because you did your job well. But

542
00:26:22.535 --> 00:26:24.935
of course if there is an epistemic condition of

543
00:26:24.935 --> 00:26:28.324
moral responsibility and we want to claim that it

544
00:26:28.324 --> 00:26:31.055
is about moral responsibility, what this resistance to evidence

545
00:26:31.055 --> 00:26:35.165
is, why it's problematic, then we're just, you know,

546
00:26:35.555 --> 00:26:37.974
uh, getting back to the same problem because moral

547
00:26:37.974 --> 00:26:40.295
responsibility implies an epistemic condition, so we still need

548
00:26:40.295 --> 00:26:42.724
to answer the question, OK, what's epistemically wrong, uh,

549
00:26:42.814 --> 00:26:45.175
as it were. So that's why there's two ways

550
00:26:45.175 --> 00:26:46.775
of explaining the data is not, are not going

551
00:26:46.775 --> 00:26:47.244
to work.

552
00:26:47.849 --> 00:26:50.729
Mhm. So, let's talk now a little bit about

553
00:26:50.729 --> 00:26:53.810
suspended judgment. That is something that you touched on

554
00:26:53.810 --> 00:26:57.290
a little bit earlier. Uh, AND you mentioned the

555
00:26:57.290 --> 00:27:01.449
examples of climate change and vaccination where, I mean,

556
00:27:01.650 --> 00:27:05.479
there, there are two situations where perhaps people shouldn't

557
00:27:05.479 --> 00:27:09.209
suspend their judgment very much because it has very

558
00:27:09.209 --> 00:27:12.510
direct consequences to their health and now to Our

559
00:27:12.880 --> 00:27:16.770
economy, environment, and so on. So, uh, but, uh,

560
00:27:16.969 --> 00:27:20.680
are there situations where you think that it is

561
00:27:20.680 --> 00:27:24.280
or it would be epistemically permissible for us to

562
00:27:24.280 --> 00:27:27.920
suspend our judgment? What would be your account of

563
00:27:27.920 --> 00:27:28.250
that?

564
00:27:29.599 --> 00:27:32.469
So What I do in the book is I

565
00:27:32.469 --> 00:27:36.010
consider all kinds of ways of accounting for what

566
00:27:36.010 --> 00:27:40.630
it is for suspension of judgment to be possible

567
00:27:40.630 --> 00:27:44.500
and in particular I distinguish between two things. First

568
00:27:44.500 --> 00:27:46.310
of all, there's one thing to suspend the judgment,

569
00:27:46.390 --> 00:27:49.150
and it's another thing to be completely neutral, which

570
00:27:49.150 --> 00:27:51.910
are sometimes people don't care, don't talk much about

571
00:27:51.910 --> 00:27:54.670
this distinction, and they don't make it clear what

572
00:27:54.670 --> 00:27:56.920
the view is about one or the other. So

573
00:27:56.920 --> 00:28:02.520
suspending judgment, um, you know, is, is just something

574
00:28:02.520 --> 00:28:05.359
that you are permitted to do in cases in

575
00:28:05.359 --> 00:28:07.839
which you don't have enough evidence to support full

576
00:28:07.839 --> 00:28:11.229
belief, right? Go ahead and have an outright belief

577
00:28:11.229 --> 00:28:13.439
that something is the case. So if you don't

578
00:28:13.439 --> 00:28:16.040
have enough evidence for that, then you're fine to

579
00:28:16.040 --> 00:28:19.489
suspend judgment, where suspension of judgment just means not

580
00:28:19.489 --> 00:28:22.270
believing it. And so basically being in an attitude

581
00:28:22.270 --> 00:28:25.839
of not believing, not fully believing. It's a completely

582
00:28:25.839 --> 00:28:29.920
different story, what neutrality is, where neutrality is being

583
00:28:29.920 --> 00:28:32.439
completely neutral of whether something is the case or

584
00:28:32.439 --> 00:28:34.910
not. So being a fifty-fifty, as it were, right?

585
00:28:35.239 --> 00:28:37.160
And why, why do I say this, that this

586
00:28:37.160 --> 00:28:39.520
is very important? Well, because in, take the case

587
00:28:39.520 --> 00:28:42.800
of vaccine, right? So many you would hear vaccine

588
00:28:42.800 --> 00:28:46.270
skeptics very often saying things like, Look, there's a

589
00:28:46.270 --> 00:28:48.790
lot of evidence that vaccines are safe, but I've,

590
00:28:48.869 --> 00:28:51.500
I've seen some people saying the contrary as well.

591
00:28:51.589 --> 00:28:53.579
So until I'm certain, I'm not going to take

592
00:28:53.579 --> 00:28:56.260
the vaccine because I'm, I want to be certain

593
00:28:56.260 --> 00:28:58.270
before I, you know, put that thing in my

594
00:28:58.270 --> 00:29:01.579
body. Well, well, that's a mistake. So say, say

595
00:29:01.579 --> 00:29:04.670
that you are in an evidential environment where you

596
00:29:04.670 --> 00:29:08.140
have many, very many testifiers giving you misleading evidence

597
00:29:08.589 --> 00:29:10.949
against the safety of vaccine, right? And then you

598
00:29:10.949 --> 00:29:13.949
are, if these testifiers have a good track record,

599
00:29:14.030 --> 00:29:16.150
you're right to trust them. You might be in

600
00:29:16.150 --> 00:29:18.589
a situation in which you don't have enough evidence

601
00:29:18.589 --> 00:29:21.430
available to you to fully believe that the vaccine

602
00:29:21.430 --> 00:29:23.910
is safe. Scientists are saying that they're safe, but

603
00:29:23.910 --> 00:29:27.430
then your family who are otherwise people that you

604
00:29:27.430 --> 00:29:29.109
trust, and you trust for a good reason. They're

605
00:29:29.109 --> 00:29:32.270
reliable, and so on. They care about you. They

606
00:29:32.270 --> 00:29:34.589
say that there's something fishy about it, that they're,

607
00:29:34.829 --> 00:29:37.689
you know, the scientists are motivated by some industry

608
00:29:37.689 --> 00:29:41.150
funder and and so on, right? So. Maybe you

609
00:29:41.150 --> 00:29:43.189
are in an evidential environment in which you don't

610
00:29:43.189 --> 00:29:45.150
have enough evidence for full belief. So then it's

611
00:29:45.150 --> 00:29:47.530
OK to suspend judgment. Here's what is not OK

612
00:29:47.530 --> 00:29:49.910
is to be fully neutral, because that is only

613
00:29:49.910 --> 00:29:52.949
OK when your evidence is really 50/50. So you

614
00:29:52.949 --> 00:29:56.550
have, you know, exactly 0.5 on one hand and

615
00:29:56.550 --> 00:29:59.459
0.5 on the other. And this matters because what

616
00:29:59.459 --> 00:30:01.969
we need for action is not full belief. What

617
00:30:01.969 --> 00:30:05.699
we need for action is just enough confidence given.

618
00:30:06.130 --> 00:30:08.670
What, as it were, the value of the outcome

619
00:30:08.670 --> 00:30:11.469
is to us. So let's go back to vaccines.

620
00:30:12.000 --> 00:30:15.390
There's no, there's no option to do nothing. You

621
00:30:15.390 --> 00:30:19.510
either go and take the vaccine, or you decide

622
00:30:19.510 --> 00:30:21.589
not to take it and you remain vulnerable to

623
00:30:21.589 --> 00:30:25.060
the virus. There's, there's no midway between these two,

624
00:30:25.189 --> 00:30:27.310
right? So what you need to do now is

625
00:30:27.310 --> 00:30:29.300
you look at the evidence for the vaccine safety

626
00:30:29.300 --> 00:30:31.349
that you have and say that it's not in.

627
00:30:31.439 --> 00:30:33.780
For full belief. In your case, it's not, I

628
00:30:33.780 --> 00:30:35.750
don't know, say that full belief is a threshold

629
00:30:35.750 --> 00:30:38.869
of 0.9 probability. Say that you are at 0.7,

630
00:30:38.910 --> 00:30:40.510
so you, you know, you don't even have enough

631
00:30:40.510 --> 00:30:43.829
for full belief. But, uh, you know, now think

632
00:30:43.829 --> 00:30:46.709
about it. So you have 0.7 probability that it's

633
00:30:46.709 --> 00:30:48.869
gonna be great. It's gonna be OK if you

634
00:30:48.869 --> 00:30:52.229
take the vaccine, right? How much probability do you

635
00:30:52.229 --> 00:30:54.069
have that you're gonna be OK if you don't

636
00:30:54.069 --> 00:30:58.410
take, um, the vaccine, right? Uh, WELL, that's not

637
00:30:58.410 --> 00:31:01.609
great, right? There's a lot of people dying without

638
00:31:01.609 --> 00:31:04.729
the vaccine. So even in cases, I guess what

639
00:31:04.729 --> 00:31:09.609
I'm saying, in which suspended judgment is justified, neutrality

640
00:31:09.609 --> 00:31:12.550
is not justified, then if neutrality is not justified,

641
00:31:12.609 --> 00:31:15.010
you still should go and take that vaccine because

642
00:31:15.010 --> 00:31:18.689
you still have enough evidential support. To act and

643
00:31:18.689 --> 00:31:21.489
go and take the vaccine. Uh, SO that's, that's

644
00:31:21.489 --> 00:31:23.410
why I, I draw this distinction and I discuss

645
00:31:23.410 --> 00:31:25.810
it in the chapter because very often you see

646
00:31:25.810 --> 00:31:28.729
epistemologists focusing either one or the other depending on

647
00:31:28.729 --> 00:31:31.689
whether they are formal epistemologists or they're more traditional

648
00:31:31.689 --> 00:31:34.410
epistemologists. So I think that these two distinctions are

649
00:31:34.410 --> 00:31:35.680
very important to the topic.

650
00:31:37.060 --> 00:31:39.319
So in the book, you also address at a

651
00:31:39.319 --> 00:31:44.400
certain point what you call their virtue responsibilist approaches

652
00:31:44.400 --> 00:31:47.760
to resistance to evidence. So what are these kinds

653
00:31:47.760 --> 00:31:50.640
of approaches and what do you think about them?

654
00:31:51.500 --> 00:31:54.560
Yeah, so virtue responsib is, um, you might think,

655
00:31:54.660 --> 00:31:57.329
on the face of it, has fantastic resources to

656
00:31:57.329 --> 00:31:59.849
deal with the phenomenon because what they care about

657
00:31:59.849 --> 00:32:04.530
are virtues and vices. Basically epistemic virtues and vices.

658
00:32:04.569 --> 00:32:08.719
So things that, uh, are things like open-mindedness or

659
00:32:08.719 --> 00:32:13.045
curiosity are. Supposed to be, uh, virtues and things

660
00:32:13.045 --> 00:32:17.045
like dogmatists are, are supposed to be, uh, vices.

661
00:32:17.094 --> 00:32:19.244
And the thought would be, well, this isn't resistance

662
00:32:19.244 --> 00:32:22.165
to evidence clearly, just a case of dogmatist and

663
00:32:22.165 --> 00:32:23.805
or a case in which this person is not

664
00:32:23.805 --> 00:32:27.125
open minded enough. So, uh, doesn't that, isn't that

665
00:32:27.125 --> 00:32:31.530
the problem? Um, SO, so the. You know, it

666
00:32:31.530 --> 00:32:34.589
may well be that sometimes in these cases of

667
00:32:34.589 --> 00:32:38.050
evidence resistance there are manifestations of these devices. It

668
00:32:38.050 --> 00:32:40.410
may well be that the person in question is

669
00:32:40.410 --> 00:32:43.089
a dogmatic person, um, and you know, you see

670
00:32:43.089 --> 00:32:45.849
this, for instance, in cases in which, uh, so

671
00:32:45.849 --> 00:32:49.560
we know from science that cognitive flexibility, so our,

672
00:32:49.729 --> 00:32:51.579
in a way our open mindedness as it were,

673
00:32:51.689 --> 00:32:56.170
decreases age. So, uh, that is one explanation for

674
00:32:56.170 --> 00:32:58.640
why what you see is that people as they

675
00:32:59.170 --> 00:33:03.079
Become older, they become more entrenched in their opinions

676
00:33:03.079 --> 00:33:05.819
and harder to, you know, to move out of

677
00:33:05.819 --> 00:33:08.630
their opinions. So, so you see that a lot

678
00:33:08.839 --> 00:33:10.680
and you know one explanation for that can be

679
00:33:10.680 --> 00:33:14.400
just epistemic. They gather so much evidence during a

680
00:33:14.400 --> 00:33:17.880
huge lifetime for their beliefs that it's harder to

681
00:33:17.880 --> 00:33:20.430
change now because it's harder to defeat that evidence.

682
00:33:20.800 --> 00:33:23.689
But another explanation unfortunately is that with age we

683
00:33:23.689 --> 00:33:27.050
all get less cognitively uh flexible, so we're less

684
00:33:27.050 --> 00:33:30.770
open to to other people's views. Uh, AND you

685
00:33:30.770 --> 00:33:32.729
might think, well, that's exactly the phenomenon that we're

686
00:33:32.729 --> 00:33:35.810
looking for, right? Um, THIS, this person is less

687
00:33:35.810 --> 00:33:38.729
open minded and more dogmatic because they have this

688
00:33:38.729 --> 00:33:42.439
decreasing cognitive flexibility. So, so that's what's going on.

689
00:33:42.770 --> 00:33:48.130
Um, UNFORTUNATELY this phenomenon doesn't, uh, need to happen

690
00:33:48.130 --> 00:33:51.880
in people who are vicious and indeed. Again, stipulating

691
00:33:51.880 --> 00:33:54.520
that everybody who resists scientific evidence is a vicious

692
00:33:54.520 --> 00:33:58.229
cogniser is a bit much because it's, it's a

693
00:33:58.229 --> 00:34:01.199
very well spread phenomenon. And again, we are a

694
00:34:01.199 --> 00:34:05.520
highly successful species which implies that we are actually

695
00:34:05.520 --> 00:34:08.918
quite good cognizers. So if we, if viciousness would

696
00:34:08.918 --> 00:34:11.560
be so widely spread, we wouldn't be doing that

697
00:34:11.560 --> 00:34:14.300
well. Uh, SO what in particular, what tends to

698
00:34:14.300 --> 00:34:17.340
happen a lot is that perfectly fine cognizers, perfectly

699
00:34:17.340 --> 00:34:21.340
virtuous cognizers make mistakes on a particular topic, right?

700
00:34:21.458 --> 00:34:25.110
So let's go to the. The, the social psychological

701
00:34:25.110 --> 00:34:28.120
explanations in terms of motivated reasoning. It might be

702
00:34:28.120 --> 00:34:30.280
that you're a fantastic organizer in all walks of

703
00:34:30.280 --> 00:34:32.879
life, but when it comes to things that affect

704
00:34:32.879 --> 00:34:36.000
your political identity, you just don't reason well anymore,

705
00:34:36.129 --> 00:34:38.199
right? You're going to believe all kinds of rubbish

706
00:34:38.199 --> 00:34:40.360
just because you are, you know, you want to

707
00:34:40.360 --> 00:34:43.250
preserve your political identity. But that doesn't mean that

708
00:34:43.250 --> 00:34:45.530
you're a vicious cogniser in general because, you know,

709
00:34:45.639 --> 00:34:48.199
this vices imply quite a lot of kind of

710
00:34:48.199 --> 00:34:50.360
disposition to for failures, so you need to be

711
00:34:50.360 --> 00:34:53.679
quite bad to be. You know, properly describable as

712
00:34:53.679 --> 00:34:56.399
vicious, um, and it can also just be a

713
00:34:56.399 --> 00:34:58.320
one off failure, and we want to be able

714
00:34:58.320 --> 00:35:01.790
to explain that a completely unmotivated one will failure

715
00:35:01.790 --> 00:35:04.560
this resistance to evidence. It might be that just

716
00:35:04.560 --> 00:35:08.719
one off, I don't appreciate exactly the evidential situation

717
00:35:08.719 --> 00:35:12.149
properly. So we are liable cognizers but not infallible.

718
00:35:12.199 --> 00:35:14.909
So that just basically implies that we fail sometimes.

719
00:35:15.000 --> 00:35:17.669
And when we do that. On just one occasion

720
00:35:17.669 --> 00:35:19.189
we still want to be able to explain what

721
00:35:19.189 --> 00:35:21.310
went wrong there and and to predict that something

722
00:35:21.310 --> 00:35:24.350
did go wrong. But if our only resource is

723
00:35:24.350 --> 00:35:26.110
to say, well, it was a manifestation of vice,

724
00:35:26.189 --> 00:35:29.000
well, no manifestation by stipulation this is a perfectly

725
00:35:29.000 --> 00:35:31.629
fine cogiter they just failed once. So that's why

726
00:35:31.629 --> 00:35:34.340
I think that although prima facie it might look

727
00:35:34.340 --> 00:35:38.000
like. But your responsibil has fantastic resources to account

728
00:35:38.000 --> 00:35:40.639
for this phenomenon and to the contrary, it's um

729
00:35:40.639 --> 00:35:43.199
it's gonna, you know, the, the danger is that

730
00:35:43.199 --> 00:35:45.479
they're gonna predict too much viciousness in the population

731
00:35:45.479 --> 00:35:47.189
in order to be able to account for all

732
00:35:47.189 --> 00:35:48.929
this, the instances of this phenomenon.

733
00:35:50.219 --> 00:35:53.649
So, uh, you also characterize in the book resistance

734
00:35:53.649 --> 00:35:58.449
to evidence as an instance of input level epistemic

735
00:35:58.449 --> 00:36:03.610
malfunctioning. Could you explain that terminology and particularly what

736
00:36:03.610 --> 00:36:07.399
malfunctioning means in this specific context?

737
00:36:07.770 --> 00:36:11.050
Yeah, so, uh, what my book says is, look.

738
00:36:11.580 --> 00:36:15.659
Uh, WE need to distinguish between evidence resistance and

739
00:36:15.659 --> 00:36:18.929
mere evidence rejection. Evidence rejection doesn't need to be

740
00:36:18.929 --> 00:36:21.500
evidence resistance. It can just be perfectly justified, right?

741
00:36:21.620 --> 00:36:24.379
If my evidential environment is such that it's populated

742
00:36:24.379 --> 00:36:27.550
with a lot of misleading evidence against the safety

743
00:36:27.550 --> 00:36:31.090
of vaccines, I am perfectly justified to reject the

744
00:36:31.090 --> 00:36:34.560
evidence of its safety. It is it true that

745
00:36:34.560 --> 00:36:36.280
it's not safe? No, of course it's not true,

746
00:36:36.320 --> 00:36:38.989
but very, we are, as a matter of fact,

747
00:36:39.239 --> 00:36:42.919
fallible cognizers and very often we are misled by

748
00:36:42.919 --> 00:36:47.080
misleading evidence. And when that happens, we are responding

749
00:36:47.080 --> 00:36:50.000
properly to this evidence because misleading evidence is evidence,

750
00:36:50.120 --> 00:36:53.060
right? So when, when we are being misled by

751
00:36:53.060 --> 00:36:56.750
it, it is a good reaction and a rational

752
00:36:56.750 --> 00:36:59.580
reaction to go with this misleading evidence, and we

753
00:36:59.580 --> 00:37:01.899
shouldn't try to change that because that's the normal

754
00:37:01.899 --> 00:37:05.330
way to function epistemically, to respond to your evidence

755
00:37:05.659 --> 00:37:07.419
because of course you don't know that it's misleading,

756
00:37:07.540 --> 00:37:12.459
right? Um, SO, so, what I'm conjecturing, although of

757
00:37:12.459 --> 00:37:15.060
course that is an empirical hypothesis, is that in

758
00:37:15.060 --> 00:37:19.719
most cases of evidence rejection. What we have is

759
00:37:19.719 --> 00:37:22.959
justified and rational evidence rejection because of this kind

760
00:37:22.959 --> 00:37:25.919
of environmental problems. Of course that's not to say

761
00:37:25.919 --> 00:37:27.679
that it's always the case. We know that there

762
00:37:27.679 --> 00:37:30.000
are cases of evidence resistance, like for instance from

763
00:37:30.000 --> 00:37:32.760
motivated reasoning. I'm not saying this case exists. All

764
00:37:32.760 --> 00:37:34.239
I'm saying is that they're going to be much

765
00:37:34.239 --> 00:37:36.879
more isolated, which makes it such that I don't

766
00:37:36.879 --> 00:37:41.860
need to stipulate this crazy widely spread irrationality hypothesis.

767
00:37:42.360 --> 00:37:44.239
But, but I still need to explain what's happening

768
00:37:44.239 --> 00:37:47.000
in this isolated kind of cases, and what I'm

769
00:37:47.000 --> 00:37:50.419
saying is, look, If proper fun the proper function

770
00:37:50.419 --> 00:37:55.330
of our cognitive system, uh, includes proper evidential takeup.

771
00:37:55.459 --> 00:37:59.489
So my, my cognitive system is not properly functioning.

772
00:37:59.679 --> 00:38:01.659
If there's a computer lying straight in front of

773
00:38:01.659 --> 00:38:04.489
me and I can't believe that there's a computer

774
00:38:04.489 --> 00:38:06.729
lying straight in front of me, that something went

775
00:38:06.729 --> 00:38:08.899
really wrong. Imagine how you would feel right now

776
00:38:08.899 --> 00:38:11.060
if I told you, actually, not sure there's a

777
00:38:11.060 --> 00:38:13.159
laptop right, right now in front of me. You'd

778
00:38:13.159 --> 00:38:15.060
really think that there's something is going wrong, uh,

779
00:38:15.179 --> 00:38:19.540
quite significantly. So my, the way in which I,

780
00:38:19.639 --> 00:38:22.360
I think about this is our, our, some of

781
00:38:22.360 --> 00:38:26.139
our bodily systems, as it were, uh, are input

782
00:38:26.139 --> 00:38:29.159
dependent. So for instance, our lungs, uh, if they

783
00:38:29.159 --> 00:38:31.760
don't take up, if there's oxygen in the environment,

784
00:38:31.800 --> 00:38:34.830
readily available, and they don't take it up, um,

785
00:38:35.040 --> 00:38:37.020
that is a sign of malfunction. Something has gone

786
00:38:37.020 --> 00:38:39.760
wrong with our lungs. Similarly, I think our cognitive

787
00:38:39.760 --> 00:38:43.040
systems, uh, are mal. Functioning if they don't take

788
00:38:43.040 --> 00:38:46.439
up very easily available evidence, um, from the environment.

789
00:38:46.560 --> 00:38:48.879
So there's nothing strange, as it were, about our

790
00:38:48.879 --> 00:38:52.120
cognitive systems. Some, uh, of our systems are input

791
00:38:52.120 --> 00:38:54.560
dependent in that way. If they don't take up

792
00:38:54.560 --> 00:38:58.919
readily available inputs, they're malfunctioning. So in that sense,

793
00:38:59.090 --> 00:39:01.709
it is an input level type of malfunction in

794
00:39:01.709 --> 00:39:04.409
that when you are this kind of system that's

795
00:39:04.409 --> 00:39:07.370
supposed to take up easily available something from the

796
00:39:07.370 --> 00:39:10.050
environment and you don't, that is one way in

797
00:39:10.050 --> 00:39:11.840
which you can uh malfunction.

798
00:39:13.060 --> 00:39:15.919
So another topic that you address in the book

799
00:39:15.919 --> 00:39:21.159
is uh defeat. So what does defeat mean and

800
00:39:21.159 --> 00:39:22.370
how do you approach it?

801
00:39:23.399 --> 00:39:26.790
So defeat, uh, defeat is a fancy epistemological term

802
00:39:26.790 --> 00:39:29.270
for just evidence against something. Uh, WE always like

803
00:39:29.270 --> 00:39:32.679
to have technical terms to pretend, um, that we're

804
00:39:32.679 --> 00:39:36.469
serious. Um, uh, BUT what is interesting about defeat,

805
00:39:36.590 --> 00:39:38.229
I think in particular when it comes to this

806
00:39:38.229 --> 00:39:41.110
resistance to evidence phenomenon, is that it comes in

807
00:39:41.110 --> 00:39:45.199
two, broad types of interest to us today and

808
00:39:45.199 --> 00:39:48.199
simplifying somehow the phenomenon, but that's what's what's important

809
00:39:48.199 --> 00:39:51.479
for us. Um, THERE'S a rebutting defeat and then

810
00:39:51.479 --> 00:39:56.000
there's undercutting defeat. Again, very sophisticated technical terms for

811
00:39:56.000 --> 00:40:00.010
actually quite simple phenomena where rebutting defeat is evidence

812
00:40:00.010 --> 00:40:03.149
against a particular proposition. So the, the case is,

813
00:40:03.239 --> 00:40:05.080
you know, you have, for instance, you have a

814
00:40:05.080 --> 00:40:08.000
testifier that says that it's raining outside and you

815
00:40:08.000 --> 00:40:11.010
have another testifier that's. IT'S not raining outside. So

816
00:40:11.010 --> 00:40:13.530
this testimony, the second testimony is a rebutting defeat

817
00:40:13.530 --> 00:40:16.489
to the first one because it affects, it's evidence

818
00:40:16.489 --> 00:40:18.610
against the proposition that was asserted by the first

819
00:40:18.610 --> 00:40:20.800
one, right? So the proposition is, it's raining outside,

820
00:40:21.010 --> 00:40:24.850
and this is defeat, a defeater, uh, to, to

821
00:40:24.850 --> 00:40:27.520
your evidence that it's raining outside because it says

822
00:40:27.520 --> 00:40:32.389
it's not raining outside, right? Uh, SO the most

823
00:40:32.389 --> 00:40:34.600
interesting for us type of defeat, however, is not

824
00:40:34.600 --> 00:40:37.399
rebutting defeat that much, is undercutting defeat. So this

825
00:40:37.399 --> 00:40:40.699
is a very pernicious sort of, uh, situation, uh,

826
00:40:40.719 --> 00:40:44.280
that can, can lead to very widespread, uh, problems

827
00:40:44.280 --> 00:40:47.479
in that this kind of defeat doesn't affect the

828
00:40:47.479 --> 00:40:50.320
probability of the proposition for you, but it affects

829
00:40:50.320 --> 00:40:53.239
the probability that your testifier, uh, is a good

830
00:40:53.239 --> 00:40:56.080
testifier, right? That the probability that your source is

831
00:40:56.080 --> 00:40:58.870
a good source. So what this defeater does is

832
00:40:58.870 --> 00:41:01.469
it tells you your source is rubbish. So the

833
00:41:01.510 --> 00:41:03.709
the case is one, for instance, where you have

834
00:41:03.709 --> 00:41:05.790
a testifier that comes and says it's raining outside,

835
00:41:05.830 --> 00:41:07.030
and you have another one who comes and says,

836
00:41:07.070 --> 00:41:09.370
don't believe anything that George says because George is

837
00:41:09.370 --> 00:41:14.129
a compulsive liar about meteorology, you know, um. So

838
00:41:14.129 --> 00:41:16.250
the reason why this is a more dangerous form

839
00:41:16.250 --> 00:41:18.530
of defeat is that it doesn't only affect one

840
00:41:18.530 --> 00:41:21.370
proposition, it affects everything that George says from now

841
00:41:21.370 --> 00:41:22.969
on, right? So if this person comes and says

842
00:41:22.969 --> 00:41:25.169
he's a, he's a compulsive liar, you're gonna be

843
00:41:25.169 --> 00:41:27.090
worried about everything that George says from now on,

844
00:41:27.290 --> 00:41:29.489
right? So it's a much more problematic form of

845
00:41:29.489 --> 00:41:32.229
defeat. Or you know, or good form of defeating

846
00:41:32.229 --> 00:41:35.149
cases in which George actually is a compulsive uh

847
00:41:35.149 --> 00:41:37.909
liar, but you can see how, you know, in

848
00:41:37.909 --> 00:41:40.510
the, in the cases of interest here like vaccine

849
00:41:40.510 --> 00:41:44.340
safety and climate change, it's the difference in, um,

850
00:41:44.389 --> 00:41:48.679
as it were dangers, um, is huge. Between these

851
00:41:48.679 --> 00:41:50.600
two types of defeaters because it's one thing, you

852
00:41:50.600 --> 00:41:52.879
know, you have your evidence from the scientists that

853
00:41:52.879 --> 00:41:55.600
vaccines are safe and climate change is happening, and

854
00:41:55.600 --> 00:41:57.399
then, you know, we have a couple of people

855
00:41:57.399 --> 00:42:00.399
telling you otherwise and you weigh these things against

856
00:42:00.399 --> 00:42:03.120
each other and, you know, the evidence in favor

857
00:42:03.120 --> 00:42:06.090
of the safety of vaccines and the climate change.

858
00:42:06.570 --> 00:42:11.570
Uh, HAPPENING is hugely, um, you know, kind of

859
00:42:11.570 --> 00:42:15.929
more weighty than the three people around the world

860
00:42:15.929 --> 00:42:19.129
who deny these claims, right? So if that was

861
00:42:19.129 --> 00:42:21.000
the only problem, I don't think we would have

862
00:42:21.000 --> 00:42:23.770
a lot of rejection of scientific evidence because it

863
00:42:23.770 --> 00:42:26.729
is overwhelming how much evidence there is that vaccines

864
00:42:26.729 --> 00:42:30.250
are saved and, and climate change is happening. Unfortunately,

865
00:42:30.290 --> 00:42:33.250
the way it works, um, if you look on

866
00:42:33.250 --> 00:42:37.120
the ground is that. These people, the, the people

867
00:42:37.120 --> 00:42:39.199
trying to deny these claims don't come and say

868
00:42:39.199 --> 00:42:41.639
just vaccines are unsafe. They come and say don't

869
00:42:41.639 --> 00:42:45.949
trust anything that. Uh, YOU know, public health authorities

870
00:42:45.949 --> 00:42:49.620
say about vaccines because they have a vested interest

871
00:42:49.909 --> 00:42:51.909
to sell you these vaccines to make some money

872
00:42:51.909 --> 00:42:54.629
for the for the vaccine industry or they have

873
00:42:54.629 --> 00:42:58.540
a vested interests to to generate this kind of

874
00:42:58.790 --> 00:43:01.750
uh herd immunity. So even though they know that

875
00:43:01.750 --> 00:43:03.669
it's there are some people for which the vaccine

876
00:43:03.669 --> 00:43:05.750
is not safe, they're not gonna tell you because

877
00:43:05.750 --> 00:43:08.310
they just want to generate herd immunity. So as

878
00:43:08.310 --> 00:43:13.000
soon as this happens. No matter how many scientists

879
00:43:13.000 --> 00:43:15.800
tell you that vaccines are safe, their entire testimony

880
00:43:15.800 --> 00:43:18.479
is undercut because now you don't trust any of

881
00:43:18.479 --> 00:43:20.639
them and you don't trust anything that they say.

882
00:43:20.719 --> 00:43:22.560
You don't trust them on vaccines, you don't trust

883
00:43:22.560 --> 00:43:24.520
them on climate change, you don't trust them on

884
00:43:24.520 --> 00:43:26.570
anything anymore. So you can see how that's a

885
00:43:26.570 --> 00:43:29.010
much more dangerous type of defeat, and you can

886
00:43:29.010 --> 00:43:31.129
see it on the ground as well, because it's

887
00:43:31.129 --> 00:43:33.610
very pervasive, right? This is the kind of discourse

888
00:43:33.610 --> 00:43:36.810
that you hear. Don't trust scientists, don't trust mainstream

889
00:43:36.810 --> 00:43:41.250
media. It's not so the evidence that the misleading

890
00:43:41.250 --> 00:43:44.629
evidence that is brought in. On this policy relevant

891
00:43:44.629 --> 00:43:48.750
topics of of high interest are very rarely actually

892
00:43:48.750 --> 00:43:51.629
affecting the proposition itself. Vaccines are safe, so very

893
00:43:51.629 --> 00:43:53.909
rarely, you know, would you see a far right

894
00:43:53.909 --> 00:43:57.030
person coming and saying actually there's a study in

895
00:43:57.030 --> 00:44:00.580
Nebraska where they tested this vaccine on, you know,

896
00:44:01.060 --> 00:44:03.580
people. With, uh, blue eyes, and it turned out

897
00:44:03.580 --> 00:44:05.840
that that that actually it's not safe for them.

898
00:44:05.919 --> 00:44:08.560
No, what they come and say is usually don't

899
00:44:08.560 --> 00:44:11.209
trust public health authorities because they're just trying to,

900
00:44:11.560 --> 00:44:14.399
uh, you know, poison us all and, and in

901
00:44:14.399 --> 00:44:18.399
order to, whatever, um, give some money to the

902
00:44:18.399 --> 00:44:20.600
industry or something like that. So. So this is

903
00:44:20.600 --> 00:44:23.040
the kind of defeat that is mostly problematic, and

904
00:44:23.040 --> 00:44:25.120
what this defeat does, as I said, is it

905
00:44:25.120 --> 00:44:29.229
decreases your probability for the trustworthiness of the source.

906
00:44:29.320 --> 00:44:31.310
So that's how it becomes very problematic.

907
00:44:32.330 --> 00:44:34.870
So, another thing that you also address in your

908
00:44:34.870 --> 00:44:39.270
book is uh epistemic dilemmas. So what is an

909
00:44:39.270 --> 00:44:43.580
epistemic dilemma and why does it, why is it

910
00:44:43.580 --> 00:44:46.040
important for us to address it in this context

911
00:44:46.040 --> 00:44:48.659
of uh resistance to evidence?

912
00:44:49.239 --> 00:44:52.870
Yeah, so one, there, there's two reasons why we

913
00:44:52.870 --> 00:44:54.669
need to talk about epistemic dilemmas if you write

914
00:44:54.669 --> 00:44:56.909
this book. One reason is purely theoretical. As soon

915
00:44:56.909 --> 00:44:59.229
as you postulate that there are such things as

916
00:44:59.229 --> 00:45:02.050
obligations in a normative domain, you might get that,

917
00:45:02.469 --> 00:45:06.270
right? Because permissions never generate dilemmas. If you're, um,

918
00:45:06.350 --> 00:45:09.030
you know, if your norm says you're permitted to

919
00:45:09.030 --> 00:45:11.909
jump in the lake, um, and this other norm

920
00:45:11.909 --> 00:45:14.959
says you're permitted to go to lunch. No, you

921
00:45:14.959 --> 00:45:16.889
know, whether you choose to jump in the lake

922
00:45:16.889 --> 00:45:18.739
or go to lunch or to the country, do

923
00:45:18.739 --> 00:45:21.149
something altogether different, none of these norms will be

924
00:45:21.149 --> 00:45:23.899
breached, right? Because they just permitted. They don't ask

925
00:45:23.899 --> 00:45:26.419
you to do it. When you have obligations, however,

926
00:45:26.459 --> 00:45:28.139
if you have two that cannot be met at

927
00:45:28.139 --> 00:45:30.060
the same time, they come in conflict, right? The

928
00:45:30.060 --> 00:45:32.379
classic case is you have promised a friend to

929
00:45:32.379 --> 00:45:34.919
go to lunch, but you encounter a child drowning

930
00:45:34.919 --> 00:45:38.139
on the, on the way. So now, evidently you

931
00:45:38.139 --> 00:45:40.300
need to jump and save the child. The more

932
00:45:40.300 --> 00:45:43.385
obligation to save the child, um, takes. But at

933
00:45:43.385 --> 00:45:45.764
the same time, you will have broken your obligation

934
00:45:45.764 --> 00:45:48.014
to keep your promise to your friend. So, you

935
00:45:48.014 --> 00:45:51.054
know, it's not a dilematic situation. It's just a

936
00:45:51.054 --> 00:45:54.094
normative conflict situation in the sense in which it's

937
00:45:54.094 --> 00:45:56.534
pretty clear which norm you should follow. You're not

938
00:45:56.534 --> 00:45:58.245
kind of like, which one should I go with?

939
00:45:58.294 --> 00:46:00.334
It's fairly evident. You should go with saving the

940
00:46:00.334 --> 00:46:03.175
child. Um, BUT I guess what I'm saying is

941
00:46:03.175 --> 00:46:04.854
that as soon as you have obligations, they might

942
00:46:04.854 --> 00:46:07.500
come in conflict. And when they come in conflict

943
00:46:07.500 --> 00:46:10.419
in a fashion where it's not really clear which

944
00:46:10.419 --> 00:46:12.500
one is stronger than the other, like in the

945
00:46:12.500 --> 00:46:14.379
case that I just described, you might be faced

946
00:46:14.379 --> 00:46:16.340
with a dilemma. So, you know, as opposed to

947
00:46:16.340 --> 00:46:18.139
what I just described with the lunch and the

948
00:46:18.139 --> 00:46:20.110
child consider a situation in which you're on your

949
00:46:20.110 --> 00:46:23.189
way to lunch. Now you have Two children drowning,

950
00:46:23.270 --> 00:46:25.020
one on the left and one on the right,

951
00:46:25.189 --> 00:46:26.909
and you still have the promise keeping to do,

952
00:46:27.030 --> 00:46:30.030
right? So the promise keeping norm is completely overridden.

953
00:46:30.070 --> 00:46:32.310
So that's, that's out the window. You're, you're fine

954
00:46:32.310 --> 00:46:34.830
being late for lunch. But these two norms, you're

955
00:46:34.830 --> 00:46:36.830
still left to them. One that says save the

956
00:46:36.830 --> 00:46:38.439
child on the left, the other one says, save

957
00:46:38.439 --> 00:46:40.709
the child on the right. Um, AND it seems

958
00:46:40.709 --> 00:46:43.510
like you're in a normative dilemma because these norms

959
00:46:43.510 --> 00:46:45.830
are equally strong, and no matter what you do,

960
00:46:45.870 --> 00:46:48.229
you're going to break a norm, that's as strong

961
00:46:48.229 --> 00:46:50.909
as the others, right? So the question is, well,

962
00:46:51.760 --> 00:46:54.560
moral dilemmas we know about them, uh, because we've

963
00:46:54.560 --> 00:46:57.959
studied moral obligation for 2000 years, but epistemic dilemmas,

964
00:46:58.000 --> 00:47:00.159
we don't know enough about them. Uh, AND now

965
00:47:00.159 --> 00:47:03.159
that you're postulating these obligations, we're gonna, aren't they,

966
00:47:03.280 --> 00:47:04.840
we gonna get them all over the place in

967
00:47:04.840 --> 00:47:08.120
the epistemic domain. And I guess what I'm saying

968
00:47:08.120 --> 00:47:10.550
is, well, first of all, no, not that much.

969
00:47:10.800 --> 00:47:13.320
I'm arguing in the chapter that actually epistemic dilemma

970
00:47:13.320 --> 00:47:16.080
are not easy to come by because what's going

971
00:47:16.080 --> 00:47:20.149
on in epistemology, um, is that you can always

972
00:47:20.149 --> 00:47:22.719
suspend judgment. You don't need to either believe P

973
00:47:22.719 --> 00:47:25.800
or no P or something like that. Um, SO

974
00:47:25.800 --> 00:47:27.919
as soon as you have that resource, actually epistemic

975
00:47:27.919 --> 00:47:31.679
dilemmas. ARE harder to come by. If anything, what

976
00:47:31.679 --> 00:47:34.530
is maybe um easiest to come by is an

977
00:47:34.530 --> 00:47:37.159
epistemic trilemmas and I, I give some examples in

978
00:47:37.159 --> 00:47:39.639
the book. Uh, BUT I guess going back to

979
00:47:39.639 --> 00:47:41.870
our phenomenon of resistance, so this is just the

980
00:47:41.879 --> 00:47:44.239
the reason why it's theoretically interesting, but why is

981
00:47:44.239 --> 00:47:47.110
it practically interesting for the phenomenon for evidence resistance?

982
00:47:47.320 --> 00:47:49.500
Well, because very often you hear that people say

983
00:47:49.500 --> 00:47:51.909
things like, I don't know what to believe anymore.

984
00:47:52.379 --> 00:47:54.719
Uh, SOME people say vaccines are safe, some people

985
00:47:54.719 --> 00:47:56.679
say they're not safe. I'm going to throw my

986
00:47:56.679 --> 00:47:59.550
hands in the air and not believe anything anymore.

987
00:47:59.719 --> 00:48:02.770
And I guess that, you know, what I'm saying

988
00:48:02.770 --> 00:48:06.239
is, no, actually that situation where it's OK for

989
00:48:06.239 --> 00:48:08.729
you to do that. Uh, SO one in which

990
00:48:08.729 --> 00:48:11.610
you are faced by this epistemic dilemma, uh, when

991
00:48:11.610 --> 00:48:13.889
it comes to, to believing or not believing something

992
00:48:13.889 --> 00:48:16.810
is not something that occurs very often, um, so

993
00:48:16.810 --> 00:48:19.030
it's not, it's not gonna be a problem. Uh,

994
00:48:19.169 --> 00:48:21.419
USUALLY when people do this, uh, they do it

995
00:48:21.419 --> 00:48:24.290
unjustifiably so it's not, it's not a phenomenon that

996
00:48:24.290 --> 00:48:27.010
is, uh, well spread, basically.

997
00:48:27.860 --> 00:48:30.840
So, uh, I have here two more topics that

998
00:48:30.840 --> 00:48:32.879
I would like to ask you about. The first

999
00:48:32.879 --> 00:48:36.719
one is skepticism. So, uh first of all, what

1000
00:48:36.719 --> 00:48:39.879
does skepticism mean in this kind of context because

1001
00:48:39.879 --> 00:48:42.959
I guess we could argue that there are different

1002
00:48:42.959 --> 00:48:47.320
kinds of skeptics out there. And uh is it

1003
00:48:47.320 --> 00:48:51.330
possible that at least in certain circumstances, in certain

1004
00:48:51.330 --> 00:48:55.989
situations, skepticism is also a form of resistance to

1005
00:48:55.989 --> 00:48:56.389
evidence.

1006
00:48:57.060 --> 00:48:59.649
Yeah, so I think, you know, sometimes skepticism is

1007
00:48:59.649 --> 00:49:02.090
warranted. So if you live in an environment in

1008
00:49:02.090 --> 00:49:05.550
which you have a lot of evidence that P

1009
00:49:05.550 --> 00:49:09.620
is false, skepticism about P is justified, right? And

1010
00:49:09.620 --> 00:49:12.879
of course this, this evidence may all be misleading,

1011
00:49:13.090 --> 00:49:15.889
but even so, it's evidence and you are supposed

1012
00:49:15.889 --> 00:49:19.169
to, you know, update your beliefs according to your

1013
00:49:19.169 --> 00:49:23.110
available evidence. So. It may well be that skepticism

1014
00:49:23.110 --> 00:49:25.739
sometimes is warranted, be it about vaccines or about

1015
00:49:25.739 --> 00:49:28.429
the existence of the external world, you know, whether

1016
00:49:28.429 --> 00:49:31.399
you're an epistemologist or someone who's interested in vaccine

1017
00:49:31.399 --> 00:49:34.780
uptake. Um, BUT I guess what I'm trying to

1018
00:49:34.780 --> 00:49:39.820
argue in that chapter is, uh, that what has

1019
00:49:39.820 --> 00:49:43.250
been assumed in the literature on skepticism, I mean

1020
00:49:43.250 --> 00:49:47.570
epistemology, which is that in a way, the skeptical,

1021
00:49:47.860 --> 00:49:50.939
the skeptic has the safe position because the skeptic

1022
00:49:50.939 --> 00:49:54.489
is just sitting there, not believing anything and wondering,

1023
00:49:54.500 --> 00:49:57.010
why should I believe that the world exists? Because,

1024
00:49:57.120 --> 00:49:58.824
you know, for all I know, maybe. It doesn't,

1025
00:49:58.905 --> 00:50:01.705
then my evidence is compatible with both and other

1026
00:50:01.705 --> 00:50:05.024
such motivations. What I'm saying is, no, actually you

1027
00:50:05.024 --> 00:50:07.185
have evidence that the world exists and you should

1028
00:50:07.185 --> 00:50:10.175
uptake it and believe that the world exists. So

1029
00:50:10.175 --> 00:50:12.544
I, what I'm basically arguing in the chapter is

1030
00:50:12.544 --> 00:50:15.375
that this assumption that skepticism is the safe position

1031
00:50:15.375 --> 00:50:18.625
and it is on us, non-skeptical epistemologists, to explain

1032
00:50:18.625 --> 00:50:20.824
to the skeptic why it's OK for them to,

1033
00:50:20.905 --> 00:50:22.784
you know, take the jump and believe that the

1034
00:50:22.784 --> 00:50:26.239
world exists, and so on. Um, IS exactly that,

1035
00:50:26.320 --> 00:50:29.250
that mistaken assumption that we had about suspension, right,

1036
00:50:29.350 --> 00:50:32.870
that that relies on the assumption that suspension is

1037
00:50:32.870 --> 00:50:35.600
the safe position, right? As soon as we discovered

1038
00:50:35.600 --> 00:50:38.149
that suspension of judgment is not safer than believing

1039
00:50:38.149 --> 00:50:41.250
that it also requires a warrant, we discovered that

1040
00:50:41.250 --> 00:50:44.199
the skeptic doesn't have anything on us because being

1041
00:50:44.199 --> 00:50:47.679
a skeptic also cannot be done without justification, I

1042
00:50:47.679 --> 00:50:48.600
guess, in a nutshell.

1043
00:50:49.260 --> 00:50:52.449
Mhm. Uh, SO, the last thing I would like

1044
00:50:52.449 --> 00:50:54.280
to ask you about is, uh, in the book,

1045
00:50:54.290 --> 00:50:58.050
you also talk about toward the end, um, about

1046
00:50:58.050 --> 00:51:03.100
misinformation and disinformation, which is something that people nowadays

1047
00:51:03.409 --> 00:51:07.489
are worrying a lot about. So, uh, taking into

1048
00:51:07.489 --> 00:51:11.129
account the main topic of the book, resistance to

1049
00:51:11.129 --> 00:51:16.090
evidence, uh, why do you also approach these questions

1050
00:51:16.090 --> 00:51:20.550
surrounding misinformation and disinformation and In what ways do

1051
00:51:20.550 --> 00:51:21.610
you approach them?

1052
00:51:22.139 --> 00:51:23.780
Yeah, the reason why I think that this topic

1053
00:51:23.780 --> 00:51:25.550
needed to be approached in this, in this book

1054
00:51:25.550 --> 00:51:30.449
is because the the these two phenomena, basically scientific

1055
00:51:30.449 --> 00:51:36.010
evidence resistance and disinformation and misinformation um are not

1056
00:51:36.010 --> 00:51:38.500
independent from each other. They, they have kind of

1057
00:51:38.500 --> 00:51:42.540
mutually reinforcing patterns. So what's going on is the

1058
00:51:42.540 --> 00:51:45.379
more disinformation you have or misinformation that you have

1059
00:51:45.379 --> 00:51:48.610
in your environment. The more misleading evidence you have,

1060
00:51:48.860 --> 00:51:51.540
uh, with the result that the more justified you're

1061
00:51:51.540 --> 00:51:54.580
going to be to resist scientific evidence, right? On

1062
00:51:54.580 --> 00:51:57.629
the other hand, As soon as you don't trust

1063
00:51:57.739 --> 00:52:02.560
scientists and expertise in general anymore, you're more vulnerable

1064
00:52:02.729 --> 00:52:06.169
to disinformation campaigns because you don't have any expertise

1065
00:52:06.169 --> 00:52:08.370
anymore to fall back on. So these are, these

1066
00:52:08.370 --> 00:52:10.629
are two phenomena that cannot be studied in isolation.

1067
00:52:10.729 --> 00:52:12.530
I think that the fact that they are currently

1068
00:52:12.530 --> 00:52:15.120
studied in isolation is a big problem and that

1069
00:52:15.120 --> 00:52:18.159
is going to hinder progress quite a bit. So

1070
00:52:18.159 --> 00:52:19.800
that's why I'm at the end of the book

1071
00:52:19.800 --> 00:52:23.419
I'm saying look let's let's look at the nature

1072
00:52:23.419 --> 00:52:27.879
of uh information misinformation disinformation in order to understand

1073
00:52:27.879 --> 00:52:31.300
how this mechanisms kind of reinforce each other evidence

1074
00:52:31.300 --> 00:52:35.520
scientific evidence rejection and disinformation. And one result that

1075
00:52:35.520 --> 00:52:38.760
I get is that again, so, so here's a

1076
00:52:38.760 --> 00:52:41.629
funny thing that's happening in this in this literature,

1077
00:52:42.120 --> 00:52:44.399
uh, for the longest time people all over the

1078
00:52:44.399 --> 00:52:48.590
scientific spectrum. Uh, WHO study misinformation and disinformation and

1079
00:52:48.590 --> 00:52:52.270
their kind of flow, um, use a dictionary definition

1080
00:52:52.270 --> 00:52:54.870
to do that, which is really funny. I mean,

1081
00:52:54.909 --> 00:52:57.750
especially, you know, in, in domains that are otherwise

1082
00:52:57.750 --> 00:53:00.389
quite critical philosophy included, that you would work with

1083
00:53:00.389 --> 00:53:02.550
the dictionary definition. I mean, you know, if we're

1084
00:53:02.550 --> 00:53:06.149
working with dictionary definition of knowledge or justification, we

1085
00:53:06.149 --> 00:53:09.310
wouldn't need epistemology anymore. And the dictionary definition of

1086
00:53:09.310 --> 00:53:14.469
this information, shockingly unsurprisingly, is false. So, uh, you

1087
00:53:14.469 --> 00:53:17.550
know, it's it's extremely anthropocentric because it's an old

1088
00:53:17.550 --> 00:53:20.270
uh definition and it gets it wrong. So the

1089
00:53:20.270 --> 00:53:22.719
way in which it's defined commonly is that it's.

1090
00:53:23.090 --> 00:53:25.709
Uh, KIND of false content that spreads with the

1091
00:53:25.709 --> 00:53:28.639
intention to mislead. That's roughly the definition that you

1092
00:53:28.639 --> 00:53:31.389
find out there. And Donel is my colleague Don

1093
00:53:31.389 --> 00:53:33.629
Felles from the US, did work a bit on

1094
00:53:33.629 --> 00:53:35.189
this and said, Well, actually it doesn't need to

1095
00:53:35.189 --> 00:53:37.689
be spread with the intention to mislead because surely

1096
00:53:37.689 --> 00:53:40.570
we, you know, machines can also spread this information

1097
00:53:40.570 --> 00:53:42.469
and that, you know, our word these days is

1098
00:53:42.469 --> 00:53:45.030
with 1, right? So it can, it can just

1099
00:53:45.030 --> 00:53:47.179
be that it's the function to mislead, not just

1100
00:53:47.179 --> 00:53:49.870
the intention, because machines maybe don't have intentions, but

1101
00:53:49.870 --> 00:53:53.169
they have functions. Uh, BUT that's about the only

1102
00:53:53.169 --> 00:53:55.250
work that we have on this information. If you

1103
00:53:55.250 --> 00:53:58.090
start working and think about it very carefully, uh,

1104
00:53:58.189 --> 00:54:01.320
first of all, importantly, this information need not be

1105
00:54:01.320 --> 00:54:05.209
false content, uh, and indeed, you know, I, I'm

1106
00:54:05.209 --> 00:54:07.090
a former journalist, and what we were taught in

1107
00:54:07.090 --> 00:54:10.250
journalist school is that disinformation campaigns are not best

1108
00:54:10.250 --> 00:54:12.409
done with false content because people are not that

1109
00:54:12.409 --> 00:54:14.919
stupid, you know, as I said, we're a highly,

1110
00:54:15.090 --> 00:54:19.030
uh, successful species cognitively. Uh, THE way in which

1111
00:54:19.030 --> 00:54:20.629
you do it if you want to have a

1112
00:54:20.629 --> 00:54:23.189
successful disinformation campaign is you do it with true

1113
00:54:23.189 --> 00:54:26.870
statements that imply a falsehood that implicate falsehood, right?

1114
00:54:26.979 --> 00:54:29.280
So you say you don't come and say climate

1115
00:54:29.280 --> 00:54:30.989
change is not happening. People are going to be

1116
00:54:30.989 --> 00:54:34.030
suspicious. You come and say something like there is.

1117
00:54:34.114 --> 00:54:37.584
Disagreement in science about climate change. Now that is

1118
00:54:37.584 --> 00:54:40.014
not a statement of course you can always find

1119
00:54:40.014 --> 00:54:41.905
a couple of crazy scientists in the middle of

1120
00:54:41.905 --> 00:54:44.465
nowhere in a village who think that climate change

1121
00:54:44.465 --> 00:54:47.145
is not happening, right? So the statement there is

1122
00:54:47.145 --> 00:54:49.745
disagreement, strictly speaking, all it says is that there's

1123
00:54:49.745 --> 00:54:53.449
at least one. Scientist in the world that disagrees.

1124
00:54:53.659 --> 00:54:55.699
So the statement is true, but when you hear

1125
00:54:55.699 --> 00:54:57.580
it on TV, what do you hear? You hear

1126
00:54:57.580 --> 00:55:01.739
there are vast and important and relevant amounts of

1127
00:55:01.739 --> 00:55:04.280
disagreement because otherwise you're thinking, why is it on

1128
00:55:04.280 --> 00:55:06.820
the news if it's only one isolated guy somewhere,

1129
00:55:07.699 --> 00:55:11.219
right? Uh, SO that is a pragmatic phenomenon. The

1130
00:55:11.219 --> 00:55:14.060
sentence that you utter doesn't mean that it's a

1131
00:55:14.060 --> 00:55:16.699
huge amount of disagreement, just means that there is

1132
00:55:16.699 --> 00:55:19.489
at least one person who disagrees, right? But what

1133
00:55:19.489 --> 00:55:22.330
the hearers here is not just what the sentence

1134
00:55:22.330 --> 00:55:25.689
means, it's also what it pragmatically conveys, and pragmatically

1135
00:55:25.689 --> 00:55:29.040
it conveys that it's a significant amount of disagreement.

1136
00:55:29.330 --> 00:55:31.850
Why? Because it's told on the news and it

1137
00:55:31.850 --> 00:55:34.050
doesn't make for a news item if it's just

1138
00:55:34.050 --> 00:55:37.169
one person disagreeing, so it's not relevant for the

1139
00:55:37.169 --> 00:55:40.149
context, right? And that's how you spread this information

1140
00:55:40.649 --> 00:55:44.429
cleverly, not by saying outright falsehoods. Now the problem

1141
00:55:44.429 --> 00:55:48.219
is that Uh, first of all, that's one thing

1142
00:55:48.219 --> 00:55:51.139
that we don't realize when we are rushed to

1143
00:55:51.139 --> 00:55:54.570
judgment to think that all these people rejecting scientific

1144
00:55:54.570 --> 00:55:57.300
evidence are completely crazy and irrational, because why would

1145
00:55:57.300 --> 00:55:59.379
they believe when they are told by a nobody

1146
00:55:59.379 --> 00:56:01.939
who's not a scientist that climate change is not

1147
00:56:01.939 --> 00:56:04.500
happening? Yeah, but that's not what's happening, right? It's

1148
00:56:04.500 --> 00:56:06.500
not, is not that they're falling for this kind

1149
00:56:06.500 --> 00:56:09.300
of blunt, uh, you know, in a way stupid

1150
00:56:09.300 --> 00:56:12.290
way of doing disinformation. No, they're falling for the

1151
00:56:12.290 --> 00:56:15.020
clever way of doing disinformation, and of course they

1152
00:56:15.020 --> 00:56:18.790
are because you're supposed to react to that pragmatic

1153
00:56:18.790 --> 00:56:21.530
in that way. That's the rational way to form

1154
00:56:21.530 --> 00:56:25.524
beliefs in that contex. Next, right? Um, SO, so

1155
00:56:25.524 --> 00:56:27.405
that's why it's important to have a proper definition

1156
00:56:27.405 --> 00:56:30.764
of disinformation because then we can see how very

1157
00:56:30.764 --> 00:56:34.245
often people reject evidence warrantedly because of the way

1158
00:56:34.245 --> 00:56:36.284
in which they're tricked by this kind of subtle

1159
00:56:36.284 --> 00:56:39.485
ways, uh, of disinforming. Another reason why it's important

1160
00:56:39.485 --> 00:56:42.445
is because we're trying to design good AIs, right?

1161
00:56:42.554 --> 00:56:44.165
So now we have the bad AIs who are

1162
00:56:44.165 --> 00:56:46.445
trying to spread disinformation. The good thing about that

1163
00:56:46.445 --> 00:56:48.044
is that we can, if we're clever enough to

1164
00:56:48.044 --> 00:56:49.645
make the bad ones, we're clever enough to do

1165
00:56:49.645 --> 00:56:52.020
the, do the good ones as well. Uh, So

1166
00:56:52.020 --> 00:56:55.699
what we need to fight LLM, disinformation is LLM

1167
00:56:56.300 --> 00:56:59.060
disinformation trackers. That's what we need because us humans

1168
00:56:59.060 --> 00:57:01.100
are not going to be able to fight uh

1169
00:57:01.100 --> 00:57:04.889
disinformation spread by LLM, um, but in order to

1170
00:57:05.100 --> 00:57:08.260
build an LLM that tracks this information properly, you'd

1171
00:57:08.260 --> 00:57:09.979
better know what it is because if we just

1172
00:57:09.979 --> 00:57:12.459
build it to track falsehoods, it's not gonna track

1173
00:57:12.459 --> 00:57:17.199
this kind of very dangerous disinformation. Um, CAMPAIGNS that

1174
00:57:17.199 --> 00:57:19.280
use this kind of pragmatic mechanisms that I that

1175
00:57:19.280 --> 00:57:21.639
I mentioned earlier. So that's, that's what they do

1176
00:57:21.639 --> 00:57:22.270
in the chapter.

1177
00:57:23.399 --> 00:57:26.399
Uh, BY the way, since you mentioned journalism and

1178
00:57:26.399 --> 00:57:29.399
you also refer to the fact that you don't

1179
00:57:29.399 --> 00:57:33.120
like the standard definition of this information because it

1180
00:57:33.120 --> 00:57:37.040
implies that it has to be intentionally spread with

1181
00:57:37.040 --> 00:57:40.550
the intent of generating false beliefs or something like

1182
00:57:40.550 --> 00:57:43.600
that in other people. I guess that's another good

1183
00:57:43.600 --> 00:57:48.199
example of exactly that would be when sometimes on

1184
00:57:48.199 --> 00:57:52.750
TV for example, they do debates between And a

1185
00:57:52.750 --> 00:57:59.070
climate scientist, expert, and another climate scientist expert who

1186
00:57:59.070 --> 00:58:03.300
is actually a science denier, and they present them

1187
00:58:03.300 --> 00:58:06.989
on an equal playing field. And then, oh, we're

1188
00:58:06.989 --> 00:58:09.070
just going to do the debate. It's for the

1189
00:58:09.070 --> 00:58:12.429
viewers to decide, uh, and I mean, no, it's

1190
00:58:12.429 --> 00:58:13.570
not for the viewers.

1191
00:58:14.790 --> 00:58:17.070
I have, I have previous work on, on this,

1192
00:58:17.110 --> 00:58:19.629
that that is a problem. Um, AND it's very.

1193
00:58:19.774 --> 00:58:21.445
People to see the problem and there is a

1194
00:58:21.445 --> 00:58:23.844
lot of empirical literature that shows that that way

1195
00:58:23.844 --> 00:58:26.044
of doing it generates a lot of false beliefs,

1196
00:58:26.084 --> 00:58:29.165
and I want to say rationally so because what

1197
00:58:29.165 --> 00:58:32.044
you're suggesting by bringing one testifier that says that

1198
00:58:32.044 --> 00:58:34.354
P and one testifier that says that non P,

1199
00:58:34.485 --> 00:58:38.794
you're generating an evidential situation that's 50/50. So you're

1200
00:58:38.794 --> 00:58:42.074
it's rational to be neutral under those circumstances because

1201
00:58:42.074 --> 00:58:44.485
the way in which these people are presented is

1202
00:58:44.485 --> 00:58:48.669
as though they have equal evidential standing. Now what

1203
00:58:48.669 --> 00:58:50.310
I want to say in defense of my colleagues,

1204
00:58:50.350 --> 00:58:52.949
the journalists is that for the longest time, you

1205
00:58:52.949 --> 00:58:55.989
know, journalism is, is, in some countries is kind

1206
00:58:55.989 --> 00:58:58.790
of, you know, officially regulated more than in others,

1207
00:58:58.949 --> 00:59:01.949
but if you look at kind of the on

1208
00:59:01.949 --> 00:59:04.550
the ontological codes that regulate journalists, they tend to

1209
00:59:04.550 --> 00:59:06.909
have this kind of 5 or 6, you know,

1210
00:59:06.989 --> 00:59:10.574
principles that should, should guide the activity. Of journalists

1211
00:59:10.824 --> 00:59:12.854
and the problem with these principles more often than

1212
00:59:12.854 --> 00:59:15.425
not is that they include two principles that often

1213
00:59:15.425 --> 00:59:18.185
contradict each other. So one principle is the reliability

1214
00:59:18.185 --> 00:59:21.584
principle that says, you know, do whatever you can

1215
00:59:21.584 --> 00:59:24.465
to, to, you know, report the truth rather than

1216
00:59:24.465 --> 00:59:27.784
not, right? So that, that's good, um, but then.

1217
00:59:28.229 --> 00:59:32.439
There is this kind of alleged objectivity or fairness

1218
00:59:32.439 --> 00:59:38.570
principle where you're supposed to bring people from all

1219
00:59:38.570 --> 00:59:41.350
corners of the debate and present them as you

1220
00:59:41.350 --> 00:59:45.959
just as you just suggested. And what is sometimes,

1221
00:59:46.010 --> 00:59:48.649
and everybody thinks these two principles are in conflict

1222
00:59:48.649 --> 00:59:51.409
because people think, you know, if there's 3 positions

1223
00:59:51.409 --> 00:59:53.370
out there, you need to bring one for, for

1224
00:59:53.370 --> 00:59:56.169
each and every one of them, right? Um, BECAUSE

1225
00:59:56.169 --> 00:59:58.770
otherwise it's not fair. But of course you know

1226
00:59:58.770 --> 01:00:01.169
that only person A is telling the truth. So

1227
01:00:01.169 --> 01:00:03.810
the principle of reliability tells you don't bring person

1228
01:00:03.810 --> 01:00:06.530
B and C, just bring only person A. Uh,

1229
01:00:06.610 --> 01:00:08.679
I think that that's, you know, it's a problem

1230
01:00:08.679 --> 01:00:10.969
with those principles, and they should be revised, but

1231
01:00:10.969 --> 01:00:12.820
I also think that there's a bad, that is

1232
01:00:12.820 --> 01:00:15.610
a bad interpretation of what the fairness or the

1233
01:00:15.610 --> 01:00:18.770
objectivity principle should say. I think the fairness, it

1234
01:00:18.770 --> 01:00:22.614
is the fairness and objectivity principles. Should give as

1235
01:00:22.614 --> 01:00:27.645
much exposure as the probability um of being right

1236
01:00:28.135 --> 01:00:30.534
uh is so you know, I mean sometimes topics

1237
01:00:30.534 --> 01:00:33.814
are complicated, you know, you, you know that you,

1238
01:00:33.895 --> 01:00:36.254
you don't really know where the truth lies, right?

1239
01:00:36.550 --> 01:00:38.080
So there are going to be people who argue

1240
01:00:38.080 --> 01:00:39.919
that P is the case and people who argue

1241
01:00:39.919 --> 01:00:41.679
that non P is the case, and you as

1242
01:00:41.679 --> 01:00:43.550
a journalist you're going to have a set of

1243
01:00:43.550 --> 01:00:47.560
evidence and your set of evidence suggests that this

1244
01:00:47.560 --> 01:00:51.120
that P is 0.7 probable and non P is

1245
01:00:51.120 --> 01:00:53.800
0.3, where that's the kind of exposure that you

1246
01:00:53.800 --> 01:00:56.439
should give if you're a truth searcher, you should

1247
01:00:56.439 --> 01:00:59.280
give an exposure that is, uh, you know, that

1248
01:00:59.280 --> 01:01:00.919
has the same weight as the evidence that you

1249
01:01:00.919 --> 01:01:02.550
have for the truth of P. IF you want

1250
01:01:02.550 --> 01:01:06.800
to still follow the, the reliability principle. So I

1251
01:01:06.800 --> 01:01:09.239
think that in a way, all I can say

1252
01:01:09.239 --> 01:01:11.669
about my colleagues, the journalists is that it's not

1253
01:01:12.360 --> 01:01:14.679
only that they're making a mistake, it's also a

1254
01:01:14.679 --> 01:01:17.280
mistake that is warranted by the best the ontological

1255
01:01:17.280 --> 01:01:19.679
codes that we have right now and which they

1256
01:01:19.679 --> 01:01:22.360
need to be revised, uh, very substantively.

1257
01:01:22.860 --> 01:01:25.649
Mhm. Yeah, I, I mean, I brought up that

1258
01:01:25.649 --> 01:01:29.169
example also because, uh, in that particular case, at

1259
01:01:29.169 --> 01:01:33.129
least it doesn't seem to me that if this

1260
01:01:33.129 --> 01:01:37.649
information is generated in those cases, it is intentionally

1261
01:01:37.649 --> 01:01:40.110
done, right? I mean, I don't think that there's

1262
01:01:40.370 --> 01:01:46.290
any journalist out there bringing on a science denier

1263
01:01:46.290 --> 01:01:49.290
just to intentionally mislead to the contrary.

1264
01:01:49.415 --> 01:01:51.284
And my colleagues indeed are very worried that they

1265
01:01:51.284 --> 01:01:52.915
have to do that, and they do it because

1266
01:01:52.915 --> 01:01:55.284
that's what the the ontological code says. And I

1267
01:01:55.284 --> 01:01:58.125
bet that who, you know, whoever this, uh, uh,

1268
01:01:58.284 --> 01:02:00.864
this, the, the fathers and mothers of this the

1269
01:02:00.864 --> 01:02:03.764
ontological code that, that put this the ontological code

1270
01:02:03.764 --> 01:02:06.004
together back in the day also had very good

1271
01:02:06.004 --> 01:02:09.205
intentions. Um, YOU know, probably they were living in

1272
01:02:09.205 --> 01:02:13.165
a simple, a simpler epistemic environment. I would, I

1273
01:02:13.165 --> 01:02:15.104
would suggest where this kind of norms would make

1274
01:02:15.104 --> 01:02:18.199
more sense. Um, SO in, indeed, most of the

1275
01:02:18.199 --> 01:02:20.439
time this kind of this is exactly what happens,

1276
01:02:20.520 --> 01:02:21.800
but I want to say that even with the

1277
01:02:21.800 --> 01:02:24.800
exact the, the example that I was mentioning earlier

1278
01:02:24.800 --> 01:02:27.520
with that there's disagreement in in science about climate

1279
01:02:27.520 --> 01:02:33.000
change, look, this, this pragmatic phenomena are studied in

1280
01:02:33.000 --> 01:02:36.199
speech act theory, which is a scientific field and

1281
01:02:36.199 --> 01:02:38.975
which most. People don't study and particularly in journalists

1282
01:02:38.975 --> 01:02:41.975
nobody teaches you uh speech act theory that much

1283
01:02:41.975 --> 01:02:44.764
so the fact that this pragmatic phenomena are generated

1284
01:02:45.094 --> 01:02:48.334
are not things that your regular journalists should be

1285
01:02:48.334 --> 01:02:50.375
aware of. So if I come on TV and

1286
01:02:50.375 --> 01:02:53.084
I say there's disagreement in science about climate change,

1287
01:02:53.455 --> 01:02:55.084
when what I mean by that is that there's

1288
01:02:55.084 --> 01:02:59.780
just 11 guy disagreeing. Uh, I cannot be blamed

1289
01:02:59.780 --> 01:03:02.540
for generating the pragmatic phenomena that I just generated

1290
01:03:02.540 --> 01:03:04.949
because I'm not aware of how these mechanisms work

1291
01:03:04.949 --> 01:03:07.659
because I haven't studied this field. So that's why

1292
01:03:07.659 --> 01:03:09.800
we need a bit more knowledge exchange from science

1293
01:03:09.800 --> 01:03:12.300
towards practice because it's not the job of the

1294
01:03:12.300 --> 01:03:15.179
journalists to know about, you know, Gri and pragmatics.

1295
01:03:15.219 --> 01:03:17.370
It's the job of the scientists to come and

1296
01:03:17.370 --> 01:03:20.580
inform, uh, the field and explain how these mechanisms

1297
01:03:20.580 --> 01:03:23.500
work in order to make for better the ontological

1298
01:03:23.500 --> 01:03:24.500
codes for journalists.

1299
01:03:24.979 --> 01:03:28.010
Mhm. So just to wrap things up and perhaps

1300
01:03:28.010 --> 01:03:32.199
to just summarize this last bit about this information,

1301
01:03:32.459 --> 01:03:35.010
uh, could you tell us about, uh, then your

1302
01:03:35.010 --> 01:03:39.919
account of this information as ignorance generating content? What,

1303
01:03:40.889 --> 01:03:43.090
when you get rid of the dictionary definition, you're

1304
01:03:43.090 --> 01:03:45.209
not left with much because we just got rid

1305
01:03:45.209 --> 01:03:47.330
of the intention, we got rid of the falsehood.

1306
01:03:47.409 --> 01:03:49.000
So what is, what is it that we're left

1307
01:03:49.000 --> 01:03:53.060
with? Uh, SO, so stepping back, looking at all

1308
01:03:53.060 --> 01:03:55.939
these possible ways in which you can, um, run

1309
01:03:55.939 --> 01:04:00.139
a disinformation campaign, what, what this way suggests is

1310
01:04:00.139 --> 01:04:02.139
that what they all have in common is that

1311
01:04:02.139 --> 01:04:04.899
they generate ignorance in the audience. You can do

1312
01:04:04.899 --> 01:04:07.989
that. Via many ways by saying something false or

1313
01:04:07.989 --> 01:04:11.110
by saying something true that has a pragmatic implicature

1314
01:04:11.110 --> 01:04:13.350
and so on and so forth, but what all

1315
01:04:13.350 --> 01:04:16.060
these ways of generating this information have in common

1316
01:04:16.060 --> 01:04:19.699
is that they have a disposition to generate ignorance

1317
01:04:19.699 --> 01:04:23.580
in your hearer. And it's important to understand that

1318
01:04:23.580 --> 01:04:26.250
because as soon as we understand that we also

1319
01:04:26.250 --> 01:04:29.340
realize that whether some content is disinformation or not

1320
01:04:29.340 --> 01:04:33.459
is highly hearer dependent. The exact same content asserted

1321
01:04:33.459 --> 01:04:35.659
in one context is going to generate a lot

1322
01:04:35.659 --> 01:04:38.379
of false beliefs asserted in another context is going

1323
01:04:38.379 --> 01:04:42.179
to generate no false beliefs, depending on the evidential

1324
01:04:42.179 --> 01:04:45.770
background of the audience. And in We already know

1325
01:04:45.770 --> 01:04:50.530
that disinformation campaigns are targeting particular audience groups because

1326
01:04:50.530 --> 01:04:53.290
they know that they're particularly vulnerable in virtue of

1327
01:04:53.290 --> 01:04:56.719
their evidential background. So just to go back to

1328
01:04:56.719 --> 01:04:59.810
the disagreement says about climate change, if I watch

1329
01:04:59.810 --> 01:05:01.830
a journalist saying that, I'm not going to follow

1330
01:05:01.830 --> 01:05:03.610
the first belief that there's a huge amount of

1331
01:05:03.610 --> 01:05:05.870
disagreement because I know how speech act theory works

1332
01:05:05.870 --> 01:05:07.409
and I know that the journalist is just making

1333
01:05:07.409 --> 01:05:10.590
a mistake. If that journalist comes on TV and

1334
01:05:10.590 --> 01:05:12.750
tells my grandmother the same thing, she will probably

1335
01:05:12.750 --> 01:05:15.469
uh get this uh false belief because she's not

1336
01:05:15.469 --> 01:05:18.790
a specialist in speech act theory. So it's important

1337
01:05:18.790 --> 01:05:21.909
that we understand that what whether content is this

1338
01:05:21.909 --> 01:05:25.949
information depends on the background evidence of the hearers,

1339
01:05:26.560 --> 01:05:31.000
because that also explains why some people are, you

1340
01:05:31.000 --> 01:05:34.959
know, more vulnerable to this, to some particular disinformation

1341
01:05:34.959 --> 01:05:39.280
messages and thereby end up rejecting scientific evidence. Well,

1342
01:05:39.399 --> 01:05:42.919
some people aren't. That's a, you know, something that's,

1343
01:05:43.000 --> 01:05:44.479
let me give you an example of how important

1344
01:05:44.479 --> 01:05:46.449
it is in the real world. So Public Health

1345
01:05:46.449 --> 01:05:50.699
Scotland. When they communicated, uh, the safety, the safety

1346
01:05:50.699 --> 01:05:52.260
of vaccines, they just did it the same with

1347
01:05:52.260 --> 01:05:54.300
all audiences, so they sent us all a little

1348
01:05:54.300 --> 01:05:56.939
flyer that said vaccines are safe, come and get

1349
01:05:56.939 --> 01:05:59.080
them. Um, THAT was, that was all they did.

1350
01:05:59.300 --> 01:06:01.979
Now of course we have very different evidential backgrounds

1351
01:06:01.979 --> 01:06:04.290
around here in Scotland because they're very diverse people.

1352
01:06:04.540 --> 01:06:07.739
So for instance, the same flyers sent um to

1353
01:06:07.739 --> 01:06:12.409
white communities generated 87% or something vaccine uptake sent

1354
01:06:12.409 --> 01:06:15.395
to African and black communities generated only 40%. uh

1355
01:06:15.395 --> 01:06:19.415
VACCINE uptake the same message. Why? Different evidential backgrounds.

1356
01:06:19.475 --> 01:06:21.554
The black community has a, you know, a lot

1357
01:06:21.554 --> 01:06:25.205
of inductive evidence of discrimination from, from the public

1358
01:06:25.205 --> 01:06:27.915
health authorities, which, you know, I don't. They also

1359
01:06:27.915 --> 01:06:30.074
have a lot of background evidence of lack of

1360
01:06:30.074 --> 01:06:34.514
trustworthiness of institutions more uh more generally and institutional

1361
01:06:34.514 --> 01:06:36.995
ignorance again, I don't have that problem, so it

1362
01:06:36.995 --> 01:06:39.435
is highly surprising that I trusted the flyer more

1363
01:06:39.435 --> 01:06:41.919
than they did. Um, SO in that, in the

1364
01:06:41.919 --> 01:06:44.639
same way in which Public Health Scotland, uh, failed

1365
01:06:44.639 --> 01:06:46.840
to kind of tailor their message to the evidential

1366
01:06:46.840 --> 01:06:48.439
background of the audience in the same way if

1367
01:06:48.439 --> 01:06:51.840
you want to spread, um, spread this information, you

1368
01:06:51.840 --> 01:06:54.750
can tailor your message to the audience's evidential background

1369
01:06:54.750 --> 01:06:58.399
and target the more vulnerable, um, communities. So that's

1370
01:06:58.399 --> 01:07:01.560
why it's important to understand that what matters is

1371
01:07:01.560 --> 01:07:03.800
the ignorance of the hearer rather than what is

1372
01:07:03.800 --> 01:07:06.419
actually written on the flyer. Mhm.

1373
01:07:06.899 --> 01:07:10.229
Great. So, the book is again resistance to Evidence.

1374
01:07:10.340 --> 01:07:12.530
I'm leaving a link to it in the description

1375
01:07:12.530 --> 01:07:15.540
of the interview and Doctor Simeon, apart from the

1376
01:07:15.540 --> 01:07:18.090
book, would you like to just tell people where

1377
01:07:18.090 --> 01:07:20.330
they can find you and the rest of your

1378
01:07:20.330 --> 01:07:21.500
work on the internet?

1379
01:07:22.310 --> 01:07:23.939
Yeah, so if you Google me, you're probably going

1380
01:07:23.939 --> 01:07:26.330
to come across my, uh, my website and my

1381
01:07:26.330 --> 01:07:29.409
university, uh, web page and where I have, um,

1382
01:07:29.419 --> 01:07:32.979
a bunch of, um, drafts and PDFs of my

1383
01:07:32.979 --> 01:07:35.020
papers if you're interested in more detail about my

1384
01:07:35.020 --> 01:07:37.659
work. Um, AND the book in good news is

1385
01:07:37.659 --> 01:07:41.419
open access with Cambridge, uh, due to very generous

1386
01:07:41.419 --> 01:07:43.459
grants from the European Research Council. So if you

1387
01:07:43.459 --> 01:07:44.889
want to read the book, you don't need to,

1388
01:07:45.219 --> 01:07:46.699
um, go ahead and buy it, but you can

1389
01:07:46.699 --> 01:07:48.530
just go on the Cambridge website where you can

1390
01:07:48.530 --> 01:07:49.459
find it open access.

1391
01:07:49.909 --> 01:07:52.709
Yeah, like I did. So and I'm very grateful

1392
01:07:52.709 --> 01:07:55.830
for that. So, Doctor Simeon, thank you so much

1393
01:07:55.830 --> 01:07:57.550
again for taking the time to come on the

1394
01:07:57.550 --> 01:07:59.500
show. It's been a real pleasure to talk with

1395
01:07:59.500 --> 01:07:59.939
you.

1396
01:08:00.229 --> 01:08:01.669
Thank you so much for having me. It's been

1397
01:08:01.669 --> 01:08:02.669
great. Thank you.

1398
01:08:03.770 --> 01:08:06.260
Hi guys, thank you for watching this interview until

1399
01:08:06.260 --> 01:08:08.439
the end. If you liked it, please share it,

1400
01:08:08.610 --> 01:08:11.399
leave a like and hit the subscription button. The

1401
01:08:11.399 --> 01:08:13.600
show is brought to you by Nights Learning and

1402
01:08:13.600 --> 01:08:17.680
Development done differently, check their website at Nights.com and

1403
01:08:17.680 --> 01:08:21.399
also please consider supporting the show on Patreon or

1404
01:08:21.399 --> 01:08:23.879
PayPal. I would also like to give a huge

1405
01:08:23.879 --> 01:08:26.990
thank you to my main patrons and PayPal supporters

1406
01:08:27.399 --> 01:08:31.200
Pergo Larsson, Jerry Mullern, Fredrik Sundo, Bernard Seyches Olaf,

1407
01:08:31.319 --> 01:08:35.040
Alexandam Castle, Matthew Whitting Berarna Wolf, Tim Hollis, Erika

1408
01:08:35.040 --> 01:08:38.249
Lenny, John Connors, Philip Fors Connolly. Then the Matter

1409
01:08:38.249 --> 01:08:42.707
Robert Windegaruyasi Zup Mark Neevs called Holbrook field governor

1410
01:08:42.707 --> 01:08:46.788
Michael Stormir, Samuel Andre, Francis Forti Agnsergoro and Hal

1411
01:08:46.788 --> 01:08:50.908
Herzognun Macha Joan Labrant John Jasent and Samuel Corriere,

1412
01:08:51.068 --> 01:08:54.738
Heinz, Mark Smith, Jore, Tom Hummel, Sardus France David

1413
01:08:54.738 --> 01:08:58.608
Sloan Wilson, asilla dearraujuru and Roach Diego Londono Correa.

1414
01:08:58.979 --> 01:09:04.578
Yannick Punteran Rosmani Charlotte blinikolbar Adamhn Pavlostaevsky nale back

1415
01:09:04.578 --> 01:09:08.618
medicine, Gary Galman Sam of Zallidrianei Poltonin John Barboza,

1416
01:09:08.658 --> 01:09:13.179
Julian Price, Edward Hall Edin Bronner, Douglas Fry, Franco

1417
01:09:13.179 --> 01:09:18.139
Bartolotti Gabrielon Corteseus Slelitsky, Scott Zachary Fish Tim Duffyani

1418
01:09:18.139 --> 01:09:22.639
Smith Jen Wieman. Daniel Friedman, William Buckner, Paul Georgianneau,

1419
01:09:23.048 --> 01:09:27.568
Luke Lovai Giorgio Theophanous, Chris Williamson, Peter Wozin, David

1420
01:09:27.568 --> 01:09:32.207
Williams, Diocosta, Anton Eriksson, Charles Murray, Alex Shaw, Marie

1421
01:09:32.207 --> 01:09:36.528
Martinez, Coralli Chevalier, bungalow atheists, Larry D. Lee Junior,

1422
01:09:36.729 --> 01:09:41.930
old Erringbo. Sterry Michael Bailey, then Sperber, Robert Grayigoren,

1423
01:09:42.129 --> 01:09:46.580
Jeff McMann, Jake Zu, Barnabas radix, Mark Campbell, Thomas

1424
01:09:46.580 --> 01:09:50.899
Dovner, Luke Neeson, Chris Storry, Kimberly Johnson, Benjamin Gilbert,

1425
01:09:51.049 --> 01:09:56.390
Jessica Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George

1426
01:09:56.390 --> 01:10:01.660
Eoriatis, Valentin Steinman, Perkrolis, Kate van Goller, Alexander Hubbert,

1427
01:10:02.439 --> 01:10:08.279
Liam Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner,

1428
01:10:08.399 --> 01:10:13.600
Ursulauddinov, Gregory Hastings, David Pinsoff Sean Nelson, Mike Levin,

1429
01:10:14.000 --> 01:10:17.399
and Jos Net. A special thanks to my producers.

1430
01:10:17.410 --> 01:10:20.160
These are Webb, Jim, Frank Lucas Steffinik, Tom Venneden,

1431
01:10:20.259 --> 01:10:24.779
Bernard Curtis Dixon, Benedict Muller, Thomas Trumbull, Catherine and

1432
01:10:24.779 --> 01:10:28.020
Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick

1433
01:10:28.020 --> 01:10:31.490
Golden, and to my executive producers, Matthew Levender, Sergio

1434
01:10:31.490 --> 01:10:34.770
Quadrian, Bogdan Kanivets, and Rosie. Thank you for all.

