WEBVTT

1
00:00:00.140 --> 00:00:03.089
Hello, everyone. Welcome to a new episode of the

2
00:00:03.089 --> 00:00:06.010
Center. I'm your host, as always, Ricardo Lopes and

3
00:00:06.010 --> 00:00:09.609
today I'm joined by Doctor Stephen Morris. He's associate

4
00:00:09.609 --> 00:00:12.760
professor of philosophy at the College of Staten Island

5
00:00:12.760 --> 00:00:16.600
slash CUNY. And today we're going to talk about

6
00:00:16.809 --> 00:00:21.229
his book Moral Damages The Case for Abolishing Morality.

7
00:00:21.370 --> 00:00:25.569
So, Doctor Morris, there it is. Doctor Morris, welcome

8
00:00:25.569 --> 00:00:27.489
to the show. It's a pleasure to have everyone.

9
00:00:28.290 --> 00:00:29.840
It's a pleasure to meet with you, Ricardo.

10
00:00:31.120 --> 00:00:35.479
So, before we get into moral abolitionism, which is

11
00:00:35.479 --> 00:00:37.310
the main topic of your book,

12
00:00:37.639 --> 00:00:37.869
how

13
00:00:37.869 --> 00:00:42.189
do people commonly approach and think about morality?

14
00:00:42.479 --> 00:00:44.990
It's a great question. It's a very complex question.

15
00:00:45.119 --> 00:00:48.000
Uh, THERE, there's no one single way, uh, that,

16
00:00:48.080 --> 00:00:50.630
that people, uh, across the world and across time,

17
00:00:50.779 --> 00:00:53.580
uh, think about morality. Um, BUT we have a

18
00:00:53.580 --> 00:00:56.900
much better understanding of how people generally conceive of

19
00:00:56.900 --> 00:00:59.290
morality now, uh, than we did in the past,

20
00:00:59.580 --> 00:01:02.490
uh, and we're still learning, of course. And, uh,

21
00:01:02.500 --> 00:01:05.330
although there is a lot of variation both between

22
00:01:05.330 --> 00:01:08.779
how people generally conceive of morality and certainly specific

23
00:01:08.779 --> 00:01:11.930
ethical beliefs, and I'm gonna use morality and ethical,

24
00:01:12.059 --> 00:01:15.669
uh, inter interchangeably. Um, SO, yeah, even though we

25
00:01:15.669 --> 00:01:18.190
find this variation, we do find a lot of

26
00:01:18.190 --> 00:01:22.309
similarity, particularly involving, uh, some, some fundamental aspects of

27
00:01:22.309 --> 00:01:25.639
people's moral views. And these include things like, and,

28
00:01:25.709 --> 00:01:27.989
and again, there, this, this is controversial. So I

29
00:01:27.989 --> 00:01:30.940
should say that, uh, somewhat controversial. There, there's, there's

30
00:01:30.940 --> 00:01:34.540
no uh complete consensus, but again, the, the research

31
00:01:34.540 --> 00:01:38.025
indicates that there are these fundamental elements. OF people's

32
00:01:38.025 --> 00:01:41.904
uh moral views, including, um, what's sometimes referred to

33
00:01:41.904 --> 00:01:45.705
as moral objectivism, this idea that whatever moral facts

34
00:01:45.705 --> 00:01:50.855
exist, they transcend individual opinions or cultural norms. Um,

35
00:01:50.904 --> 00:01:54.345
THERE'S also the idea of obligation that, uh, moral

36
00:01:54.345 --> 00:01:57.864
facts, uh, moral requirements are somewhat, uh, are somehow

37
00:01:57.864 --> 00:02:00.065
obligatory on all of us to act in accordance

38
00:02:00.065 --> 00:02:03.105
with. Um, THERE'S a close connection, and I'm gonna

39
00:02:03.105 --> 00:02:05.849
focus on this throughout my talk. As I do

40
00:02:05.849 --> 00:02:09.449
in the book, close connection between morality and punishment

41
00:02:09.449 --> 00:02:12.699
and blame, uh, very, very similar concepts, a very

42
00:02:12.699 --> 00:02:15.369
specific notion of blame. Uh, AND, and this is

43
00:02:15.369 --> 00:02:18.970
perhaps the most controversial, uh, aspect of my, my

44
00:02:18.970 --> 00:02:21.250
conception of, of the kind of morality that I

45
00:02:21.250 --> 00:02:25.800
attribute to most individuals, uh, non-hilosophers, laypersons, or, or

46
00:02:25.850 --> 00:02:27.789
What we call the folk. And that is this

47
00:02:27.789 --> 00:02:31.600
retributivist element. Now, think of retributivism as a sort

48
00:02:31.600 --> 00:02:33.919
of blame, uh, the, the, the sort of eye

49
00:02:33.919 --> 00:02:37.229
for an eye basis for punishment, right? That, uh,

50
00:02:37.240 --> 00:02:40.190
people are punished in a way that fits with,

51
00:02:40.199 --> 00:02:43.320
uh, their, their actions, their responses, uh, to their

52
00:02:43.320 --> 00:02:45.960
actions. If they've done bad, uh, they deserve punishment,

53
00:02:46.000 --> 00:02:47.910
if they've done. Good, they would deserve some sort

54
00:02:47.910 --> 00:02:50.889
of reward. Uh, WHERE the punishment and reward, uh,

55
00:02:50.910 --> 00:02:55.000
corresponds to the degree of rightness or wrongness or

56
00:02:55.000 --> 00:02:58.110
rightness of their actions, right? So, uh, think about

57
00:02:58.110 --> 00:03:01.250
punishment. If you're punishing someone on retributiveist grounds, you're

58
00:03:01.250 --> 00:03:03.729
doing it just because they, in a sense, deserve

59
00:03:03.729 --> 00:03:07.509
to be punishment, be punished. It's intrinsically right. Uh,

60
00:03:07.550 --> 00:03:09.639
THIS is different from punishment, let's say, with an

61
00:03:09.639 --> 00:03:13.229
eye towards shaping their behavior or deterring others, uh,

62
00:03:13.350 --> 00:03:15.789
having some sort of consequentialist stand in the future.

63
00:03:15.910 --> 00:03:19.550
So, um, in my book, then, I, I use

64
00:03:19.550 --> 00:03:22.979
a, a notion, uh, of, um, morality that I'm

65
00:03:22.979 --> 00:03:26.690
arguing we ought to abolish that contains these elements,

66
00:03:26.699 --> 00:03:31.770
uh, objectivism, it's realist, uh, uh, it is, uh,

67
00:03:31.789 --> 00:03:34.630
tied closely to blame, uh, and retributism.

68
00:03:36.190 --> 00:03:39.710
Uh, AND what is moral abolitionism then? What does

69
00:03:39.710 --> 00:03:42.070
that mean and what is the kind of moral

70
00:03:42.070 --> 00:03:45.380
abolitionism you are arguing for in the book?

71
00:03:46.520 --> 00:03:50.479
OK. So, the moral abolitionism basically is to try

72
00:03:50.479 --> 00:03:54.399
to eliminate insofar as possible, all remnants of morality

73
00:03:54.399 --> 00:03:57.800
from our deliberations, from our language, um, and, and

74
00:03:57.800 --> 00:04:00.600
from our, uh, social institutions in the way that

75
00:04:00.600 --> 00:04:04.410
society is, is organized. Um, NOW, this can be,

76
00:04:04.889 --> 00:04:08.449
it is easier said than done. Um, BUT nonetheless,

77
00:04:08.539 --> 00:04:11.479
there, there are good reasons for being an abolitionist

78
00:04:11.479 --> 00:04:15.059
about morality. So, uh, one reason that people would

79
00:04:15.059 --> 00:04:17.730
argue for abolishing morality is that if you take

80
00:04:17.730 --> 00:04:19.779
the view that there are no moral facts, which

81
00:04:19.779 --> 00:04:22.480
is a views uh referred to generally. IS error

82
00:04:22.480 --> 00:04:25.119
theory. The idea that, uh, when people speak about

83
00:04:25.119 --> 00:04:28.880
morality, they are assuming or asserting the existence of

84
00:04:28.880 --> 00:04:31.709
moral facts. But as it turns out, there are

85
00:04:31.709 --> 00:04:33.720
no moral facts. And so, and, and I'll talk

86
00:04:33.720 --> 00:04:35.519
about this later on, and so that they're in

87
00:04:35.519 --> 00:04:38.839
error. So one argument, which is not the main

88
00:04:38.839 --> 00:04:41.390
one I should mention, for moral abolitionism is that

89
00:04:42.089 --> 00:04:45.170
Because moral facts don't exist, um, when we speak

90
00:04:45.170 --> 00:04:47.959
about them, we're speaking falsely. So for, uh, uh,

91
00:04:47.970 --> 00:04:51.209
for epistemic reasons, uh, we ought to do away

92
00:04:51.209 --> 00:04:53.040
with this kind of speech, uh, in the same

93
00:04:53.040 --> 00:04:56.410
way that we shouldn't make, um, uh, assertions that

94
00:04:56.410 --> 00:04:58.459
we, that we take to be truthful about unicorns

95
00:04:58.459 --> 00:05:01.290
and the like. Now, the main reason, the other

96
00:05:01.290 --> 00:05:05.700
reason for, uh, abolishing morality is that The claim

97
00:05:05.700 --> 00:05:08.750
is, is that morality in, in, in a particular

98
00:05:08.750 --> 00:05:10.579
sense in the sense that I'm speaking with, uh,

99
00:05:10.589 --> 00:05:14.510
of, uh, actually, uh, causes more harm than benefits

100
00:05:14.510 --> 00:05:17.630
uh for, for uh human society. And so, this

101
00:05:17.630 --> 00:05:20.429
is a purely pragmatic argument, all right? So it's

102
00:05:20.429 --> 00:05:22.589
not an argument, clearly not an argument based on

103
00:05:22.589 --> 00:05:25.190
morality. It's an argument about what would be best

104
00:05:25.190 --> 00:05:27.589
for human beings, for individual lives and for our

105
00:05:27.589 --> 00:05:30.269
lives collectively. And the claim is that we're better

106
00:05:30.269 --> 00:05:33.269
off without morality. And this is certainly a very

107
00:05:33.269 --> 00:05:36.230
provocative and, and controversial and that at the time

108
00:05:36.230 --> 00:05:38.589
where I'm, I'm talking about it, um, not a

109
00:05:38.589 --> 00:05:41.029
very popular view, although it does seem to be

110
00:05:41.029 --> 00:05:45.149
getting, gaining some traction, um, And so, one of

111
00:05:45.149 --> 00:05:48.989
the main um uh uh things I'm trying to

112
00:05:48.989 --> 00:05:51.750
accomplish in this book is get a dialogue going,

113
00:05:51.829 --> 00:05:54.790
you know, is morality, uh, a good thing overall

114
00:05:54.790 --> 00:05:57.470
for, for humanity, as, as virtually everybody thinks, and

115
00:05:57.470 --> 00:05:59.350
I'm gonna argue, no, it's not.

116
00:06:00.980 --> 00:06:03.410
Right, and we're going to get into more detail

117
00:06:03.410 --> 00:06:07.220
in in terms of your view of moral evolutionism

118
00:06:07.220 --> 00:06:10.420
later on in our conversation. But tell us now

119
00:06:10.420 --> 00:06:14.049
about the analogy you established in the book between

120
00:06:14.049 --> 00:06:17.079
morality and the sickle cell trait.

121
00:06:18.309 --> 00:06:21.850
OK. So, I, I use the analogy comparing uh

122
00:06:21.850 --> 00:06:25.130
the human moral faculty to the uh sickle cell

123
00:06:25.130 --> 00:06:26.989
trait in human beings. And I think there's a

124
00:06:26.989 --> 00:06:31.290
lot of similarities between the two. NOW, a, a

125
00:06:31.290 --> 00:06:35.100
major part of my project in, in this book,

126
00:06:35.209 --> 00:06:37.799
um, moral damage is the case for abolishing morality.

127
00:06:38.359 --> 00:06:41.519
Is that, uh, morality, the moral faculty is, is

128
00:06:41.519 --> 00:06:44.440
an adaptation. It was selected for because it conferred

129
00:06:44.440 --> 00:06:48.040
survival advantages onto our ancestors, OK? Uh, NOW, this

130
00:06:48.040 --> 00:06:50.920
is a very, uh, this view is not, I,

131
00:06:51.040 --> 00:06:54.000
I would say there's no, it's not a unanimous

132
00:06:54.000 --> 00:06:58.730
opinion among Uh, philosophers, but it, it, it's becoming,

133
00:06:58.769 --> 00:07:01.649
I think, the, the most, uh, or, or becoming

134
00:07:01.649 --> 00:07:05.100
very accepted. I, I think particularly among moral psychologists,

135
00:07:05.250 --> 00:07:08.980
uh, among scientists who look into morality, right? This

136
00:07:08.980 --> 00:07:11.690
idea, you know, how, where does this, this, uh,

137
00:07:11.815 --> 00:07:14.515
Um, propensity to, to, to think of the world

138
00:07:14.515 --> 00:07:17.845
in moral terms come from. And, uh, the best

139
00:07:17.845 --> 00:07:20.084
sort of explanation that I've seen, and again, I

140
00:07:20.084 --> 00:07:24.484
think a lot of psychologists, uh, evolutionary biologists, neuroscientists,

141
00:07:24.605 --> 00:07:26.684
and, and a lot of increasing numbers of philosophers,

142
00:07:26.725 --> 00:07:28.404
I should say, would back me up on this.

143
00:07:28.765 --> 00:07:32.325
It's that we developed this because it was extremely

144
00:07:32.325 --> 00:07:35.924
important to help our ancestors cooperate in groups, right?

145
00:07:36.084 --> 00:07:40.070
So, uh, certainly, there's other facets of our Uh,

146
00:07:40.079 --> 00:07:42.510
psyche, we have empathy, we have our, you know,

147
00:07:42.799 --> 00:07:46.209
prudential considerations that, that lead us to cooperate with

148
00:07:46.209 --> 00:07:50.000
individuals. Um, BUT oftentimes, those sort of catalysts to

149
00:07:50.000 --> 00:07:53.480
cooperative behavior can be overrun by selfish reasons we

150
00:07:53.480 --> 00:07:55.959
have. And so, the idea is morality is a

151
00:07:55.959 --> 00:07:58.279
sort of, uh, as, as a bulwark or an

152
00:07:58.279 --> 00:08:01.160
extra layer of support to, uh, get us to

153
00:08:01.160 --> 00:08:05.750
act, um, uh, cooperatively with other people. OK. So,

154
00:08:06.040 --> 00:08:10.070
now, uh, As it is an adaptation, I, I

155
00:08:10.070 --> 00:08:13.309
agree with the, the consensus view here that morality

156
00:08:13.309 --> 00:08:18.109
was generally very uh important in, in conferring survival

157
00:08:18.109 --> 00:08:20.940
advantages to our ancestors, OK? In the same way,

158
00:08:20.950 --> 00:08:22.700
and this goes back to the sickle cell trait.

159
00:08:22.910 --> 00:08:25.670
Uh, THE sickle cell trait was very important for

160
00:08:25.670 --> 00:08:31.019
helping our ancient ancestors, uh, resist malaria, OK. Um,

161
00:08:31.940 --> 00:08:35.520
But in more modern times, for several reasons, these

162
00:08:35.520 --> 00:08:38.200
traits, uh, I argue now are maladaptive. Certainly, the

163
00:08:38.200 --> 00:08:41.239
sickle cell trait, um, although it can make one

164
00:08:41.239 --> 00:08:44.190
more resistant, uh, to malaria, it brings its own

165
00:08:44.190 --> 00:08:47.710
ill health effects that generally make it maladaptive, OK?

166
00:08:47.869 --> 00:08:51.520
And, and furthermore, we don't really need uh the

167
00:08:51.520 --> 00:08:54.960
sickle cell trait anymore, at least insofar as we

168
00:08:54.960 --> 00:08:57.799
have access to certain kinds of drugs, uh, that

169
00:08:57.799 --> 00:09:01.409
can actually fight off malaria, all right? So, uh,

170
00:09:02.000 --> 00:09:05.299
Sickle cell used to be adaptive, no longer adaptive,

171
00:09:05.419 --> 00:09:08.159
and it's, it's no longer necessary because at least

172
00:09:08.159 --> 00:09:10.479
for most populations, since we have drugs that can

173
00:09:10.479 --> 00:09:14.309
deal with it. Likewise, I argue, although morality used

174
00:09:14.309 --> 00:09:18.429
to be adaptive, adaptive in, in small close-knit groups,

175
00:09:18.679 --> 00:09:22.159
um, in competition with other small close-knit groups, uh,

176
00:09:22.239 --> 00:09:24.599
it no longer is. All right. So, uh, with

177
00:09:24.599 --> 00:09:29.080
the onset of agrarian civilization, about 10 or 1000

178
00:09:29.080 --> 00:09:30.659
or so years ago, a little bit before that,

179
00:09:30.679 --> 00:09:33.895
we're talking and And the preceding late Pleistocene period.

180
00:09:34.145 --> 00:09:38.775
Um, MORALITY, I'm, I'm arguing has generally been maladaptive.

181
00:09:38.895 --> 00:09:41.775
Uh, ALTHOUGH it certainly has its advantages, still does,

182
00:09:41.905 --> 00:09:45.215
I'm gonna argue that overall, it's no longer adaptive

183
00:09:45.215 --> 00:09:48.635
and, and is more overall harmful than helpful. And

184
00:09:48.984 --> 00:09:51.104
like the drugs that can treat malaria, I think

185
00:09:51.104 --> 00:09:53.414
we have non-moral resources that can provide us with

186
00:09:53.414 --> 00:09:55.984
the advantages that morality used to confer and still

187
00:09:55.984 --> 00:09:59.414
confers, um, with, uh, without all the negative baggage,

188
00:09:59.424 --> 00:10:00.775
uh, that comes along with morality.

189
00:10:01.719 --> 00:10:05.010
So you mentioned there our morality has been the

190
00:10:05.010 --> 00:10:09.369
result of evolution and how our moral faculty holds

191
00:10:09.369 --> 00:10:13.849
its existence to evolutionary factors and you mentioned that

192
00:10:13.849 --> 00:10:18.530
they involve our the propensity to encourage cooperation among

193
00:10:18.530 --> 00:10:23.669
our ancient ancestors and people today. But what other

194
00:10:23.669 --> 00:10:27.130
theoretical and empirical arguments do you have against the

195
00:10:27.130 --> 00:10:29.650
existence of objective moral truths?

196
00:10:30.719 --> 00:10:33.690
Yeah, so there's, there's a lot of different kinds

197
00:10:33.690 --> 00:10:35.690
of arguments. I'll just give a, a, a summary,

198
00:10:35.780 --> 00:10:37.330
Ricardo, and if you'd like me to go into

199
00:10:37.330 --> 00:10:41.489
detail, feel free to ask. So, um, And again,

200
00:10:41.570 --> 00:10:43.369
let me make sure I understand the question. So

201
00:10:43.369 --> 00:10:46.409
the, the question is about, uh, why I, I

202
00:10:46.409 --> 00:10:49.080
believe morality doesn't exist. Was, was that the primary

203
00:10:49.080 --> 00:10:49.780
question? Uh,

204
00:10:49.849 --> 00:10:54.289
I mean, the the main theoretical and empirical arguments

205
00:10:54.289 --> 00:10:57.080
against the existence of objective moral truths.

206
00:10:57.434 --> 00:11:00.434
Objective moral truths, great. Um, YES, uh, thank you

207
00:11:00.434 --> 00:11:03.664
for the clarification. So there, there's, I'll, I'll talk

208
00:11:03.664 --> 00:11:06.275
about some of the major arguments that have been

209
00:11:06.275 --> 00:11:09.515
given that, that I find persuasive. So, um, probably

210
00:11:09.515 --> 00:11:12.674
the, the, the first major argument, uh, that was

211
00:11:12.674 --> 00:11:16.174
given, well, uh, at least in contemporary times, uh,

212
00:11:16.275 --> 00:11:19.104
for why moral truths most likely do not exist,

213
00:11:19.234 --> 00:11:22.114
uh, comes from the error theorist JL Mackey. Again,

214
00:11:22.145 --> 00:11:24.554
error theory is the claim that moral truths don't

215
00:11:24.554 --> 00:11:26.625
exist. When we assert them, we're speaking in error.

216
00:11:27.210 --> 00:11:30.049
And uh he used his so-called argument of queerness

217
00:11:30.049 --> 00:11:32.570
to say that if moral truths existed, they'd be

218
00:11:32.570 --> 00:11:36.450
very strange, um, unlike anything else that we're aware

219
00:11:36.450 --> 00:11:39.049
of, and this gives a sort of a a

220
00:11:39.049 --> 00:11:41.369
reason to reject them, right? So the idea is

221
00:11:41.369 --> 00:11:43.919
that if, if moral truths exist, and he had,

222
00:11:43.969 --> 00:11:46.010
he conceived the morality as the way I, I

223
00:11:46.010 --> 00:11:48.530
argue most people do in this objective sense that

224
00:11:48.530 --> 00:11:54.559
transcends, uh, individual, uh, viewpoints or cultural perspectives. If

225
00:11:54.559 --> 00:11:57.559
they exist, uh, they are sort of categorically binding,

226
00:11:57.650 --> 00:12:00.359
right? So, regardless of what we want, regardless of

227
00:12:00.359 --> 00:12:03.359
what desires we have, uh, there are still, if

228
00:12:03.359 --> 00:12:05.799
these facts exist, moral reasons for us to do

229
00:12:05.799 --> 00:12:08.119
something. And this idea that you can have a

230
00:12:08.119 --> 00:12:10.760
reason for doing something that doesn't correspond with any

231
00:12:10.760 --> 00:12:13.039
desire that you have, is one of the things

232
00:12:13.039 --> 00:12:15.280
that he found very strange. Another thing he found

233
00:12:15.280 --> 00:12:18.239
very strange is this idea Um, how would we,

234
00:12:18.450 --> 00:12:20.890
uh, come to perceive these moral facts? There would

235
00:12:20.890 --> 00:12:23.849
have to be some sort of, uh, uh, intuitional

236
00:12:23.849 --> 00:12:27.049
ability we have, uh, that again, seems like, unlike

237
00:12:27.049 --> 00:12:29.690
anything that we're aware of, um, because it doesn't

238
00:12:29.690 --> 00:12:32.489
seem, despite what a lot of philosophers say, that

239
00:12:32.489 --> 00:12:35.599
we can come to know moral truths through, um,

240
00:12:35.809 --> 00:12:39.609
uh, a priori reasoning or, or, um, empirical observations.

241
00:12:39.869 --> 00:12:41.940
It would have to be some other sort of

242
00:12:41.940 --> 00:12:44.940
extrasensory, uh, extra sensory intuition we have, which seems

243
00:12:44.940 --> 00:12:47.369
very strange. OK. So that, that's one argument against

244
00:12:47.369 --> 00:12:50.580
the existence of uh objective moral truths. Another one,

245
00:12:50.619 --> 00:12:52.940
and, and, and one that I find actually uh

246
00:12:52.940 --> 00:12:57.219
more uh compelling is the arguments against free will.

247
00:12:57.340 --> 00:13:00.380
Now, this is a very complex subject, philosophers are

248
00:13:00.380 --> 00:13:04.140
still arguing about this. Um, And they will into

249
00:13:04.140 --> 00:13:07.140
the foreseeable future, I'm sure. Um, BUT there are

250
00:13:07.140 --> 00:13:12.729
very strong arguments against free will, and Although, again,

251
00:13:12.809 --> 00:13:15.130
there's no consensus on what free will even means,

252
00:13:15.169 --> 00:13:17.640
and this is, this is complicated this subject along

253
00:13:17.640 --> 00:13:19.719
with morality, I should say, for a long time.

254
00:13:19.890 --> 00:13:23.169
Philosophers sort of like having these verbal disagreements because

255
00:13:23.169 --> 00:13:26.080
they can't agree with these, these philosophical terms mean.

256
00:13:26.330 --> 00:13:29.500
But I think, uh, part of the, the research

257
00:13:29.500 --> 00:13:31.210
that I, that I began with some colleagues back

258
00:13:31.210 --> 00:13:34.859
in the early 2000s sought to clarify. What, what

259
00:13:35.229 --> 00:13:38.630
the folk or or non-hilosophers understand free will to

260
00:13:38.630 --> 00:13:41.000
be. And this has been actually clarified, I think

261
00:13:41.000 --> 00:13:43.909
even more so by the work of moral psychologists

262
00:13:43.909 --> 00:13:47.539
like Aimm Sharif and, and Corey Clark, um, and,

263
00:13:47.549 --> 00:13:50.140
and they've come to, uh, I, I, I think,

264
00:13:50.549 --> 00:13:53.460
really help establish the view that, that the folk

265
00:13:53.460 --> 00:13:55.729
understanding of free will is. IDEA that we are

266
00:13:55.729 --> 00:13:59.530
in a sense, uh very roughly put uncaused causers,

267
00:13:59.650 --> 00:14:02.809
OK. Uh, IF we have free will, we're not

268
00:14:02.809 --> 00:14:05.450
determined to do what we do. We just uh

269
00:14:05.460 --> 00:14:06.849
uh uh the reality of free will kind of

270
00:14:06.849 --> 00:14:09.609
corresponds to our, uh, experiences of it. You know,

271
00:14:09.690 --> 00:14:11.929
if I'm deciding what ice cream to choose, I

272
00:14:11.929 --> 00:14:15.010
just choose strawberry over vanilla, OK? And, and nothing

273
00:14:15.010 --> 00:14:18.289
caused me to do that. OK. Um, AND, and

274
00:14:18.289 --> 00:14:20.969
this understanding of being an uncaust, uh, certainly a

275
00:14:20.969 --> 00:14:23.929
lot of philosophers believe, and, and some individuals that

276
00:14:23.929 --> 00:14:25.460
believe that we can have free will, even if

277
00:14:25.460 --> 00:14:27.849
we are determined. But I argue that all these

278
00:14:27.849 --> 00:14:31.710
arguments fail. Uh, THERE, there are certainly very compelling

279
00:14:31.710 --> 00:14:34.679
arguments against having any sort of free will. And

280
00:14:34.679 --> 00:14:36.330
and why am I talking about free will, by

281
00:14:36.330 --> 00:14:38.530
the way? Um, THE reason I'm talking about free

282
00:14:38.530 --> 00:14:41.210
will is if there is a consensus among philosophers,

283
00:14:41.359 --> 00:14:44.239
it's, uh, considered, it's the view that free will

284
00:14:44.239 --> 00:14:48.659
is necessary to be morally responsible. OK. So, if

285
00:14:48.659 --> 00:14:51.219
we don't have free will, um, in a sense,

286
00:14:51.299 --> 00:14:54.780
we cannot be morally responsible where, again, this moral

287
00:14:54.780 --> 00:14:57.659
responsibility can be understood in different ways, but typically,

288
00:14:57.700 --> 00:15:00.979
it's understood if you're not morally responsible, um, it

289
00:15:00.979 --> 00:15:04.619
would be inappropriate, uh, to, to, um, attribute moral

290
00:15:04.619 --> 00:15:06.760
blame or moral praise to you. All right. So

291
00:15:06.760 --> 00:15:10.340
the idea basically, if there's no free will, Again,

292
00:15:11.359 --> 00:15:13.799
this is not uh a consensus view, but, but

293
00:15:13.799 --> 00:15:15.080
one way to think about free will, if there's

294
00:15:15.080 --> 00:15:17.789
no free will, there's no morality. I certainly believe

295
00:15:17.789 --> 00:15:21.979
that to be true, um, A lot of uh

296
00:15:21.979 --> 00:15:24.299
free will skeptics believe that as well. So, uh,

297
00:15:24.359 --> 00:15:26.460
I, I, I believe that the arguments against free

298
00:15:26.460 --> 00:15:28.809
will give us really good reason to believe to

299
00:15:28.809 --> 00:15:32.299
reject the concept of morality that, that most individuals

300
00:15:32.299 --> 00:15:36.059
have. OK. Uh, BEYOND that, there are empirical reasons

301
00:15:36.059 --> 00:15:38.544
for error theory. I spoke about the evolution of

302
00:15:38.544 --> 00:15:41.104
the moral faculty. Uh, AND if this is true,

303
00:15:41.184 --> 00:15:43.744
the basic idea behind this, this is also sometimes

304
00:15:43.744 --> 00:15:45.744
called moral nativism. I'm gonna try not to throw

305
00:15:45.744 --> 00:15:48.744
too much jargon out there. Um, BUT the idea

306
00:15:48.744 --> 00:15:52.344
is that if morality evolved, OK, then we would

307
00:15:52.344 --> 00:15:55.299
believe in These moral claims, and we would make

308
00:15:55.299 --> 00:15:58.270
moral claims, even if they had no actual existence.

309
00:15:58.419 --> 00:16:01.650
OK. The idea is the moral faculty evolved not

310
00:16:01.650 --> 00:16:04.690
to help us perceive actual facts in the world,

311
00:16:04.820 --> 00:16:09.099
but just to cooperate, OK? Um, THE, the moral

312
00:16:09.099 --> 00:16:12.880
psychologist Jonathan Haidt makes an analogy between our, uh,

313
00:16:12.890 --> 00:16:15.820
sense, uh our perception of taste, right? Uh, THERE,

314
00:16:15.940 --> 00:16:20.059
there's no inherent, um, property of sweetness out there

315
00:16:20.059 --> 00:16:22.500
in the world, right? It's just something our brains

316
00:16:22.500 --> 00:16:25.700
have sort of conjured up, uh, to drive certain

317
00:16:25.700 --> 00:16:28.729
behaviors, namely, to get us to eat certain kinds

318
00:16:28.729 --> 00:16:31.979
of, um, foods that, that are safe, that are

319
00:16:31.979 --> 00:16:34.609
not toxic, uh, that can provide a quick source

320
00:16:34.609 --> 00:16:36.969
of calories and so forth. So, the idea is,

321
00:16:37.179 --> 00:16:40.880
if morality did evolve, it would be Roughly put

322
00:16:40.880 --> 00:16:44.719
an amazing coincidence if our views about morality actually

323
00:16:44.719 --> 00:16:47.559
tracked facts in the world, OK? And the idea

324
00:16:47.559 --> 00:16:52.119
is, given that um the moral faculty, moral faculty

325
00:16:52.119 --> 00:16:56.159
evolved, there's simply no independent reason to think that

326
00:16:56.159 --> 00:16:58.710
our intuitions about morality track anything in the world,

327
00:16:58.840 --> 00:17:03.380
OK? Um, SO that's another argument against, uh, moral

328
00:17:03.380 --> 00:17:07.219
realism. And then another one which, which is controversial,

329
00:17:07.260 --> 00:17:09.699
these are all somewhat controversial, right? Uh, BUT more

330
00:17:09.699 --> 00:17:12.569
controversial, I'd say, is this idea of that, uh,

331
00:17:12.579 --> 00:17:15.180
the con the power of conscious will is illusory,

332
00:17:15.459 --> 00:17:18.050
right? Now, this goes back to, this has been

333
00:17:18.050 --> 00:17:20.260
discussed a lot. So, uh, I imagine a lot,

334
00:17:20.300 --> 00:17:22.108
a lot of your viewers are already familiar with

335
00:17:22.108 --> 00:17:24.219
the, with the limited experiments and so forth. But

336
00:17:24.219 --> 00:17:26.930
the idea is that, um, the, the idea that

337
00:17:26.987 --> 00:17:31.046
Consciousness actually, if, if it's at all productive of

338
00:17:31.046 --> 00:17:34.006
our actions, it, it tends to promote, uh, produce

339
00:17:34.006 --> 00:17:36.927
a lot fewer actions than we are aware of,

340
00:17:37.126 --> 00:17:40.296
right? So, if we do any actions pretty much

341
00:17:40.296 --> 00:17:42.886
in our conscious state, you know, whether I'm picking

342
00:17:42.886 --> 00:17:45.366
up a, a marker or drinking something or speaking

343
00:17:45.366 --> 00:17:48.006
to you, I think that I'm consciously willing uh

344
00:17:48.006 --> 00:17:51.207
this to happen. But what neuroscientific research has shown

345
00:17:51.207 --> 00:17:52.886
is that a lot of, uh, a lot of

346
00:17:52.886 --> 00:17:54.884
times, most of the time in We seem to

347
00:17:54.884 --> 00:17:59.024
be wrong, right? That, uh, generally, it's, it's unconscious

348
00:17:59.024 --> 00:18:01.593
processes that are, that are sort of making us

349
00:18:01.593 --> 00:18:05.394
behave automatically. And then our consciousness, uh, sort of

350
00:18:05.394 --> 00:18:08.953
interprets this, um, to, to uh understanding that we

351
00:18:08.953 --> 00:18:11.073
are the causes of this. But now, now, again,

352
00:18:11.193 --> 00:18:13.543
like I said, this is controversial, this is ongoing.

353
00:18:13.634 --> 00:18:15.813
But I certainly think the neuroscience points in this

354
00:18:15.813 --> 00:18:18.223
way. I mentioned Jonathan Haidt. He thinks that as

355
00:18:18.223 --> 00:18:22.600
much as 99% of our quote unquote intentional actions

356
00:18:22.600 --> 00:18:26.610
might actually be caused by these unconscious processes. Now,

357
00:18:27.020 --> 00:18:29.581
if this is true, if our consciousness plays a

358
00:18:29.581 --> 00:18:31.980
much weaker role in producing our actions that we're

359
00:18:31.980 --> 00:18:34.691
aware of, uh, then we would seem to lack

360
00:18:34.691 --> 00:18:37.490
free will for these over these actions and therefore,

361
00:18:37.640 --> 00:18:39.860
uh, we would not seem to be morally responsible

362
00:18:39.860 --> 00:18:41.941
for them. So, those are some of what I

363
00:18:41.941 --> 00:18:45.740
consider to be the strongest arguments um against the

364
00:18:45.740 --> 00:18:48.340
existence of, of, uh, objective moral truths.

365
00:18:49.060 --> 00:18:53.400
So earlier, you've already mentioned error theory and also

366
00:18:53.400 --> 00:18:57.079
you've also talked a little bit about retributivism. So

367
00:18:57.079 --> 00:18:59.910
tell us about the particular kind of error theory

368
00:19:00.199 --> 00:19:03.959
that you espouse in the book, namely retributivist moral

369
00:19:03.959 --> 00:19:05.000
anti-realism.

370
00:19:06.150 --> 00:19:10.589
Right, so, I, I, I'm attacking a very what

371
00:19:10.589 --> 00:19:12.709
a lot of people consider to be possibly an

372
00:19:12.709 --> 00:19:18.640
overly narrow understanding of um Of, of moral realism,

373
00:19:18.770 --> 00:19:23.140
right? So, what I'm calling um retributivist, uh, moral

374
00:19:23.140 --> 00:19:26.880
realism, right? And as I mentioned, so this is

375
00:19:26.880 --> 00:19:29.869
the kind of, of moral realism according to which,

376
00:19:30.160 --> 00:19:32.650
um, there are these moral facts out there in

377
00:19:32.650 --> 00:19:34.640
the world, moral properties in the world, uh, that

378
00:19:34.640 --> 00:19:38.964
are attributable to human beings, and which warrant. RETRIBUTIVIST,

379
00:19:39.135 --> 00:19:43.334
uh, either attitudes or behaviors toward them, right? Uh

380
00:19:43.334 --> 00:19:44.814
SUCH that if someone acts in a way that's

381
00:19:44.814 --> 00:19:49.775
morally wrong, genuinely morally wrong, they are appropriate targets

382
00:19:49.775 --> 00:19:53.055
of retributivist actions. OK. So, think eye for an

383
00:19:53.055 --> 00:19:55.625
eye, right? So, uh, you, you stole something, you're

384
00:19:55.625 --> 00:19:57.415
morally wrong for doing that, you need to be

385
00:19:57.415 --> 00:20:02.270
punished. And and because it's retributist, um, the, the

386
00:20:02.270 --> 00:20:07.510
more consequentialist forward-looking purposes of, of, uh, punishment are

387
00:20:07.510 --> 00:20:10.109
thrown aside. Now, I'm not saying people don't sometimes

388
00:20:10.109 --> 00:20:12.949
punish for forward-looking purposes. They clearly do. But as

389
00:20:12.949 --> 00:20:14.709
the way in the, in the sense that most

390
00:20:14.709 --> 00:20:17.939
people think of morality, I argue. There's this uh

391
00:20:18.069 --> 00:20:22.349
uh integral uh retributivist component that, that can't be

392
00:20:22.349 --> 00:20:25.310
separated from it, right? So, to say someone's morally

393
00:20:25.310 --> 00:20:27.069
wrong, at least is in part to say that

394
00:20:27.069 --> 00:20:30.569
they deserve to be punished, um, for on retributivist

395
00:20:30.569 --> 00:20:34.270
grounds, strictly retributist grounds. Now, uh, what I'm arguing

396
00:20:34.270 --> 00:20:37.670
is that the kind of uh moral realism that's

397
00:20:37.729 --> 00:20:42.670
Required uh to justify those retributivist judgments are all

398
00:20:42.670 --> 00:20:45.510
wrong, OK? So, uh, the, the kind of error

399
00:20:45.510 --> 00:20:47.939
theory that I'm arguing is that when any, whenever

400
00:20:47.939 --> 00:20:52.339
anybody makes a, a moral assertion that implies, um,

401
00:20:52.349 --> 00:20:56.310
the appropriateness of retributivism towards an individual, they are

402
00:20:56.310 --> 00:20:59.800
in error, they're wrong. All right. And, and I,

403
00:20:59.839 --> 00:21:02.160
I, I can talk about this later. So 11

404
00:21:02.160 --> 00:21:03.439
question I think a lot of people would have

405
00:21:03.439 --> 00:21:05.400
is, well, yes, if you understand morality in this

406
00:21:05.400 --> 00:21:10.670
very narrow way, that um imparts retributivism, well, sure,

407
00:21:10.719 --> 00:21:12.560
that might not exist, but there, why think of

408
00:21:12.560 --> 00:21:14.959
morality like that? There's, there's certainly other ways to

409
00:21:14.959 --> 00:21:16.569
think about it. It doesn't have to be retributivist,

410
00:21:16.640 --> 00:21:19.760
it can be purely forward-looking. There are these constructivist

411
00:21:19.760 --> 00:21:22.479
accounts of morality and so forth. Um, AND I

412
00:21:22.479 --> 00:21:24.760
could talk about this in more detail. Later. But

413
00:21:24.760 --> 00:21:28.439
my basic response is, when we talk about morality

414
00:21:28.439 --> 00:21:30.239
in different ways, and I think philosophers have a

415
00:21:30.239 --> 00:21:34.640
very nasty habit, not intentionally, perhaps, but understanding terms

416
00:21:34.640 --> 00:21:37.640
in different ways. Whether it's free will, uh, moral

417
00:21:37.640 --> 00:21:41.640
realism, um, What have you. When they talk about

418
00:21:41.640 --> 00:21:43.920
it in this different, these different ways, not only

419
00:21:43.920 --> 00:21:47.030
does it add unnecessary confusion, uh, to the issue,

420
00:21:47.160 --> 00:21:49.280
but it makes it very difficult to make any

421
00:21:49.280 --> 00:21:51.839
sort of progress on these debates because philosophers really

422
00:21:51.839 --> 00:21:55.079
are speaking past each other. So, uh, one of

423
00:21:55.079 --> 00:21:57.239
the main project, uh, uh, main aims of my

424
00:21:57.239 --> 00:21:59.599
book is to try to get philosophers and, and

425
00:21:59.599 --> 00:22:03.079
scientists on the same page, uh, about what morality.

426
00:22:03.170 --> 00:22:04.680
IS. And I think the best way to do

427
00:22:04.680 --> 00:22:07.040
that, and I'm not alone in saying this, is

428
00:22:07.040 --> 00:22:09.239
that we should speak of these philosophical terms in

429
00:22:09.239 --> 00:22:12.000
a way that corresponds to the way that non-hilosophers

430
00:22:12.000 --> 00:22:15.319
or non-academics generally conceive of them. And as I've

431
00:22:15.319 --> 00:22:18.400
talked about, because the folk, when they, generally, when

432
00:22:18.400 --> 00:22:21.839
they talk about morality, they are imparting retributivism. They

433
00:22:21.839 --> 00:22:24.630
are making a, a, a, a judgment that is,

434
00:22:24.680 --> 00:22:27.920
is, um, to it has retributivism as a central

435
00:22:27.920 --> 00:22:31.469
component. OK. So, if we want to then revise

436
00:22:31.469 --> 00:22:33.439
morality, to be like, oh, no, it just means

437
00:22:33.439 --> 00:22:36.000
doing nice things for people, um, we're, we're gonna

438
00:22:36.000 --> 00:22:38.300
be confusing people, right? Because if that's all I

439
00:22:38.300 --> 00:22:40.540
mean, is that, oh, when I say this person

440
00:22:40.540 --> 00:22:43.000
acted morally, let's say they're, they're doing something nice

441
00:22:43.000 --> 00:22:45.239
for somebody. Then if I say, you know, such

442
00:22:45.239 --> 00:22:49.619
and such is moral, this could reinforce these, these

443
00:22:49.619 --> 00:22:53.599
uh alternate conceptions of morality, right? When somebody hears

444
00:22:53.599 --> 00:22:55.560
me saying morality in a sense, it doesn't impart

445
00:22:55.560 --> 00:22:58.859
retributivism, because I don't specify that, uh, they might

446
00:22:58.859 --> 00:23:03.160
think of me as, as actually, um, uh, espousing

447
00:23:03.160 --> 00:23:05.780
this idea that, yeah, there is retribuous moral realism.

448
00:23:05.959 --> 00:23:07.839
Uh, AND, and so, that's why I think it's

449
00:23:07.839 --> 00:23:11.119
important that we settle on specific philosophical notions. In

450
00:23:11.119 --> 00:23:15.569
this case, moral realism. That correspond uh to to

451
00:23:15.569 --> 00:23:17.329
how the folk view them, and the folk view

452
00:23:17.329 --> 00:23:20.650
them as having this this uh important retributivist component.

453
00:23:22.400 --> 00:23:26.319
And what do you think about alternative concepts conceptions

454
00:23:26.319 --> 00:23:31.050
of morality, like, for example, a constructivist moral perspective

455
00:23:31.050 --> 00:23:35.050
based either on cultural or personal values.

456
00:23:35.609 --> 00:23:38.209
Good, and this really sort of dovetails nicely with

457
00:23:38.209 --> 00:23:42.109
the last question. So, let's say We all come

458
00:23:42.109 --> 00:23:44.459
to agree, which I don't see happening anytime soon,

459
00:23:44.670 --> 00:23:47.510
uh, that there is no what I'm calling retributivist

460
00:23:47.510 --> 00:23:50.979
moral realism. That is, uh, retributivism is never, um,

461
00:23:50.989 --> 00:23:54.130
a, a, a justifiable attitude or behavior towards innovative

462
00:23:54.130 --> 00:23:56.270
body. It just doesn't exist. It, it's not pertinent

463
00:23:56.270 --> 00:23:58.949
to human beings. OK, fine. Well, why don't we

464
00:23:58.949 --> 00:24:02.189
just preserve morality and understand it in a different

465
00:24:02.189 --> 00:24:06.109
sense. So let's revise the term, right? Um, And

466
00:24:06.109 --> 00:24:08.699
there's been a, a bunch of different ways that

467
00:24:08.709 --> 00:24:11.189
that philosophers have, have thought about revising the terms.

468
00:24:11.229 --> 00:24:16.829
So, uh, you mentioned constructivist accounts. Um, ONE constructivist

469
00:24:16.829 --> 00:24:20.060
account is, is called, uh, referred to generally as

470
00:24:20.060 --> 00:24:24.089
Kantian constructionism, named after the philosopher Kant. Um, AND,

471
00:24:24.160 --> 00:24:26.319
and the basic idea here is that there are

472
00:24:26.319 --> 00:24:29.040
these sort of, they may not be these mind-independent

473
00:24:29.040 --> 00:24:32.079
moral properties, but there is some sort of mind-independent

474
00:24:32.079 --> 00:24:37.400
property that, um, is, let's say, non-objective, OK? So

475
00:24:37.400 --> 00:24:40.599
we can get rid of objectivism, um, and, and

476
00:24:40.599 --> 00:24:43.439
that can, uh, form the basis of of an

477
00:24:43.439 --> 00:24:45.920
account of morality. Now, I think if we look

478
00:24:45.920 --> 00:24:48.489
at these Accounts of morality and detail. So John

479
00:24:48.489 --> 00:24:51.410
Rawls and, and his notion of um justice and

480
00:24:51.410 --> 00:24:54.099
fairness, I, I, I think is a really good

481
00:24:54.099 --> 00:24:56.359
example of this. And the problem I see for

482
00:24:56.359 --> 00:24:57.489
a view like this, even though I think there's

483
00:24:57.489 --> 00:24:59.250
a lot to say in favor of it, is

484
00:24:59.250 --> 00:25:02.209
I think it tries getting objective moral properties in

485
00:25:02.209 --> 00:25:04.209
through the back door, right? So, what do I

486
00:25:04.209 --> 00:25:07.489
mean by that? So, without going into the details

487
00:25:07.489 --> 00:25:11.380
of Rawls's account, um, He suggests that if we're

488
00:25:11.380 --> 00:25:13.699
trying to find out what kind of principles of

489
00:25:13.699 --> 00:25:15.979
justice there are that we should abide by, we,

490
00:25:16.020 --> 00:25:18.020
we should use this thought experiment where we're sort

491
00:25:18.020 --> 00:25:21.459
of at this um um pre-moral state where a

492
00:25:21.459 --> 00:25:24.290
bunch of these individuals come together and decide what,

493
00:25:24.300 --> 00:25:26.939
what society would be best for them based basically

494
00:25:26.939 --> 00:25:29.920
on self-interest. And, but, but one thing that he

495
00:25:29.920 --> 00:25:33.040
does is he builds this concept of fairness into

496
00:25:33.319 --> 00:25:36.560
the, this, this theoretical construct, right? That sort of

497
00:25:36.560 --> 00:25:39.280
uh restricts the kinds of, of rules that will

498
00:25:39.280 --> 00:25:41.239
come out of it. And in doing that, he's

499
00:25:41.239 --> 00:25:43.920
essentially saying, well, let's assume that fairness is an

500
00:25:43.920 --> 00:25:46.760
objective moral good, uh, that, that any society should

501
00:25:46.760 --> 00:25:48.989
abide by. So, that, that's why I think uh

502
00:25:49.000 --> 00:25:52.459
uh the, the Kantian constructivist views really, uh, I,

503
00:25:52.560 --> 00:25:55.939
I think are another form of, of Or fall

504
00:25:55.939 --> 00:25:59.020
come very close to, to moral objectivism, and so

505
00:25:59.020 --> 00:26:01.030
fall prey to the kinds of objections uh for

506
00:26:01.030 --> 00:26:05.680
error theory that I mentioned. Another account is referred

507
00:26:05.680 --> 00:26:08.520
to sometimes as human constructivism. This is where you

508
00:26:08.520 --> 00:26:11.199
get the sort of relativistic views, right? Where, let's

509
00:26:11.199 --> 00:26:14.319
say in individuals, there's, there's different variations of all

510
00:26:14.319 --> 00:26:16.640
these views I should mention. And um, so an

511
00:26:16.640 --> 00:26:19.880
individual's own views about morality or maybe society's views

512
00:26:19.880 --> 00:26:23.989
about morality, um, Can shape what morality is to

513
00:26:23.989 --> 00:26:26.550
me. So think subjectivism and relativism, right? So if

514
00:26:26.550 --> 00:26:30.510
you're a moral subjectivist, roughly speaking, uh, moral right

515
00:26:30.510 --> 00:26:32.829
and wrong for you is determined by your own

516
00:26:32.829 --> 00:26:34.790
views about right and wrong, right? If you think

517
00:26:34.790 --> 00:26:36.510
stealing's wrong, then it's wrong for you to steal.

518
00:26:36.630 --> 00:26:39.270
If you think stealing is OK, it's not wrong

519
00:26:39.270 --> 00:26:42.709
for you to steal. Um, WHEREAS, um, ethical relativism,

520
00:26:43.050 --> 00:26:45.569
sometimes referred to as cultural relativism, is the view

521
00:26:45.569 --> 00:26:49.239
that, um, our moral obligations stem from what our

522
00:26:49.250 --> 00:26:52.500
society, the greater culture that we're in, uh, believes.

523
00:26:52.650 --> 00:26:54.709
But the problem with these views is I, uh,

524
00:26:54.719 --> 00:26:58.239
they, they seem not to be unable to ground.

525
00:26:59.160 --> 00:27:03.810
The retributivist aspects of people's moral views, right? Um,

526
00:27:03.979 --> 00:27:05.780
NOT to mention it makes, there's a bunch of

527
00:27:05.780 --> 00:27:07.890
other objections against these zoos, but I, but I

528
00:27:07.890 --> 00:27:09.739
think that one of the main ones is that

529
00:27:09.739 --> 00:27:13.020
it makes it very difficult to ground retributivism, right?

530
00:27:13.099 --> 00:27:14.859
So, I, I use the example in a book.

531
00:27:14.959 --> 00:27:17.540
Imagine that you have this sort of odd cultural

532
00:27:17.540 --> 00:27:21.099
norm where the death penalty is uh deserved for

533
00:27:21.099 --> 00:27:24.969
uh what we would consider being extremely trivial, um,

534
00:27:25.119 --> 00:27:28.979
um, crime, all right? Um, YOU know, maybe, maybe

535
00:27:28.979 --> 00:27:30.619
when you're walking, you start with your right foot.

536
00:27:30.660 --> 00:27:32.319
That's not the example I used. And let's say

537
00:27:32.319 --> 00:27:34.599
that you, you know, society is really just sort

538
00:27:34.599 --> 00:27:37.869
of, uh, uh, uh, hammer this into your brain,

539
00:27:38.000 --> 00:27:41.000
you fully accept it and so forth. And, and

540
00:27:41.000 --> 00:27:43.400
one day something happens, you're startled and you start

541
00:27:43.400 --> 00:27:45.349
walking with your right foot, and then someone's like,

542
00:27:45.599 --> 00:27:47.959
uh, 00, you violated the number one rule of

543
00:27:47.959 --> 00:27:49.469
our society, you need to be put to death,

544
00:27:49.640 --> 00:27:52.439
OK? Because they believe that on retributist grounds. So

545
00:27:52.439 --> 00:27:54.640
let's say you agree, oh yeah, but um, I,

546
00:27:54.719 --> 00:27:56.760
I think from an objective standpoint in a sense,

547
00:27:56.800 --> 00:28:00.109
or or or at least from a strong intuition.

548
00:28:00.390 --> 00:28:03.170
A person is not deserving of retributivism, right? So,

549
00:28:03.380 --> 00:28:05.540
um, I, I think a lot of those constructivist

550
00:28:05.540 --> 00:28:09.459
views can't ground retributivism. Uh, THE forward-looking arguments, uh,

551
00:28:09.560 --> 00:28:11.579
fall prey to the same kind of objection, right?

552
00:28:11.680 --> 00:28:13.859
That I've talked about before. If, if, and, and

553
00:28:13.859 --> 00:28:16.140
a forward-looking argument, by the way, for morality, I

554
00:28:16.140 --> 00:28:18.219
should just specify, look, when we say people are

555
00:28:18.219 --> 00:28:22.260
morally responsible, we're saying perhaps that they are liable

556
00:28:22.260 --> 00:28:24.859
to be punished, but only in a way that

557
00:28:24.859 --> 00:28:30.319
aims toward um preventing these kinds of, uh, Problematic

558
00:28:30.319 --> 00:28:33.199
behaviors of the future, or maybe deters other people

559
00:28:33.199 --> 00:28:35.760
from doing it, um, or makes them a better

560
00:28:35.760 --> 00:28:37.510
person, right? So you're doing it with a what

561
00:28:37.520 --> 00:28:40.670
what sometimes referred to as a consequentialist aim, OK?

562
00:28:40.770 --> 00:28:44.750
You're, you're, you're punishing now, not for backward-looking reasons.

563
00:28:44.839 --> 00:28:46.849
They did something wrong, they deserve to be punished.

564
00:28:47.000 --> 00:28:49.880
You're punishing for forward-looking reasons. You're try you're trying

565
00:28:49.880 --> 00:28:52.119
to prove something in the future. And we can

566
00:28:52.119 --> 00:28:55.104
just forward-looking punishment, but we do all the time

567
00:28:55.104 --> 00:28:58.584
to animals, right? Um, TO young children. Uh, WE'RE

568
00:28:58.584 --> 00:29:01.064
not saying that they're, you know, inherently evil or

569
00:29:01.064 --> 00:29:02.824
anything like that, but we don't want our dog

570
00:29:02.824 --> 00:29:04.984
to, you know, go to the bathroom on the

571
00:29:04.984 --> 00:29:07.864
floor. We don't want our child to steal cookies

572
00:29:07.864 --> 00:29:10.344
from a sibling or something, right? So we might

573
00:29:10.344 --> 00:29:12.905
punish them with an eye towards their benefit and

574
00:29:12.905 --> 00:29:15.025
towards good things in the future. OK. So, why

575
00:29:15.025 --> 00:29:17.920
don't we just Uh, except a kind of morality

576
00:29:17.920 --> 00:29:20.400
like that. Well, again, on the one hand, and

577
00:29:20.400 --> 00:29:22.680
this is the main argument I have, you are

578
00:29:22.680 --> 00:29:25.150
now revising morality in a way that is too

579
00:29:25.150 --> 00:29:29.119
divergent from how people ordinarily speak about it, OK?

580
00:29:29.359 --> 00:29:32.640
Um, NOBODY disagrees. I don't disagree, no error theorists

581
00:29:32.640 --> 00:29:36.599
disagree, no disagrees, no free will skeptic disagrees that

582
00:29:36.599 --> 00:29:41.949
forward-looking punishment is, um, sometimes preferable and sometimes allowable.

583
00:29:42.150 --> 00:29:46.010
Uh OUR society relies on it, OK? Um, SO

584
00:29:46.010 --> 00:29:48.959
if, if forward-looking punishment and morality was all there

585
00:29:48.959 --> 00:29:51.689
was to morality or, or free will and these

586
00:29:51.689 --> 00:29:53.969
kinds of things, there'd be no disagreement about these

587
00:29:53.969 --> 00:29:56.689
issues among philosophers. Uh, SO the very fact that

588
00:29:56.689 --> 00:30:01.390
there is this disagreement shows that forward-looking punishment isn't

589
00:30:01.390 --> 00:30:04.369
really what we're primarily concerned about, whether we're talking

590
00:30:04.369 --> 00:30:07.010
about free will, whether we're talking about moral realism.

591
00:30:07.089 --> 00:30:11.130
So the, these revisionist attempts, these, these alternatives to

592
00:30:11.130 --> 00:30:13.719
uh moral. IS more error theory and I, I

593
00:30:13.719 --> 00:30:16.520
should mention. Um, I argue simply don't work. They,

594
00:30:16.599 --> 00:30:20.119
they don't, um, fit the bill of, of how

595
00:30:20.119 --> 00:30:23.189
most people understand morality. It is an in effect

596
00:30:23.189 --> 00:30:27.479
changing the subject, and it, it doesn't go very

597
00:30:27.479 --> 00:30:30.560
far in, uh, helping us, uh, it doesn't really

598
00:30:30.560 --> 00:30:32.800
do anything, I argue, in helping us resolve these

599
00:30:32.800 --> 00:30:35.760
age-old philosophical questions that gave rise to the debates

600
00:30:35.760 --> 00:30:38.160
in the first place, and it just adds unnecessary

601
00:30:38.160 --> 00:30:38.680
confusion.

602
00:30:39.489 --> 00:30:43.270
So this next question will probably also help us

603
00:30:43.270 --> 00:30:45.630
understand a little bit better why you think that

604
00:30:45.630 --> 00:30:49.589
the belief in morality does humanity more harm than

605
00:30:49.589 --> 00:30:52.900
good. So what do we know about the historical

606
00:30:52.900 --> 00:30:56.579
and present role that morality has played in both

607
00:30:56.910 --> 00:30:59.540
preventing and promoting violence?

608
00:31:01.260 --> 00:31:03.739
So, this is one of the main subjects of

609
00:31:03.739 --> 00:31:05.979
my book, right? So this, this gets to, I,

610
00:31:06.020 --> 00:31:07.540
I, I think the aspect of the book that

611
00:31:07.540 --> 00:31:09.540
is most controversial, and that really sort of sets

612
00:31:09.540 --> 00:31:12.900
my book apart from, uh, so far as I

613
00:31:12.900 --> 00:31:15.260
can tell, any other book on the subject of

614
00:31:15.260 --> 00:31:17.650
philosophy that's been written. So far as I'm aware,

615
00:31:17.900 --> 00:31:21.010
um, no other, uh, book of this length or,

616
00:31:21.020 --> 00:31:24.459
or of this depth has tried to uh establish

617
00:31:24.459 --> 00:31:27.660
that, uh, the empirical case that morality causes more

618
00:31:27.660 --> 00:31:31.469
problems than it solves. Uh, AND again, before I

619
00:31:31.469 --> 00:31:33.670
address the, the question, Ricard, I'll just say, you

620
00:31:33.670 --> 00:31:35.459
know, my, my aim here, I, I don't take

621
00:31:35.459 --> 00:31:36.989
my book to be a final word on this

622
00:31:36.989 --> 00:31:39.300
subject, although I do think I offer very compelling

623
00:31:39.300 --> 00:31:43.109
evidence, uh, showing that morality, uh, is problematic, more

624
00:31:43.109 --> 00:31:46.349
problematic than helpful, um, in terms of violence, let's

625
00:31:46.349 --> 00:31:48.349
say, which I'll discuss shortly. But again, I want

626
00:31:48.349 --> 00:31:51.380
to get a dialogue going. So, so, uh, generally,

627
00:31:51.390 --> 00:31:55.510
people, non-moral abolitionists, just assume that, oh, morality is

628
00:31:55.510 --> 00:31:56.859
a good thing. And I, I, I could talk

629
00:31:56.859 --> 00:31:59.750
about that later as well. Um, BUT there's Very

630
00:31:59.750 --> 00:32:02.219
good reason to say that that is generally not

631
00:32:02.219 --> 00:32:05.069
the case, both historically and at present. And, and

632
00:32:05.069 --> 00:32:08.949
violence is a good example. So, certainly, morality plays

633
00:32:08.949 --> 00:32:11.390
a role in inhibiting violence, but it is very

634
00:32:11.390 --> 00:32:13.469
hard to identify when it does that. Because I

635
00:32:13.469 --> 00:32:14.949
think most of the times it does that. These

636
00:32:14.949 --> 00:32:17.430
are sort of, you know, battles inside of people's

637
00:32:17.430 --> 00:32:20.229
brains where they're deliberating, uh, man, this person makes

638
00:32:20.229 --> 00:32:22.109
me really angry, or do I want to join

639
00:32:22.109 --> 00:32:24.750
this violent gang or this, this, this army or

640
00:32:24.750 --> 00:32:27.670
something. Um, AND, and maybe there's moral considerations that

641
00:32:27.670 --> 00:32:30.900
lead them, um, that inhibit them from, from doing

642
00:32:30.900 --> 00:32:33.380
these things or engaging in violence. So, it, it

643
00:32:33.380 --> 00:32:37.050
is, uh, the, the, I should say it, the,

644
00:32:37.060 --> 00:32:39.819
my opponents at a disadvantage here because there are

645
00:32:39.819 --> 00:32:44.619
a lot clearer, uh, examples of morality, um, causing

646
00:32:44.619 --> 00:32:48.020
violence than preventing violence. Nonetheless, I, I don't think

647
00:32:48.020 --> 00:32:50.219
it, it, it's unfair of me to appeal to

648
00:32:50.219 --> 00:32:52.989
a appeal to these, uh, empirical reasons and, uh,

649
00:32:53.150 --> 00:32:55.989
uh, for, for the, the main reason that as

650
00:32:55.989 --> 00:32:58.469
I'll, uh, after my discussion, hopefully, uh, you and

651
00:32:58.469 --> 00:33:01.670
your viewers will, will agree, um, it's hard to

652
00:33:01.670 --> 00:33:06.560
imagine morality inhibiting, uh, making, uh, uh, making up

653
00:33:06.560 --> 00:33:08.630
for the amount of violence that it's caused in

654
00:33:08.630 --> 00:33:11.520
terms of inhibiting it, right? Uh, SO, what are

655
00:33:11.569 --> 00:33:13.939
Some of the ways that morality causes violence. Well,

656
00:33:14.439 --> 00:33:17.199
in the book, I separate it between small scale

657
00:33:17.199 --> 00:33:20.349
violence and large-scale violence. It's really hard to sort

658
00:33:20.349 --> 00:33:22.400
of make a, a, a clear distinction between the

659
00:33:22.400 --> 00:33:26.000
two. I, I, I think I define small-scale violence

660
00:33:26.000 --> 00:33:29.359
somewhat arbitrarily as, as uh acts that take the

661
00:33:29.359 --> 00:33:31.810
lives of 250 people or less or something. And,

662
00:33:31.920 --> 00:33:36.650
and Uh, no, so, with, with small-scale violence that,

663
00:33:36.689 --> 00:33:39.810
that's more uh motivated by morality, uh, think of

664
00:33:39.810 --> 00:33:42.689
terrorism, OK? Uh, ACTS of terrorism that affect less

665
00:33:42.689 --> 00:33:47.209
than 250 people, right? Um, THINK of, uh, domestic

666
00:33:47.209 --> 00:33:49.949
violence. A lot of time that is given a,

667
00:33:49.959 --> 00:33:52.569
uh, moral justification, uh, in, in places as we

668
00:33:52.569 --> 00:33:54.614
see, uh, such as India. Where there is a

669
00:33:54.614 --> 00:33:57.535
lot of domestic abuse abuse against women. And, and

670
00:33:57.535 --> 00:34:00.145
women, oddly, um, according to a lot of the

671
00:34:00.145 --> 00:34:02.964
surveys that I've read, uh, actually believe that, uh,

672
00:34:02.974 --> 00:34:06.015
the use of violence from husbands against wives and

673
00:34:06.015 --> 00:34:08.975
so forth, um, women seem to think that's more

674
00:34:08.975 --> 00:34:12.054
justified than, than men, uh, which is surprising and

675
00:34:12.054 --> 00:34:14.925
eye-opening and, and, and unfortunate, I should also add.

676
00:34:15.254 --> 00:34:19.090
Um, So, other sorts of small uh kinds of

677
00:34:19.090 --> 00:34:23.208
violence or small-scale violence. Um, SO think of violence

678
00:34:23.208 --> 00:34:26.800
against outgroups, right? So, violence against women, violence against,

679
00:34:26.850 --> 00:34:31.090
uh, political dissidents, uh, against, uh, the LGBTQ plus

680
00:34:31.090 --> 00:34:36.010
community, um, ethnic, racial minorities. This is all, oftentimes,

681
00:34:36.090 --> 00:34:39.679
sparked by moral reasons. So people perpetuating this violence

682
00:34:39.679 --> 00:34:42.478
think, oh, you know, this group is, is evil,

683
00:34:42.489 --> 00:34:44.639
or they need to be put down. Or I'm

684
00:34:44.639 --> 00:34:47.040
gonna engage in an act of terrorism because I

685
00:34:47.040 --> 00:34:48.800
think it's my moral duty to do so for

686
00:34:48.800 --> 00:34:51.188
my country or my religion or my political group,

687
00:34:51.360 --> 00:34:54.560
right? OK. Um, AND, and, and, uh, I, I

688
00:34:54.560 --> 00:34:56.478
should also say, when I'm talking about acts of

689
00:34:56.478 --> 00:35:00.959
violence, I focus on fatalities, all right? But the

690
00:35:00.959 --> 00:35:05.510
amount of non-fatal injuries resulting from morally motivated violence,

691
00:35:06.360 --> 00:35:09.149
certainly well exceeds the number of fatalities. OK. Still,

692
00:35:09.159 --> 00:35:11.750
the number of fatalities is certainly high. Opening. And

693
00:35:11.750 --> 00:35:13.669
we see that particularly when we talk about large-scale

694
00:35:13.669 --> 00:35:17.429
violence. So here I'm talking about um ritualistic sacrifice

695
00:35:17.429 --> 00:35:20.709
in the distant past, um, the execution of witches,

696
00:35:20.790 --> 00:35:24.459
which is sort of portrayed somewhat graphically on, on

697
00:35:24.750 --> 00:35:28.030
my book cover, um, and uh also war and

698
00:35:28.030 --> 00:35:31.510
genocide, right? So, uh, uh, a lot of commentators

699
00:35:31.510 --> 00:35:35.790
on war, uh, Ian Hinkfuss, uh, Joshua Green, have

700
00:35:35.790 --> 00:35:40.070
basically said, look, without morality, Uh, certainly in recent

701
00:35:40.070 --> 00:35:43.590
times, without morality, uh, being, uh, a useful tool

702
00:35:43.590 --> 00:35:46.629
for to drumming up support for war, it's impossible

703
00:35:46.629 --> 00:35:49.030
to imagine that large scale wars would have taken

704
00:35:49.030 --> 00:35:53.530
place, right? Almost any war. We can think of

705
00:35:53.530 --> 00:35:55.810
in in the past several 100 years, a large

706
00:35:55.810 --> 00:35:59.030
scale war, uh, has been justified on moral grounds,

707
00:35:59.090 --> 00:36:01.929
and this has been instrumental in getting people to

708
00:36:01.929 --> 00:36:04.330
join the cause, right? Um, WE see this in

709
00:36:04.330 --> 00:36:06.969
World War II with Hitler, um, we see it

710
00:36:06.969 --> 00:36:10.290
in the wars of religion, um, 30 Years' War.

711
00:36:10.495 --> 00:36:13.854
Uh, THE, the Muslim conquest of India, all these

712
00:36:13.854 --> 00:36:16.514
kinds of, uh, uh, wars where, where millions of

713
00:36:16.514 --> 00:36:18.995
people, um, perhaps up to a billion people, according

714
00:36:18.995 --> 00:36:21.935
to some estimates, have died, um, that would not

715
00:36:21.935 --> 00:36:24.895
almost certainly have not occurred without people believing that

716
00:36:24.895 --> 00:36:26.895
it was the morally right thing to do to

717
00:36:26.895 --> 00:36:30.600
go to war. Now, In saying this, I allow,

718
00:36:30.800 --> 00:36:33.159
um, and, and we see this even nowadays, right?

719
00:36:33.270 --> 00:36:36.149
So think of Putin, uh, and, and his invasion

720
00:36:36.149 --> 00:36:39.879
um of Ukraine. Uh, HE gave a moral justification

721
00:36:39.879 --> 00:36:42.120
for it, right? One of the moral justifications I

722
00:36:42.120 --> 00:36:44.879
recall is that, uh, well, you know, uh, ethnic

723
00:36:44.879 --> 00:36:49.860
Russians are being persecuted, um. Ukraine was persecuting the

724
00:36:49.860 --> 00:36:51.989
Jewish population or something, and so there are these

725
00:36:51.989 --> 00:36:54.830
moral justifications. Now, I think he's full of garbage.

726
00:36:54.870 --> 00:36:58.590
I don't think he, uh, uh, really genuinely believed

727
00:36:58.590 --> 00:37:00.629
these reasons for going to war, but he still

728
00:37:00.629 --> 00:37:02.550
had to give a moral argument to get his

729
00:37:02.550 --> 00:37:05.429
country in favor of doing it, right? Um, AND

730
00:37:05.429 --> 00:37:07.429
so this is the idea. Even if a leader

731
00:37:07.429 --> 00:37:10.429
doesn't morally believe that going to war or, or

732
00:37:10.429 --> 00:37:12.760
committing Genocide is the right thing to do. Um,

733
00:37:12.830 --> 00:37:15.040
I, I, I think Hitler actually thought it was

734
00:37:15.040 --> 00:37:16.520
the moral thing to do. But even if they

735
00:37:16.520 --> 00:37:19.239
don't, it's important that he get, they get their

736
00:37:19.239 --> 00:37:22.060
country, uh, uh, their citizens and their, their, their

737
00:37:22.060 --> 00:37:25.070
other uh political leaders to agree with it, OK?

738
00:37:25.280 --> 00:37:28.199
And then genocide, oftentimes we find out there is

739
00:37:28.199 --> 00:37:31.780
a, a central, uh, moral justification given for it,

740
00:37:32.000 --> 00:37:36.639
for the Holocaust, um, For, um, the, the, the

741
00:37:36.639 --> 00:37:41.179
uh uh Rwandan genocide, uh, for, uh, the Cambodian

742
00:37:41.179 --> 00:37:45.159
genocide, and, and this idea of racial purity, we

743
00:37:45.159 --> 00:37:48.000
find is, is, is a very frequent contributor to

744
00:37:48.000 --> 00:37:51.030
the justification here. And as I'll talk about, purity

745
00:37:51.030 --> 00:37:53.639
plays a very important role. Uh, A lot of

746
00:37:53.639 --> 00:37:56.459
people in, in, in the West, uh, academics and,

747
00:37:56.520 --> 00:38:01.699
and highly educated individuals. Their moral perspective generally doesn't

748
00:38:01.699 --> 00:38:04.020
include this notion of purity, but it does for

749
00:38:04.020 --> 00:38:05.979
the majority of the world. It has for the

750
00:38:05.979 --> 00:38:08.459
majority of the past. And as I'll talk about,

751
00:38:08.540 --> 00:38:12.459
it, it's extremely damaging. OK. Um, SO those are

752
00:38:12.459 --> 00:38:15.735
some of the ways that morality helps perpetuate. SCALE

753
00:38:15.735 --> 00:38:20.334
violence, small-scale violence. And so, uh, I, I, I,

754
00:38:20.455 --> 00:38:23.254
I know that the philosopher, uh, um, uh, and,

755
00:38:23.324 --> 00:38:26.064
and error theorist and, and opponent of more abolitionists,

756
00:38:26.074 --> 00:38:28.544
I should say, uh, Richard Joyce, uh, who, who

757
00:38:28.544 --> 00:38:30.135
I'll, I'll talk about in more depth in a

758
00:38:30.135 --> 00:38:35.949
moment. Um. Uh, I forgot I was uh going

759
00:38:35.949 --> 00:38:37.939
there first. Oh yeah. So he makes the case

760
00:38:37.939 --> 00:38:39.659
that he, one of the reasons he rejects moral

761
00:38:39.659 --> 00:38:42.879
abolitionism is that he thinks, look, we can't really

762
00:38:42.879 --> 00:38:46.310
establish, for instance, that morality has caused more violence

763
00:38:46.310 --> 00:38:48.310
than it's inhibited. And, and I agree with that,

764
00:38:48.429 --> 00:38:50.510
right? There's, we, we, we have a lot of

765
00:38:50.510 --> 00:38:52.389
empirical information, so I think we can make a

766
00:38:52.389 --> 00:38:55.469
very compelling case for it, but we'll never know

767
00:38:55.469 --> 00:38:58.966
a complete answer, right? Um, NONETHELESS, and, and, and

768
00:38:58.966 --> 00:39:01.516
I'll, uh, I'll, I'll end this, uh, uh, question

769
00:39:01.516 --> 00:39:04.476
by saying this. Um, IT'S just simply hard for

770
00:39:04.476 --> 00:39:07.795
me to imagine that whatever role morality's played in

771
00:39:07.795 --> 00:39:12.436
preventing violence, it balances out the millions or billions

772
00:39:12.436 --> 00:39:15.996
of lives that have either, uh, been killed or

773
00:39:15.996 --> 00:39:18.635
severely wounded, and, and the other lives have been

774
00:39:18.635 --> 00:39:21.471
devastated by loved ones and, and friends and died

775
00:39:21.681 --> 00:39:25.281
um from uh violence caused by morality. And, and

776
00:39:25.281 --> 00:39:27.281
so because of that, I, I, I think it's,

777
00:39:27.322 --> 00:39:30.681
it's, uh, I, I think the burden, the philosophical

778
00:39:30.681 --> 00:39:33.041
burden of, of proof is on uh anybody who

779
00:39:33.041 --> 00:39:34.961
thinks that morality is actually more of a preventer

780
00:39:34.961 --> 00:39:37.562
of violence, uh, um, uh, than a, than a

781
00:39:37.562 --> 00:39:40.251
catalyst for it, especially since when it comes to,

782
00:39:40.541 --> 00:39:42.322
and I'll, I, I, let me, let me say

783
00:39:42.322 --> 00:39:44.201
this one thing to, to sort of motivate my

784
00:39:44.201 --> 00:39:49.810
position. So, when it comes to preventing violence, There

785
00:39:49.810 --> 00:39:53.330
are a lot of non-moral factors that prevent people

786
00:39:53.330 --> 00:39:55.719
from engaging in violence. And so, I think one,

787
00:39:56.729 --> 00:39:58.610
important study that was done is I think that

788
00:39:58.610 --> 00:40:01.939
in World War 2, something like, I, I, I

789
00:40:01.939 --> 00:40:03.889
can't recall the exact number, but something, and, and

790
00:40:03.889 --> 00:40:06.689
you feel free to everybody out there to research

791
00:40:06.689 --> 00:40:09.570
this on your own, uh, to verify. Um, BUT

792
00:40:09.570 --> 00:40:12.610
something like only 20% of soldiers in the field

793
00:40:12.610 --> 00:40:15.250
in World War II, like, fired their guns like

794
00:40:15.250 --> 00:40:19.429
at the opponent or something. Um, EVEN though Not

795
00:40:19.429 --> 00:40:21.590
only was it not considered immoral to do it,

796
00:40:21.629 --> 00:40:23.679
but there were moral reasons for doing it, right?

797
00:40:23.909 --> 00:40:25.429
So, a lot of the training was to try

798
00:40:25.429 --> 00:40:27.280
to get the soldiers to think they are the

799
00:40:27.280 --> 00:40:29.350
enemy, they're immoral, you have to kill them. It's

800
00:40:29.350 --> 00:40:32.219
your moral duty to do that. And most still

801
00:40:32.219 --> 00:40:34.260
couldn't do it, right? And what does this show?

802
00:40:34.300 --> 00:40:36.979
This shows that we don't need morality to inhibit

803
00:40:36.979 --> 00:40:39.780
violence. Sometimes, sure, it does. But a lot of

804
00:40:39.780 --> 00:40:43.030
times we have these non-moral um aspects of our,

805
00:40:43.129 --> 00:40:46.939
our, um our, our, our psyche that, that is,

806
00:40:46.959 --> 00:40:50.020
is sufficient for preventing violence. Uh WHEREAS when it

807
00:40:50.020 --> 00:40:53.260
comes to promoting violence or, or uh uh perpetuating

808
00:40:53.260 --> 00:40:56.580
violence, oftentimes, moral reasons are the main reasons, and

809
00:40:56.580 --> 00:40:58.739
without them, violence like, uh, uh, particularly on a

810
00:40:58.739 --> 00:41:00.020
large scale, wouldn't be committed.

811
00:41:00.850 --> 00:41:03.570
So I think that you probably already partly answered

812
00:41:03.570 --> 00:41:07.070
this next question when you talked about their uh

813
00:41:07.070 --> 00:41:12.820
about uh war particularly, but how does morality influence

814
00:41:12.820 --> 00:41:13.399
politics?

815
00:41:14.649 --> 00:41:18.860
Yeah. Um, SO I, yeah, so this is, and

816
00:41:18.860 --> 00:41:20.570
we're, we're seeing this played out in real time,

817
00:41:20.620 --> 00:41:22.540
by the way, particularly in the United States and,

818
00:41:22.620 --> 00:41:25.459
and across the world as well, right? Um, SO,

819
00:41:25.469 --> 00:41:30.939
I, I'll start by, um, giving a quotation by

820
00:41:30.939 --> 00:41:33.739
Donald Trump, President Donald Trump, uh, in, in a

821
00:41:33.739 --> 00:41:36.810
2023 meeting of, of, of CPAC, where he said,

822
00:41:37.060 --> 00:41:40.139
I am your justice, I am your retribution. Right?

823
00:41:41.040 --> 00:41:44.159
So, clearly, morality plays an important role in politics

824
00:41:44.159 --> 00:41:47.560
all across the spectrum, all right? And I, I,

825
00:41:47.639 --> 00:41:50.870
I think, obviously, Donald Trump is a polarizing figure.

826
00:41:51.040 --> 00:41:53.719
Um, MANY on the right support him, uh, in

827
00:41:53.719 --> 00:41:56.229
the United States, I, I should mention, uh, uh,

828
00:41:56.239 --> 00:41:58.750
support him fervently, whereas a lot on the left,

829
00:41:58.879 --> 00:42:03.729
uh, really despise him. Um, BUT both sides have

830
00:42:03.729 --> 00:42:07.110
very strong, what they consider to be very strong

831
00:42:07.110 --> 00:42:11.399
moral reasons uh for, for their position. OK. Um,

832
00:42:11.649 --> 00:42:14.250
SO, when I'm talking about in, in the book

833
00:42:14.250 --> 00:42:15.959
and the chapter, when I talk about morality and

834
00:42:15.959 --> 00:42:20.639
politics, my argument is that morality, um, Generally today,

835
00:42:20.679 --> 00:42:23.669
but almost certainly in the past as well, causes

836
00:42:23.669 --> 00:42:26.540
more problems than it solves. Uh, MORALITY, when it's

837
00:42:26.540 --> 00:42:29.199
involved in political decisions, we tend to have worse

838
00:42:29.199 --> 00:42:31.199
political decisions. And, and, and how do I cash

839
00:42:31.199 --> 00:42:34.110
out worse? Just think in terms of beneficial outcomes.

840
00:42:34.360 --> 00:42:36.790
OK, so, so what's some evidence for that? So,

841
00:42:37.040 --> 00:42:42.145
um, generally speaking, the research indicates that liberal policies,

842
00:42:42.354 --> 00:42:46.114
politically liberal policies tend to promote better outcomes than

843
00:42:46.114 --> 00:42:48.754
conservative policies. Now, there are exceptions, of course. I'm

844
00:42:48.754 --> 00:42:51.175
not saying that all liberal policies are good, all

845
00:42:51.175 --> 00:42:54.675
conservative policies are bad, but generally speaking, when we

846
00:42:54.675 --> 00:42:58.554
talk about uh conservative economic policies, for instance, so,

847
00:42:58.715 --> 00:43:04.639
uh, deregulation, Uh, supply side economics, uh, not providing

848
00:43:04.639 --> 00:43:08.590
universal healthcare, um, eliminating programs for the social safety

849
00:43:08.590 --> 00:43:12.000
net. Research shows that when these, uh, these policies

850
00:43:12.000 --> 00:43:14.469
are put into place, a society generally does worse

851
00:43:14.469 --> 00:43:17.320
off. We see things like lower life expectancy. I'm

852
00:43:17.320 --> 00:43:20.000
talking, again, I, I'm, I'm gonna be talking specifically

853
00:43:20.000 --> 00:43:22.520
in the United States, and I should mention, um,

854
00:43:22.560 --> 00:43:26.629
it, it, it There are, there's wide variation between

855
00:43:26.629 --> 00:43:31.199
different conceptions of conservatism and liberalism. Uh, CONSERVATIVES, as

856
00:43:31.199 --> 00:43:33.159
a group can believe very different things, as can

857
00:43:33.159 --> 00:43:37.270
liberals, particularly across countries. So, across countries, rather. So,

858
00:43:37.639 --> 00:43:40.820
Um, the average conservative in the United States is

859
00:43:40.820 --> 00:43:43.479
much more right-wing than the average conservative in, say,

860
00:43:43.580 --> 00:43:47.780
uh, uh, Europe. OK. Um, THAT said, so, as

861
00:43:47.780 --> 00:43:50.379
I mentioned, a lot of evidence generally shows that

862
00:43:50.379 --> 00:43:53.540
liberal policies do better than conservative policies and producing

863
00:43:53.540 --> 00:43:59.919
good outcomes. So, um, Ocalix, uh, uh, Kazarian, uh,

864
00:43:59.949 --> 00:44:03.860
in, in 2014 and his colleagues did a study

865
00:44:04.129 --> 00:44:08.149
where they actually spent, I, I think, better part

866
00:44:08.149 --> 00:44:09.909
of a decade and a half to two decades

867
00:44:10.149 --> 00:44:13.550
looking at how Europe was shaped, uh, the impact

868
00:44:13.550 --> 00:44:16.350
of Europe on political policies, and they showed that

869
00:44:16.350 --> 00:44:19.389
the liberal policies brought about better outcomes than conservative

870
00:44:19.389 --> 00:44:24.770
policies. OK. Um, CRIMINAL justice. Uh, SO, so, uh,

871
00:44:24.810 --> 00:44:28.659
oh, and, and so, Assuming that someone agrees, yeah,

872
00:44:28.780 --> 00:44:31.739
OK, well, in general, conservative policies bring about uh

873
00:44:31.739 --> 00:44:33.590
better results than liberal policies. But what does this

874
00:44:33.590 --> 00:44:36.020
have to do with morality? As I said, liberals

875
00:44:36.020 --> 00:44:41.229
appeal the moral justifications, conservatives do. Conservatives tend to

876
00:44:41.229 --> 00:44:45.709
uh use more moral justifications than liberals. OK. So,

877
00:44:45.764 --> 00:44:48.395
Um, what's the evidence for this? Well, uh, Jim

878
00:44:48.395 --> 00:44:50.834
Everett and his colleagues in a, in a 2020

879
00:44:50.834 --> 00:44:53.794
study actually showed that conservatives and, and it was

880
00:44:53.794 --> 00:44:57.655
done on, uh, American, uh, conservatives tend to moralize,

881
00:44:57.915 --> 00:45:01.274
um, more issues and make more attributions of moral

882
00:45:01.274 --> 00:45:05.300
wrongness than liberals. All right. So, um, And this

883
00:45:05.300 --> 00:45:07.120
seems to be a pattern that we find that

884
00:45:07.120 --> 00:45:12.479
people who identify as conservatives, generally, objective morality tends

885
00:45:12.479 --> 00:45:15.479
to play a larger role um in their lives

886
00:45:15.479 --> 00:45:17.520
and in their moral views than, than it does

887
00:45:17.520 --> 00:45:20.320
for liberals. I, I, I, I should also mention

888
00:45:21.120 --> 00:45:23.929
Um, stepping outside of the, the, the, the conservative

889
00:45:23.929 --> 00:45:27.250
liberal liberal divide for a second. Um, AN important

890
00:45:27.250 --> 00:45:30.530
study that was done was in 2016, uh, by

891
00:45:30.530 --> 00:45:33.560
Stankoff and Lee. And what they did is they,

892
00:45:33.570 --> 00:45:38.330
uh, tried identifying, um, they, they split, uh, countries

893
00:45:38.330 --> 00:45:41.270
and regions around the world in particular. GROUPS, basically,

894
00:45:41.489 --> 00:45:44.149
geographic groups, but not necessarily. There was also the

895
00:45:44.149 --> 00:45:47.209
Anglo group, which included the United States, Canada, Australia,

896
00:45:47.250 --> 00:45:50.489
and so forth. And then assess them according to

897
00:45:50.489 --> 00:45:55.439
three different criteria, um, morality, uh, religiosity, and, and,

898
00:45:55.449 --> 00:45:59.129
uh, nastiness, which nastiness is a pro-violence attitude. OK.

899
00:45:59.530 --> 00:46:01.850
Um, AND what they found is a, a, a

900
00:46:01.850 --> 00:46:04.929
very strong correlation, uh, well, a, a, a somewhat

901
00:46:04.929 --> 00:46:08.090
weak correlation among individuals, right? That the more moral

902
00:46:08.090 --> 00:46:10.570
somebody was, the nastier they were, the more prone

903
00:46:10.570 --> 00:46:12.800
to violence they were. And, and this was, uh,

904
00:46:12.810 --> 00:46:16.330
also, um, consistent with findings, uh, uh, uh, I

905
00:46:16.330 --> 00:46:19.040
think, uh, by the Global Peace Index as well.

906
00:46:19.300 --> 00:46:24.250
And, um, So, you, you've got this correspondence between

907
00:46:24.250 --> 00:46:26.649
this, this, and, and a very strong correlation, I

908
00:46:26.649 --> 00:46:29.850
should say, between the level of morality of a

909
00:46:29.850 --> 00:46:32.969
given country and or a region and how uh

910
00:46:32.969 --> 00:46:35.399
nasty it is, or prone to violence it is.

911
00:46:35.570 --> 00:46:38.510
OK? Um, WHERE the, the most, the, the, the

912
00:46:38.570 --> 00:46:41.830
Uh, the countries that, that, uh, are considered measures

913
00:46:41.830 --> 00:46:44.429
most moral, all right, by very different measures, they

914
00:46:44.429 --> 00:46:46.870
did a variety of measures to, to, to uh

915
00:46:46.870 --> 00:46:49.699
measure this. Um, THE, the countries that are most

916
00:46:49.699 --> 00:46:51.830
moral tend to be the most nasty, and the

917
00:46:51.830 --> 00:46:53.429
countries that are least moral tend to be the

918
00:46:53.429 --> 00:46:56.790
least nasty. So right there, You get this, this

919
00:46:56.790 --> 00:46:59.709
fairly unique study, I think, which shows a very

920
00:46:59.709 --> 00:47:02.510
strong, now, now they, uh, uh, yeah, a very

921
00:47:02.510 --> 00:47:07.129
strong uh correlation between the level of uh morality

922
00:47:07.129 --> 00:47:09.989
in in the role that it plays in individuals'

923
00:47:09.989 --> 00:47:13.550
lives and, and the greater cultural, greater culture, and

924
00:47:13.550 --> 00:47:17.760
the amount of violence, OK? Um, NOW, stepping more

925
00:47:17.760 --> 00:47:21.510
specifically into politics. So what are some, uh, specific

926
00:47:21.510 --> 00:47:25.600
examples of how morality brings about, um, negative policies?

927
00:47:25.679 --> 00:47:27.800
I, I, I talked about generally conservatives tend to

928
00:47:27.800 --> 00:47:30.360
be more moral, and I don't mean objectively moral.

929
00:47:30.479 --> 00:47:32.479
I'm just, uh, I think about the, the belief

930
00:47:32.479 --> 00:47:35.360
in morality. The, the belief in morality among conservatives

931
00:47:35.360 --> 00:47:38.040
tends to be stronger than that, uh, among liberals,

932
00:47:38.080 --> 00:47:40.199
they tend to be more objective than liberals, liberals

933
00:47:40.199 --> 00:47:43.800
tend to be more relativistic. Um, OK, but you

934
00:47:43.800 --> 00:47:47.439
also find that where morality shapes policies, you have

935
00:47:47.439 --> 00:47:51.159
these very negative outcomes or a negative outcomes where

936
00:47:51.159 --> 00:47:53.429
had the policies been shaped just by, let's say,

937
00:47:53.439 --> 00:47:56.639
self-interest, um, the policies would likely have been very

938
00:47:56.639 --> 00:47:59.719
different, um, and, and much more beneficial. So, in

939
00:47:59.719 --> 00:48:03.030
the United States, think gun control, right? So, what,

940
00:48:03.159 --> 00:48:06.750
uh, there's for, for viewers who aren't aware, um,

941
00:48:06.870 --> 00:48:09.466
there Gun violence in, in the United States is,

942
00:48:09.516 --> 00:48:12.355
is higher than anywhere in the world. Um, WE,

943
00:48:12.426 --> 00:48:14.855
we have more guns per capita. I believe than

944
00:48:14.855 --> 00:48:16.506
anybody in the world. There, there's certainly more guns

945
00:48:16.506 --> 00:48:19.105
than, than citizens in the United States, and more

946
00:48:19.105 --> 00:48:21.266
mass shootings than any country around the world. All

947
00:48:21.266 --> 00:48:24.206
right? Now, from a pure self-interest standpoint, and by

948
00:48:24.206 --> 00:48:26.516
the way, I should mention my alma mater, Florida

949
00:48:26.516 --> 00:48:28.946
State, where I, where I received my PhD, there

950
00:48:28.946 --> 00:48:31.266
was just a mass shooting, uh, uh, a few

951
00:48:31.266 --> 00:48:34.731
days. THAT, that seemed to be spawned uh by

952
00:48:34.731 --> 00:48:37.771
someone with, with white supremacist tendencies, and he was

953
00:48:37.771 --> 00:48:40.531
also diagnosed as having some, some mental concerns as

954
00:48:40.531 --> 00:48:43.451
well. But that aside, the, the, the major concern

955
00:48:43.451 --> 00:48:45.132
here is, why are there so many guns in

956
00:48:45.132 --> 00:48:48.041
the US um where you've had these mass shootings,

957
00:48:48.052 --> 00:48:50.971
where, you know, you, it's typical in the United

958
00:48:50.971 --> 00:48:53.011
States to actually have these drills where students have

959
00:48:53.011 --> 00:48:57.260
to prepare for these shootings. Um, Why do we

960
00:48:57.260 --> 00:48:59.699
have this? Um, IS it because people think it's

961
00:48:59.699 --> 00:49:02.010
in their self-interest to have this many guns? No.

962
00:49:02.219 --> 00:49:04.370
I, I, I can't imagine anybody in their right

963
00:49:04.370 --> 00:49:06.699
mind being informed thinking, oh, we need this many

964
00:49:06.699 --> 00:49:08.889
guns in the United States. The reason why we

965
00:49:08.889 --> 00:49:10.580
have such a lack of gun control in the

966
00:49:10.580 --> 00:49:12.830
United States is the moral arguments given on the

967
00:49:12.830 --> 00:49:15.949
political right. That generally appeal to this, this, this

968
00:49:15.949 --> 00:49:18.229
right to own guns, where they appeal to the

969
00:49:18.229 --> 00:49:21.770
US Constitution, um, to, to make the moral claim

970
00:49:21.770 --> 00:49:24.070
that yes, you know, it's unfortunate that we have

971
00:49:24.070 --> 00:49:26.629
these gun debts, but it would simply be morally

972
00:49:26.629 --> 00:49:29.469
wrong to infringe on someone's right to own a

973
00:49:29.469 --> 00:49:32.360
gun, right? Um, SO that's, that's one example, but

974
00:49:32.360 --> 00:49:35.040
there's, there's plenty of them. Uh, COVID, uh, vaccines

975
00:49:35.040 --> 00:49:38.060
in the United States. Again, from a purely self-interested

976
00:49:38.060 --> 00:49:41.320
standpoint, it seems like, yes, everybody should go get

977
00:49:41.320 --> 00:49:45.800
the COVID vaccine. It, it's, it's generally extremely been

978
00:49:45.800 --> 00:49:48.945
shown, been, been proven to be extremely safe. Um,

979
00:49:49.195 --> 00:49:50.834
AND without it, you know, there have been in

980
00:49:50.834 --> 00:49:53.354
the United States alone, millions of deaths, but people

981
00:49:53.354 --> 00:49:58.235
gave arguments on, again, primarily rights-based, individual rights on

982
00:49:58.235 --> 00:50:00.784
why it's wrong for, for the government to say,

983
00:50:00.794 --> 00:50:03.314
force people to have vaccines or why people should

984
00:50:03.314 --> 00:50:07.939
abstain from having vaccines. Um. But, but there's plenty

985
00:50:07.939 --> 00:50:10.540
of other examples I can use. So, uh, the

986
00:50:10.540 --> 00:50:15.060
basic point where morality gets involved, and, and, and

987
00:50:15.060 --> 00:50:17.860
I should, let me, let me mention this as

988
00:50:17.860 --> 00:50:20.659
well, because this is important. Um, ONE of the

989
00:50:20.659 --> 00:50:22.389
main objections I think to my view is, well,

990
00:50:22.580 --> 00:50:24.949
certainly morality has played a role in some very

991
00:50:24.949 --> 00:50:28.020
important political movements, right? Think of the eradication of

992
00:50:28.020 --> 00:50:32.159
slavery, think of civil rights, um, uh. The push

993
00:50:32.159 --> 00:50:34.919
for universal healthcare in Europe and the United States,

994
00:50:34.949 --> 00:50:38.469
and so forth. And absolutely, I agree, that's correct.

995
00:50:38.719 --> 00:50:42.570
Um, But in terms of, let's say, issues that

996
00:50:42.570 --> 00:50:46.209
were, there were uh sought social justice, right? Uh,

997
00:50:46.250 --> 00:50:49.399
THE eradication of slavery and uh the, the uh

998
00:50:49.409 --> 00:50:51.929
uh extension of civil rights to women or, or

999
00:50:51.929 --> 00:50:55.159
ethnic minorities and so forth. Yes, those, those, uh,

1000
00:50:55.169 --> 00:50:58.489
moral arguments did play an important role. Um, BUT

1001
00:50:58.489 --> 00:51:00.929
what I think people fail to realize is that

1002
00:51:00.929 --> 00:51:04.879
these, um, oppressive institutions were upheld to begin with,

1003
00:51:04.889 --> 00:51:07.520
and probably existed because of moral arguments in their

1004
00:51:07.520 --> 00:51:11.010
favor, favor rather. So for every argument being given

1005
00:51:11.010 --> 00:51:13.560
that they're morally wrong, they are the individuals arguing

1006
00:51:13.560 --> 00:51:15.719
it was morally right. And in fact, uh, in

1007
00:51:15.719 --> 00:51:19.560
the Confederacy, um, the vice president argued that, you

1008
00:51:19.560 --> 00:51:21.800
know, ours is the first, I'm paraphrasing, but, but

1009
00:51:21.800 --> 00:51:23.879
uh the, the Confederacy is the first union to

1010
00:51:23.879 --> 00:51:28.590
be established on um the natural law view, uh,

1011
00:51:28.600 --> 00:51:31.590
the moral natural law view that Blacks are inferior

1012
00:51:31.590 --> 00:51:35.870
to whites, right? Uh, SO, so, while I, I

1013
00:51:35.870 --> 00:51:38.629
don't deny the role that morality is played in,

1014
00:51:38.669 --> 00:51:43.310
and positive political movements, it generally is also responsible

1015
00:51:43.310 --> 00:51:45.830
for the negative, uh, uh, movements as, uh, movements

1016
00:51:45.830 --> 00:51:48.629
as well. And I don't think you need morality.

1017
00:51:48.739 --> 00:51:50.790
Certainly, it, it, it's playing a role, it's played

1018
00:51:50.790 --> 00:51:53.550
an important role in the past, but that worm

1019
00:51:53.550 --> 00:51:55.500
seems to have turned, right? So we're talking about

1020
00:51:55.500 --> 00:51:56.979
social justice. Justice. What do we have now in

1021
00:51:56.979 --> 00:52:00.100
the United States, you know, right? So, um, we,

1022
00:52:00.179 --> 00:52:02.219
we now have a government that's going again, you

1023
00:52:02.219 --> 00:52:05.300
know, opposing social justice seems almost to be like

1024
00:52:05.300 --> 00:52:08.780
their main, uh, uh, aim, right? Uh, THE, this,

1025
00:52:08.860 --> 00:52:11.979
the, the, the fight against wokeness and so forth.

1026
00:52:12.000 --> 00:52:14.060
And I think most people don't even understand what

1027
00:52:14.060 --> 00:52:17.419
that means, but basically just refers to, um, being

1028
00:52:17.419 --> 00:52:19.979
aware of social, uh, injustice and and trying to

1029
00:52:19.979 --> 00:52:23.689
make amends for it. Um, AND, and the, the,

1030
00:52:23.709 --> 00:52:27.800
the, uh, political, uh, aims of, of the US

1031
00:52:27.800 --> 00:52:31.199
government right now is antithetical to social justice, right?

1032
00:52:31.439 --> 00:52:32.760
Um, YOU know, you, you see sort of the

1033
00:52:32.760 --> 00:52:35.120
resurgence of the, of the KKK, all this sort

1034
00:52:35.120 --> 00:52:36.949
of stuff that I would consider to be awful.

1035
00:52:37.159 --> 00:52:39.679
Uh, NOT morally awful. I just really, really don't

1036
00:52:39.679 --> 00:52:41.919
like it, and I think it's, it's, uh, uh,

1037
00:52:41.929 --> 00:52:44.000
adverse to the welfare of, of the country and

1038
00:52:44.000 --> 00:52:47.959
perhaps the world, right? Um, SO, Yeah, so, so,

1039
00:52:48.040 --> 00:52:51.750
you know, what I, I, I think, without morality,

1040
00:52:51.929 --> 00:52:55.520
to sort of bolster the, these oppressive political movements,

1041
00:52:55.659 --> 00:52:58.719
um, these negative political movements, if we just relied

1042
00:52:58.719 --> 00:53:01.320
on things like empathy and self-interest, which, again, I'll

1043
00:53:01.320 --> 00:53:04.399
talk about more in, in the future, um, in

1044
00:53:04.399 --> 00:53:07.330
this, in this discussion, uh, we would have better

1045
00:53:07.330 --> 00:53:10.479
policy. Last thing I'll mention, um, is that one

1046
00:53:10.479 --> 00:53:12.689
of the main things in the United States is

1047
00:53:13.060 --> 00:53:15.389
You know, you, you find, uh, when I talk

1048
00:53:15.389 --> 00:53:19.750
about conservative fiscal policy, economic policy, um, clearly, we

1049
00:53:19.750 --> 00:53:22.510
see from the Reagan years to today where sort

1050
00:53:22.510 --> 00:53:26.709
of supply side economics and deregulation is, um, More

1051
00:53:26.709 --> 00:53:28.590
or less been, been one of the driving forces

1052
00:53:28.590 --> 00:53:31.199
in, in, in uh economics in the US, you

1053
00:53:31.199 --> 00:53:33.590
see a greater and greater discrepancy of wealth that

1054
00:53:33.590 --> 00:53:35.949
has clearly been hurting the poor, the middle, and

1055
00:53:35.949 --> 00:53:38.270
the working class in the United States. And yet

1056
00:53:38.270 --> 00:53:42.389
you find poor and working class uh Republicans, uh,

1057
00:53:42.429 --> 00:53:45.149
or a lot of them, particularly whites, uh, voting

1058
00:53:45.149 --> 00:53:48.340
Republican. And this is, this is sort of very

1059
00:53:48.340 --> 00:53:50.669
odd, right? Because it seems to go clearly against

1060
00:53:50.669 --> 00:53:54.295
their, their financial self-interest. But as Jonathan Haidt said,

1061
00:53:54.375 --> 00:53:57.045
yes, but according to what their perceived moral interests,

1062
00:53:57.216 --> 00:54:00.645
they're doing what they think is right, right? Um,

1063
00:54:00.656 --> 00:54:03.216
AND there was a 2019 study by the Cato

1064
00:54:03.216 --> 00:54:06.085
Institute. I think Laura Ekins was, was the, um,

1065
00:54:06.156 --> 00:54:08.736
the author or conductor of the study. And it's

1066
00:54:08.736 --> 00:54:11.295
very interesting because what it showed was that a

1067
00:54:11.295 --> 00:54:15.855
lot of the disagreement between conservatives and liberals regarding

1068
00:54:15.855 --> 00:54:21.132
economic policy, um, is Driven by moral considerations, particularly

1069
00:54:21.132 --> 00:54:24.062
free will, right? And, and in opposition to socialism,

1070
00:54:24.281 --> 00:54:27.271
right? With government involvement in, in the, uh, uh,

1071
00:54:27.281 --> 00:54:30.681
in the economy to help uh improve the social

1072
00:54:30.681 --> 00:54:33.122
safety net and so forth. Um, AND, and it,

1073
00:54:33.241 --> 00:54:35.241
and it's very interesting. Again, what it showed is

1074
00:54:35.241 --> 00:54:38.162
that people generally have, if you're a Republican, you

1075
00:54:38.162 --> 00:54:41.602
think people are responsible for their outcomes, right? Um,

1076
00:54:41.642 --> 00:54:43.481
IF someone's poor, they're just not working hard enough,

1077
00:54:43.642 --> 00:54:46.739
you know, um, If someone's rich, it's all through

1078
00:54:46.739 --> 00:54:50.500
their own efforts. Whereas, uh, liberals, uh, progressives, Democrats

1079
00:54:50.500 --> 00:54:52.330
generally have the view that, no, there's a lot

1080
00:54:52.330 --> 00:54:55.590
of, even if, if, if personal responsibility plays some

1081
00:54:55.590 --> 00:54:58.419
role, there's a lot of external factors that come

1082
00:54:58.419 --> 00:55:00.820
into play here that in a sense, make it

1083
00:55:00.820 --> 00:55:04.459
unfair that some people have obscene amounts of wealth

1084
00:55:04.459 --> 00:55:06.939
while other people are, you know, maybe working multiple

1085
00:55:06.939 --> 00:55:10.419
jobs and barely Scraping by. So, um, these are

1086
00:55:10.419 --> 00:55:12.340
some of the ways where I think that morality

1087
00:55:12.340 --> 00:55:17.040
really results in a negative, uh, political policy, economic

1088
00:55:17.040 --> 00:55:19.899
criminal justice, um, that I didn't really go into

1089
00:55:19.899 --> 00:55:23.489
criminal justice, but yeah, the, the, the, the retributive

1090
00:55:23.489 --> 00:55:25.979
basis for criminal justice that we find in the

1091
00:55:25.979 --> 00:55:28.780
United States is responsible for the negative outcomes we

1092
00:55:28.780 --> 00:55:31.919
have there, right? So, if we Got rid of

1093
00:55:31.919 --> 00:55:34.639
these moral considerations, all right? The good with the

1094
00:55:34.639 --> 00:55:37.909
bad, and just relied on things like natural empathy,

1095
00:55:38.040 --> 00:55:41.120
OK, um, reason, a better awareness of the, the

1096
00:55:41.120 --> 00:55:43.949
factors that shape us, uh, and, and, and self-interest,

1097
00:55:44.040 --> 00:55:46.429
you know, prudential self-interest, I think we'd have much,

1098
00:55:46.479 --> 00:55:48.639
much better policy in the United States and beyond.

1099
00:55:50.050 --> 00:55:55.649
On an individual level, would abolishing morality also impact

1100
00:55:55.649 --> 00:55:57.010
individual well-being?

1101
00:55:59.100 --> 00:56:02.139
OK. So, this is uh a question and I,

1102
00:56:02.179 --> 00:56:03.939
I addressed this in a, in another chapter of

1103
00:56:03.939 --> 00:56:07.540
the book. Um, YOU know, would sort of brass

1104
00:56:07.540 --> 00:56:10.459
tacks, would we be happier with without morality in

1105
00:56:10.459 --> 00:56:13.060
our lives? This is a more difficult question, I

1106
00:56:13.060 --> 00:56:14.739
think. I, I, I think the, the sort of

1107
00:56:14.739 --> 00:56:17.659
empirical evidence you can bring to uh weigh on

1108
00:56:17.659 --> 00:56:21.370
the question of whether Morality has, uh, you know,

1109
00:56:21.679 --> 00:56:24.439
it's effect on violence, its effect on politics. I

1110
00:56:24.439 --> 00:56:26.800
think you can make a a a very compelling

1111
00:56:26.800 --> 00:56:29.320
arguments that uh morality is negative, uh, when it

1112
00:56:29.320 --> 00:56:31.860
comes to its impact on, on, uh, violence and

1113
00:56:31.860 --> 00:56:34.760
politics. With regard to individual happiness, it's a little

1114
00:56:34.760 --> 00:56:37.179
more tricky, but I still think that the ledger

1115
00:56:37.179 --> 00:56:38.800
comes out on the side of doing away with

1116
00:56:38.800 --> 00:56:42.030
morality, moral abolitionism. The main reason is this, if

1117
00:56:42.030 --> 00:56:44.274
we got rid of morality, You know, I don't

1118
00:56:44.274 --> 00:56:46.594
see this happening anytime soon, but perhaps in some

1119
00:56:46.594 --> 00:56:50.304
idealistic situation, um, that, that I think would be

1120
00:56:50.554 --> 00:56:52.514
better off in, in the future. I mean, I

1121
00:56:52.514 --> 00:56:54.435
admit I could be wrong about this. I encourage

1122
00:56:54.435 --> 00:56:57.314
people to do their own, uh, uh, empirical work.

1123
00:56:57.395 --> 00:57:02.195
Um, I, I'm, I'm interested in, in finding coherent

1124
00:57:02.195 --> 00:57:05.354
arguments empirically based, particularly that call my conclusions to

1125
00:57:05.354 --> 00:57:08.379
the question. But, but, you know, uh, uh, I

1126
00:57:08.379 --> 00:57:11.020
earnestly tried to find out what, what, uh, looked

1127
00:57:11.020 --> 00:57:13.939
into what the best uh um uh empirical evidence

1128
00:57:13.939 --> 00:57:15.899
has to say, and it, it did seem to,

1129
00:57:16.100 --> 00:57:18.780
to suggest we're better off without morality. So, when

1130
00:57:18.780 --> 00:57:21.340
it comes to individuals, I think overall we'd be

1131
00:57:21.340 --> 00:57:23.530
better just because if we got rid of morality,

1132
00:57:23.659 --> 00:57:26.020
we would have less violence. We'd have better political

1133
00:57:26.020 --> 00:57:30.320
policy, OK? Um, And this is the main reason

1134
00:57:30.320 --> 00:57:32.419
why I think overall we'd be better off, but

1135
00:57:32.879 --> 00:57:34.850
You also have to consider the role that morality

1136
00:57:34.850 --> 00:57:38.969
plays in people's lives, right? And certainly, morality does

1137
00:57:38.969 --> 00:57:42.889
seem to be a source of um uh well-being

1138
00:57:42.889 --> 00:57:44.370
for a lot of people. It does give them

1139
00:57:44.370 --> 00:57:48.010
some, some satisfaction, some happiness. So reflecting positively on

1140
00:57:48.010 --> 00:57:50.530
our behavior. Some, some have argued, you know, with

1141
00:57:50.530 --> 00:57:53.010
uh one of the things that makes life enjoyable

1142
00:57:53.010 --> 00:57:55.290
is to, to look in the mirror and identify

1143
00:57:55.290 --> 00:57:57.639
yourself as, you know, you are a morally upright

1144
00:57:57.639 --> 00:58:00.649
individual. You're doing the right kinds of things. Um,

1145
00:58:00.659 --> 00:58:02.739
I, I agree that that's important, but here's the

1146
00:58:02.739 --> 00:58:05.379
thing, I believe that about myself a lot, right?

1147
00:58:05.550 --> 00:58:06.520
I, I, I try to be a good person.

1148
00:58:06.580 --> 00:58:08.780
I'm not saying I, I always do nice things

1149
00:58:08.780 --> 00:58:12.260
and I can't, you know, act problematically sometimes. But

1150
00:58:12.260 --> 00:58:13.560
generally, when I look at the mirror I'm like,

1151
00:58:13.659 --> 00:58:15.500
yeah, yeah, I think I'm a good person. I

1152
00:58:15.500 --> 00:58:16.860
try to do nice things for people and so

1153
00:58:16.860 --> 00:58:19.000
forth. And I don't believe in morality, right? So

1154
00:58:19.000 --> 00:58:22.760
I, I, I, I am a, a card-carrying error

1155
00:58:22.760 --> 00:58:25.449
theorist and, and moral abolitionist. And I still get

1156
00:58:25.449 --> 00:58:28.830
that. SENSE of uh satisfaction when I, when I

1157
00:58:28.830 --> 00:58:30.659
do things that I consider to be nice for

1158
00:58:30.659 --> 00:58:33.669
people, right? Um, I, I sometimes regret things if

1159
00:58:33.669 --> 00:58:36.189
I do something negative, negatively to some people. If

1160
00:58:36.189 --> 00:58:38.790
I'm overly harsh with my child, um, you know,

1161
00:58:39.030 --> 00:58:40.310
reflect back on it, oh man, that was a

1162
00:58:40.310 --> 00:58:43.070
bad decision, right? So, I think in rejecting morality,

1163
00:58:43.149 --> 00:58:47.229
we wouldn't necessarily have, have to sacrifice, um, that

1164
00:58:47.229 --> 00:58:50.770
feeling of gratification we get from uh re reflecting

1165
00:58:50.770 --> 00:58:54.419
positively on, on ourselves, our character, our behavior. Now,

1166
00:58:55.110 --> 00:58:59.350
A more problematic, uh, aspect of, of being a

1167
00:58:59.350 --> 00:59:03.260
moral abolitionist or a moral anti-realist, is that morality,

1168
00:59:03.590 --> 00:59:06.149
people who have strong moral views seem to have

1169
00:59:06.149 --> 00:59:09.110
a greater sense of purpose and meaning. And uh

1170
00:59:09.110 --> 00:59:12.429
the uh the, the Cato Institute article that I

1171
00:59:12.429 --> 00:59:16.389
actually mentioned, the 2019 article, um, it actually did

1172
00:59:16.389 --> 00:59:18.659
some uh surveys on this, and what it found

1173
00:59:18.659 --> 00:59:22.780
is that um people who uh reject free will,

1174
00:59:22.959 --> 00:59:26.080
um, And therefore, I think people who probably reject

1175
00:59:26.080 --> 00:59:29.530
morality as well, um, have significantly less purpose and

1176
00:59:29.530 --> 00:59:32.850
meaning in their life than people who have strong

1177
00:59:32.850 --> 00:59:36.610
beliefs and, and personal responsibility, OK? Um, AND that

1178
00:59:36.610 --> 00:59:39.590
is problematic because, uh, a sense of, of, of

1179
00:59:39.590 --> 00:59:42.064
purpose has been shown to To be a very

1180
00:59:42.064 --> 00:59:45.625
positive influence in a person's life. So, um, you

1181
00:59:45.625 --> 00:59:47.165
know, if we give up on morality, you know,

1182
00:59:47.544 --> 00:59:49.824
do we have to worry about falling prey to

1183
00:59:49.824 --> 00:59:51.665
this sort of, well, what's the point? You know,

1184
00:59:51.814 --> 00:59:55.264
this, this, this, um, negatively nihilistic attitude on things.

1185
00:59:55.344 --> 00:59:57.014
And, and I don't think so. I should say,

1186
00:59:57.185 --> 01:00:01.024
also in that article in the Cato Institute, it

1187
01:00:01.024 --> 01:00:06.360
showed that, um, Democrats, uh, and liberals also and

1188
01:00:06.370 --> 01:00:09.520
and non-religious individuals also have a very low sense

1189
01:00:09.520 --> 01:00:12.010
of purpose and meaning, right? So, what, what sort

1190
01:00:12.010 --> 01:00:14.489
of came across whereas conservatives generally have a very

1191
01:00:14.489 --> 01:00:17.129
high sense of purpose in their lives, but what

1192
01:00:17.129 --> 01:00:19.570
came across this article is religions seem to be

1193
01:00:19.570 --> 01:00:22.370
playing a major role here, right? That people, it

1194
01:00:22.370 --> 01:00:24.370
might have been religion because conservatives tend to be

1195
01:00:24.370 --> 01:00:28.810
more religious, um, uh, and, and, and, you know,

1196
01:00:29.050 --> 01:00:33.479
religious people tend to have very religiously, um, Inform

1197
01:00:33.479 --> 01:00:35.830
morality or or very strong beliefs on free will,

1198
01:00:36.040 --> 01:00:39.000
that could explain why, um, you know, these certain

1199
01:00:39.000 --> 01:00:41.000
groups have, have, uh, uh, a lot of sense

1200
01:00:41.000 --> 01:00:43.550
of purpose in their lives, and the more left-wing,

1201
01:00:43.719 --> 01:00:47.719
uh, atheists, agnostic view, uh, individuals don't, OK. But

1202
01:00:47.719 --> 01:00:49.800
one, so that is some cause for concern in

1203
01:00:49.800 --> 01:00:52.429
terms of individual happiness. Although I will say this,

1204
01:00:52.600 --> 01:00:55.050
um, if we break this down to brass tacks,

1205
01:00:55.159 --> 01:00:58.399
what we find is that the more religious and

1206
01:00:58.399 --> 01:01:01.550
the more moral societies going by stankoff and 2016

1207
01:01:01.550 --> 01:01:04.510
analysis are less happy, right? So, the happiest countries

1208
01:01:04.510 --> 01:01:08.590
in the world are Scandinavia, New Zealand, um, Japan.

1209
01:01:08.719 --> 01:01:11.479
And these tend to be, um, the, the countries

1210
01:01:11.479 --> 01:01:14.389
that are less moralistic in terms of, uh, uh,

1211
01:01:14.399 --> 01:01:16.639
uh, the, the extent to which they accept objective

1212
01:01:16.639 --> 01:01:20.959
morality, and much less religious. OK. So, yeah, you

1213
01:01:20.959 --> 01:01:23.560
know, there's some facets of being moral and religious

1214
01:01:23.560 --> 01:01:26.000
that, that make you happier than, than average, let's

1215
01:01:26.000 --> 01:01:32.280
say. But overall, um, Since the, the less moral

1216
01:01:32.280 --> 01:01:36.520
and objectively moral, and let's say when I say

1217
01:01:36.520 --> 01:01:38.199
objectively moral, let me make clear, I'm not saying

1218
01:01:38.199 --> 01:01:40.239
they're immoral. I, I, I meant the belief in

1219
01:01:40.239 --> 01:01:43.229
morality. I call this psych uh psychological morality, as

1220
01:01:43.229 --> 01:01:46.199
opposed to metaphysical morality where you actually are moral.

1221
01:01:46.479 --> 01:01:47.840
So when I say less moral, I think of,

1222
01:01:47.919 --> 01:01:50.239
you know, less of a belief and objective morality,

1223
01:01:50.320 --> 01:01:53.280
right? Um, So, yeah, overall, if you want to

1224
01:01:53.280 --> 01:01:55.719
be a happy person, live in a society that

1225
01:01:55.719 --> 01:01:58.120
is characterized by less morality and less religion. That,

1226
01:01:58.189 --> 01:02:01.080
that's what the, the research shows. However, within countries,

1227
01:02:01.189 --> 01:02:04.159
again, yeah, there is some evidence that conservatives are

1228
01:02:04.159 --> 01:02:05.959
happier than liberals, but a lot of this has

1229
01:02:05.959 --> 01:02:08.840
to do with things like system justification, uh, uh,

1230
01:02:08.889 --> 01:02:11.439
uh, a failure to recognize the problems in society,

1231
01:02:11.479 --> 01:02:14.645
and so forth. Um, But even if one isn't

1232
01:02:14.645 --> 01:02:17.095
moral, if one's an anti-realist, if one's a free

1233
01:02:17.095 --> 01:02:20.085
will skeptic or non-religious or what have you, um,

1234
01:02:20.416 --> 01:02:23.575
evidence shows that individuals who, let's say, engage in

1235
01:02:23.575 --> 01:02:27.615
pro-social activities like volunteering, and so forth, have very

1236
01:02:27.615 --> 01:02:30.575
strong, um, senses of purpose and meaning in their

1237
01:02:30.575 --> 01:02:33.335
lives, right? So it's not hopeless, right? So if

1238
01:02:33.335 --> 01:02:36.471
you're a Uh, an anti-realist or a moral abolitionist,

1239
01:02:37.021 --> 01:02:38.521
that you're not gonna, you don't have to worry

1240
01:02:38.521 --> 01:02:40.461
about going through life without a sense of purpose.

1241
01:02:40.632 --> 01:02:41.912
You know, you can find a sense of purpose

1242
01:02:41.912 --> 01:02:43.872
in a job or, or what have you, or

1243
01:02:43.872 --> 01:02:46.652
a political movement. Um, BUT I think volunteering to,

1244
01:02:46.711 --> 01:02:49.751
to help people is, has been shown a very

1245
01:02:49.751 --> 01:02:51.511
effective way of finding a sense of meaning and

1246
01:02:51.511 --> 01:02:56.409
purpose and happiness in one's life. Um, Yeah, finally,

1247
01:02:56.489 --> 01:02:58.570
I, I, I would just uh a couple other

1248
01:02:58.570 --> 01:03:04.229
points about um being a a moral anti-realist's connection

1249
01:03:04.229 --> 01:03:07.510
to individual happiness. Um, ALTHOUGH there's been some individuals

1250
01:03:07.510 --> 01:03:09.590
who have argued, well, if you're immoral, you this

1251
01:03:09.590 --> 01:03:12.750
damages personal relationships. I, I, I think that's more

1252
01:03:12.750 --> 01:03:16.110
or less been debunked. Um, I, I, I Uh,

1253
01:03:16.270 --> 01:03:18.790
this is anecdotal, obviously, but pretty much, I, I

1254
01:03:18.790 --> 01:03:22.070
think every anti-realist I've spoken to or, or free

1255
01:03:22.070 --> 01:03:24.229
will skeptic has said the same thing, it hasn't

1256
01:03:24.229 --> 01:03:26.790
affected our relationships at all with people. And, and,

1257
01:03:26.830 --> 01:03:29.350
and it can even be beneficial insofar as, let's

1258
01:03:29.350 --> 01:03:32.899
say if you reject, um, retributist moral realism or,

1259
01:03:32.909 --> 01:03:35.280
or this very strong sense of libertarian free will,

1260
01:03:35.510 --> 01:03:38.159
you're more forgiving towards others. You're more forgiving towards

1261
01:03:38.206 --> 01:03:41.256
Yourself, right? Um, IF you don't believe in morality,

1262
01:03:41.355 --> 01:03:44.226
you're less likely to feel guilt, which is, uh,

1263
01:03:44.236 --> 01:03:47.716
can, can, uh, take away from your happiness. You're

1264
01:03:47.716 --> 01:03:50.595
not as likely to be and let's say, the

1265
01:03:50.595 --> 01:03:52.996
idea of eternal damnation, which I think more than

1266
01:03:52.996 --> 01:03:55.956
any other idea might be responsible for more suffering

1267
01:03:55.956 --> 01:03:59.355
for humanity. So, there are some advantages for actually

1268
01:03:59.355 --> 01:04:03.602
being anti-realestate. I think. Um, BUT, uh, summary here,

1269
01:04:03.761 --> 01:04:06.721
you know, in the grand scheme of things, would

1270
01:04:06.721 --> 01:04:09.602
we be happier, be more realist or anti-realists? Well,

1271
01:04:09.761 --> 01:04:11.672
it's, it's really hard to tell. But again, I

1272
01:04:11.672 --> 01:04:16.241
think because morality, uh, has a negative impact generally

1273
01:04:16.241 --> 01:04:19.721
on society, we'd probably be better off if we

1274
01:04:19.721 --> 01:04:22.322
lived in an a uh uh uh an abolitionist

1275
01:04:22.322 --> 01:04:22.632
world.

1276
01:04:23.590 --> 01:04:26.459
So, I also need to ask you, are there

1277
01:04:26.459 --> 01:04:32.659
preferable alternatives to moral abolitionism available to error theorists?

1278
01:04:33.870 --> 01:04:36.550
Yeah, I, well, obviously I don't think so, but,

1279
01:04:36.590 --> 01:04:39.110
but this, I, I, I, I think here, I'm

1280
01:04:39.110 --> 01:04:42.270
a lot more open, uh, uh, to opposing points

1281
01:04:42.270 --> 01:04:43.989
of view. And I'm, I'm still open to posing

1282
01:04:43.989 --> 01:04:46.850
opposing points of view. So, I mentioned Richard Joyce.

1283
01:04:46.909 --> 01:04:50.360
Now, Richard Joyce is probably the, the, the most,

1284
01:04:50.389 --> 01:04:53.949
the foremost authority on what's known as moral fictionalism.

1285
01:04:54.070 --> 01:04:58.100
Now, Richard Joyce is a um Very well-known, very

1286
01:04:58.100 --> 01:05:01.300
accomplished, uh, philosopher, uh, and a friend of mine,

1287
01:05:01.399 --> 01:05:03.939
even though we disagree on the specifics. We are

1288
01:05:03.939 --> 01:05:07.330
both error theorists, OK? But he rejects moral abolitionism,

1289
01:05:07.580 --> 01:05:11.040
um, And what he argues for is a position

1290
01:05:11.040 --> 01:05:15.280
called moral fictionalism. Now, it's predicated on the idea

1291
01:05:15.280 --> 01:05:20.120
that morality generally plays um a beneficial role in

1292
01:05:20.120 --> 01:05:23.000
the lives of people and, and society. So, this

1293
01:05:23.000 --> 01:05:24.760
is obviously a point which he, he and I

1294
01:05:24.760 --> 01:05:28.215
disagree, OK? But what do you Basically argues is

1295
01:05:28.215 --> 01:05:31.774
that because fictionalism is beneficial, or I'm sorry, because

1296
01:05:31.774 --> 01:05:35.254
morality is beneficial, we ought to try to, he

1297
01:05:35.254 --> 01:05:38.014
doesn't like using the word pretend, but it, I

1298
01:05:38.014 --> 01:05:40.094
argue it more or less amounts to that. We

1299
01:05:40.094 --> 01:05:43.965
ought to pretend to, that, that morality is real.

1300
01:05:44.094 --> 01:05:48.679
Now, he doesn't think we should Directly fool ourselves

1301
01:05:48.679 --> 01:05:50.600
into thinking morality is real. That, that's a view

1302
01:05:50.600 --> 01:05:53.629
that's more associated with a view called moral conservationism,

1303
01:05:53.760 --> 01:05:56.000
which is that, yes, error theory is true, but

1304
01:05:56.000 --> 01:05:59.429
we can actually believe that moral moral truths hold

1305
01:05:59.429 --> 01:06:03.040
when we're not consciously aware of our metaphysical concerns

1306
01:06:03.040 --> 01:06:05.879
about morality, right? OF our arguments for error theory

1307
01:06:05.879 --> 01:06:09.649
and such. Um, Joyce's view is more along the

1308
01:06:09.649 --> 01:06:13.090
lines of, um, he, he, he conveys it to

1309
01:06:13.090 --> 01:06:15.290
getting lost in a good movie, right? If we're

1310
01:06:15.290 --> 01:06:17.169
in a movie theater and, and we're watching a

1311
01:06:17.169 --> 01:06:19.439
movie, and we're really caught up in the plot,

1312
01:06:19.570 --> 01:06:21.530
you know, we're identifying with the characters, we're really

1313
01:06:21.530 --> 01:06:23.729
sort of empathizing with what's going on, we're feeling

1314
01:06:23.729 --> 01:06:26.455
their fears and their excitement, and so. Forth. Um,

1315
01:06:26.614 --> 01:06:28.375
IF someone were to ask me, you know, nudge

1316
01:06:28.375 --> 01:06:29.854
me during the movie and the seat next to

1317
01:06:29.854 --> 01:06:30.895
me and be like, Hey, you know this is

1318
01:06:30.895 --> 01:06:32.774
fake, right? I'd be like, oh, yeah, I, I

1319
01:06:32.774 --> 01:06:34.534
know it's fake. But, but I'm still caught up

1320
01:06:34.534 --> 01:06:37.215
in it and still experiencing it. Um, AND, and

1321
01:06:37.215 --> 01:06:39.735
that's sort of the, the idea of moral fictionalism

1322
01:06:39.735 --> 01:06:42.574
for Joyce. So he thinks because morality plays an

1323
01:06:42.574 --> 01:06:45.610
important role in our lives, We should try to,

1324
01:06:45.729 --> 01:06:48.159
to take this view, even though we're aware, if,

1325
01:06:48.250 --> 01:06:50.800
if, um if, if asked about it, we, we'd

1326
01:06:50.800 --> 01:06:52.800
be aware and we are aware on some level

1327
01:06:53.010 --> 01:06:55.449
that morality is fake, we should still try to

1328
01:06:55.449 --> 01:06:57.699
be motivated by it, still allow it to play

1329
01:06:57.699 --> 01:06:59.729
the role it plays in society since it's mostly

1330
01:06:59.729 --> 01:07:04.429
beneficial. Now, I obviously reject this view, um, and,

1331
01:07:04.510 --> 01:07:06.469
and one of the reasons I do is that

1332
01:07:06.469 --> 01:07:08.389
I think that for Joyce, it's hard for me

1333
01:07:08.389 --> 01:07:11.750
to imagine that this sort of pretend morality or

1334
01:07:11.750 --> 01:07:14.750
what I'll call moral pretension, um, can really play

1335
01:07:14.750 --> 01:07:17.469
that strong of a motivational role in our behavior

1336
01:07:17.469 --> 01:07:20.100
if we acknowledge that it's fake, right? And one

1337
01:07:20.100 --> 01:07:21.659
way to think about it is this. I, I,

1338
01:07:21.760 --> 01:07:24.139
I think a lot of times, even true believers

1339
01:07:24.139 --> 01:07:27.219
are able to sort of push aside um moral

1340
01:07:27.219 --> 01:07:29.939
scruples if let's say they think their, their self-interest

1341
01:07:29.939 --> 01:07:32.260
to be better promoted by something. So, if I

1342
01:07:32.260 --> 01:07:35.830
find $1000 in an abandoned wallet with the identification

1343
01:07:35.830 --> 01:07:38.449
of the owner, I might think, well, I, I

1344
01:07:38.449 --> 01:07:39.770
know it's morally wrong to do it, but I

1345
01:07:39.770 --> 01:07:41.989
need the money. I'm taking that, right? Um, SO,

1346
01:07:42.050 --> 01:07:45.889
and, and people can rationalize their, their actions, even

1347
01:07:45.889 --> 01:07:48.290
if there's sort of moral concerns, right? My, my

1348
01:07:48.290 --> 01:07:51.209
point being is it, because it's so easy for

1349
01:07:51.209 --> 01:07:55.169
people to ignore, uh, moral consultations of themselves when

1350
01:07:55.169 --> 01:07:57.330
behaving, it's gonna be that much easier to do

1351
01:07:57.330 --> 01:08:00.353
it if someone acknowledges that. REALLY doesn't exist like

1352
01:08:00.353 --> 01:08:03.312
a moral fictionist. So, I just don't see morality

1353
01:08:03.312 --> 01:08:06.992
playing, uh, or this this moral pretense playing a

1354
01:08:06.992 --> 01:08:11.153
significant role in our behaviors, um, in as much

1355
01:08:11.153 --> 01:08:13.992
as Joyce does. Now, Joyce would argue, one of

1356
01:08:13.992 --> 01:08:15.693
the things he'd say is, look, we don't need

1357
01:08:15.693 --> 01:08:19.312
morality, uh, uh, moral fictionalism to be as strongly

1358
01:08:19.312 --> 01:08:22.095
more motivating as a genuine belief in morality, but

1359
01:08:22.095 --> 01:08:24.996
as long as it gives us some motivational nudge,

1360
01:08:25.295 --> 01:08:29.404
then it's worth having, OK? But the thing, the,

1361
01:08:29.475 --> 01:08:31.175
my main objection to him, and something I don't

1362
01:08:31.175 --> 01:08:33.335
think he really considers is that the concern is

1363
01:08:33.335 --> 01:08:37.845
that by speaking in of morality, in the um

1364
01:08:38.095 --> 01:08:39.975
general way that people do. And, and, and he

1365
01:08:39.975 --> 01:08:41.536
says, you know, we, we should more or less,

1366
01:08:41.725 --> 01:08:44.255
uh again, I, I'm, I'm leaving a lot of

1367
01:08:44.255 --> 01:08:46.576
stuff out here, obviously, but, but generally thinks we

1368
01:08:46.576 --> 01:08:48.734
should act and speak and deliberate in the way

1369
01:08:48.734 --> 01:08:51.569
that we normally do. The main problem I see

1370
01:08:51.569 --> 01:08:54.810
for that is that by doing so, you're reinforcing

1371
01:08:54.810 --> 01:08:58.129
folk morality, OK? With all the negative things that

1372
01:08:58.129 --> 01:09:00.569
go along with it, right? So if Richard Joyce

1373
01:09:00.569 --> 01:09:03.310
and moral fictionists go out there and say, um,

1374
01:09:03.330 --> 01:09:05.729
yeah, you know, it's, it's morally wrong to steal

1375
01:09:05.729 --> 01:09:07.649
or morally wrong to do this or that, I

1376
01:09:07.649 --> 01:09:10.145
think most people We're gonna interpret them as, as

1377
01:09:10.145 --> 01:09:13.225
speaking, understanding morality in the same way they and

1378
01:09:13.225 --> 01:09:15.825
their, their friends and their family did, uh, which

1379
01:09:15.825 --> 01:09:19.024
is, oh, yeah, OK, so this person's espousing retributivism.

1380
01:09:19.104 --> 01:09:21.584
They're saying this person acted morally wrong and needs

1381
01:09:21.584 --> 01:09:24.104
to be punished on retributivist grounds. And that for

1382
01:09:24.104 --> 01:09:27.578
the reasons I've mentioned is problematic. So, I think

1383
01:09:27.578 --> 01:09:30.099
part of the reasons that that Joyce really fails

1384
01:09:30.099 --> 01:09:32.148
to consider this, or, or has in a lot

1385
01:09:32.148 --> 01:09:34.499
of his writings, is he just assumes that morality

1386
01:09:34.499 --> 01:09:36.559
is overall good. But, but since I believe it's

1387
01:09:36.559 --> 01:09:39.578
not, there's a genuine concern that the moral fiction

1388
01:09:39.578 --> 01:09:42.019
is is going to reinforce the negative morality that

1389
01:09:42.019 --> 01:09:44.538
they think is we should do without. So, all

1390
01:09:44.538 --> 01:09:46.578
in all, I think better to just get rid

1391
01:09:46.578 --> 01:09:49.258
of the whole thing, given that morality is generally

1392
01:09:49.258 --> 01:09:51.849
bad, um, and, and, and go from there.

1393
01:09:52.720 --> 01:09:55.180
And what do you make of a forward looking

1394
01:09:55.180 --> 01:09:58.899
account of morality like utilitarianism? Do you think that

1395
01:09:58.899 --> 01:10:02.649
it could lead to better outcomes than moral abolitionism?

1396
01:10:03.899 --> 01:10:08.180
This is another one of those. Perspectives that I'm,

1397
01:10:08.259 --> 01:10:10.379
I'm, I'm very open to. I, I certainly think

1398
01:10:10.379 --> 01:10:13.250
if, if there were to be a, a global

1399
01:10:13.250 --> 01:10:16.350
uh change from uh, uh, uh, a more principle

1400
01:10:16.620 --> 01:10:20.859
based, non-consequential deontological perspective morality, which I think most

1401
01:10:20.859 --> 01:10:23.459
people have. I, I think utilitarian morality is, is

1402
01:10:23.459 --> 01:10:29.330
a very um Uh, uh, recent, uh, development, uh,

1403
01:10:29.379 --> 01:10:32.020
in terms of the, the, uh, influence it has

1404
01:10:32.020 --> 01:10:35.100
over people. I, I, I certainly think for the

1405
01:10:35.100 --> 01:10:37.700
highly educated in the West, it's a, a, a

1406
01:10:37.700 --> 01:10:41.220
primary, uh, way of thinking about morality. I think

1407
01:10:41.220 --> 01:10:42.580
there's a lot to say for it. I think

1408
01:10:42.580 --> 01:10:45.649
if everybody changed their view to utilitarianism, we'd probably

1409
01:10:45.649 --> 01:10:49.439
be better off, OK? Um, MOST of the academics

1410
01:10:49.493 --> 01:10:51.763
That I discussed in my book at length, whether

1411
01:10:51.763 --> 01:10:55.592
we're talking height, whether we're talking Peter Singer, um,

1412
01:10:55.803 --> 01:10:59.323
um, Steven Pinker, uh, a lot of these individuals,

1413
01:10:59.453 --> 01:11:04.522
they all advocate some sort of forward-looking utilitarianism, right?

1414
01:11:04.683 --> 01:11:07.283
So, the, their main aims, the kind of societies

1415
01:11:07.283 --> 01:11:10.083
they seem to, to favor, are these very, uh,

1416
01:11:10.092 --> 01:11:13.905
liberal societies founded on these utilitarian principle. Uh, NOT,

1417
01:11:13.945 --> 01:11:16.706
not, not, not exclusively, perhaps. Um, AND so I

1418
01:11:16.706 --> 01:11:18.746
think there's a lot to say for utilitarianism. Uh,

1419
01:11:18.905 --> 01:11:20.795
YOU know, Steven Pinker in his, in his excellent

1420
01:11:20.795 --> 01:11:23.465
book, The Better Angels of Our Nature, um, has

1421
01:11:23.465 --> 01:11:25.905
argued that this sort of shift in the West

1422
01:11:25.905 --> 01:11:30.175
towards a more utilitarian, um, outcome-based moral perspective, uh,

1423
01:11:30.186 --> 01:11:33.065
rather than a, a, a more dogmatic principle-based the

1424
01:11:33.065 --> 01:11:36.625
ontological perspective, has brought about a lot of, um,

1425
01:11:37.310 --> 01:11:40.709
Important positive changes in society, such as a decrease

1426
01:11:40.709 --> 01:11:44.500
in violence, OK. That said, I would still argue

1427
01:11:44.830 --> 01:11:47.939
that we'd be better off trying to push moral

1428
01:11:47.939 --> 01:11:51.299
abolitionism than trying to uh uh push, uh, uh,

1429
01:11:51.310 --> 01:11:54.035
a utilitarian moral perspective. One of the reasons is

1430
01:11:54.035 --> 01:11:57.185
this. I think that moral abolitionism simply is a

1431
01:11:57.185 --> 01:12:01.955
much philosophically stronger position than utilitarianism, right? So, the

1432
01:12:01.955 --> 01:12:03.725
arguments for error theory. If the arguments for error

1433
01:12:03.725 --> 01:12:06.314
theory that I've given are correct, OK, utilitarianism is

1434
01:12:06.314 --> 01:12:10.064
false, right? At least as a moral theory. Now,

1435
01:12:10.754 --> 01:12:12.515
part of the problem here, and, and we see

1436
01:12:12.515 --> 01:12:19.240
this is that, um, assuming uh that error theory

1437
01:12:19.240 --> 01:12:22.720
is, is correct, there's simply no objective criterion that

1438
01:12:22.720 --> 01:12:25.120
you could uh uh appeal to to try to

1439
01:12:25.120 --> 01:12:28.600
convince someone that let's say their, their Kian deontological

1440
01:12:28.600 --> 01:12:33.470
perspective is inferior to your utilitarian perspective. Right? Um,

1441
01:12:33.740 --> 01:12:35.740
AND, and the fact that we're still ethicists are

1442
01:12:35.740 --> 01:12:37.419
still arguing about this, by the way, is indicative

1443
01:12:37.419 --> 01:12:39.700
of the fact that there are no moral truths

1444
01:12:39.700 --> 01:12:41.419
about this, at least none that we really have

1445
01:12:41.419 --> 01:12:43.060
access to. We're having the same debates that we've

1446
01:12:43.060 --> 01:12:45.899
had for hundreds of years. No real progress is

1447
01:12:45.899 --> 01:12:49.569
being made here. And, and my, my explanation for

1448
01:12:49.569 --> 01:12:51.649
this is, there's no more progress to be made.

1449
01:12:51.779 --> 01:12:54.660
There, there are no answers about objective worldries. They

1450
01:12:54.660 --> 01:12:57.740
don't exist, right? So, um, yeah, if, if you

1451
01:12:57.740 --> 01:12:59.859
could, if there was an argument out there that

1452
01:12:59.859 --> 01:13:03.140
you could appeal to, to, to uh reliably convert

1453
01:13:03.140 --> 01:13:06.379
uh deontologists to utilitarians, I would say, hey, let's

1454
01:13:06.379 --> 01:13:07.540
give it a try, but I don't think it's

1455
01:13:07.540 --> 01:13:10.660
out there. Um, AND in fact, you see, again,

1456
01:13:10.819 --> 01:13:13.299
what one pattern that we see is a lot

1457
01:13:13.299 --> 01:13:16.560
of reversal. Um, SO, I, I don't want to

1458
01:13:16.560 --> 01:13:18.600
get too off, uh, uh, off track on, on

1459
01:13:18.600 --> 01:13:21.399
the present question, Ricardo, but, um, you know, one

1460
01:13:21.399 --> 01:13:24.379
of the arguments, uh, that, that people like Steven

1461
01:13:24.379 --> 01:13:26.790
Pinker have given, even individuals who are very aware

1462
01:13:26.790 --> 01:13:30.490
of morality's problems like Pinker, like height, um, they

1463
01:13:30.490 --> 01:13:33.160
seem to subscribe to, uh, the idea that the

1464
01:13:33.160 --> 01:13:36.240
sort of moral arc of the universe, uh, is

1465
01:13:36.240 --> 01:13:38.879
long and, and tends towards justice, right? Or, or

1466
01:13:38.879 --> 01:13:41.520
bends towards justice. Um, THIS idea that, yeah, you

1467
01:13:41.520 --> 01:13:44.339
know, society has its ups and downs, but there's

1468
01:13:44.339 --> 01:13:47.384
an, uh, uh, Constant upward trajectory. That has been

1469
01:13:47.384 --> 01:13:49.705
the case, I think. Um, AND, and I think

1470
01:13:49.705 --> 01:13:52.825
this transition to utilitarianism has a lot to do

1471
01:13:52.825 --> 01:13:56.694
with it, this transition away from, uh, religious dogma,

1472
01:13:56.944 --> 01:14:01.015
um, particularly among political leaders and, and, and, uh,

1473
01:14:01.024 --> 01:14:03.984
academic leaders and so forth. But we could very

1474
01:14:03.984 --> 01:14:06.254
well be seeing a a change in that trajectory.

1475
01:14:06.265 --> 01:14:09.745
And I think the Trump presidency, um, uh, sort

1476
01:14:09.745 --> 01:14:13.015
of indicates that, right? Where the, these, um, you

1477
01:14:13.015 --> 01:14:18.819
know, a heavily sort of deontological moral perspective based

1478
01:14:18.819 --> 01:14:23.899
on, um, let's say religion, for instance, this idea

1479
01:14:23.899 --> 01:14:27.100
of, um, you know, retrievatist rightness and wrongness, you

1480
01:14:27.100 --> 01:14:28.979
know, giving people their just desserts, this sort of

1481
01:14:28.979 --> 01:14:31.339
thing. Uh, IT, it seems to be trending upward,

1482
01:14:31.459 --> 01:14:33.810
OK. So, um, I, I don't think we can

1483
01:14:33.810 --> 01:14:35.939
really rely on the trend we've seen in the

1484
01:14:35.939 --> 01:14:38.734
past since Say the, the early to mid-nineteenth century

1485
01:14:38.734 --> 01:14:42.294
to today where there's less violence, more economic prosperity,

1486
01:14:42.415 --> 01:14:45.814
more social justice, we are now seeing um moral

1487
01:14:45.814 --> 01:14:48.254
forces working against this and turning these trends in

1488
01:14:48.254 --> 01:14:51.055
the opposite direction. Uh, GOING back to Joyce for

1489
01:14:51.055 --> 01:14:54.444
a uh for a moment, um, so yeah, so,

1490
01:14:54.455 --> 01:14:58.029
um. Oh, I'm sorry, for looking at accounts of

1491
01:14:58.029 --> 01:15:01.439
morality. Yeah. So one argument against uh utilitarianism is

1492
01:15:01.439 --> 01:15:04.040
that there's just no objective criterion by which we

1493
01:15:04.040 --> 01:15:06.919
could establish that it's superior to any other ethical

1494
01:15:06.919 --> 01:15:09.000
position, because in my view, according to the error

1495
01:15:09.000 --> 01:15:12.390
theory, they're all false. Another problem with utilitarianism is,

1496
01:15:12.680 --> 01:15:16.259
although it's generally beneficial, it can still have some

1497
01:15:16.259 --> 01:15:20.830
very um negative results, OK? So, um, even if

1498
01:15:20.830 --> 01:15:24.149
people subscribe to a uh a purely utilitarian view,

1499
01:15:24.359 --> 01:15:28.339
um, It could lead to a very sort of

1500
01:15:28.339 --> 01:15:31.140
coldness and result in some sort of horrific events.

1501
01:15:31.220 --> 01:15:33.939
So, for instance, um, the bombs that were dropped

1502
01:15:33.939 --> 01:15:36.220
on Japan, uh, at the end of World War

1503
01:15:36.220 --> 01:15:38.859
Two, were basically done on the basis of a

1504
01:15:38.859 --> 01:15:41.299
cost-benefit analysis. At least this is what a lot

1505
01:15:41.299 --> 01:15:45.609
of, um, Historians believe that, that Truman's decision was

1506
01:15:45.609 --> 01:15:48.209
based on, OK, we could end the war now,

1507
01:15:48.379 --> 01:15:51.040
save more American lives by dropping the bombs, perhaps,

1508
01:15:51.220 --> 01:15:52.970
maybe the war would end anyway, and we would,

1509
01:15:53.049 --> 01:15:55.649
we would save some Japanese lives, but American lives

1510
01:15:55.649 --> 01:15:57.290
are more important, so let's just drop the bombs,

1511
01:15:57.370 --> 01:16:00.479
right? Um, THE extermination of the Jews was done,

1512
01:16:00.609 --> 01:16:03.169
uh, partly on the basis of a, a utilitarian

1513
01:16:03.169 --> 01:16:07.569
calculation, right? So, without the sort of concern for

1514
01:16:07.569 --> 01:16:10.640
other individuals, Right? Without the, the ability to sort

1515
01:16:10.640 --> 01:16:14.250
of weigh people's lives uh uh uh as more

1516
01:16:14.250 --> 01:16:17.569
or less being equal, utilitarianism can, can give rise

1517
01:16:17.569 --> 01:16:20.640
to some gruesome, uh, outcomes, right? So, I, I

1518
01:16:20.640 --> 01:16:22.410
don't, tho those are some of the reasons why

1519
01:16:22.410 --> 01:16:26.970
I think moral abolitionism, um, informed by empathy, informed

1520
01:16:26.970 --> 01:16:30.959
by prudential self-interest, would be better than trying to

1521
01:16:31.129 --> 01:16:33.490
uh push this more utilitarian, uh, line.

1522
01:16:34.700 --> 01:16:38.680
I think you've already alluded partially to this question

1523
01:16:38.680 --> 01:16:42.919
when earlier you mentioned uh how we can compare

1524
01:16:42.919 --> 01:16:46.270
societies in terms of how much morality they have.

1525
01:16:46.279 --> 01:16:50.200
And you mentioned, for example, the Scandinavian countries, Japan,

1526
01:16:50.359 --> 01:16:54.080
New Zealand, and so on. Do human societies need

1527
01:16:54.080 --> 01:16:56.390
morality to function well?

1528
01:16:57.399 --> 01:17:00.640
Uh, I, I don't. I, I think that morality's

1529
01:17:00.640 --> 01:17:02.879
primary use, and the reason why it's so ingrained

1530
01:17:02.879 --> 01:17:04.600
in us and why so many people just think

1531
01:17:04.600 --> 01:17:08.229
of it as being obviously correct, um, is that

1532
01:17:08.229 --> 01:17:12.109
it was so very important for our, our evolutionary

1533
01:17:12.109 --> 01:17:14.680
ancestors, particularly, I think, in, in the small groups,

1534
01:17:14.839 --> 01:17:18.419
um, Perhaps the onset of early and agrarian societies.

1535
01:17:18.549 --> 01:17:21.109
Um, BUT it's no longer necessary. You know, I,

1536
01:17:21.149 --> 01:17:22.990
I, I think if we had a society of

1537
01:17:22.990 --> 01:17:26.310
moral abolitionists, you know, that were rational, empathetic, had

1538
01:17:26.310 --> 01:17:29.120
these other non-moral resources, uh, we would prosper a

1539
01:17:29.120 --> 01:17:31.220
lot better. And I think, again, we see that,

1540
01:17:31.470 --> 01:17:35.540
um, there, there is at least Suggestive evidence. I,

1541
01:17:35.560 --> 01:17:37.569
I keep referring back to the stankoff and Lee

1542
01:17:37.569 --> 01:17:40.229
article because I think it's very important, um, where

1543
01:17:40.229 --> 01:17:42.839
the, the countries that they identify as being most

1544
01:17:42.839 --> 01:17:46.359
moral, most religious, uh, tend to be the less

1545
01:17:46.359 --> 01:17:48.720
the least effective in terms of outcomes, in terms

1546
01:17:48.720 --> 01:17:51.680
of happiness, in terms of economic prosperity, uh, in

1547
01:17:51.680 --> 01:17:55.359
terms of violence, right? Um, AND this is a

1548
01:17:55.359 --> 01:17:57.160
major thing, and I, I, I may talk about

1549
01:17:57.160 --> 01:17:59.270
this in, in, in a little more depth shortly,

1550
01:17:59.439 --> 01:18:06.910
but Most people, certainly most academics, and uh I,

1551
01:18:06.990 --> 01:18:09.069
I think David Lewis called this sort, not referring

1552
01:18:09.069 --> 01:18:11.430
to the subject, but the argument from the incredulous

1553
01:18:11.430 --> 01:18:13.620
stare, right? Where if, if an abolitionist come out,

1554
01:18:13.709 --> 01:18:15.430
comes out and says, oh, we're better off without

1555
01:18:15.430 --> 01:18:17.540
morality, people just kind of, what are you talking

1556
01:18:17.540 --> 01:18:20.799
about, right? IT'S so foreign and preposterous to a

1557
01:18:20.799 --> 01:18:23.310
lot of people. And I think that's because people,

1558
01:18:23.680 --> 01:18:26.839
academics, you know, and, and, and Jonathan Haidt had

1559
01:18:26.839 --> 01:18:30.080
a uh a moniker or an acronym for this

1560
01:18:30.080 --> 01:18:33.240
kind of moral perspective that academics share called weird

1561
01:18:33.240 --> 01:18:38.629
morality. Um, WESTERN educated, industrialized, rich and democratic. Uh,

1562
01:18:38.720 --> 01:18:41.990
AND it's weird, um, primarily because it is not

1563
01:18:41.990 --> 01:18:45.629
the norm, whether we're talking historically or, uh, uh,

1564
01:18:45.640 --> 01:18:48.720
or whether we're talking most people's moral views in

1565
01:18:48.720 --> 01:18:50.959
the world today. Um, WE tend to think of

1566
01:18:50.959 --> 01:18:53.560
morality as, oh, and I, when I say us,

1567
01:18:53.640 --> 01:18:57.629
I mean us weird, highly educated Western people, right?

1568
01:18:57.799 --> 01:18:59.729
We think of morality in terms of, oh, it's

1569
01:18:59.729 --> 01:19:03.160
just, you know, showing care for people, not doing

1570
01:19:03.160 --> 01:19:05.529
harm. Maybe giving people the freedom to do what

1571
01:19:05.529 --> 01:19:08.459
they want, um, and acting fairly. How can that

1572
01:19:08.459 --> 01:19:11.220
possibly be bad? Well, generally, it's not, but this

1573
01:19:11.220 --> 01:19:14.459
isn't what most people's concept of morality is. Most

1574
01:19:14.459 --> 01:19:17.250
people's concept of morality has things like, uh uh

1575
01:19:17.259 --> 01:19:21.129
uh involves things like, um, submission to authority, um,

1576
01:19:21.339 --> 01:19:25.339
uh, in-group tribal loyalty or loyalty, um, and also

1577
01:19:25.339 --> 01:19:28.549
sanctity, purity, which I've mentioned. And these, these, uh,

1578
01:19:28.569 --> 01:19:31.100
three foundations, and, and Jonathan Haidt was the one

1579
01:19:31.100 --> 01:19:34.720
who came up with this foundational view, um. And,

1580
01:19:34.729 --> 01:19:36.629
and this itself was based on another view, uh,

1581
01:19:36.689 --> 01:19:38.890
a similar view of, of morality, uh, by, by

1582
01:19:38.890 --> 01:19:41.770
Alan Fisk, I believe. Um, BUT, but the idea

1583
01:19:41.770 --> 01:19:43.850
is that there's a lot more to morality than

1584
01:19:43.850 --> 01:19:45.689
being nice and not harming and letting people do

1585
01:19:45.689 --> 01:19:48.120
whatever they want. OK. There, there's the, you know,

1586
01:19:48.250 --> 01:19:52.129
uh, enforcing the pecking order, enforcing group loyalty, um,

1587
01:19:52.250 --> 01:19:56.140
ostracizing or oppressing individuals who don't meet cultural. And

1588
01:19:56.140 --> 01:19:58.729
as a purity, and these forces lead to violence,

1589
01:19:58.890 --> 01:20:01.890
OK? So, um, I, I certainly don't think we

1590
01:20:01.890 --> 01:20:04.729
need morality to have a successful societies. I think

1591
01:20:04.729 --> 01:20:07.009
if we got, did away with it, we do

1592
01:20:07.009 --> 01:20:09.319
away with a lot of the problems that, uh,

1593
01:20:09.330 --> 01:20:12.209
sort of haunt contemporary society, and we also have

1594
01:20:12.209 --> 01:20:15.109
other non-moral resources that can allow us um to,

1595
01:20:15.209 --> 01:20:17.410
to persevere and, and, um, be successful.

1596
01:20:18.640 --> 01:20:22.919
So earlier I've asked you particularly about the, about

1597
01:20:22.919 --> 01:20:26.709
preventing violence, but more broadly, what do you make

1598
01:20:26.709 --> 01:20:31.240
of the contributions of morality to the betterment of

1599
01:20:31.240 --> 01:20:35.680
society across history? Earlier you mentioned, for example, civil

1600
01:20:35.680 --> 01:20:38.879
rights movements and stuff like that. So what do

1601
01:20:38.879 --> 01:20:39.720
you make of it?

1602
01:20:40.200 --> 01:20:43.540
Yeah, I, I think certainly social justice movements are

1603
01:20:43.540 --> 01:20:46.339
very important. Again, moving away from the ancient history,

1604
01:20:46.419 --> 01:20:49.009
where it basically just allowed our groups to, uh,

1605
01:20:49.020 --> 01:20:52.020
have less infighting and work together and be more

1606
01:20:52.020 --> 01:20:56.220
effective at, uh, overcoming challenges, um, both within and

1607
01:20:56.220 --> 01:20:58.379
outside of the group, whether we're talking, um, you

1608
01:20:58.379 --> 01:21:01.580
know, methods of hunting, uh, methods of, of fighting

1609
01:21:01.580 --> 01:21:04.490
other hostile tribes and so forth. But moving towards,

1610
01:21:04.509 --> 01:21:07.459
uh, more recent centuries, I think, yeah, morality has,

1611
01:21:07.580 --> 01:21:11.250
has played an instrumental role, um, Well, I, I,

1612
01:21:11.330 --> 01:21:13.029
I shouldn't say internal, cause I think without it,

1613
01:21:13.049 --> 01:21:15.089
we still, we would have achieved a lot of

1614
01:21:15.089 --> 01:21:19.330
these accomplishments. But doing away with, with uh slavery,

1615
01:21:19.609 --> 01:21:22.080
right? Uh, ALLOWING giving women the right to vote,

1616
01:21:22.200 --> 01:21:25.959
uh, extending civil rights, these kinds of things, um,

1617
01:21:26.729 --> 01:21:31.370
more concern for handicapped individuals, um, uh, more, more

1618
01:21:31.370 --> 01:21:33.976
concerned with the the welfare of outgroups, whether they're

1619
01:21:33.976 --> 01:21:38.976
ethnic or religious, or, um, uh, sexual, uh, gender,

1620
01:21:39.025 --> 01:21:41.335
what have you. I, I, I think certainly morality

1621
01:21:41.335 --> 01:21:43.456
has played an important role here, and I think

1622
01:21:43.456 --> 01:21:48.485
it has tended, um, to be generally beneficial. But

1623
01:21:48.936 --> 01:21:51.456
again, uh, that's really hard to say though, cause

1624
01:21:51.456 --> 01:21:53.295
again, I, I, I, I don't even know, know

1625
01:21:53.295 --> 01:21:56.472
if I I'd say generally beneficial because without morality,

1626
01:21:56.662 --> 01:21:59.771
you wouldn't have had these, this widespread, let's say,

1627
01:21:59.981 --> 01:22:03.622
uh, animosity towards the LGBTQ plus community in the

1628
01:22:03.622 --> 01:22:07.312
past or ethnic minorities, or, um, you know, the

1629
01:22:07.321 --> 01:22:09.142
the desire to keep women in their place by

1630
01:22:09.142 --> 01:22:11.282
not allowing them to vote or get positions of,

1631
01:22:11.301 --> 01:22:14.861
of, of, uh, of, of power, right? So, um,

1632
01:22:16.200 --> 01:22:19.169
Yeah, so I, I, I think really, at best,

1633
01:22:19.250 --> 01:22:21.729
I, I think we can say morality's been a

1634
01:22:21.729 --> 01:22:25.609
wash in terms of um promoting social justice, right?

1635
01:22:25.810 --> 01:22:28.770
Because again, I, I think it's overdetermined in terms

1636
01:22:28.770 --> 01:22:31.330
of social justice. Cause without morality, you would have

1637
01:22:31.330 --> 01:22:34.569
still had the groups themselves wanting to not be

1638
01:22:34.569 --> 01:22:37.509
oppressed, right? You would still have empathy. Pathetic people

1639
01:22:37.509 --> 01:22:40.149
thinking, you know, why are we treating these people

1640
01:22:40.149 --> 01:22:41.879
so badly when they act in ways that doesn't

1641
01:22:41.879 --> 01:22:44.169
harm us? And they're even maybe beneficial to society.

1642
01:22:44.259 --> 01:22:46.669
They're, you know, when it comes to women's suffrage

1643
01:22:46.669 --> 01:22:48.589
or leadership, you know, I, I, a lot of

1644
01:22:48.589 --> 01:22:51.109
women, I, I'm speaking as an individual, random individual

1645
01:22:51.109 --> 01:22:53.660
for like 100 years ago, let's say, or more.

1646
01:22:53.790 --> 01:22:55.029
Um, I know a lot of women who are

1647
01:22:55.029 --> 01:22:57.109
smarter than men and, and better people, you know,

1648
01:22:57.229 --> 01:22:58.939
why? Why should we oppress them? It doesn't really

1649
01:22:58.939 --> 01:23:01.850
make sense. Um, YOU know, we, we've become more

1650
01:23:01.850 --> 01:23:04.160
reasonable and more rational. So a lot of the,

1651
01:23:04.290 --> 01:23:07.649
the stereotypes that we had about women and, and,

1652
01:23:07.669 --> 01:23:09.649
you know, ethnic groups and so forth in the

1653
01:23:09.649 --> 01:23:13.450
past, um, thanks to, to being, becoming better educated

1654
01:23:13.450 --> 01:23:15.819
and, and, uh, with, with the development of science,

1655
01:23:15.950 --> 01:23:20.169
science, rather, we, we know that. THOSE old stereotypes

1656
01:23:20.169 --> 01:23:24.330
were false, right? So, um, yeah, morality's had a

1657
01:23:24.330 --> 01:23:28.240
benefit, uh, but, uh, it, it, it, it, it's

1658
01:23:28.490 --> 01:23:32.490
sort of motivated social oppression while it's fought against

1659
01:23:32.490 --> 01:23:35.620
it. Um, AND I, I don't really think it,

1660
01:23:35.629 --> 01:23:38.569
it was necessary ultimately, to try to overcome these,

1661
01:23:38.729 --> 01:23:41.649
these, uh, negative aspects of society. And again, I

1662
01:23:41.649 --> 01:23:45.149
think, particularly nowadays, morality seems to be doing, playing

1663
01:23:45.149 --> 01:23:50.020
much larger of a role in fostering, um, or

1664
01:23:50.020 --> 01:23:53.049
a social injustice than actually trying to promote it.

1665
01:23:53.149 --> 01:23:54.950
Again, and, and the United States and in other

1666
01:23:54.950 --> 01:23:58.870
countries, we're finding this as well, anti-immigrant sentiment, um,

1667
01:23:59.439 --> 01:24:02.839
Uh, scaling back women's reproductive rights, all this sort

1668
01:24:02.839 --> 01:24:05.189
of thing, uh, doing away with the, uh, uh,

1669
01:24:05.200 --> 01:24:07.959
the DEI initiatives in the United States. Uh, AGAIN,

1670
01:24:08.120 --> 01:24:10.759
and, and this is all based on, or at

1671
01:24:10.759 --> 01:24:15.589
least given a moral justification, you know, whether Donald

1672
01:24:15.589 --> 01:24:19.339
Trump has any real moral views, I don't know,

1673
01:24:19.470 --> 01:24:22.470
um, but, but certainly, uh, uh, his, his supporters

1674
01:24:22.470 --> 01:24:24.299
do. I would argue most of his supporters do.

1675
01:24:24.350 --> 01:24:26.430
And I would argue most of his supporters, for

1676
01:24:26.430 --> 01:24:29.709
them, morality is playing much uh uh having a

1677
01:24:29.709 --> 01:24:31.430
much stronger role in their lives in terms of

1678
01:24:31.430 --> 01:24:33.870
like objective moral truths, playing much of a stronger

1679
01:24:33.870 --> 01:24:35.470
role in their lives than it is on, on

1680
01:24:35.470 --> 01:24:36.470
the more liberal side.

1681
01:24:37.580 --> 01:24:40.790
So you already at least mentioned them earlier, but

1682
01:24:40.790 --> 01:24:45.140
tell us more about the three necessary resources you

1683
01:24:45.140 --> 01:24:49.120
argue we have at our disposal to benefit humanity

1684
01:24:49.350 --> 01:24:54.979
in the world without morality, namely empathy, prudential, self-interest,

1685
01:24:55.109 --> 01:24:55.870
and reason.

1686
01:24:56.950 --> 01:24:59.419
Yeah. So, uh, a big part of my book

1687
01:24:59.419 --> 01:25:02.350
is not just to argue that, you know, morality

1688
01:25:02.350 --> 01:25:05.259
doesn't exist, uh, or that morality is bad, but,

1689
01:25:05.470 --> 01:25:09.200
OK, well, it's when it comes to error theory,

1690
01:25:09.270 --> 01:25:11.350
this is known as the, the what now problem,

1691
01:25:11.549 --> 01:25:13.419
right? OK. So, what do we do now? Do,

1692
01:25:13.549 --> 01:25:15.080
if error. THEORY is true, if there are no

1693
01:25:15.080 --> 01:25:18.009
moral truths, do we become abolitionists? Do we become

1694
01:25:18.160 --> 01:25:21.000
fictionalists? Do we just try to, you know, some

1695
01:25:21.000 --> 01:25:23.720
other position? Do, are we propagandists? Do we, do

1696
01:25:23.720 --> 01:25:26.770
we hide the, the, uh, um, claim that morality

1697
01:25:26.770 --> 01:25:28.640
doesn't exist for fear that it might result in

1698
01:25:28.640 --> 01:25:32.279
a Lord of the Flies type society? Um, And,

1699
01:25:32.290 --> 01:25:35.689
and to, to motivate the abolitionist position, effectively, I

1700
01:25:35.689 --> 01:25:38.290
think, you need to point out that, um, yeah,

1701
01:25:38.370 --> 01:25:40.919
morality is bad, but certainly, it's got its good

1702
01:25:40.919 --> 01:25:43.169
points. Now, um, so if we get rid of

1703
01:25:43.169 --> 01:25:46.209
morality, well, do we necessarily have to do away

1704
01:25:46.209 --> 01:25:48.129
with the benefits it offers? And I argue that

1705
01:25:48.129 --> 01:25:51.330
no, we, we have uh non-moral resources, which I've

1706
01:25:51.330 --> 01:25:53.609
already talked about to some extent. We have uh

1707
01:25:53.609 --> 01:25:58.529
empathy, self-interest, prudential self-interest, and reason. And I think

1708
01:25:58.529 --> 01:26:03.540
that those three elements, Excuse me, are sufficient to

1709
01:26:03.540 --> 01:26:05.620
provide us with all the main benefits that morality

1710
01:26:05.620 --> 01:26:09.779
is given. Empathy is especially important. And um empathy

1711
01:26:09.779 --> 01:26:12.740
basically just the ability, people understand it in different

1712
01:26:12.740 --> 01:26:14.620
ways, but, uh, I'm gonna use it in the

1713
01:26:14.620 --> 01:26:16.770
sense of it's the ability to understand what somebody

1714
01:26:16.770 --> 01:26:20.109
is experiencing and the ability uh to have concern

1715
01:26:20.109 --> 01:26:22.620
for, for that individual. HAVE concern for, for their

1716
01:26:22.620 --> 01:26:26.959
well-being, let's say. Um, AND, um, research has shown

1717
01:26:27.810 --> 01:26:31.629
Pretty much conclusively that empathy is, is highly important

1718
01:26:31.629 --> 01:26:36.430
in motivating pro-social behavior, downplaying anti-social behavior, and it's

1719
01:26:36.430 --> 01:26:40.549
much uh more of a reliable catalyst for pro-social

1720
01:26:40.549 --> 01:26:43.339
behavior and doing away with negative anti-social behavior than

1721
01:26:43.339 --> 01:26:46.790
moralities, right? So, certainly, some aspects of morality, you

1722
01:26:46.790 --> 01:26:51.180
know, the, uh, Jonathan Haidt's, uh, care, harm Foundation,

1723
01:26:51.350 --> 01:26:56.589
uh, fairness Foundation, perhaps. Uh, YEAH, those foundations. Attaches

1724
01:26:56.589 --> 01:26:59.990
to the welfare of others, but other moral foundations

1725
01:26:59.990 --> 01:27:04.549
work against that, uh, authority, loyalty, oftentimes, uh, purity

1726
01:27:04.549 --> 01:27:08.109
and so forth, uh, sanctity. Um, WHEREAS empathy is

1727
01:27:08.109 --> 01:27:10.509
pretty much, uh, a guarantee that if you feel

1728
01:27:10.509 --> 01:27:14.229
empathy for somebody, particularly empathic concern, you are gonna

1729
01:27:14.229 --> 01:27:17.029
want to do what you perceive anyway, to be

1730
01:27:17.029 --> 01:27:20.819
in that individual's, uh, um, best interests, right? So,

1731
01:27:20.950 --> 01:27:24.399
uh, it, it, it's a very important, um, Fat

1732
01:27:24.399 --> 01:27:28.319
to have. Now, self-interest, obviously, self-interest can be a

1733
01:27:28.319 --> 01:27:32.000
negative thing. Certainly, uh, a lot of um anti-social

1734
01:27:32.000 --> 01:27:35.160
behavior is motivated by self-interest. Uh, THIS is why,

1735
01:27:35.399 --> 01:27:37.419
you know, self-interest is the reason why the philosopher

1736
01:27:37.419 --> 01:27:40.040
David Hume thought that we needed morality to sort

1737
01:27:40.040 --> 01:27:44.319
of overcome the our, our more self-interested uh motives

1738
01:27:44.319 --> 01:27:47.580
that lead to um conflict and so forth. But

1739
01:27:48.009 --> 01:27:50.609
If we have a better understanding of what's in

1740
01:27:50.609 --> 01:27:52.729
our self-interest, I think it will see that it

1741
01:27:52.729 --> 01:27:56.930
is actually um more uh conducive to pro-social behavior

1742
01:27:56.930 --> 01:28:00.450
than anti-social behavior. Why, why think that? Um, STUDY

1743
01:28:00.450 --> 01:28:04.490
after study has shown that, um, the happiest people

1744
01:28:04.490 --> 01:28:07.924
tend to be the most altruistic. OK. That volunteering,

1745
01:28:07.964 --> 01:28:11.004
for instance, is a huge um contributor to a

1746
01:28:11.004 --> 01:28:14.044
person's happiness. They've done studies where they've given people

1747
01:28:14.044 --> 01:28:16.564
some money and they, they are either associ uh,

1748
01:28:16.604 --> 01:28:18.564
they, they're assigned to either one group that spends

1749
01:28:18.564 --> 01:28:20.205
it on others, or a group that spends it

1750
01:28:20.205 --> 01:28:22.325
on themselves. The group that spends it on themselves

1751
01:28:22.325 --> 01:28:27.390
are consistently happier, right? Um, And so, even though

1752
01:28:27.390 --> 01:28:29.549
our, our sort of primitive minds might tell us,

1753
01:28:29.839 --> 01:28:32.399
no, you know, we, we need to achieve power

1754
01:28:32.399 --> 01:28:35.000
and material wealth in the world, um, and then

1755
01:28:35.000 --> 01:28:37.200
you end up being like Elon Musk, perhaps, if

1756
01:28:37.200 --> 01:28:40.000
you're fortunate. Uh, IT actually turns out that people

1757
01:28:40.000 --> 01:28:42.529
are that are greedy and materialistic are less happy,

1758
01:28:42.640 --> 01:28:45.439
right? So, with a, with a more enlightened view

1759
01:28:45.439 --> 01:28:48.515
of self-interest, I think rather than leading Under normal

1760
01:28:48.515 --> 01:28:51.354
circumstances where, where we have enough resources to get

1761
01:28:51.354 --> 01:28:53.475
by on and so forth, in a, in a

1762
01:28:53.475 --> 01:28:56.734
society of moderate, uh, uh, wealth, um, I think,

1763
01:28:56.745 --> 01:29:01.035
uh, enlightened self-interest, uh, would actually, uh, inform us

1764
01:29:01.035 --> 01:29:04.595
that our interests, our, our happiness is in line

1765
01:29:04.595 --> 01:29:07.435
with other people rather than being opposed to other

1766
01:29:07.435 --> 01:29:11.939
people by, let's say, Best being uh uh attained

1767
01:29:11.939 --> 01:29:14.459
by, by fighting them for goods or resources and

1768
01:29:14.459 --> 01:29:17.580
so forth. And then finally, um, oh, and, and

1769
01:29:17.580 --> 01:29:19.620
by the way, you get sort of pos, what

1770
01:29:19.620 --> 01:29:22.259
with the psychological research shows here with regard to

1771
01:29:22.259 --> 01:29:24.779
self-interest and happiness is you get this positive sort

1772
01:29:24.779 --> 01:29:27.620
of feedback loop. So, uh, generally speaking, if you

1773
01:29:27.620 --> 01:29:29.859
act nice towards others, you'll feel good, and then

1774
01:29:29.859 --> 01:29:32.064
if you feel good or happy, you take to

1775
01:29:32.064 --> 01:29:34.895
be more pro-social, right? So, individuals who tend to

1776
01:29:34.895 --> 01:29:37.865
be jerks are less happy. Um, AND, and it

1777
01:29:37.865 --> 01:29:41.125
makes sense from a uh uh evolutionary, uh, standpoint,

1778
01:29:41.334 --> 01:29:43.845
um, which is kind of an account I give

1779
01:29:43.845 --> 01:29:46.174
of how altruism evolved. I, I, I won't talk

1780
01:29:46.174 --> 01:29:48.975
about that right now, but, but basically, self-interest is

1781
01:29:48.975 --> 01:29:52.254
very happy, uh, very happy, uh, uh, very important

1782
01:29:52.254 --> 01:29:55.379
in terms of understanding. How we can have a

1783
01:29:55.379 --> 01:29:58.220
more pro-social society. So, if we learn, if, if

1784
01:29:58.220 --> 01:30:02.700
we understand the research indicating that pro-social behavior contributes

1785
01:30:02.700 --> 01:30:05.009
to happiness and that happiness in, in, in turn

1786
01:30:05.009 --> 01:30:08.270
contributes to pro-social behavior, um, that's the way that

1787
01:30:08.270 --> 01:30:12.020
a society can really prosper. Uh, FINALLY, reason, um,

1788
01:30:12.265 --> 01:30:14.384
Reason, uh, uh, again, and I'm appealing to the

1789
01:30:14.384 --> 01:30:19.345
empirical work here. So, research indicates that intelligence is

1790
01:30:19.345 --> 01:30:24.495
highly correlated to less violence, uh, less dogmatism, um,

1791
01:30:24.665 --> 01:30:27.725
and actually, uh, increased empathy, uh, for, because the

1792
01:30:27.725 --> 01:30:31.665
ability to, to, to, uh, do perspective taking and

1793
01:30:31.665 --> 01:30:36.450
understand uh how others are feeling. OK. Uh, AND

1794
01:30:36.450 --> 01:30:40.209
so, the ideas, and also intelligence, sure, it's innate,

1795
01:30:40.410 --> 01:30:42.490
but, but it can be improved by things like

1796
01:30:42.490 --> 01:30:45.009
education, right? So in a society that, that is

1797
01:30:45.009 --> 01:30:48.220
more geared towards critical thinking and, and boosting intelligence

1798
01:30:48.220 --> 01:30:51.009
through things like education, you would have again, positive

1799
01:30:51.009 --> 01:30:56.709
outcomes, um, And also, with intelligence, uh, and reason,

1800
01:30:56.930 --> 01:30:59.459
you're able to understand how best to achieve the

1801
01:30:59.459 --> 01:31:01.660
happiness that we want in our lives, OK? And

1802
01:31:01.660 --> 01:31:04.410
then finally, it's, I'll say that, uh, going briefly

1803
01:31:04.410 --> 01:31:08.290
back to, to politics, um, there are both empathy

1804
01:31:08.290 --> 01:31:13.490
or each of empathy, self-interest, and reason are attached

1805
01:31:13.490 --> 01:31:16.689
to beneficial public policies. So, for instance, liberals tend

1806
01:31:16.689 --> 01:31:19.250
to be more empathetic. I mentioned how liberal policies

1807
01:31:19.250 --> 01:31:23.200
generally Produce better outcomes than conservative policies, um, self-interest,

1808
01:31:23.290 --> 01:31:25.810
I mentioned that if we voted basically on self-interest

1809
01:31:25.810 --> 01:31:30.209
rather than these deontological moral, um, perspectives, we probably

1810
01:31:30.209 --> 01:31:33.839
have better public policies, whether we're talking gun control,

1811
01:31:34.089 --> 01:31:37.310
um, a progressive taxation, what have you, and then

1812
01:31:37.310 --> 01:31:40.850
also a reason, obviously, if we, uh, if reason

1813
01:31:40.850 --> 01:31:42.609
plays a greater role in our lives, we'll probably

1814
01:31:42.609 --> 01:31:46.100
have more effective, uh, political, um, policies.

1815
01:31:47.629 --> 01:31:49.709
Why do you think that even most of the

1816
01:31:49.709 --> 01:31:55.160
scientists themselves, including the moral psychologists, share the opinion

1817
01:31:55.160 --> 01:31:59.540
that morality is an indispensable ally in the quest

1818
01:31:59.540 --> 01:32:00.899
for a better future?

1819
01:32:01.770 --> 01:32:03.649
Yeah, so I, I alluded to this a little

1820
01:32:03.649 --> 01:32:05.390
bit ago, but I'll, I'll expand more on it,

1821
01:32:05.490 --> 01:32:08.490
right? I think it primarily stems from what I

1822
01:32:08.490 --> 01:32:11.490
was talking about, about how philosophers and scientists and

1823
01:32:11.490 --> 01:32:16.569
academics generally, um come from communities and are in

1824
01:32:16.569 --> 01:32:20.890
embedded in communities where morality takes a very peculiar

1825
01:32:20.890 --> 01:32:22.890
form, the, the weird form that I talked about,

1826
01:32:22.970 --> 01:32:25.975
right? So, um, We tend to think of morality

1827
01:32:25.975 --> 01:32:29.245
in terms of doing no harm, um, being kind,

1828
01:32:29.694 --> 01:32:32.535
uh, allowing people to express themselves freely, as long

1829
01:32:32.535 --> 01:32:35.245
as they're not harming others, um, and being fair.

1830
01:32:35.375 --> 01:32:37.174
OK. And, and when you conceive of morality in

1831
01:32:37.174 --> 01:32:40.254
those terms, yeah, intuitively, it's bizarre to think, wait,

1832
01:32:40.345 --> 01:32:42.495
we'd be better without this? How? Uh, YOU know,

1833
01:32:42.615 --> 01:32:44.654
how would I be better? Uh, HOW would we

1834
01:32:44.654 --> 01:32:49.120
as a society be better without morality? MOTIVATING us

1835
01:32:49.120 --> 01:32:52.350
to be nice or not harming other individuals. If

1836
01:32:52.350 --> 01:32:54.560
this was the way, if everybody in the world

1837
01:32:54.560 --> 01:32:57.160
shared this weird moral perspective, I would say, yeah,

1838
01:32:57.439 --> 01:32:59.799
it, it probably is better for us. But as

1839
01:32:59.799 --> 01:33:02.270
I mentioned, this, and as Haight points out, and,

1840
01:33:02.279 --> 01:33:05.680
and other anthropologists have shown, this is not the

1841
01:33:05.680 --> 01:33:08.240
notion of morality that most of the world shares,

1842
01:33:08.319 --> 01:33:11.529
OK? Uh, MOST of the world shares, uh, So,

1843
01:33:11.540 --> 01:33:14.720
Heights 5 foundations, just I've talked about them, but

1844
01:33:14.839 --> 01:33:18.080
um there's actually 6, but you have uh care,

1845
01:33:18.390 --> 01:33:24.080
um, fairness, uh loyalty, authority, sanctity, and then, uh,

1846
01:33:24.089 --> 01:33:29.450
freedom, uh, liberty, OK. And whereas liberals and weird

1847
01:33:29.450 --> 01:33:34.970
people basically just emphasize care, um, fairness, And liberty,

1848
01:33:35.299 --> 01:33:38.919
um, conservatives sort of, uh, or most people's morality,

1849
01:33:39.029 --> 01:33:42.419
not including conservatives, uh, give equal weight to all

1850
01:33:42.419 --> 01:33:45.020
those foundations, OK? And in fact, studies have shown

1851
01:33:45.020 --> 01:33:48.089
they even give less weight to, let's say, care

1852
01:33:48.089 --> 01:33:50.779
and fairness than they do to the other ones,

1853
01:33:51.100 --> 01:33:54.100
including sanctity and purity, right? Um, AND if we

1854
01:33:54.100 --> 01:33:57.180
look at those foundations, we, we see that oftentimes,

1855
01:33:57.890 --> 01:34:00.569
Uh, the, the, the binding foundations that bind us

1856
01:34:00.569 --> 01:34:03.839
to our community, like loyalty, purity, and, and, uh,

1857
01:34:03.850 --> 01:34:09.439
and, um, sanctity, um, oftentimes are, uh, bring about

1858
01:34:09.810 --> 01:34:14.200
violence, bring about, uh, conflict within groups, OK? Um,

1859
01:34:14.290 --> 01:34:16.330
AND I mean, this is what we're talking, which

1860
01:34:16.379 --> 01:34:21.370
Burning, persecution of ethnic minorities, oppression of women, um,

1861
01:34:21.529 --> 01:34:24.609
insofar as these, as they usually are, motivated by

1862
01:34:24.609 --> 01:34:27.290
morality, it's these foundations, right? And this is what

1863
01:34:27.290 --> 01:34:29.330
I think makes it difficult for a lot of

1864
01:34:29.330 --> 01:34:32.660
academics to understand that your moral view does not

1865
01:34:32.660 --> 01:34:36.810
capture most people's morality, OK? And I, I, I

1866
01:34:36.810 --> 01:34:38.810
think, you know, uh, one response there is this,

1867
01:34:38.930 --> 01:34:40.930
well, OK, well then we just get rid of

1868
01:34:40.930 --> 01:34:43.009
that bad morality and just have, you know, try

1869
01:34:43.009 --> 01:34:45.560
to make people adopt our weird kind of morality.

1870
01:34:45.649 --> 01:34:48.350
But as I pointed out, If error theory is

1871
01:34:48.350 --> 01:34:50.990
correct, and it seems to be, there's no reason

1872
01:34:50.990 --> 01:34:53.620
we should expect this to ever occur, right? So,

1873
01:34:53.750 --> 01:34:56.750
for someone whose morality is predicated on religion and

1874
01:34:56.750 --> 01:35:00.140
purity, and, and deference to authority, you know, um,

1875
01:35:00.149 --> 01:35:02.549
them going ahead and stoning, uh, you know, by,

1876
01:35:02.629 --> 01:35:05.270
by stoning, I mean, throwing rocks and killing women,

1877
01:35:05.350 --> 01:35:07.160
uh, in the Middle East that have been uh

1878
01:35:07.169 --> 01:35:09.575
uh convicted of adultery or something, you could say,

1879
01:35:09.674 --> 01:35:10.884
you know, you could have a weird person say,

1880
01:35:10.995 --> 01:35:12.915
no, you shouldn't do this because it's, it's harming

1881
01:35:12.915 --> 01:35:15.754
her, needlessly. Um, THAT'S not gonna resonate with them

1882
01:35:15.754 --> 01:35:18.595
at all. They're gonna say, we realize it's harming

1883
01:35:18.595 --> 01:35:20.875
her, and she deserves it. Um, YOU know, she

1884
01:35:20.875 --> 01:35:23.515
was impure. She violated our religion and our, and

1885
01:35:23.515 --> 01:35:27.104
our societal, uh, norms and so forth, right? Um,

1886
01:35:27.754 --> 01:35:30.325
AND so, I, I, I think it's pretty much

1887
01:35:31.509 --> 01:35:33.240
I, I, I could be wrong about this, and,

1888
01:35:33.270 --> 01:35:34.600
and if I am wrong about it, it'd be

1889
01:35:34.600 --> 01:35:37.250
nice if the weird morality could perpetuate. It's, it,

1890
01:35:37.399 --> 01:35:39.600
again, to this point, it's been having more and

1891
01:35:39.600 --> 01:35:42.319
more influence, but certainly, I think with the advent

1892
01:35:42.319 --> 01:35:44.359
of social media, perhaps, where people are more in

1893
01:35:44.359 --> 01:35:47.910
their sort of echo chambers, um, and maybe not

1894
01:35:47.910 --> 01:35:50.899
exposed to arguments in favor of more uh weird

1895
01:35:50.899 --> 01:35:54.509
morality or, or utilitarian morality, it's having, uh, uh,

1896
01:35:54.520 --> 01:35:57.120
finding less traction among its opponents. And again, I,

1897
01:35:57.200 --> 01:36:00.990
I, I think, um, Given that error theory is

1898
01:36:00.990 --> 01:36:04.390
probably true, uh, we should not expect, uh, weird

1899
01:36:04.390 --> 01:36:07.509
morality to, to spread across the globe, uh, as

1900
01:36:07.509 --> 01:36:10.270
it has to this point. So, um, that's in

1901
01:36:10.270 --> 01:36:12.390
answer your question then, Ricardo, I think that explains

1902
01:36:12.390 --> 01:36:15.779
it. I think people are generally academics, particularly, are,

1903
01:36:15.790 --> 01:36:18.750
are, have a very narrow conception of morality that's

1904
01:36:18.750 --> 01:36:20.580
not applicable to, to most people.

1905
01:36:22.140 --> 01:36:24.589
So let me ask you a little bit about

1906
01:36:24.680 --> 01:36:28.040
uh today's problems. What would you say are the

1907
01:36:28.040 --> 01:36:33.229
greatest challenges facing us today? And why is it

1908
01:36:33.229 --> 01:36:35.799
that, uh, on, I mean, on what grounds is

1909
01:36:35.799 --> 01:36:38.720
it that in the book you argue that morality

1910
01:36:38.720 --> 01:36:42.430
is also ill-suited to, for dealing with them?

1911
01:36:43.680 --> 01:36:46.839
So, I think the best way for, for someone

1912
01:36:46.839 --> 01:36:49.770
who's perhaps skeptical of, of uh error theory or

1913
01:36:49.770 --> 01:36:52.330
particularly moral abolitionism, uh, to get them to see

1914
01:36:52.330 --> 01:36:56.319
why, um, morality is more problematic than helpful, is

1915
01:36:56.319 --> 01:36:59.720
again, look at today's political climate, particularly in the

1916
01:36:59.720 --> 01:37:05.129
United States, right? Um, Has morality, you know, what,

1917
01:37:05.319 --> 01:37:08.000
what role, positive role in, in let's say the

1918
01:37:08.000 --> 01:37:10.919
last 10 or 20 years, has morality really played?

1919
01:37:10.990 --> 01:37:14.540
Um, CERTAINLY politically and, and, and socially, I, I

1920
01:37:14.540 --> 01:37:18.720
think it, it, it's hard to deny that morality

1921
01:37:18.720 --> 01:37:21.319
has been more problematic than helpful. So, if we

1922
01:37:21.319 --> 01:37:24.830
look at the greatest sort of challenges facing Uh,

1923
01:37:24.950 --> 01:37:27.540
you know, the United States, where I, I'm not,

1924
01:37:27.629 --> 01:37:30.229
uh, uh, uh, uh, a supporter of Trump. I,

1925
01:37:30.310 --> 01:37:33.140
I think that the Trump presidency is, is, is,

1926
01:37:33.149 --> 01:37:35.709
uh, causing a lot of problems. Um, IF we

1927
01:37:35.709 --> 01:37:38.669
look at gun control, right? Um, WE, we see

1928
01:37:38.669 --> 01:37:42.589
that morality is playing a much bigger role in

1929
01:37:42.589 --> 01:37:46.279
enabling these kinds of social movements, uh, than opposing

1930
01:37:46.279 --> 01:37:48.229
them. Whereas if we just, if, if we took

1931
01:37:48.229 --> 01:37:50.100
morality out of the way, right? And and if

1932
01:37:50.100 --> 01:37:51.509
we got rid of morality, what would we have?

1933
01:37:51.580 --> 01:37:54.589
Well, uh, Um, the, the sort of Christian fundamentalism

1934
01:37:54.589 --> 01:37:57.259
that's a driving force in right in, in Republican

1935
01:37:57.259 --> 01:38:00.290
politics today, that would fall by the wayside. Um,

1936
01:38:00.470 --> 01:38:04.390
THE, the, um, avid supporters of the Second Amendment

1937
01:38:04.390 --> 01:38:07.950
that enable our, uh, ludicrous gun laws would fall

1938
01:38:07.950 --> 01:38:11.259
by the wayside. Um, IF, if we just relied

1939
01:38:11.259 --> 01:38:15.549
on enlightened self-interest, reason, empathy, uh, I, I, I,

1940
01:38:15.629 --> 01:38:19.229
I can't imagine, um, some of these policies or,

1941
01:38:19.270 --> 01:38:22.180
or, you know, Coming into play or, or, or

1942
01:38:22.180 --> 01:38:24.700
somebody like Trump being president. I, it's just very

1943
01:38:24.700 --> 01:38:27.140
difficult for me to imagine. Now, certainly, certainly people

1944
01:38:27.140 --> 01:38:30.049
could be motivated by self-interest to vote for Trump

1945
01:38:30.049 --> 01:38:31.819
or if you're a gun manufacturer to allow for

1946
01:38:31.819 --> 01:38:33.629
gun laws, but that would be a very small

1947
01:38:33.629 --> 01:38:35.259
amount of people, I think. I, I, I could

1948
01:38:35.259 --> 01:38:38.859
be wrong about this again, um, but certainly, morality

1949
01:38:38.859 --> 01:38:40.379
seems to be playing a huge role. You know,

1950
01:38:40.419 --> 01:38:42.660
the people who oppose social justice in the United

1951
01:38:42.660 --> 01:38:46.339
States really see it as reverse discrimination. It's unfair.

1952
01:38:46.419 --> 01:38:51.080
It's morally wrong, OK? Um, And also, morality, and

1953
01:38:51.080 --> 01:38:52.520
I haven't mentioned this, but one of the real

1954
01:38:52.520 --> 01:38:57.859
downsides of morality is that it, uh, Really prevents,

1955
01:38:58.240 --> 01:39:02.080
um, it, it promotes dogmatism and prevents sort of

1956
01:39:02.080 --> 01:39:04.189
an open discussion of things. If, if you believe,

1957
01:39:04.350 --> 01:39:07.669
if you have a moral conviction, um, the neuroscientist

1958
01:39:07.669 --> 01:39:10.069
John Deitee actually makes this point in a recent

1959
01:39:10.069 --> 01:39:13.470
article, if you have a moral conviction about something,

1960
01:39:13.709 --> 01:39:16.910
you're more, much less likely to take opposing views

1961
01:39:16.910 --> 01:39:19.270
into consideration, and to call your views in the

1962
01:39:19.270 --> 01:39:21.435
uh In question, right? And so we get this

1963
01:39:21.435 --> 01:39:24.515
closed-mindedness that, you know, when you combine with the

1964
01:39:24.515 --> 01:39:27.075
echo chamber given by social media, and that, you

1965
01:39:27.075 --> 01:39:30.875
know, people tend to be, um, particularly Republicans, I

1966
01:39:30.875 --> 01:39:33.484
think, uh, in the United States, where, you know,

1967
01:39:33.794 --> 01:39:35.395
everybody around you is a Republican, and so they

1968
01:39:35.395 --> 01:39:38.555
just kind of reinforce your views. Um, YOU'RE, you're

1969
01:39:38.555 --> 01:39:41.794
not really getting exposed to this other sort of

1970
01:39:41.794 --> 01:39:46.359
information, um. And so, I, I, I think that

1971
01:39:46.359 --> 01:39:49.600
again, morality is gonna play a much bigger, uh

1972
01:39:49.660 --> 01:39:51.180
uh uh uh bigger role in, in causing these,

1973
01:39:51.279 --> 01:39:55.419
these, these problematic current circumstances we have than preventing

1974
01:39:55.419 --> 01:39:57.109
them. Now, I, I could be wrong about this.

1975
01:39:57.240 --> 01:40:01.509
Certainly, the left is motivated by uh moral considerations.

1976
01:40:01.680 --> 01:40:05.200
Um, BUT, and those moral considerations might turn out

1977
01:40:05.200 --> 01:40:07.399
to, to motivate them to, to lead to, to

1978
01:40:07.399 --> 01:40:12.430
good outcomes in the future. I'm just very pessimistic

1979
01:40:12.430 --> 01:40:15.350
about this, OK? Uh, I, I, I, it, it,

1980
01:40:15.370 --> 01:40:18.779
it's uncertain how things are gonna turn out, and

1981
01:40:18.779 --> 01:40:21.390
I, and I think again, we, uh, a much

1982
01:40:21.770 --> 01:40:26.390
Uh, better, um, uh, we have much better prospects

1983
01:40:26.390 --> 01:40:29.759
for, uh, sort of reaching a sort of idealized

1984
01:40:29.759 --> 01:40:33.930
utopia where that, that height and pier and singer

1985
01:40:33.930 --> 01:40:36.209
favor where people's needs are best met and you

1986
01:40:36.209 --> 01:40:37.729
have a good amount of freedom and so forth

1987
01:40:37.729 --> 01:40:39.770
in society, if we were to get rid of

1988
01:40:39.770 --> 01:40:43.600
morality completely. We, we don't need moral arguments um

1989
01:40:43.600 --> 01:40:46.149
to, to argue why it would be a good

1990
01:40:46.149 --> 01:40:48.870
thing in a sense, to have a society characterized

1991
01:40:48.870 --> 01:40:52.470
by prosperity and inequality and equal rights and these

1992
01:40:52.470 --> 01:40:54.270
kinds of things. You, you don't need morality to

1993
01:40:54.270 --> 01:40:57.629
do that. I, empathy, self-interest, and reason are enough

1994
01:40:57.629 --> 01:41:01.149
to do that. Um, MORALITY, however, is needed to

1995
01:41:01.149 --> 01:41:03.879
give a, a, a, a sort of, um, strong,

1996
01:41:03.910 --> 01:41:06.029
compelling reason to a lot of people why we

1997
01:41:06.029 --> 01:41:07.339
should not have these things.

1998
01:41:08.410 --> 01:41:12.629
I have here one final question then, and I

1999
01:41:12.629 --> 01:41:15.220
think it's a very important one. Do you think

2000
01:41:15.220 --> 01:41:18.879
that it is actually feasible at all to abolish

2001
01:41:18.879 --> 01:41:22.430
morality? And I mean, because there are many complications

2002
01:41:22.430 --> 01:41:26.100
here, the fact that it has a biological basis,

2003
01:41:26.149 --> 01:41:29.430
it is found in every human society ever documented.

2004
01:41:29.819 --> 01:41:32.859
And it is sort of a facet of our

2005
01:41:32.859 --> 01:41:36.779
existence that is to some extent already ingrained in

2006
01:41:36.779 --> 01:41:40.040
our collective psyche. So what do you make of

2007
01:41:40.040 --> 01:41:40.379
that?

2008
01:41:41.259 --> 01:41:43.990
Yeah. So, right. So, uh, you know, I, I,

2009
01:41:44.069 --> 01:41:47.350
I agree that morality is, is, uh, deeply ingrained

2010
01:41:47.350 --> 01:41:49.520
in our psyches, that it, uh, in our psyche,

2011
01:41:49.790 --> 01:41:52.870
uh, collectively, um, that it's the product of biological

2012
01:41:52.870 --> 01:41:55.950
factors. It, it's, you know, culturally ingrained and so

2013
01:41:55.950 --> 01:41:58.509
forth. So, is it realistic to think that we

2014
01:41:58.509 --> 01:42:02.029
could ever Sort of get from under its influence.

2015
01:42:02.080 --> 01:42:05.720
Um, IT may not be possible to completely eradicate

2016
01:42:05.720 --> 01:42:08.000
it, uh, from our thoughts, you know, uh, and,

2017
01:42:08.040 --> 01:42:11.160
and have these sort of, um, moral, uh, residual

2018
01:42:11.160 --> 01:42:13.569
moral elements coming to mind and so forth. But

2019
01:42:13.569 --> 01:42:16.319
I certainly think it's possible to downplay them and

2020
01:42:16.319 --> 01:42:19.600
suppress morality's influence to the extent where it plays,

2021
01:42:19.680 --> 01:42:23.935
um, an increasingly negligible role over our Our deliberations,

2022
01:42:24.095 --> 01:42:26.654
our behaviors, and our societies, right? And, and I

2023
01:42:26.654 --> 01:42:29.165
think, uh, you know, anecdotally, I, I think in,

2024
01:42:29.254 --> 01:42:31.774
in, in, in my life, certainly, I, I, I

2025
01:42:31.774 --> 01:42:34.365
think I, I don't really conceive of the world

2026
01:42:34.365 --> 01:42:38.384
in moral terms. Moral motivations don't really motivate me

2027
01:42:38.384 --> 01:42:41.685
that much, although I still have these sorts of,

2028
01:42:42.015 --> 01:42:43.685
for lack of a better word, principles that I

2029
01:42:43.685 --> 01:42:47.095
abide by, but I, I don't see them as

2030
01:42:47.095 --> 01:42:51.660
being morally justified, right? Um, So, uh, so I,

2031
01:42:51.779 --> 01:42:54.299
I think, quick short answer to your question is

2032
01:42:54.299 --> 01:42:56.939
yes. I think we can downplay them enough to

2033
01:42:56.939 --> 01:43:00.580
where the, the moral abolitionist program can be adopted,

2034
01:43:00.620 --> 01:43:03.979
that it is, it is, um, psychologically feasible for

2035
01:43:03.979 --> 01:43:05.500
people. And, and one way to think about it,

2036
01:43:05.540 --> 01:43:07.379
you know, why think that? Well, think about religion,

2037
01:43:07.459 --> 01:43:11.060
right? So, um, I, I think like morality, religion

2038
01:43:11.060 --> 01:43:14.259
has an evolutionary basis, like very similar to morality.

2039
01:43:14.339 --> 01:43:17.540
I think it helped, uh, in-group, um, bonding and

2040
01:43:17.540 --> 01:43:21.850
cooperation. Uh, IT, it, it's obviously very culturally ingrained.

2041
01:43:22.020 --> 01:43:24.250
Every society, basically in the history of the world,

2042
01:43:24.450 --> 01:43:26.859
uh, has had religion, but we see that its

2043
01:43:26.859 --> 01:43:30.759
influence has been decreasing, um, particularly since, um, you

2044
01:43:30.759 --> 01:43:32.790
know, the, the Enlightenment period, but really sort of

2045
01:43:32.790 --> 01:43:35.729
gained steam in the 20th century and beyond, uh,

2046
01:43:35.740 --> 01:43:39.020
to where I think in a lot of people's

2047
01:43:39.020 --> 01:43:41.540
lives, growing numbers of people are, are identifying as

2048
01:43:41.540 --> 01:43:44.490
agnostic and atheists. Um, AND it's, and it's having

2049
01:43:44.490 --> 01:43:47.100
less of an influence, uh, in, in some senses

2050
01:43:47.100 --> 01:43:51.740
negligible in, in people's lives. Um, SO, I think

2051
01:43:51.740 --> 01:43:54.100
religion gives us good evidence that, yeah, morality can

2052
01:43:54.100 --> 01:43:57.140
be something like that, where we can understand by

2053
01:43:57.140 --> 01:44:01.339
being exposed to arguments against moral realism that, you

2054
01:44:01.339 --> 01:44:03.540
know, I, I, I know I have a tendency

2055
01:44:03.540 --> 01:44:05.959
to think of things as good or bad, um,

2056
01:44:05.979 --> 01:44:09.129
but, but there's really no basis in reality. Um,

2057
01:44:09.379 --> 01:44:12.529
AND, and furthermore, there's actually evidence that That, uh,

2058
01:44:12.540 --> 01:44:15.759
moral anti-realism is actually gaining more of a foothold

2059
01:44:15.759 --> 01:44:18.490
in countries like the United States. So, uh, Thomas

2060
01:44:18.490 --> 01:44:23.279
Posler and, uh, Jennifer Wright in 2020, um, did,

2061
01:44:23.290 --> 01:44:26.160
uh, a survey on, on people's moral views. Now,

2062
01:44:26.529 --> 01:44:28.509
we should take this with a grain of salt.

2063
01:44:28.529 --> 01:44:30.410
I don't, I, I think there are some, some

2064
01:44:30.410 --> 01:44:34.479
problems, um. With the study, such that I, I

2065
01:44:34.479 --> 01:44:36.479
don't think it's a representative sample of the world

2066
01:44:36.479 --> 01:44:39.189
and perhaps not even within the United States. Um,

2067
01:44:39.279 --> 01:44:42.000
I believe it was done primarily with college students

2068
01:44:42.000 --> 01:44:46.120
and, and, um, uh, Amazon Turk, uh, uh, participants,

2069
01:44:46.240 --> 01:44:50.080
right, which, which tend to be uh more Western

2070
01:44:50.080 --> 01:44:52.560
industrialized and so forth that, than your average person

2071
01:44:52.560 --> 01:44:55.160
around the world. But nonetheless, what they found was,

2072
01:44:55.240 --> 01:44:59.180
was pretty eye-opening. So they found um that um

2073
01:45:00.240 --> 01:45:04.479
Roughly 23 to 29% of their subjects, which is

2074
01:45:04.479 --> 01:45:10.330
significant, um, sided with uh anti-realism, as I understand

2075
01:45:10.330 --> 01:45:12.689
it. But more, more importantly, I think, roughly 19

2076
01:45:12.689 --> 01:45:16.009
to 20%, according to their surveys, identified as either

2077
01:45:16.009 --> 01:45:20.370
non-cognitivists, um, and 4 to 9% even identified as

2078
01:45:20.370 --> 01:45:24.520
error theorists, OK? Which, uh, non-cognitivism for, for those

2079
01:45:24.520 --> 01:45:26.729
of your viewers who aren't familiar, is basically the

2080
01:45:26.729 --> 01:45:29.140
view that That, uh, there are no real moral

2081
01:45:29.140 --> 01:45:32.140
truths because there are, in a sense, moral assertions

2082
01:45:32.140 --> 01:45:36.020
are just emotional, uh, assertions, right? It's an expression

2083
01:45:36.020 --> 01:45:38.859
of emotion. If I say stealing is bad, I'm

2084
01:45:38.859 --> 01:45:42.029
basically saying, stealing, boo, don't do that, right? Um,

2085
01:45:42.180 --> 01:45:44.339
BUT if you're a non-cognitivist, you're not a moral

2086
01:45:44.339 --> 01:45:48.109
realist. You reject moral realism. And so, if their

2087
01:45:48.109 --> 01:45:51.979
surveys are right, if they're somewhat representative, um, that

2088
01:45:51.979 --> 01:45:54.379
means that we have, again, upwards to 9% of

2089
01:45:54.379 --> 01:45:59.859
the US population identifying as um error theorists, and

2090
01:45:59.859 --> 01:46:02.350
up to 20% identifying as non-cognitist. Now, I don't

2091
01:46:02.350 --> 01:46:06.259
think it's representative, OK? But it's still showing, it

2092
01:46:06.259 --> 01:46:10.339
indicates that it's certainly not impossible for people um

2093
01:46:10.339 --> 01:46:12.819
to suppress their, their belief and morality in the

2094
01:46:12.819 --> 01:46:15.020
way that moral the way role morality plays in

2095
01:46:15.020 --> 01:46:18.029
their lives. Um, ON the basis of this, uh,

2096
01:46:18.819 --> 01:46:20.899
on the basis of these considerations, I don't think

2097
01:46:20.899 --> 01:46:23.919
it's a stretch to say That as arguments for

2098
01:46:23.919 --> 01:46:28.319
error theory, um, and perhaps arguments for moral abolitionism

2099
01:46:28.319 --> 01:46:32.450
become more well known, more expressed, uh, philosophers, moral

2100
01:46:32.450 --> 01:46:35.930
psychologists start to, to, to see, uh, the, the,

2101
01:46:35.970 --> 01:46:38.529
the merits of these arguments that they'll gain more

2102
01:46:38.529 --> 01:46:42.060
of attraction in the academic public, uh, academic world,

2103
01:46:42.290 --> 01:46:45.549
and in the world beyond. So, um, You know,

2104
01:46:45.629 --> 01:46:47.229
what's gonna happen in the future? I don't know.

2105
01:46:47.390 --> 01:46:50.310
Certainly, I think moral abolitionism and and uh moral

2106
01:46:50.310 --> 01:46:54.629
error theory are uh minority views among uh philosophers,

2107
01:46:54.709 --> 01:46:57.020
but I, I, I think the numbers are growing,

2108
01:46:57.149 --> 01:46:58.990
and I think that's attributable to the fact that

2109
01:46:58.990 --> 01:47:01.270
the, the arguments in favor of them are very

2110
01:47:01.270 --> 01:47:04.790
strong, and hopefully, again, my book will help, uh,

2111
01:47:05.689 --> 01:47:07.729
Increase this trend. Cause I, I, I really do

2112
01:47:07.729 --> 01:47:12.410
think, um, that overall society and would benefit from

2113
01:47:12.410 --> 01:47:15.410
the adoption of moral abolitionism. I mean, morality doesn't

2114
01:47:15.410 --> 01:47:17.009
seem to be working. I don't think it's worked

2115
01:47:17.009 --> 01:47:19.569
for a long time. Um, I, I think it's

2116
01:47:19.569 --> 01:47:22.410
try to time to try something different. And I

2117
01:47:22.410 --> 01:47:27.660
think that moral abolitionism is, is, um, Much more

2118
01:47:27.660 --> 01:47:30.580
grounded in reality than uh than moral realism is.

2119
01:47:32.109 --> 01:47:35.080
Great. So the book is again moral damages, the

2120
01:47:35.080 --> 01:47:38.799
case for abolishing morality, and of course, I'm leaving

2121
01:47:38.799 --> 01:47:41.160
a link to it in the description of the

2122
01:47:41.160 --> 01:47:44.830
interview. And uh Doctor Morris, apart from the book,

2123
01:47:45.000 --> 01:47:48.120
are there any places on the internet where people

2124
01:47:48.120 --> 01:47:49.470
can find your work?

2125
01:47:50.220 --> 01:47:52.709
So I do have a website where that has

2126
01:47:52.709 --> 01:47:55.189
a link to my CV, uh, and, and some

2127
01:47:55.189 --> 01:48:00.509
of my uh research. It's uh uh Steven GeorgeMorris.com,

2128
01:48:00.629 --> 01:48:07.520
all one word. Um. And, and, uh, beyond that,

2129
01:48:07.640 --> 01:48:10.990
uh, you can contact me, uh, through email, um,

2130
01:48:11.000 --> 01:48:13.750
at my institution, the College of Staten Island. So,

2131
01:48:14.080 --> 01:48:16.879
um, yeah, uh, let's get the dialogue going. I,

2132
01:48:16.959 --> 01:48:18.600
I, I realize, uh, a lot of people are

2133
01:48:18.600 --> 01:48:20.669
gonna disagree with what I, what I've, uh, said,

2134
01:48:20.779 --> 01:48:23.319
uh, at least some aspects, but, uh, again, I

2135
01:48:23.319 --> 01:48:25.120
think the compelling, uh, there is a lot of

2136
01:48:25.120 --> 01:48:27.160
compelling evidence in, in favor of the conclusions that

2137
01:48:27.160 --> 01:48:30.640
I draw, and I'm, I'd be interested about coming

2138
01:48:30.640 --> 01:48:34.410
across, um, empirical evidence or, or, uh, philosophical arguments

2139
01:48:34.410 --> 01:48:36.419
to the contrary. And uh yeah.

2140
01:48:37.419 --> 01:48:40.359
OK, so great. Thank you so much again for

2141
01:48:40.359 --> 01:48:42.220
taking the time to come on the show. It's

2142
01:48:42.220 --> 01:48:44.479
been a very interesting conversation.

2143
01:48:45.319 --> 01:48:47.359
Thank you very much for having me, Ricardo. I

2144
01:48:47.359 --> 01:48:48.560
appreciate it and I, I think you do a

2145
01:48:48.560 --> 01:48:49.439
great job with your channel.

2146
01:48:50.799 --> 01:48:53.319
Hi guys, thank you for watching this interview until

2147
01:48:53.319 --> 01:48:55.470
the end. If you liked it, please share it,

2148
01:48:55.640 --> 01:48:58.430
leave a like and hit the subscription button. The

2149
01:48:58.430 --> 01:49:00.629
show is brought to you by Nights Learning and

2150
01:49:00.629 --> 01:49:04.709
Development done differently, check their website at Nights.com and

2151
01:49:04.709 --> 01:49:08.430
also please consider supporting the show on Patreon or

2152
01:49:08.430 --> 01:49:10.910
PayPal. I would also like to give a huge

2153
01:49:10.910 --> 01:49:14.020
thank you to my main patrons and PayPal supporters

2154
01:49:14.020 --> 01:49:18.240
Perergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyches Olaf,

2155
01:49:18.350 --> 01:49:21.600
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

2156
01:49:21.729 --> 01:49:25.020
Erika Lenny, John Connors, Philip Fors Connolly. Then the

2157
01:49:25.020 --> 01:49:28.419
Mari Robert Windegaruyasi Zup Mark Nes calling in Holbrook

2158
01:49:28.419 --> 01:49:33.339
field governor Michael Stormir, Samuel Andre Francis Forti Agnseroro

2159
01:49:33.339 --> 01:49:37.930
and Hal Herzognun Macha Joan Labray and Samuel Corriere,

2160
01:49:38.089 --> 01:49:41.770
Heinz, Mark Smith, Jore, Tom Hummel, Sardus France David

2161
01:49:41.770 --> 01:49:46.370
Sloan Wilson, asila dearauurumen Roach Diego London Correa. Yannick

2162
01:49:46.370 --> 01:49:51.609
Punteran Rosmani Charlotte blinikol Barbara Adamhn Pavlostaevsky nale back

2163
01:49:51.609 --> 01:49:56.160
medicine, Gary Galman Sam of Zallirianeioltonin John Barboza, Julian

2164
01:49:56.160 --> 01:50:00.810
Price, Edward Hall Edin Bronner, Douglas Fre Franca Bartolotti

2165
01:50:00.810 --> 01:50:05.169
Gabriel Pons Corteseus Slelitsky, Scott Zachary Fish Tim Duffyani

2166
01:50:05.169 --> 01:50:09.669
Smith John Wieman. Daniel Friedman, William Buckner, Paul Georgianneau,

2167
01:50:10.080 --> 01:50:14.600
Luke Lovai Giorgio Theophanous, Chris Williamson, Peter Wozin, David

2168
01:50:14.600 --> 01:50:19.250
Williams, Diota Anton Eriksson, Charles Murray, Alex Shaw, Marie

2169
01:50:19.250 --> 01:50:23.560
Martinez, Coralli Chevalier, bungalow atheists, Larry V. Lee Junior,

2170
01:50:23.759 --> 01:50:28.959
old Erringbo. Sterry Michael Bailey, then Sperber, Robert Grassyigoren,

2171
01:50:29.160 --> 01:50:33.600
Jeff McMann, Jake Zu, Barnabas radix, Mark Campbell, Thomas

2172
01:50:33.600 --> 01:50:37.930
Dovner, Luke Neeson, Chris Storry, Kimberly Johnson, Benjamin Gilbert,

2173
01:50:38.080 --> 01:50:43.430
Jessica Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George

2174
01:50:43.430 --> 01:50:48.660
Eoriatis, Valentin Steinman, Perrolis, Kate van Goller, Alexander Aubert,

2175
01:50:49.479 --> 01:50:55.319
Liam Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner,

2176
01:50:55.439 --> 01:51:00.200
Ursula Gudinov, Gregory Hastings, David Pinsoff, Sean Nelson, Mike

2177
01:51:00.200 --> 01:51:03.859
Levin, and Jos Net. A special thanks to my

2178
01:51:03.859 --> 01:51:06.700
producers. These are Webb, Jim, Frank Lucas Steffinik, Tom

2179
01:51:06.700 --> 01:51:11.580
Venneden, Bernardin Curtis Dixon, Benedic Muller, Thomas Trumbull, Catherine

2180
01:51:11.580 --> 01:51:14.859
and Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and

2181
01:51:14.859 --> 01:51:18.220
Nick Golden, and to my executive producers Matthew Levender,

2182
01:51:18.310 --> 01:51:21.490
Sergio Quadrian, Bogdan Kanivets, and Rosie. Thank you for

2183
01:51:21.490 --> 01:51:21.810
all.

