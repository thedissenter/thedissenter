WEBVTT

1
00:00:00.129 --> 00:00:02.869
Hello, everyone. Welcome to a new episode of the

2
00:00:02.930 --> 00:00:05.599
Center. I'm your host, Ricardo Lobs. And today I'm

3
00:00:05.610 --> 00:00:08.310
joined by Doctor Peter Els. He is Professor of

4
00:00:08.319 --> 00:00:12.229
Psychology and Science Communication at Beth Spa University. His

5
00:00:12.239 --> 00:00:15.220
research looks at the behavioral and well being effects

6
00:00:15.229 --> 00:00:17.979
of playing video games. And he is also interested

7
00:00:17.989 --> 00:00:22.200
in me scientific issues regarding best practice in digital

8
00:00:22.209 --> 00:00:26.620
technology effects research. And today we're talking about his

9
00:00:26.629 --> 00:00:30.930
recent book, Unlocked The Real Science of Screen time

10
00:00:30.940 --> 00:00:34.500
and How to spend it better. So, Doctor Els,

11
00:00:34.509 --> 00:00:36.619
welcome to the show. It's a pleasure to everyone.

12
00:00:37.259 --> 00:00:38.639
Thank you very much for having me. It's great

13
00:00:38.650 --> 00:00:39.080
to be here.

14
00:00:40.080 --> 00:00:43.500
So of course, your book is uh a lot

15
00:00:43.509 --> 00:00:47.700
about, let's say a response to many moral panics

16
00:00:47.709 --> 00:00:51.340
that people have been expressing in recent years surrounding

17
00:00:51.680 --> 00:00:57.009
uh different kinds of technologies but particularly screen technologies.

18
00:00:57.110 --> 00:00:59.500
So where do you think that perhaps some of

19
00:00:59.509 --> 00:01:03.939
these moral panic surrounding, for example, the internet, social

20
00:01:03.950 --> 00:01:08.129
media, uh smartphones and so on? Stem from

21
00:01:09.529 --> 00:01:12.019
it's a really good question. Um I always, I

22
00:01:12.029 --> 00:01:14.419
always tend to caveat the answer to this by

23
00:01:14.430 --> 00:01:17.790
saying that the problem with moral panics is that

24
00:01:17.800 --> 00:01:20.319
you don't know that you're in one until you're

25
00:01:20.330 --> 00:01:22.629
out of it. So we always see them in

26
00:01:22.639 --> 00:01:26.629
a retrospective sense. Um, AND I think it's important

27
00:01:26.639 --> 00:01:30.989
to bear that in mind actually because, um, you

28
00:01:31.000 --> 00:01:32.949
know, you, you never know because you never know

29
00:01:32.959 --> 00:01:34.730
whether you're actually in a moral panic or not.

30
00:01:34.739 --> 00:01:36.900
You just have to be careful not to be

31
00:01:37.470 --> 00:01:41.000
just dismissive straight off the bat just because this,

32
00:01:41.010 --> 00:01:42.860
you know, feels like everything that's happened before. It

33
00:01:42.870 --> 00:01:45.110
could well be the case that something comes along

34
00:01:45.120 --> 00:01:47.199
at some point that actually we need, we do

35
00:01:47.209 --> 00:01:51.069
need to really worry about. Um, WHAT I probably

36
00:01:51.080 --> 00:01:53.779
would say is that with the current concerns around

37
00:01:53.790 --> 00:01:57.169
things like social media and smartphones, um, you know,

38
00:01:57.180 --> 00:02:00.389
the evidence base isn't great as to whether they,

39
00:02:00.400 --> 00:02:03.620
they actually do cause harm or not. Um So

40
00:02:03.629 --> 00:02:05.970
it feels like a moral panic and certainly we're

41
00:02:05.980 --> 00:02:08.679
going through the same sorts of motions. So we

42
00:02:08.690 --> 00:02:11.500
see the same sorts of cycles of arguments, the

43
00:02:11.509 --> 00:02:14.160
way that the rhetoric builds up. It looks very

44
00:02:14.169 --> 00:02:17.149
similar to what we've seen before. Uh So that

45
00:02:17.160 --> 00:02:19.919
very big caveat, uh why, why do we find

46
00:02:19.929 --> 00:02:23.279
ourselves in this? Um It's a good question uh

47
00:02:23.289 --> 00:02:25.880
because we, we, we keep finding ourselves in this

48
00:02:25.889 --> 00:02:29.550
situation right here. Whenever a new, well, in modern

49
00:02:29.559 --> 00:02:33.070
day times, a new digital technology comes along, we

50
00:02:33.080 --> 00:02:37.600
very often tend to view that with suspicion, um,

51
00:02:37.740 --> 00:02:40.369
and varying levels of concern about how this is

52
00:02:40.380 --> 00:02:44.199
impacting people, particularly kids, uh particularly uh uh young

53
00:02:44.210 --> 00:02:49.630
people in adolescents. Um I think very often that

54
00:02:49.639 --> 00:02:55.419
happens because these things sort of really take hold.

55
00:02:55.429 --> 00:02:59.270
Right. So before screen, before social media and smartphones,

56
00:02:59.279 --> 00:03:03.119
it was video games. Uh BEFORE video games, it

57
00:03:03.130 --> 00:03:05.360
was, you know, radio and television and all those

58
00:03:05.369 --> 00:03:09.610
sorts of things that these are really disruptive technologies,

59
00:03:09.619 --> 00:03:12.339
you know, they appear pretty much out of the

60
00:03:12.350 --> 00:03:13.990
blue or what feels like out of the blue

61
00:03:14.000 --> 00:03:17.039
to, to the general public. Um They take hold

62
00:03:17.050 --> 00:03:22.050
pretty quickly and they become ubiquitous really quickly and

63
00:03:22.059 --> 00:03:25.320
if you don't know much about them or you

64
00:03:25.330 --> 00:03:27.929
have a particular perception of them, that's not, they

65
00:03:27.940 --> 00:03:32.539
may be informed by ex expertise of use. Um

66
00:03:32.550 --> 00:03:34.809
I, I think our default very often is to

67
00:03:34.820 --> 00:03:36.860
view them with suspicion or worry and concern and

68
00:03:36.869 --> 00:03:40.580
yeah, that's an entirely reasonable thing to uh to

69
00:03:40.589 --> 00:03:43.059
do. Right. So the problem that we've got with

70
00:03:43.070 --> 00:03:45.570
the big smartphone, social media debate at the minute

71
00:03:45.580 --> 00:03:50.210
is that these things are so embedded within our

72
00:03:50.220 --> 00:03:55.899
society and culture now, um everybody uses them. Everybody

73
00:03:55.910 --> 00:04:01.339
has that sort of lived experience of not having

74
00:04:01.350 --> 00:04:03.699
a good time on your phone at some point.

75
00:04:03.720 --> 00:04:05.990
And for some people that's really serious, right? Some

76
00:04:06.000 --> 00:04:08.789
people just really horrible things that happen out there.

77
00:04:09.210 --> 00:04:11.740
Uh Nobody is saying that that doesn't happen, but

78
00:04:11.750 --> 00:04:13.500
for most of us, it's not that level of

79
00:04:13.509 --> 00:04:16.500
extremity. It's things like, oh, you know, we wanted

80
00:04:16.510 --> 00:04:17.899
to go to bed at 10 o'clock and we

81
00:04:17.910 --> 00:04:19.980
ended up going to bed at 1030 or 11

82
00:04:19.988 --> 00:04:23.250
because we were scrolling mindlessly on Instagram or whatever.

83
00:04:23.269 --> 00:04:25.519
And that feels not great. It feels a bit

84
00:04:25.529 --> 00:04:30.950
unnatural. We tend to focus on those negatives more

85
00:04:30.959 --> 00:04:34.429
than the positives. So when you start really kind

86
00:04:34.440 --> 00:04:37.049
of asking people interrogated people, you know, what do

87
00:04:37.059 --> 00:04:38.440
you use your phones for? What do you use

88
00:04:38.450 --> 00:04:41.970
social media for? Although those sorts of negative things

89
00:04:41.980 --> 00:04:44.839
might come to, to light first. Actually, there's lots

90
00:04:44.850 --> 00:04:48.769
of positives around them, social connection. Um For me,

91
00:04:48.779 --> 00:04:53.190
it's things like uh meeting uh new researchers and

92
00:04:53.369 --> 00:04:55.350
over the past 10 years, it's been a really

93
00:04:55.359 --> 00:04:58.839
good way of generating new sort of research, ideas

94
00:04:58.850 --> 00:05:01.049
and science communication projects and things like that. And

95
00:05:01.059 --> 00:05:03.100
we tend to forget those things. But even like

96
00:05:03.109 --> 00:05:05.429
just using a phone, you know, the, the the

97
00:05:05.440 --> 00:05:08.750
sheer level of convenience about it, there are really

98
00:05:08.760 --> 00:05:10.869
good things that phones allow you to do, but

99
00:05:10.880 --> 00:05:13.890
you won't even necessarily notice. So, you know, we

100
00:05:13.899 --> 00:05:17.589
don't really get lost anymore. Um So you don't

101
00:05:17.600 --> 00:05:19.269
have the bad experience of getting lost and being

102
00:05:19.279 --> 00:05:21.549
super late for a meeting or, or an event

103
00:05:21.559 --> 00:05:23.359
because that never happened because you got there on

104
00:05:23.369 --> 00:05:25.149
time because you were using the map function on

105
00:05:25.160 --> 00:05:27.230
your phone. So those sorts of good things, those

106
00:05:27.239 --> 00:05:29.890
convenient things we don't tend to notice as much.

107
00:05:29.920 --> 00:05:32.709
So we all have this lived experience where these

108
00:05:32.720 --> 00:05:36.570
negative aspects of technology use maybe are a little

109
00:05:36.579 --> 00:05:39.410
bit more salient than the positives. So when people

110
00:05:39.420 --> 00:05:43.170
come along and say smartphones are, are really bad

111
00:05:43.179 --> 00:05:45.130
for us or social media is really bad for

112
00:05:45.140 --> 00:05:46.839
us, we, we sit up and take note of

113
00:05:46.850 --> 00:05:48.989
that because it, it kind of fits with our

114
00:05:49.000 --> 00:05:52.559
world view really. And that's been true. I think

115
00:05:52.570 --> 00:05:56.920
of every cycle of panic, technology, panic through, through

116
00:05:56.929 --> 00:06:02.200
history, it just feels orders of magnitudes la larger.

117
00:06:02.250 --> 00:06:04.350
Uh NOW with the current debates that we're having.

118
00:06:04.869 --> 00:06:05.339
Mhm

119
00:06:05.489 --> 00:06:09.309
So would you say then that people tend to

120
00:06:09.320 --> 00:06:15.059
look at screen technologies in a mostly negative way?

121
00:06:16.209 --> 00:06:19.290
I think we default to that for sure. Um

122
00:06:19.299 --> 00:06:23.000
If you, if you ask people quickly um and

123
00:06:23.010 --> 00:06:24.670
then move on to something else, you know, we

124
00:06:24.679 --> 00:06:27.899
default to these very surface levels perceptions of, of

125
00:06:27.910 --> 00:06:29.859
uh of screens. And I think one of the

126
00:06:29.869 --> 00:06:33.350
problems there is that that is increasingly those perceptions

127
00:06:33.359 --> 00:06:38.390
are increasingly colored by this very big debate that

128
00:06:38.399 --> 00:06:39.630
we're we're having. So one of the things that

129
00:06:39.640 --> 00:06:41.149
I talk about in my new book is this

130
00:06:41.160 --> 00:06:44.670
idea of um the influence of presumed media influence,

131
00:06:45.230 --> 00:06:47.850
which, you know, there, there's various interpretations of it.

132
00:06:47.859 --> 00:06:49.049
And one of them that I take in the

133
00:06:49.059 --> 00:06:50.559
book that I think is a useful one is

134
00:06:50.570 --> 00:06:54.989
that, you know, we are told constantly literally constantly

135
00:06:55.000 --> 00:06:58.480
now that screens are bad for us that social

136
00:06:58.489 --> 00:07:02.269
media is really bad. Of course, that's going to

137
00:07:02.279 --> 00:07:04.190
start embedding in there. So there's, you know, there's

138
00:07:04.200 --> 00:07:07.269
tons of research from psychology that shows that if

139
00:07:07.279 --> 00:07:09.649
you're presented with a, a claim or an argument,

140
00:07:09.660 --> 00:07:11.890
you know, regardless of whether you believe in it

141
00:07:11.899 --> 00:07:14.630
or not or believe it, if you're repeatedly presented

142
00:07:14.640 --> 00:07:16.549
with it over time, you start to, it becomes

143
00:07:16.559 --> 00:07:18.649
more salient, right? And then eventually over time you

144
00:07:18.660 --> 00:07:21.630
start becoming more swayed to thinking that that's correct.

145
00:07:21.640 --> 00:07:24.619
And we are literally told constantly that screens are

146
00:07:24.630 --> 00:07:27.869
bad for us. Um So it's unsurprising in a

147
00:07:27.880 --> 00:07:30.429
way that everybody was, you know, if you're told

148
00:07:30.559 --> 00:07:32.119
all the time, screens are bad for you and

149
00:07:32.130 --> 00:07:33.559
then somebody comes along and says, do you think

150
00:07:33.570 --> 00:07:35.200
screens are bad for you? Of course, you're gonna

151
00:07:35.209 --> 00:07:37.480
say yes, um straight off the bat. But when

152
00:07:37.489 --> 00:07:39.500
you start getting people really thinking about it and

153
00:07:39.510 --> 00:07:43.059
really interrogating how that technology works for them in

154
00:07:43.070 --> 00:07:45.839
their lives, that's when you get more nuanced, more

155
00:07:45.850 --> 00:07:49.760
rich, more interesting responses and, and data from people.

156
00:07:50.519 --> 00:07:52.989
So let me ask you about a particular concept

157
00:07:53.000 --> 00:07:56.540
that is relevant for this conversation. So what is

158
00:07:56.549 --> 00:08:00.579
screen, screen time? What does that mean exactly.

159
00:08:01.269 --> 00:08:04.899
So screen time is literally the amount of time

160
00:08:04.910 --> 00:08:08.809
that you spend on screen based activities, whether that's,

161
00:08:08.820 --> 00:08:11.570
you know, software or apps or video games in

162
00:08:11.579 --> 00:08:14.179
a given time period. So if you're looking at

163
00:08:14.190 --> 00:08:16.350
this from a scientific point of view in terms

164
00:08:16.359 --> 00:08:18.329
of trying to do some research on it, you

165
00:08:18.339 --> 00:08:20.929
can define it as, you know, how much time

166
00:08:20.940 --> 00:08:23.359
did you spend on screens over the past 24

167
00:08:23.369 --> 00:08:26.019
hours or over the past week or past year

168
00:08:26.029 --> 00:08:28.059
or whatever? Um Or you can be a little

169
00:08:28.070 --> 00:08:31.450
bit more specific. So rather than screen time, uh

170
00:08:31.459 --> 00:08:33.119
you can say, you know, how much time did

171
00:08:33.130 --> 00:08:35.789
you spend on social media yesterday or how much

172
00:08:35.799 --> 00:08:38.859
time did you spend on tiktok yesterday? So you

173
00:08:38.869 --> 00:08:41.880
can ask more fine grained versions of that uh

174
00:08:41.890 --> 00:08:45.849
that question. But in the, in the research literature

175
00:08:45.859 --> 00:08:48.820
at large, these things are all considered under the

176
00:08:48.830 --> 00:08:52.270
general banner of of screen time. So, yeah, my,

177
00:08:52.280 --> 00:08:54.710
my book is about that. It's about various different

178
00:08:54.719 --> 00:08:57.369
aspects of of screen time. I think, you know,

179
00:08:58.179 --> 00:09:01.010
in the 34 months that it's been out, the

180
00:09:01.020 --> 00:09:04.700
conversation has really shifted, nobody really talks that much

181
00:09:04.710 --> 00:09:07.469
about screen time generally anymore. We're talking about smartphones

182
00:09:07.479 --> 00:09:09.520
and social media. But in a sense, they're the

183
00:09:09.530 --> 00:09:12.580
same thing, right? You know, uh how much time

184
00:09:12.590 --> 00:09:16.299
did you spend on your smartphone yesterday is basically

185
00:09:16.309 --> 00:09:18.859
asking how much screen time you had. Uh And

186
00:09:18.869 --> 00:09:19.880
you go to a lot of detail in the

187
00:09:19.890 --> 00:09:22.909
book about why screen time is essentially a meaningless

188
00:09:22.919 --> 00:09:26.789
concept for research purposes because it's literally anything and

189
00:09:26.799 --> 00:09:30.760
everything, right? I think if you're saying that screen

190
00:09:30.770 --> 00:09:34.750
time has an effect, certainly now given the level

191
00:09:34.760 --> 00:09:36.679
of complexity of the debate and what we know.

192
00:09:36.690 --> 00:09:39.039
And from the research, if you're still saying that

193
00:09:39.049 --> 00:09:42.169
screen time is an effect has an effect. What

194
00:09:42.179 --> 00:09:44.650
you're, what you've, what you've now increasingly got to

195
00:09:44.659 --> 00:09:47.000
do is make an argument for why? Literally, I

196
00:09:47.010 --> 00:09:48.989
know this sounds a bit facetious, but literally, the

197
00:09:49.000 --> 00:09:54.770
screen itself is theoretically meaningful in some way. And

198
00:09:54.780 --> 00:09:56.900
I think people increasingly realize that that's a bit

199
00:09:56.909 --> 00:09:58.580
of a silly argument to make. But I don't

200
00:09:58.590 --> 00:10:01.710
think anybody is trying to say that it's literally

201
00:10:01.719 --> 00:10:04.750
the g the glass screen. Uh THAT'S, that's the

202
00:10:04.760 --> 00:10:06.830
problem. It's the stuff that we do on it,

203
00:10:06.840 --> 00:10:11.960
right? But even then, so smartphones are basically the

204
00:10:11.969 --> 00:10:14.099
same thing. You know, there's, we don't, when we,

205
00:10:14.109 --> 00:10:16.210
I think when we talk about smartphones, we talk

206
00:10:16.219 --> 00:10:18.539
about smartphone bands and things like that. It's not

207
00:10:18.549 --> 00:10:20.989
actually the smartphone that we're worried about. It's the

208
00:10:21.000 --> 00:10:23.330
stuff that we're doing on it. So you drill

209
00:10:23.340 --> 00:10:25.630
a little bit deeper and say, OK, well, actually

210
00:10:25.640 --> 00:10:27.229
the thing that we're worried about at the minute

211
00:10:27.239 --> 00:10:30.450
is social media, but then you've got the same

212
00:10:30.460 --> 00:10:33.320
problem, right? So what is social media? How do

213
00:10:33.330 --> 00:10:35.440
you define that? I've still not seen a good

214
00:10:35.450 --> 00:10:39.059
consistent consensus definition of social media. I've seen people

215
00:10:39.070 --> 00:10:41.679
try to come up with that and there've been

216
00:10:41.690 --> 00:10:45.640
some really good efforts, but there are always things.

217
00:10:45.659 --> 00:10:47.869
So, you know, you take these sorts of definitions

218
00:10:47.880 --> 00:10:53.229
sometimes that um you think will just cover tiktok

219
00:10:53.239 --> 00:10:56.679
and Instagram and Twitter and things like that. But

220
00:10:56.690 --> 00:10:58.719
actually, if you look, if you, if you're specific

221
00:10:58.729 --> 00:11:01.179
about it and as scientists, we need to be

222
00:11:01.190 --> 00:11:04.260
specific about it. Um It also covers things that

223
00:11:04.270 --> 00:11:07.280
you maybe wouldn't want to be worried about, you

224
00:11:07.289 --> 00:11:10.669
know, Minecraft or the chat function in World of

225
00:11:10.679 --> 00:11:14.409
Warcraft or Google classroom. Uh THOSE sorts of the

226
00:11:14.419 --> 00:11:19.010
comments section on youtube, these sorts of um, software

227
00:11:19.020 --> 00:11:22.640
and, and platforms, you can, you can define them

228
00:11:22.650 --> 00:11:26.650
as social media with these sorts of definitions. And

229
00:11:26.659 --> 00:11:29.210
it's interesting to me because um I kind of

230
00:11:29.219 --> 00:11:31.489
get some pushback on this sometimes that this is

231
00:11:31.500 --> 00:11:34.200
like a really nerdy conversation to have that, you

232
00:11:34.210 --> 00:11:36.349
know, I'm coming along and saying, well, definitions are

233
00:11:36.359 --> 00:11:38.299
important and we need to make sure that we

234
00:11:38.309 --> 00:11:40.169
define our things properly. You know, people come along

235
00:11:40.179 --> 00:11:42.450
and say, come on like this is, this is

236
00:11:42.460 --> 00:11:45.429
derailing the conversation. Nobody cares about definitions. We know

237
00:11:45.440 --> 00:11:48.719
that there's problems with whatever, but it's actually a

238
00:11:48.729 --> 00:11:51.799
really important part of that because everybody feels as

239
00:11:51.809 --> 00:11:56.349
though there's a problem going on, but we need

240
00:11:56.359 --> 00:11:59.460
to define that properly. A in order to test

241
00:11:59.469 --> 00:12:01.469
it and b in order to do something about

242
00:12:01.479 --> 00:12:03.580
it. And I find it really interesting that when

243
00:12:03.590 --> 00:12:05.900
you push people sometimes on that, you say, OK,

244
00:12:05.909 --> 00:12:08.340
well, what is it specifically about smartphones? What is

245
00:12:08.349 --> 00:12:11.049
it specifically about social media? They start to get

246
00:12:11.059 --> 00:12:14.580
very defensive and very angry and aggressive. They don't

247
00:12:14.590 --> 00:12:16.820
really give you a good answer. And I think

248
00:12:16.830 --> 00:12:20.030
part of the reason for that is they have

249
00:12:20.039 --> 00:12:23.390
this sort of sense, that stuff is bad, particularly

250
00:12:23.400 --> 00:12:29.679
for kids, but it's not easily definable, but that

251
00:12:29.690 --> 00:12:32.820
shouldn't stop us from doing something about it because

252
00:12:32.830 --> 00:12:34.940
we need to, we need to protect the kids.

253
00:12:35.380 --> 00:12:38.619
Um, AND, and I, it, it, it, it creates

254
00:12:38.630 --> 00:12:42.299
a very difficult situation. Right. It's very, it's, it's

255
00:12:42.309 --> 00:12:45.119
a very toxic environment at the minute. A to

256
00:12:45.130 --> 00:12:48.239
even have the debate about screen time or social

257
00:12:48.250 --> 00:12:51.330
media or whatever, but b to do the research

258
00:12:51.340 --> 00:12:55.260
in it as well. So um yeah, that is

259
00:12:55.270 --> 00:12:58.590
a very long winded way of saying um screen

260
00:12:58.599 --> 00:13:02.520
time is, is effectively very poorly defined. Um And

261
00:13:02.530 --> 00:13:04.500
that actually causes quite a lot of problems.

262
00:13:05.809 --> 00:13:08.219
And what do you make by the way of

263
00:13:08.229 --> 00:13:12.690
claims made by people and their research and public

264
00:13:12.700 --> 00:13:17.640
campaign by people like Jinwan Ji and Jonathan Hyde

265
00:13:17.729 --> 00:13:22.549
against screen based technologies. And in their case, more

266
00:13:22.559 --> 00:13:24.950
specifically about social media,

267
00:13:26.150 --> 00:13:31.929
I genuinely think that everybody in this debate. So

268
00:13:31.940 --> 00:13:35.640
the scientists and researchers who are, are publishing research

269
00:13:35.650 --> 00:13:39.169
and making arguments that screens are bad, the scientists

270
00:13:39.179 --> 00:13:41.710
and researchers that are publishing research and making arguments

271
00:13:41.719 --> 00:13:43.729
that it's more complicated than that and that we

272
00:13:43.739 --> 00:13:46.440
need to think about this differently. I genuinely think

273
00:13:46.450 --> 00:13:47.650
that all of those people are trying to do

274
00:13:47.659 --> 00:13:51.049
the right thing. I don't think there's any um

275
00:13:51.609 --> 00:13:55.460
anybody with sort of ill intent there, genuinely, like

276
00:13:55.469 --> 00:13:57.549
all of the conversations that I've had with researchers

277
00:13:57.559 --> 00:13:59.229
in this area. People are trying to figure out

278
00:13:59.239 --> 00:14:01.630
what's actually going on so that we can do

279
00:14:01.640 --> 00:14:05.590
something that works because I think, I don't think

280
00:14:05.599 --> 00:14:09.820
there's anybody out there who says, who believes that,

281
00:14:10.099 --> 00:14:12.380
you know, there aren't some bad things that, that

282
00:14:12.390 --> 00:14:15.130
go on online, uh, that can be really impactful

283
00:14:15.140 --> 00:14:18.690
and really harmful. Um, Candace Hodges. Uh There's a

284
00:14:18.700 --> 00:14:21.210
really great quote from her from a, a recent

285
00:14:21.219 --> 00:14:25.010
um recent article which is essentially that you can

286
00:14:25.020 --> 00:14:29.280
take the viewpoint that the research on social media

287
00:14:29.289 --> 00:14:32.840
and its effects on mental health is terrible. And

288
00:14:32.849 --> 00:14:35.239
at the same time, you can take the viewpoint

289
00:14:35.250 --> 00:14:37.919
that tech companies need to do something they need

290
00:14:37.929 --> 00:14:40.640
to step up and design their products better. Those

291
00:14:40.650 --> 00:14:43.760
are not mutually exclusive positions to take. And I'm

292
00:14:43.770 --> 00:14:45.429
exactly the same. I whole both of those positions.

293
00:14:45.440 --> 00:14:48.369
I think the research is really poor and really

294
00:14:48.380 --> 00:14:52.289
conflicting in lots of ways that doesn't let tech

295
00:14:52.299 --> 00:14:55.090
companies off the hook. There are things that they

296
00:14:55.099 --> 00:14:56.969
need to do better and that needs to happen

297
00:14:56.979 --> 00:15:02.010
really quickly. So there, there's another caveat for you

298
00:15:02.150 --> 00:15:04.719
um in terms of the sort of research that

299
00:15:04.729 --> 00:15:07.400
comes out that seems to suggest that there are

300
00:15:07.409 --> 00:15:10.919
these, there are these links, my worry at the

301
00:15:10.929 --> 00:15:14.299
minute with not just that research, but also the

302
00:15:14.309 --> 00:15:16.559
research on the other side that suggests that, you

303
00:15:16.570 --> 00:15:19.010
know, these, these links aren't, aren't as, as uh

304
00:15:19.020 --> 00:15:21.520
as worrisome as other people make them out to

305
00:15:21.530 --> 00:15:25.369
be. We're kind of fumbling around in the dark.

306
00:15:25.570 --> 00:15:27.479
So if you look at the trajectory of research

307
00:15:27.489 --> 00:15:32.460
over the past seven or eight years, 90% of

308
00:15:32.469 --> 00:15:35.710
it is based in longitudinal research. So where you

309
00:15:35.719 --> 00:15:37.599
go away, you find these big data sets where

310
00:15:37.609 --> 00:15:40.320
loads of data has already been collected, really important

311
00:15:40.330 --> 00:15:43.520
data sets and you find some questions about screen

312
00:15:43.530 --> 00:15:45.390
time or social media or whatever it is that

313
00:15:45.400 --> 00:15:47.070
you're interested in. You find some questions about mental

314
00:15:47.080 --> 00:15:49.679
health and you do these, you, you do these

315
00:15:49.690 --> 00:15:52.460
analyses that you think make sense. So what what

316
00:15:52.469 --> 00:15:54.169
seems to have happened is that we started off,

317
00:15:54.179 --> 00:15:56.080
some people have done some analyses and found a

318
00:15:56.090 --> 00:15:59.150
correlation effect that looks big and scary and that

319
00:15:59.159 --> 00:16:02.210
set the scene for the debate. So you find

320
00:16:02.219 --> 00:16:06.030
these correlations, these links between screen time going up

321
00:16:06.039 --> 00:16:09.340
over time and teenage happiness going down over time

322
00:16:09.659 --> 00:16:11.619
and then somebody comes along and, and does a

323
00:16:11.630 --> 00:16:13.179
little bit more of a specific study and says,

324
00:16:13.190 --> 00:16:14.580
well, hang on that. I'm not sure that that

325
00:16:14.590 --> 00:16:18.460
correlation actually means anything and then the focus shifts.

326
00:16:18.469 --> 00:16:21.130
So it's maybe not about screen time anymore. It's

327
00:16:21.140 --> 00:16:24.130
maybe about social media instead. And then you go

328
00:16:24.140 --> 00:16:25.929
through the same cycle and then the focus shifts

329
00:16:25.940 --> 00:16:28.330
again and it's maybe not so much about social

330
00:16:28.340 --> 00:16:31.409
media for everybody. It's social media for girls. And

331
00:16:31.419 --> 00:16:33.630
then on the other side of the equation we're

332
00:16:33.640 --> 00:16:36.239
talking about literally, I think we originally started talking

333
00:16:36.250 --> 00:16:38.979
about this in terms of happiness. Uh And then

334
00:16:38.989 --> 00:16:41.349
that sh it bounces around. Sometimes we're talking about

335
00:16:41.359 --> 00:16:45.080
depression or self harm or suicide rates or anxiety

336
00:16:45.090 --> 00:16:48.169
or self esteem or body image. And we call

337
00:16:48.179 --> 00:16:50.130
all of those things mental well being, but they're

338
00:16:50.140 --> 00:16:53.159
very different things right now. If you sort of

339
00:16:53.169 --> 00:16:54.890
take a step back from all of this and

340
00:16:54.900 --> 00:16:57.479
go all right, if you, if you were trying

341
00:16:57.489 --> 00:16:59.289
to de design, if if none of this research

342
00:16:59.299 --> 00:17:02.010
existed and you were trying to design a scientific

343
00:17:02.020 --> 00:17:05.229
study being the best possible scientist that you could

344
00:17:05.239 --> 00:17:08.520
do, you could be, what do you start off

345
00:17:08.530 --> 00:17:11.140
with? And the ancillary is to start with a

346
00:17:11.150 --> 00:17:17.239
theoretical framework theoretically, why would I expect this thing

347
00:17:17.249 --> 00:17:20.598
whatever this thing is to impact this aspect of

348
00:17:20.608 --> 00:17:23.208
mental health, you're very specific about that you build

349
00:17:23.218 --> 00:17:25.428
a theoretical framework by and large, that's not really

350
00:17:25.438 --> 00:17:28.968
happened in this space, which partly explains why you

351
00:17:28.979 --> 00:17:32.428
get this bouncing around this conflation of different ideas

352
00:17:32.438 --> 00:17:35.229
that we try to make sense of a very

353
00:17:35.239 --> 00:17:39.770
disparate area of research by saying what is the

354
00:17:39.780 --> 00:17:42.880
effect of I mean today, it's what is the

355
00:17:42.890 --> 00:17:45.569
effect of social media on mental health seven years

356
00:17:45.579 --> 00:17:47.150
ago, it's what was the effect of screen time

357
00:17:47.160 --> 00:17:52.189
on mental health. But those two concepts, social media

358
00:17:52.199 --> 00:17:56.209
and mental health cover so many different things. Even

359
00:17:56.219 --> 00:17:59.369
even if you look beyond the platform level, like

360
00:17:59.380 --> 00:18:04.189
if you ask people how they use Instagram, you

361
00:18:04.199 --> 00:18:06.250
get 100 people, you get 100 different answers, right?

362
00:18:06.260 --> 00:18:07.910
Because there's so many different ways in which you

363
00:18:07.920 --> 00:18:12.400
can use it. Some people are active content generators.

364
00:18:12.689 --> 00:18:16.050
Some people don't make videos or posts, but they

365
00:18:16.060 --> 00:18:18.630
comment on other people's, some people don't do any

366
00:18:18.640 --> 00:18:21.780
of that, they just scroll um for their own

367
00:18:21.790 --> 00:18:24.170
interest. Some people use it for work. Some people

368
00:18:24.180 --> 00:18:26.430
use it for hobbies. Some people, you know, all

369
00:18:26.439 --> 00:18:28.310
these and all of those things can happen for

370
00:18:28.319 --> 00:18:30.260
the same person at different times of the day.

371
00:18:30.420 --> 00:18:33.469
You start thinking about one social media platform. In

372
00:18:33.479 --> 00:18:36.099
that way, you start to realize, well, talking about

373
00:18:36.109 --> 00:18:38.800
social media in a general sense, doesn't, you're never

374
00:18:38.810 --> 00:18:41.650
gonna get anywhere, right? Um Because then you start

375
00:18:41.660 --> 00:18:45.709
having to think, well, what's the impact of commenting

376
00:18:45.959 --> 00:18:49.790
on Instagram on mental health versus what's the impact

377
00:18:49.800 --> 00:18:55.209
of using direct messages on mental health? Yeah, you,

378
00:18:55.219 --> 00:18:57.800
you, you really wanna start teasing those and that's

379
00:18:57.810 --> 00:18:59.290
sort of in a way that's an interesting route

380
00:18:59.300 --> 00:19:02.449
to go down because, you know, again, theoretically, if

381
00:19:02.459 --> 00:19:05.489
you found that um some of these mechanisms had

382
00:19:05.500 --> 00:19:08.079
impacts on others, then it's not so much about

383
00:19:08.089 --> 00:19:10.099
banning social media or getting rid of it. It's

384
00:19:10.109 --> 00:19:12.729
about how do we, how do we design it

385
00:19:13.069 --> 00:19:16.540
to maximize well being and minimize the potential harmful

386
00:19:16.550 --> 00:19:18.359
effects and I think that's what a lot of

387
00:19:18.369 --> 00:19:19.869
people are saying in this area, they're not necessarily

388
00:19:19.880 --> 00:19:21.609
saying go away and do that level of research,

389
00:19:21.619 --> 00:19:23.390
but certainly in terms of design, that's what we

390
00:19:23.400 --> 00:19:25.589
need to be thinking about. So that's sort of

391
00:19:25.599 --> 00:19:27.479
the trajectory of the research literature, one of the

392
00:19:27.489 --> 00:19:30.079
things and also the debate as well. One of

393
00:19:30.089 --> 00:19:31.520
the things that I struggle with here though is

394
00:19:31.530 --> 00:19:37.699
that, um, it feels like certainly in the public

395
00:19:37.709 --> 00:19:42.869
debate around screens, it feels like the replication crisis

396
00:19:42.880 --> 00:19:45.939
in psychology never happened. You know, that was a

397
00:19:45.949 --> 00:19:47.849
thing that happened 10 years ago. And we've kind

398
00:19:47.859 --> 00:19:50.599
of forgotten about it because what's happening if you

399
00:19:50.609 --> 00:19:52.489
look at the debate is people are shifting their

400
00:19:52.500 --> 00:19:56.089
viewpoints, their positions all the time. I, I really

401
00:19:56.099 --> 00:19:58.829
struggle with this when I talk to journalists um

402
00:19:58.839 --> 00:20:01.540
about my book about, about this stuff, generally that

403
00:20:01.550 --> 00:20:04.760
maybe the conversation will start by saying, you know,

404
00:20:04.770 --> 00:20:07.380
what's your view on this idea that social media

405
00:20:07.390 --> 00:20:10.109
causes mental health problems? And, you know, we'll have

406
00:20:10.119 --> 00:20:13.079
the sort of conversation that we're having here and

407
00:20:13.089 --> 00:20:14.609
then at some point they'll go. Yeah. But if

408
00:20:14.619 --> 00:20:17.750
you look at the data on suicide rates, you

409
00:20:17.760 --> 00:20:20.010
know, how do you explain that? And then there's

410
00:20:20.020 --> 00:20:21.939
a sort of different conversation there and then later

411
00:20:21.949 --> 00:20:23.890
on, they'll say, OK, yeah. But if you look

412
00:20:23.900 --> 00:20:26.520
at um what happens with, you know, I don't

413
00:20:26.530 --> 00:20:29.459
know, body image and Instagram, they're all different conversations

414
00:20:29.469 --> 00:20:31.910
they're bouncing around and it becomes very difficult to

415
00:20:31.920 --> 00:20:35.969
have that conversation because the goalposts keep shifting. One

416
00:20:35.979 --> 00:20:37.619
of the things that we don't see in the

417
00:20:37.630 --> 00:20:43.969
literature is uh much modern day research adhering to

418
00:20:44.459 --> 00:20:46.369
um all the lessons that we learned from the

419
00:20:46.380 --> 00:20:50.760
replication crisis, basically. So there are very few preregistered

420
00:20:50.770 --> 00:20:54.800
studies in this area. Um There are very few

421
00:20:54.810 --> 00:20:58.359
open data sets. There's, you know, when you ask

422
00:20:58.369 --> 00:21:00.829
people sometimes for their data sets, even though they

423
00:21:00.839 --> 00:21:04.680
say they're available, you don't get them uh which

424
00:21:04.689 --> 00:21:06.839
you know, happens a lot in lots of areas

425
00:21:06.849 --> 00:21:08.959
of psychology and science. Beyond that, you know, we

426
00:21:08.969 --> 00:21:11.239
make these statements to say data is available upon

427
00:21:11.250 --> 00:21:15.199
reasonable request, which is essentially a meaningless statement because

428
00:21:15.209 --> 00:21:18.680
people are still very reluctant to share their data.

429
00:21:18.969 --> 00:21:23.489
Uh MATERIALS aren't shared, uh analysis, scripts aren't shared.

430
00:21:23.640 --> 00:21:27.400
Um HYPOTHESES aren't preregistered, all those sorts of things.

431
00:21:28.069 --> 00:21:31.089
And we know from like 15 years of the

432
00:21:31.099 --> 00:21:34.489
replication crisis. Now, nearly that those things, they're not,

433
00:21:34.500 --> 00:21:37.459
they're not fixed, they don't fix everything, but they

434
00:21:37.469 --> 00:21:39.959
really help if what you're trying to figure out

435
00:21:39.969 --> 00:21:44.609
is actual, more convincing, more reassuring answers to things.

436
00:21:44.619 --> 00:21:46.339
Those are the sorts of things that you need

437
00:21:46.349 --> 00:21:48.780
to do and it's happening in some places. Uh

438
00:21:48.790 --> 00:21:50.560
It's not happening across the board though.

439
00:21:52.319 --> 00:21:55.939
But then with all of those limitations that you

440
00:21:55.949 --> 00:21:59.680
described there, when it comes to these studies surrounding

441
00:21:59.689 --> 00:22:03.839
the relationship or the link between um social media

442
00:22:03.849 --> 00:22:07.219
use or internet use more generally. And mental health,

443
00:22:07.280 --> 00:22:11.780
is there anything concrete we can say about that

444
00:22:11.790 --> 00:22:14.739
link at this point in time or not?

445
00:22:15.410 --> 00:22:19.819
Um So I think the most, so the best

446
00:22:19.829 --> 00:22:22.089
that we can say in a, in a sort

447
00:22:22.099 --> 00:22:24.880
of evidence based way at the moment is that

448
00:22:24.890 --> 00:22:29.469
there are associations between social media and poor mental

449
00:22:29.479 --> 00:22:32.810
health. So social media going up mental health go

450
00:22:32.819 --> 00:22:38.089
down, but they're very, very small associations. Um And

451
00:22:38.099 --> 00:22:41.949
there's this increasing uh and I, I hope it

452
00:22:41.959 --> 00:22:45.500
becomes a more vocal, more noticeable point that's being

453
00:22:45.510 --> 00:22:50.510
made in the literature that um social media is

454
00:22:50.520 --> 00:22:55.000
one part of the puzzle that uh that tells

455
00:22:55.010 --> 00:22:59.729
us something meaningful about mental health. What, what's gone

456
00:22:59.739 --> 00:23:01.709
a bit wrong? I think with certainly with the

457
00:23:01.719 --> 00:23:05.319
public debate is that it feels like everybody's trying

458
00:23:05.329 --> 00:23:07.349
to run around and figure out what is the

459
00:23:07.359 --> 00:23:11.719
main cause of mental health problems. What is the

460
00:23:11.729 --> 00:23:15.250
soul or the route driver? And I don't think

461
00:23:15.260 --> 00:23:18.949
that's the right approach to take because mental health

462
00:23:18.959 --> 00:23:22.780
is really complex. It's always going to be impacted

463
00:23:22.790 --> 00:23:27.530
by a multitude of factors that all interact with

464
00:23:27.540 --> 00:23:33.160
each other in really interesting and sometimes counterintuitive ways.

465
00:23:33.910 --> 00:23:36.170
So, you know, if you take it to its

466
00:23:36.180 --> 00:23:37.829
logical conclusion, if you say, you know, we're trying

467
00:23:37.839 --> 00:23:40.890
to find what's, what's the one big thing that's

468
00:23:40.900 --> 00:23:43.849
affected teenage mental health over the past 10 years

469
00:23:43.859 --> 00:23:46.010
with a view to if we can find that

470
00:23:46.020 --> 00:23:50.680
convincingly, we can remove it from the equation. Um

471
00:23:50.790 --> 00:23:53.410
You will not magically fix anything. If you do

472
00:23:53.420 --> 00:23:56.380
that, you will not create a utopian state where

473
00:23:56.390 --> 00:23:59.839
teenage mental health is suddenly fine because again, that's

474
00:23:59.849 --> 00:24:02.859
not how mental health works. Mental health is complicated.

475
00:24:02.869 --> 00:24:05.719
It's affected by a lot of interacting factors. It's

476
00:24:05.729 --> 00:24:09.339
affected by an ecosystem of factors. Social media will

477
00:24:09.349 --> 00:24:11.140
be a part of that. Of course, it will,

478
00:24:11.349 --> 00:24:13.000
you know, it has to be, be weird not

479
00:24:13.010 --> 00:24:14.739
to be right that it would be a very

480
00:24:14.750 --> 00:24:17.829
strange thing if that had no impact whatsoever. But

481
00:24:17.839 --> 00:24:19.910
increasingly what we're seeing in, in some of the

482
00:24:19.920 --> 00:24:22.170
best research out there is that yes, it's a

483
00:24:22.180 --> 00:24:24.689
factor. It's a very, very small factor and actually

484
00:24:24.699 --> 00:24:27.150
there are other things which are not the main

485
00:24:27.160 --> 00:24:29.569
cause the, the root cause, but they are bigger

486
00:24:29.579 --> 00:24:31.550
effects and maybe we should be looking at those

487
00:24:31.560 --> 00:24:32.400
sorts of things first.

488
00:24:33.589 --> 00:24:38.150
But then of course, we've been focusing here mostly

489
00:24:38.160 --> 00:24:42.569
on the supposed uh negative effects of social media.

490
00:24:42.579 --> 00:24:46.160
But aren't there also positive effects?

491
00:24:46.800 --> 00:24:49.020
Yeah. And this is, it's a great point because

492
00:24:49.030 --> 00:24:51.680
this is absolutely where some of that complexity comes

493
00:24:51.689 --> 00:24:55.619
into it. Right. And, and again, I've had a

494
00:24:55.630 --> 00:24:58.920
lot of conversations with people ironically online on, on

495
00:24:58.930 --> 00:25:02.069
social media where it genuinely seems to be the

496
00:25:02.079 --> 00:25:04.680
case that the view that they're taking is that

497
00:25:04.689 --> 00:25:07.579
screens are inherently harmful and bad. And that the

498
00:25:07.589 --> 00:25:11.430
they, they're only that that, you know, you pick

499
00:25:11.439 --> 00:25:13.849
up a phone and you log on to the

500
00:25:13.859 --> 00:25:17.209
internet and then you're instantly bombarded with bad stuff.

501
00:25:18.319 --> 00:25:22.130
Um, THAT'S not my experience of being on the

502
00:25:22.140 --> 00:25:23.800
internet or being on social media or being on

503
00:25:23.810 --> 00:25:26.170
phones. It's not the experience of a lot of,

504
00:25:26.270 --> 00:25:28.000
you know, if you actually ask teenage about it,

505
00:25:28.010 --> 00:25:32.630
that's not their experience either. It's way more complicated

506
00:25:32.640 --> 00:25:37.069
than that. So and again, this is the thing

507
00:25:37.079 --> 00:25:39.260
right that we miss in this, that some, there

508
00:25:39.270 --> 00:25:41.810
are some aspects of social media, use of smartphone,

509
00:25:41.819 --> 00:25:43.469
use of going on the internet that can be

510
00:25:43.479 --> 00:25:46.939
really positive and supportive for mental health. There are

511
00:25:46.949 --> 00:25:49.900
some aspects that can be really detrimental to it

512
00:25:50.270 --> 00:25:53.020
and critically you, you as an individual can experience

513
00:25:53.030 --> 00:25:58.160
both. So there's a really interesting line of research

514
00:25:58.170 --> 00:26:00.560
that's been kind of brewing over the past six

515
00:26:00.569 --> 00:26:04.349
years or so. That suggests that WW well, first

516
00:26:04.359 --> 00:26:07.140
off our, our online lives mirror our offline lives

517
00:26:07.150 --> 00:26:09.280
in really important ways. It's not to say that

518
00:26:09.290 --> 00:26:12.199
they're exactly the same thing, but there are important

519
00:26:12.209 --> 00:26:15.459
differences as well. But what that also means is

520
00:26:15.469 --> 00:26:18.599
that sometimes you can see things that are going

521
00:26:18.609 --> 00:26:21.650
on or vulnerabilities that you have in your offline

522
00:26:21.660 --> 00:26:25.290
life can predict the kinds of risks that you

523
00:26:25.300 --> 00:26:30.520
come across online, particularly for, for teenagers. Now, so

524
00:26:30.530 --> 00:26:32.400
one example of this might be, you know, if

525
00:26:32.410 --> 00:26:37.040
you, if you're feeling depressed and lonely, you might

526
00:26:37.050 --> 00:26:40.439
go online on social media or wherever, to try

527
00:26:40.449 --> 00:26:43.420
and connect with people uh and to try and

528
00:26:43.630 --> 00:26:46.939
seek out information about how to help with, with

529
00:26:46.949 --> 00:26:53.589
depression. No, that could either be successful or disastrous,

530
00:26:53.599 --> 00:26:56.489
right? Depending on, you know, the, the routes that

531
00:26:56.500 --> 00:26:57.869
you can take and, and this is what we

532
00:26:57.880 --> 00:26:59.510
see in the research literature, right? So, so in

533
00:26:59.520 --> 00:27:02.869
some cases, you know, you'll have a really successful

534
00:27:02.880 --> 00:27:06.300
outcome day, you know, you'll find uh a really

535
00:27:06.310 --> 00:27:09.229
supportive group of people or you'll find a more

536
00:27:09.239 --> 00:27:12.310
kind of formalized support group or you'll find resources

537
00:27:12.319 --> 00:27:17.729
that really help you. In other cases, you'll find

538
00:27:17.739 --> 00:27:21.000
stuff that is really harmful to your mental health.

539
00:27:21.410 --> 00:27:23.750
So maybe a better example here is if, if

540
00:27:23.760 --> 00:27:25.660
you're um you know, if you're struggling with an

541
00:27:25.670 --> 00:27:28.439
eating disorder and you go online to try and

542
00:27:28.449 --> 00:27:32.199
seek out help. Um AND you find a forum

543
00:27:32.209 --> 00:27:35.050
of people with who who are also dealing with

544
00:27:35.140 --> 00:27:38.130
uh with eating disorders. That could be a very

545
00:27:38.140 --> 00:27:40.410
positive forum where people are trying to help each

546
00:27:40.420 --> 00:27:43.660
other. There are also horrible situations out there where

547
00:27:43.670 --> 00:27:46.680
there are people setting up spaces to, you almost

548
00:27:46.689 --> 00:27:51.829
encourage people to, to go further. Um So, you

549
00:27:51.839 --> 00:27:54.979
know, which one you find has a drastic effect

550
00:27:54.989 --> 00:28:00.699
on your mental health? Um There um whether, which

551
00:28:00.709 --> 00:28:02.390
route you go down, whether or not you find

552
00:28:02.400 --> 00:28:04.969
the, the, the supportive stuff or the harmful stuff,

553
00:28:04.979 --> 00:28:06.839
you know, it certainly has nothing to do with

554
00:28:06.849 --> 00:28:09.459
social media or screens, right? It has everything to

555
00:28:09.469 --> 00:28:12.410
do with other things. It has, uh, you know,

556
00:28:12.420 --> 00:28:15.619
things that impact that are, um, do you have

557
00:28:15.630 --> 00:28:18.369
a support network around you? Do you have people

558
00:28:18.489 --> 00:28:20.109
in your life that you trust and that you

559
00:28:20.119 --> 00:28:23.130
can talk to and talk things through with a,

560
00:28:23.140 --> 00:28:25.109
so you can get ideas and advice on where

561
00:28:25.119 --> 00:28:27.619
the right places are to go. But b if

562
00:28:27.630 --> 00:28:30.020
you find yourself on the wrong route, on the

563
00:28:30.030 --> 00:28:33.040
harmful route, you've got people to talk to about

564
00:28:33.050 --> 00:28:34.140
it. And I think that's one of the other

565
00:28:34.150 --> 00:28:36.000
things that we don't maybe acknowledge as much in

566
00:28:36.010 --> 00:28:38.410
the literature. You know, that's certainly a, it's certainly

567
00:28:38.420 --> 00:28:40.770
a signal in the research literature that if you

568
00:28:40.780 --> 00:28:44.609
come across or if teenagers come across uncomfortable or

569
00:28:45.040 --> 00:28:48.770
what they perceive to be harmful content online j

570
00:28:48.780 --> 00:28:50.750
because they've come across it, it is not necessarily

571
00:28:50.760 --> 00:28:54.089
guaranteed that they will be harmed by it. Again,

572
00:28:54.099 --> 00:28:56.689
other things will impact that so you can dampen

573
00:28:56.699 --> 00:28:59.569
the effects of that. Um OO of, of that

574
00:28:59.579 --> 00:29:03.810
eventuality again, with, through support networks through open communication.

575
00:29:03.819 --> 00:29:07.099
Can they talk to their parents or caregivers, friends,

576
00:29:07.109 --> 00:29:10.209
family about this and work it through and get

577
00:29:10.219 --> 00:29:13.800
help that sort of way? Are they being trained

578
00:29:13.810 --> 00:29:17.689
in digital literacy and resilience skills beforehand? So that

579
00:29:17.699 --> 00:29:22.959
they know what a an identity theft scam looks

580
00:29:22.969 --> 00:29:27.989
like so that they don't get defrauded? Um AS

581
00:29:28.000 --> 00:29:30.949
an example that they know what sorts of sites

582
00:29:30.959 --> 00:29:33.310
are appropriate and inappropriate for them to navigate and

583
00:29:33.319 --> 00:29:35.849
why and all of those sorts of things. So

584
00:29:35.859 --> 00:29:37.910
what you see then is this sort of you

585
00:29:37.920 --> 00:29:39.290
start thinking about it in that way and it

586
00:29:39.300 --> 00:29:43.880
immediately becomes more complicated, right? That of course, social

587
00:29:43.890 --> 00:29:45.579
media can't be the soul or the root cause

588
00:29:45.589 --> 00:29:47.900
of it in the, in the way that anything

589
00:29:47.910 --> 00:29:52.449
can that all of these things interact. So one

590
00:29:52.459 --> 00:29:54.189
of the, one of the kind of concerns for

591
00:29:54.199 --> 00:29:56.859
me then is that if you, if you partly

592
00:29:56.869 --> 00:29:59.040
what's happened in the debate is that we've set

593
00:29:59.050 --> 00:30:01.880
off by going, you know, screen slash social media

594
00:30:01.890 --> 00:30:04.369
are the bad thing, right? And then a bunch

595
00:30:04.380 --> 00:30:05.880
of scientists come along and said, we don't think

596
00:30:05.890 --> 00:30:07.660
that's the case. That's not what the evidence sets.

597
00:30:08.000 --> 00:30:10.459
What that immediately sets up is a debate where

598
00:30:10.469 --> 00:30:12.880
you have a back and forth where one side

599
00:30:12.890 --> 00:30:14.560
then goes. OK. Well, if it's not social media,

600
00:30:14.569 --> 00:30:17.599
what is it? And then people start suggesting loads

601
00:30:17.609 --> 00:30:19.569
of things and we go through that list one

602
00:30:19.579 --> 00:30:22.089
by one. Well, maybe it was the global financial

603
00:30:22.099 --> 00:30:24.760
crisis. Well, no, it's not that because of this

604
00:30:24.770 --> 00:30:28.219
reasonable argument. Well, maybe it was something else that

605
00:30:28.229 --> 00:30:30.270
happened. Well, it's not that because you kind of

606
00:30:30.280 --> 00:30:32.560
go through the list one by one crossing them

607
00:30:32.569 --> 00:30:34.310
out. This isn't the main cause this isn't the

608
00:30:34.319 --> 00:30:37.619
main cause you kind of don't you sort of

609
00:30:37.630 --> 00:30:39.189
leave social media to the end. Right. You go.

610
00:30:39.199 --> 00:30:41.510
Well, this is the only thing that explains everything,

611
00:30:41.520 --> 00:30:43.089
right. It's the only thing that happened on a

612
00:30:43.099 --> 00:30:46.819
global scale and it's just the wrong way of

613
00:30:46.829 --> 00:30:50.000
looking at it because that list that you've created

614
00:30:50.010 --> 00:30:53.199
all of those things matter in some way, shape

615
00:30:53.209 --> 00:30:56.160
or form, they will all interact and matter with,

616
00:30:56.800 --> 00:30:58.839
interact with each other and matter and have an

617
00:30:58.849 --> 00:31:02.430
effect on uh on mental health or educational outcomes,

618
00:31:02.439 --> 00:31:04.800
whatever it is that you're interested in and we've

619
00:31:04.810 --> 00:31:08.790
just not developed that level of complexity yet to

620
00:31:08.800 --> 00:31:13.569
say, look, this thing has negative impacts. It also

621
00:31:13.579 --> 00:31:16.239
has some positives. So how do we, how do

622
00:31:16.250 --> 00:31:19.310
we balance that? How do we understand that? And

623
00:31:19.319 --> 00:31:21.790
I think that's why for me the, the, the

624
00:31:21.800 --> 00:31:25.910
general framing of the debate, which is, is social

625
00:31:25.920 --> 00:31:30.579
media causing mental ill health or is x whatever

626
00:31:30.589 --> 00:31:32.989
thing you're interested in causing mental health, ill health,

627
00:31:33.150 --> 00:31:36.930
it's just the wrong way to frame it. So,

628
00:31:36.939 --> 00:31:38.680
uh and again, I can't take credit for this.

629
00:31:38.689 --> 00:31:40.900
I think it's uh work by Candice Rodgers and

630
00:31:40.910 --> 00:31:43.420
um and uh M and Jensen that this idea

631
00:31:43.430 --> 00:31:47.420
that a better question to ask here is why

632
00:31:47.670 --> 00:31:51.310
do some people do some teenagers, do some adults?

633
00:31:51.400 --> 00:31:55.969
Why do some people really struggle online and why

634
00:31:55.979 --> 00:31:59.410
do other people in seemingly similar situations? Why do

635
00:31:59.420 --> 00:32:04.560
they thrive and how do we understand those pathways

636
00:32:04.569 --> 00:32:07.050
to those different routes? And if you can understand

637
00:32:07.060 --> 00:32:08.849
that, then that's when you can actually make a

638
00:32:08.859 --> 00:32:11.319
difference, right? Because you can start to see well,

639
00:32:11.329 --> 00:32:15.380
these particular aspects of the way that we navigate

640
00:32:15.390 --> 00:32:16.920
the internet or the way that we use social

641
00:32:16.930 --> 00:32:21.550
media don't work, they need fixing. Um And it's

642
00:32:21.560 --> 00:32:24.989
much harder for tech companies to ignore that sort

643
00:32:25.000 --> 00:32:27.989
of line of questioning and line of complaint. It's,

644
00:32:28.000 --> 00:32:29.339
I think it's very easy for them when you

645
00:32:29.349 --> 00:32:32.430
come along and say social media is melting everybody's

646
00:32:32.439 --> 00:32:37.229
brains. Um It's just such a, a, an un

647
00:32:37.239 --> 00:32:39.239
evidenced thing to say that it's very easy for

648
00:32:39.250 --> 00:32:41.229
a tech company to say, you know, that's, that's

649
00:32:41.239 --> 00:32:42.390
a silly thing to say. We're not going to

650
00:32:42.400 --> 00:32:44.959
engage with that and that's what you don't want

651
00:32:44.969 --> 00:32:46.010
them to do. You don't want to dis them

652
00:32:46.020 --> 00:32:48.369
to disengage with the conversation cos then nothing's gonna

653
00:32:48.380 --> 00:32:48.969
change, right?

654
00:32:50.390 --> 00:32:53.709
So another kind of claims that people have been

655
00:32:53.719 --> 00:32:58.359
making are related to a supposed association between uh

656
00:32:58.369 --> 00:33:03.140
screen based technologies and sleep. And they say that

657
00:33:03.349 --> 00:33:07.250
uh using screen based based technologies can ruin our

658
00:33:07.260 --> 00:33:11.729
sleep, our sleeping patterns, our circadian rhythm, stuff like

659
00:33:11.739 --> 00:33:15.489
that. Uh What can we tell about that from

660
00:33:15.500 --> 00:33:17.969
the best research we have on it?

661
00:33:19.569 --> 00:33:21.229
I, well, it's a good question. I think, I

662
00:33:21.239 --> 00:33:23.719
don't think we have much good research on this

663
00:33:23.790 --> 00:33:26.469
at the minute. So, um that's kind of part

664
00:33:26.479 --> 00:33:29.619
of the problem I think. Um So there's, there's

665
00:33:29.630 --> 00:33:31.459
a couple of things going on here. One is

666
00:33:31.469 --> 00:33:34.849
that there's been this consistent thread again in public

667
00:33:34.859 --> 00:33:38.670
debates about screens over the, the past at least

668
00:33:38.680 --> 00:33:42.189
10 years or so that, um, blue light, so

669
00:33:42.199 --> 00:33:45.979
blue length, uh blue wavelength, um in the light

670
00:33:45.989 --> 00:33:49.790
spectrum, um impact and interrupt sleep. And there's a

671
00:33:49.800 --> 00:33:53.229
biological mechanism going on there. And I should say

672
00:33:53.239 --> 00:33:55.569
that's absolutely true. You know, that's, that's not, I

673
00:33:55.579 --> 00:33:57.680
don't think that's controversial in any way. It's absolutely

674
00:33:57.689 --> 00:34:00.219
the case that there's this biological mechanism that blue

675
00:34:00.229 --> 00:34:03.060
light, um, interacts with sleep where it gets a

676
00:34:03.069 --> 00:34:06.270
bit more shaky is that, um, screens emit blue

677
00:34:06.280 --> 00:34:11.010
light, therefore, screens impact sleep in a biological sense.

678
00:34:12.780 --> 00:34:14.918
Um, I went down a bit of a rabbit

679
00:34:14.929 --> 00:34:18.040
hole with, with this question, um, in the book

680
00:34:18.050 --> 00:34:21.668
because, um, you know, I, I sort of taken

681
00:34:21.679 --> 00:34:23.360
the view that, you know, I'm, I'm, I'm generally

682
00:34:23.370 --> 00:34:26.850
quite skeptical about many claims about the screens, but

683
00:34:26.860 --> 00:34:29.010
I'm willing to change my mind if the evidence

684
00:34:29.020 --> 00:34:32.179
makes sense. And I, and I fully expected at

685
00:34:32.188 --> 00:34:34.830
the, at some point in writing, unlocked that I'd

686
00:34:34.840 --> 00:34:36.958
have a chapter where I go. Here's this thing

687
00:34:36.969 --> 00:34:39.370
that people say about screens that I don't think

688
00:34:39.379 --> 00:34:40.958
is right. And then we look at the evidence

689
00:34:40.969 --> 00:34:41.850
and that by the end of it I go,

690
00:34:41.860 --> 00:34:44.780
oh, actually, yeah, that makes sense. That's good. But,

691
00:34:44.790 --> 00:34:46.100
you know, now we can be more evidence based

692
00:34:46.110 --> 00:34:47.899
on it and it didn't really happen and it

693
00:34:47.909 --> 00:34:51.159
certainly didn't happen with sleep. Um, BUT there's there's

694
00:34:51.168 --> 00:34:53.780
this weird thing going on here. Right. So, um

695
00:34:54.149 --> 00:34:57.850
screens, screens do have an impact on sleep, but

696
00:34:57.860 --> 00:35:02.600
we don't need to resort to biological discussions, biological

697
00:35:02.610 --> 00:35:04.409
mechanisms to relate to that. So if you look

698
00:35:04.419 --> 00:35:06.669
at the, the, the, the, the research, the data

699
00:35:06.679 --> 00:35:09.500
on, on blue light from screens and sleep, I

700
00:35:09.510 --> 00:35:11.750
don't think there's anything there. You know, if you're

701
00:35:11.760 --> 00:35:14.030
worried about blue light from your phones at night,

702
00:35:14.040 --> 00:35:18.389
don't be um, it's not having a meaningful impact

703
00:35:18.399 --> 00:35:20.229
as far as I can tell from the literature

704
00:35:20.959 --> 00:35:23.350
where screens maybe have more of an impact is

705
00:35:23.360 --> 00:35:27.090
in two ways, two interrelated ways. One is literally,

706
00:35:27.100 --> 00:35:28.989
you know, if you're not thinking about what you're

707
00:35:29.000 --> 00:35:30.679
doing on your phone and you're just spending loads

708
00:35:30.689 --> 00:35:34.239
of time in the evening doing something mindlessly, you're

709
00:35:34.250 --> 00:35:36.669
on the risk of doing it too long. And

710
00:35:36.679 --> 00:35:38.199
then the, the impact on sleep is you have

711
00:35:38.209 --> 00:35:41.280
less sleep, right. So that's actually one important important

712
00:35:41.290 --> 00:35:43.510
thing for teenagers in particular because we see a

713
00:35:43.520 --> 00:35:46.209
lot of pressures on teenage sleep when they get

714
00:35:46.219 --> 00:35:50.669
to uh being a teenager insofar as you know,

715
00:35:50.679 --> 00:35:52.540
if you look at school start times they tend

716
00:35:52.550 --> 00:35:54.939
to get um earlier in the morning. So there's

717
00:35:54.949 --> 00:35:57.530
that impact at the end of sleep and bedtime

718
00:35:57.540 --> 00:36:00.810
for various reasons, get later. So parents become more

719
00:36:00.820 --> 00:36:05.239
relaxed about enforcing uh sleep rules. Kids have more

720
00:36:05.250 --> 00:36:09.639
autonomy to do stuff and kids have phones. So

721
00:36:09.649 --> 00:36:11.419
yeah, they're more likely to go to bed early

722
00:36:11.429 --> 00:36:13.530
later and get up earlier. And that's a really

723
00:36:13.540 --> 00:36:17.520
big thing because teenagers need, need sleep. But that's

724
00:36:17.530 --> 00:36:20.739
been true forever. Right. That, that story's been true

725
00:36:20.750 --> 00:36:23.939
forever. I guess the key question there is have

726
00:36:23.949 --> 00:36:30.139
phones exacerbated the issue and maybe, yes, I think

727
00:36:30.149 --> 00:36:32.610
there. But, you know, there's, again, there's really mixed

728
00:36:32.620 --> 00:36:35.530
signals in the literature. Um, ONE of the most

729
00:36:35.540 --> 00:36:38.610
interesting lines of research that I saw there is,

730
00:36:38.620 --> 00:36:42.639
and again, we need to ask kids like teenage,

731
00:36:42.649 --> 00:36:45.239
we're talking about teenagers a lot in this debate

732
00:36:45.250 --> 00:36:47.610
and they are, you know, people are talking to

733
00:36:47.620 --> 00:36:50.060
them but not many. Uh AND actually they're the

734
00:36:50.070 --> 00:36:51.330
ones that we need to talk to, to figure

735
00:36:51.340 --> 00:36:53.600
out what's going on. You actually talk to teenagers

736
00:36:53.610 --> 00:36:56.300
about what they're doing and what's happened over time

737
00:36:56.310 --> 00:36:58.840
is that there's this set of social norms that

738
00:36:58.850 --> 00:37:02.520
have developed that there's an expectation that you are

739
00:37:02.530 --> 00:37:08.860
available to talk to your schoolmates classmates online late

740
00:37:08.870 --> 00:37:13.270
ish at night and they're aware of this, they're

741
00:37:13.280 --> 00:37:15.649
aware that that's a problem, right? They wanna go

742
00:37:15.659 --> 00:37:19.850
to sleep, but it's not because screens are addictive

743
00:37:19.860 --> 00:37:23.169
or blue light is sending out anti sleep rays

744
00:37:23.179 --> 00:37:25.729
or anything like that. It's because there's a social

745
00:37:25.739 --> 00:37:28.360
expectation that they should be around. If a if

746
00:37:28.370 --> 00:37:30.590
a message comes through that they should be able

747
00:37:30.600 --> 00:37:32.330
to respond to it straight away and if they

748
00:37:32.340 --> 00:37:35.479
don't, that has consequences basically in the playground the

749
00:37:35.489 --> 00:37:37.239
day after. So if you're not on your phone

750
00:37:37.250 --> 00:37:39.959
at night when the big drama's kicking off and

751
00:37:39.969 --> 00:37:41.739
you don't know anything about it the day after

752
00:37:41.750 --> 00:37:45.250
you're immediately isolated from that conversation and that can

753
00:37:45.260 --> 00:37:47.370
cause knock on problems. So, teenagers are very kind

754
00:37:47.379 --> 00:37:51.399
of wary of that. Um, NOT all teenagers though.

755
00:37:51.409 --> 00:37:53.120
So, you know, you, you and you talk to

756
00:37:53.129 --> 00:37:54.350
them and some of them are like, you know,

757
00:37:54.360 --> 00:37:56.830
what, what's the, if, if there's one word that

758
00:37:56.840 --> 00:37:59.800
you could associate with a teenager, it's rebel. Um,

759
00:37:59.810 --> 00:38:02.189
AND there are some teenagers that rebel against this.

760
00:38:02.199 --> 00:38:04.000
They're like, you know, I'm, I, I don't want

761
00:38:04.010 --> 00:38:05.310
to be involved in that cos I wanna go

762
00:38:05.320 --> 00:38:07.820
to sleep. Uh, AND, you know, there's all sorts

763
00:38:07.830 --> 00:38:09.649
of things going on there. But how do you

764
00:38:09.659 --> 00:38:11.520
fix that? Oh, it's nothing. I don't think it's

765
00:38:11.530 --> 00:38:13.870
anything to do with taking away screens or banning

766
00:38:13.879 --> 00:38:15.600
social media or anything like that. You need to

767
00:38:15.610 --> 00:38:19.280
try and change social norms. Again, it's through digital

768
00:38:19.290 --> 00:38:22.389
literacy skills that you talk to kids and, and

769
00:38:22.399 --> 00:38:25.600
try and establish these appropriate norms around. You don't

770
00:38:25.610 --> 00:38:28.350
need to be having those conversations at 10 o'clock

771
00:38:28.360 --> 00:38:31.090
at night. Um, THEY can wait until the morning

772
00:38:31.100 --> 00:38:33.929
or they can happen earlier in the day. Uh,

773
00:38:33.939 --> 00:38:36.649
YOU know, instilling values around the importance of sleep

774
00:38:36.659 --> 00:38:38.409
and, and stuff like that and how to use

775
00:38:38.419 --> 00:38:41.570
technology in a more mindful way that aligns with

776
00:38:41.580 --> 00:38:44.610
the goals that we have will help just as

777
00:38:44.620 --> 00:38:46.889
much. Well, it will help more than I think

778
00:38:46.899 --> 00:38:51.729
bands would because they provide long term sustainable skills

779
00:38:51.739 --> 00:38:56.250
for kids. The other thing around screens and sleep

780
00:38:56.260 --> 00:38:58.550
is so I talked to a, a sleep expert

781
00:38:58.560 --> 00:38:59.669
from a book and one of the things that

782
00:38:59.679 --> 00:39:02.250
really stuck with me from what he said was

783
00:39:02.260 --> 00:39:03.800
that, you know, he sees people come into the

784
00:39:03.810 --> 00:39:07.229
clinic and they've got sleep problems and they'll say

785
00:39:07.239 --> 00:39:09.590
stuff like, you know, I, I go up to

786
00:39:09.600 --> 00:39:12.409
bed at night and I'm really tired, I'm absolutely

787
00:39:12.419 --> 00:39:14.629
shattered. I go into the bedroom and as soon

788
00:39:14.639 --> 00:39:17.209
as I see the bed, something changes, you know,

789
00:39:17.219 --> 00:39:20.350
I'm wide awake and he was like that, that's

790
00:39:20.360 --> 00:39:22.310
a real red flag for me because what that's

791
00:39:22.320 --> 00:39:25.830
saying is that you've developed a really negative sleep

792
00:39:25.840 --> 00:39:28.739
association with your bed. So if you look at

793
00:39:28.750 --> 00:39:31.149
like all of the sleep hygiene literature, it says,

794
00:39:31.159 --> 00:39:32.879
you know, basically the only thing, the only things

795
00:39:32.889 --> 00:39:34.760
that you should be doing in bed are sleeping

796
00:39:34.770 --> 00:39:38.320
and having sex, um, anything else try and not

797
00:39:38.330 --> 00:39:40.270
do it in bed because there's always a risk

798
00:39:40.280 --> 00:39:43.360
that you develop in association with that thing. Um

799
00:39:43.370 --> 00:39:45.110
So the classic one for us then is we

800
00:39:45.120 --> 00:39:46.729
go to bed at night and we get on

801
00:39:46.739 --> 00:39:49.449
our phones. Now, some of us and I think

802
00:39:49.459 --> 00:39:52.949
a lot of us do that to help get

803
00:39:52.959 --> 00:39:55.570
to sleep. It's a way for us to like

804
00:39:55.580 --> 00:39:59.360
unwind and, and break the day and just sort

805
00:39:59.370 --> 00:40:00.729
of wind down a little bit, you know, and

806
00:40:00.739 --> 00:40:03.010
I've talked to people, they, they'll put a podcast

807
00:40:03.020 --> 00:40:05.129
on or a video on not to watch, but

808
00:40:05.139 --> 00:40:08.320
just to have like background white noise and that's

809
00:40:08.330 --> 00:40:10.139
fine. You know, if that helps you get to

810
00:40:10.149 --> 00:40:12.530
sleep, that's not a problem at all. The thing

811
00:40:12.540 --> 00:40:13.800
is you need to be a bit more mindful

812
00:40:13.810 --> 00:40:16.520
of this in, in, in so far as if

813
00:40:16.530 --> 00:40:18.699
that thing that you're doing on your phone starts

814
00:40:18.709 --> 00:40:22.100
to become more cognitively arousing more stressful, you know,

815
00:40:22.110 --> 00:40:24.500
kicking in a bit of adrenaline or whatever starts

816
00:40:24.510 --> 00:40:25.689
to stress you out and make you feel more

817
00:40:25.699 --> 00:40:29.100
awake. Obviously, that's not gonna help you sleep. So

818
00:40:29.110 --> 00:40:31.300
if you can catch yourself doing that, that's the

819
00:40:31.310 --> 00:40:35.260
first stage to doing something about it. Um So

820
00:40:35.270 --> 00:40:37.570
yeah, there's this weird relationship in the research literature

821
00:40:37.580 --> 00:40:39.820
around screens and sleep in the, you know, what

822
00:40:39.830 --> 00:40:42.239
we've not talked about at all is, is that

823
00:40:42.250 --> 00:40:44.800
biological component? Cos I don't think we need to,

824
00:40:45.229 --> 00:40:49.070
but we very often, um you know, all of

825
00:40:49.080 --> 00:40:51.540
these conversations around screens, we very often feel the

826
00:40:51.550 --> 00:40:55.350
need to revert to biological descriptions and explanations and

827
00:40:55.360 --> 00:40:57.810
that's true of the blue light and, and uh

828
00:40:57.820 --> 00:41:00.399
sleep thing. But I think it's also true just

829
00:41:00.409 --> 00:41:02.489
generally how we talk about screens that, you know,

830
00:41:02.500 --> 00:41:05.909
we say that they're addictive, that they're biologically addictive

831
00:41:05.919 --> 00:41:08.649
that they, you know, they're a, they're a dopamine

832
00:41:08.659 --> 00:41:11.229
machine or they increase dopamine and then that means

833
00:41:11.239 --> 00:41:15.199
that they're super bad when you can explain the

834
00:41:15.209 --> 00:41:18.530
way that people interact with digital technology and the

835
00:41:18.540 --> 00:41:21.360
good and the bad without reverting to those. And

836
00:41:21.370 --> 00:41:24.550
I think in reverting to those, you create problematic

837
00:41:24.560 --> 00:41:25.969
frameworks of talking about them.

838
00:41:27.290 --> 00:41:29.780
So let me ask you now about another kind

839
00:41:29.790 --> 00:41:32.709
of worry that people express and in this particular

840
00:41:32.719 --> 00:41:37.830
case specifically, I would guess that many educators would

841
00:41:37.840 --> 00:41:41.590
worry about this. So what about the link or

842
00:41:41.600 --> 00:41:45.550
the supposed link between screen based technology and our

843
00:41:45.560 --> 00:41:48.610
ability to focus in the sense that it would

844
00:41:48.620 --> 00:41:52.090
disrupt our ability to focus? I mean, does the

845
00:41:52.100 --> 00:41:55.229
literature really support that idea or not?

846
00:41:56.449 --> 00:41:59.139
I say these are all great questions and I

847
00:41:59.149 --> 00:42:01.159
think the very frustrating thing probably for you and,

848
00:42:01.169 --> 00:42:03.360
and, and, and the listeners is that very often

849
00:42:03.370 --> 00:42:06.600
the answer to those questions is uh we don't

850
00:42:06.610 --> 00:42:09.989
really know or it's mixed. So again, like there's

851
00:42:10.000 --> 00:42:13.870
a, there's a disconnect between the big general conversations

852
00:42:13.879 --> 00:42:15.540
that we're having about this stuff and what the

853
00:42:15.550 --> 00:42:17.969
actual science could say very often, what you find

854
00:42:17.979 --> 00:42:21.360
is that the big conversations are very certain, they're

855
00:42:21.370 --> 00:42:25.219
very clear and confident that there is an association.

856
00:42:25.919 --> 00:42:28.300
And then actually, when you drill into the research,

857
00:42:28.600 --> 00:42:32.020
you find a bit of a mess um and,

858
00:42:32.030 --> 00:42:34.000
and attention and, and focus I think is a

859
00:42:34.010 --> 00:42:36.850
really good example of that. So and it's the

860
00:42:36.860 --> 00:42:39.120
same issues over and over again. Um And I

861
00:42:39.129 --> 00:42:41.110
know I'm gonna sound like a bit of a

862
00:42:41.129 --> 00:42:45.060
broken record, broken nerdy record about this. But um

863
00:42:45.340 --> 00:42:48.139
everything's really poorly defined in the research literature. It

864
00:42:48.149 --> 00:42:49.580
makes it hard to figure out what it is

865
00:42:49.590 --> 00:42:52.340
precisely that people are worrying about. So we've seen

866
00:42:52.350 --> 00:42:54.419
loads of claims over the past few years that

867
00:42:54.429 --> 00:42:59.429
um social media screens are a driver of poorer

868
00:42:59.439 --> 00:43:05.139
attention spans. Uh And then sometimes that's framed as

869
00:43:05.590 --> 00:43:09.280
distract ability, like increased distractibility, a decrease in ability

870
00:43:09.290 --> 00:43:12.050
to concentrate or a decrease in ability to focus.

871
00:43:12.120 --> 00:43:14.219
Now, actually, all of those things are very different

872
00:43:14.229 --> 00:43:17.350
things. Certainly from a scientific, from a cognitive psychological

873
00:43:17.360 --> 00:43:21.620
perspective, they're very different things. Um Attention le let's

874
00:43:21.629 --> 00:43:23.840
not go into attention because it's, it's a bit

875
00:43:23.850 --> 00:43:25.959
of a minefield, but, you know, attention is not

876
00:43:25.969 --> 00:43:29.060
one singular thing. There's all sorts of things that

877
00:43:29.070 --> 00:43:31.389
you can consider to be different types of attention.

878
00:43:31.399 --> 00:43:34.050
They all have different mechanisms in the brain. But

879
00:43:34.060 --> 00:43:36.689
yeah, I think when really, when we're talking about

880
00:43:36.699 --> 00:43:39.820
attention spans and things like that, we're talking about

881
00:43:39.830 --> 00:43:43.469
um not being able to concentrate on stuff. And

882
00:43:43.479 --> 00:43:45.010
I think again, that's one of those things where

883
00:43:45.020 --> 00:43:47.570
we all feel that right? We all feel as

884
00:43:47.580 --> 00:43:51.010
though this has got worse that it's harder to

885
00:43:51.020 --> 00:43:55.260
concentrate on things nowadays. Um Why is that the

886
00:43:55.270 --> 00:43:58.699
case? Uh Well, one reason could be that um

887
00:43:58.909 --> 00:44:03.669
screens are emitting distraction rays in some way, shape

888
00:44:03.679 --> 00:44:05.179
or form or the way that we set these

889
00:44:05.189 --> 00:44:07.939
things up means that it's eroding that form of

890
00:44:07.949 --> 00:44:11.010
attention. Uh, AND that's the thing that people worry

891
00:44:11.020 --> 00:44:14.800
about. Right. Um, ANOTHER thing could be that this

892
00:44:14.810 --> 00:44:16.780
is what always happens to everybody when they get

893
00:44:16.790 --> 00:44:19.320
older. Right. You know, I could probably focus on

894
00:44:19.330 --> 00:44:22.459
things better when I was younger because I had

895
00:44:22.469 --> 00:44:25.300
fewer things to worry about. Um, BUT if you

896
00:44:25.310 --> 00:44:28.540
asked 18 year old me, I'd say that I

897
00:44:28.550 --> 00:44:30.520
really struggled to focus on things like writing an

898
00:44:30.530 --> 00:44:33.219
essay cos it could be a bit boring sometimes.

899
00:44:33.620 --> 00:44:36.110
Um So it's hard to tease those things apart.

900
00:44:36.120 --> 00:44:37.159
And I did, you know, I don't, I'm not

901
00:44:37.169 --> 00:44:39.320
saying that we should be really dismissive here or

902
00:44:39.330 --> 00:44:41.590
facetious because, you know, if there are real problems

903
00:44:41.600 --> 00:44:44.040
there, I, I would want to know about them

904
00:44:44.050 --> 00:44:45.399
and I'd want to try and figure them out.

905
00:44:47.199 --> 00:44:48.649
But again, if you look at, if you look

906
00:44:48.659 --> 00:44:50.879
at the literature, it's, it's not, it's just not

907
00:44:50.889 --> 00:44:53.469
clear cut. There's, there's no good evidence to, to

908
00:44:53.479 --> 00:44:55.669
my mind that suggests that our attention spans have

909
00:44:55.679 --> 00:45:00.149
been declining over time. Certainly not in the past

910
00:45:00.159 --> 00:45:02.659
five or six years with the advent and explosion

911
00:45:02.669 --> 00:45:04.530
of social media. I mean, if that was the

912
00:45:04.540 --> 00:45:08.959
case, I think our ability to atte like our

913
00:45:08.969 --> 00:45:12.889
cognitive abilities, the cognitive architecture in our brains that's

914
00:45:13.149 --> 00:45:16.830
developed over thousands and thousands of years and evolved

915
00:45:16.840 --> 00:45:20.520
over that time. Um If it was that fragile,

916
00:45:21.389 --> 00:45:25.229
we probably would have been gone extinct. Uh MUCH

917
00:45:25.239 --> 00:45:28.389
earlier than this for other reasons. Right. So again,

918
00:45:28.399 --> 00:45:31.489
there's a lot of like fear based, this is

919
00:45:31.500 --> 00:45:32.810
a thing that we really need to be scared

920
00:45:32.820 --> 00:45:34.500
about and we need to be scared about now.

921
00:45:34.510 --> 00:45:38.889
Element to this. Um THERE is, there's a line

922
00:45:38.899 --> 00:45:43.399
of research, um that goes back 1012 years, at

923
00:45:43.409 --> 00:45:45.280
least that looks at this in the case in

924
00:45:45.290 --> 00:45:48.340
the context of multitasking. And again, I think that's

925
00:45:48.350 --> 00:45:50.520
actually when you really try and interrogate what people's

926
00:45:50.530 --> 00:45:54.840
worries are. It's, it's this idea that, um you

927
00:45:54.850 --> 00:45:57.679
know, we've got screens everywhere now and you're flicking

928
00:45:57.689 --> 00:46:01.060
your attention, your, your point of focus between lots

929
00:46:01.070 --> 00:46:03.520
of different things and that has to be detrimental

930
00:46:03.800 --> 00:46:06.070
and it's supported by research. And that there's this

931
00:46:06.080 --> 00:46:08.739
idea in, in cognitive psychology called the switch cost

932
00:46:08.750 --> 00:46:11.419
effect that if you switch your attention between tasks,

933
00:46:11.800 --> 00:46:14.520
there's a time cost associated with that very, very

934
00:46:14.530 --> 00:46:17.570
small. But if that builds up over time, then

935
00:46:17.580 --> 00:46:19.840
of course, you'll, you know, you'll see impacts on

936
00:46:19.850 --> 00:46:24.080
your ability to do work. Well, there was a

937
00:46:24.090 --> 00:46:25.929
really big paper that came out on that about

938
00:46:25.939 --> 00:46:29.530
15 years ago and then about 10 years later,

939
00:46:29.540 --> 00:46:31.629
uh a bunch of scientists did a meta review

940
00:46:31.639 --> 00:46:33.810
on what's happened since then. And, and the sort

941
00:46:33.820 --> 00:46:37.120
of sad story there was that there's not been

942
00:46:37.129 --> 00:46:39.729
any good research to support it. Basically that you

943
00:46:39.739 --> 00:46:41.340
find all of these, you know, you find some

944
00:46:41.350 --> 00:46:44.129
effects showing that multitasking is bad, some effects showing

945
00:46:44.139 --> 00:46:45.770
that actually, it's not something that you need to

946
00:46:45.780 --> 00:46:48.090
worry about all of those sorts of things. So

947
00:46:48.100 --> 00:46:49.669
again, it's trying to make sense of a very

948
00:46:49.679 --> 00:46:53.939
messy, very conflicting literature that sometimes is presented as

949
00:46:53.949 --> 00:46:57.649
coherent when I don't think it is one of

950
00:46:57.659 --> 00:46:59.010
the, one of the things that really stuck with

951
00:46:59.020 --> 00:47:01.199
me when I was, um thinking about this for

952
00:47:01.209 --> 00:47:06.340
a lot was one of the big analogies in,

953
00:47:06.350 --> 00:47:08.989
uh, cognitive psychology around attention is that we have

954
00:47:09.000 --> 00:47:12.320
this spotlight focus. And that certainly is, is, is

955
00:47:12.330 --> 00:47:14.590
almost like our experience of attention that, you know,

956
00:47:14.600 --> 00:47:17.699
I'm focusing on this thing at this particular point.

957
00:47:17.719 --> 00:47:19.770
And, you know, by and large, I'm not really

958
00:47:19.780 --> 00:47:24.070
looking or attending to anything in the periphery. But,

959
00:47:24.080 --> 00:47:26.979
you know, if something comes along, that's interesting, I'll

960
00:47:26.989 --> 00:47:29.000
flick my attention to it. You know, the classic,

961
00:47:29.010 --> 00:47:30.719
we, we always used to talk about this in,

962
00:47:30.729 --> 00:47:34.060
uh underground psychology lectures. You know, you're, you're focusing,

963
00:47:34.070 --> 00:47:36.719
you're attending to the lecture, uh, slides on the

964
00:47:36.729 --> 00:47:38.959
screen. But if a tiger runs, jumps into the

965
00:47:38.969 --> 00:47:40.479
room, you know, you're gonna look at that really

966
00:47:40.489 --> 00:47:42.840
quickly and you're not interested in the lecture slides

967
00:47:42.850 --> 00:47:45.399
anymore. But the, yeah, the analogy is there that,

968
00:47:45.409 --> 00:47:47.399
you know, if that's how attention works, that and

969
00:47:47.409 --> 00:47:49.449
you've got your phone on your desk and the

970
00:47:49.459 --> 00:47:51.969
phone pings off with a notification that whatever you

971
00:47:51.979 --> 00:47:54.820
were doing, you look at your phone and then

972
00:47:54.830 --> 00:47:57.179
you're stuck then because you're on your phone and

973
00:47:57.189 --> 00:47:59.879
it's very hard to switch back or even if

974
00:47:59.889 --> 00:48:01.800
you can switch back, you're doing that repeatedly and

975
00:48:01.810 --> 00:48:03.659
that's to the detriment of whatever you were doing

976
00:48:03.669 --> 00:48:08.050
before. Um, THERE'S this line of research that that's

977
00:48:08.060 --> 00:48:09.669
been coming up recently that I think is a

978
00:48:09.679 --> 00:48:13.250
really interesting alternative way of thinking about how attention

979
00:48:13.260 --> 00:48:16.550
works. That it's not this, you're not in this

980
00:48:16.560 --> 00:48:19.629
like, tunnel vision mode, that's partly what you're doing.

981
00:48:19.639 --> 00:48:21.270
You know, you have to be focusing on something

982
00:48:21.280 --> 00:48:23.489
like. But actually, if you look at what's going

983
00:48:23.500 --> 00:48:26.850
on in the brain, your brain is very good

984
00:48:26.860 --> 00:48:30.570
at creating maps of your environment. So you have

985
00:48:30.580 --> 00:48:33.030
auditory maps, but you literally have a visual map

986
00:48:33.040 --> 00:48:35.260
at the back of your brain that maps out

987
00:48:35.270 --> 00:48:38.020
what, what, what's what light's coming in and hitting

988
00:48:38.030 --> 00:48:40.959
your retina. Um But kind of overlaid on this

989
00:48:40.969 --> 00:48:43.290
is what we might call an attention priority map.

990
00:48:43.679 --> 00:48:45.379
So you can imagine this is like a topical,

991
00:48:45.389 --> 00:48:48.790
a topographical map of your environmental space with peaks

992
00:48:48.800 --> 00:48:50.729
and troughs and where there's a peak on that

993
00:48:50.739 --> 00:48:54.429
map that corresponds to something that's really interesting and

994
00:48:54.439 --> 00:48:57.729
important that's worthy of your attention and where the

995
00:48:57.739 --> 00:49:00.739
map's flat, there's nothing interesting going on. So for

996
00:49:00.750 --> 00:49:03.020
me at the minute, you know, there's a massive

997
00:49:03.030 --> 00:49:06.149
peak on my topographical map for, for my screen

998
00:49:06.159 --> 00:49:08.310
and my camera cos that's where I'm looking at

999
00:49:08.320 --> 00:49:10.149
the minute I've got a screen over here and

1000
00:49:10.159 --> 00:49:12.300
maybe that's a small peak. Um I don't have

1001
00:49:12.310 --> 00:49:13.870
my phone on me at the minute, but maybe

1002
00:49:13.879 --> 00:49:16.000
if I have my phone on my desk, there

1003
00:49:16.010 --> 00:49:17.550
might be nothing there at the minute because it's

1004
00:49:17.560 --> 00:49:21.629
switched off. Now, what affects that topographical map? That

1005
00:49:21.639 --> 00:49:25.889
heat map, that priority map is sure, bottom up,

1006
00:49:25.899 --> 00:49:29.800
salient sensory things. So, you know, if a bright

1007
00:49:29.810 --> 00:49:32.830
light appears on my, on my desk, that will

1008
00:49:32.840 --> 00:49:35.219
create a bit of a, a peak on that

1009
00:49:35.229 --> 00:49:37.520
map, uh because it's something that's maybe of interest.

1010
00:49:37.530 --> 00:49:39.260
So if my phone goes off and the screen

1011
00:49:39.270 --> 00:49:43.889
lights up, there's something but critically, those maps and

1012
00:49:43.899 --> 00:49:47.790
therefore our attention aren't solely driven by bottom up

1013
00:49:47.800 --> 00:49:51.290
basic sensory processes. There are also top down processes

1014
00:49:51.300 --> 00:49:53.239
as well. And what we mean by that is

1015
00:49:53.250 --> 00:49:57.419
things like um what are your goals at the

1016
00:49:57.429 --> 00:50:01.520
minute? What are your motivations? What's your prior experience,

1017
00:50:01.530 --> 00:50:04.020
your prior learning history almost when you've been in

1018
00:50:04.030 --> 00:50:06.169
this situation before? And things have gone right or

1019
00:50:06.179 --> 00:50:08.459
wrong? What have you kind of learned from that?

1020
00:50:08.919 --> 00:50:10.530
So they're gonna go into detail in this with

1021
00:50:10.540 --> 00:50:12.100
an example of the book. And I say, you

1022
00:50:12.110 --> 00:50:15.229
know, I'm sat on my computer writing my book,

1023
00:50:15.239 --> 00:50:17.330
but you know, this is a good example now.

1024
00:50:17.340 --> 00:50:20.310
Yeah, I, I, yeah, I knew that this interview

1025
00:50:20.320 --> 00:50:21.719
was coming along for a few days and I've

1026
00:50:21.729 --> 00:50:23.899
been doing some preparation for it and, and I

1027
00:50:23.909 --> 00:50:25.850
want this to go well, you know, I want

1028
00:50:25.860 --> 00:50:27.459
to feel as though I'm not stumbling over my

1029
00:50:27.469 --> 00:50:28.909
words and things like that and I want to

1030
00:50:28.919 --> 00:50:31.360
focus on the questions that you're asking me. So,

1031
00:50:31.370 --> 00:50:36.010
for me, there's a massive priority peak around. Ironically,

1032
00:50:36.020 --> 00:50:38.459
my screen and, and my camera at the minute,

1033
00:50:38.570 --> 00:50:40.629
if my phone was on my desk and it

1034
00:50:40.639 --> 00:50:43.919
pinged off. Well, I don't wanna, I don't wanna

1035
00:50:43.929 --> 00:50:46.389
pick it up and start reading the message or

1036
00:50:46.409 --> 00:50:48.439
answering a phone call or something like that would

1037
00:50:48.449 --> 00:50:51.060
be weird because you're recording this interview and, you

1038
00:50:51.070 --> 00:50:53.449
know, it does not align with how I want

1039
00:50:53.459 --> 00:50:55.610
this to go and what my goals are. So

1040
00:50:55.620 --> 00:50:57.000
if my phone were on my desk and it

1041
00:50:57.010 --> 00:50:59.040
pinged off, there might be a little peak of

1042
00:50:59.050 --> 00:51:01.679
activity on that map, but not enough to shift

1043
00:51:01.689 --> 00:51:05.429
my attention. So I think, thinking about it in

1044
00:51:05.439 --> 00:51:08.340
that sense and, you know, that's a vast oversimplification

1045
00:51:08.350 --> 00:51:11.550
of what's going on in the brain. Um But

1046
00:51:11.560 --> 00:51:13.830
basically, you know, you have these layers of different

1047
00:51:13.840 --> 00:51:17.489
maps like that, um going on there. You know,

1048
00:51:17.500 --> 00:51:18.750
if you think about it in the, in this

1049
00:51:18.760 --> 00:51:20.550
sort of sense, like what's going on cos we've,

1050
00:51:20.560 --> 00:51:22.149
you know, I could say all this stuff and

1051
00:51:22.159 --> 00:51:24.570
say, you know, attention spans aren't collapsing this stuff,

1052
00:51:24.580 --> 00:51:28.310
but we still feel really distractible still. Yeah. How

1053
00:51:28.320 --> 00:51:30.370
do we maybe explain that with that sort of

1054
00:51:30.379 --> 00:51:36.040
thinking then? Um MAYBE it's like it, it's distractibility

1055
00:51:36.050 --> 00:51:38.399
then, right? That uh and I have lots of

1056
00:51:38.409 --> 00:51:41.469
conversations with my, my students about this. You know,

1057
00:51:41.479 --> 00:51:43.590
they'll, they'll come in and for meetings with me

1058
00:51:43.929 --> 00:51:46.060
and we'll talk about how their work's going and

1059
00:51:46.070 --> 00:51:49.300
they'll say they're really struggling. They, they know that

1060
00:51:49.310 --> 00:51:52.100
they've got this essay in but they just, you

1061
00:51:52.110 --> 00:51:54.149
know, when they sit down to, to write it,

1062
00:51:54.379 --> 00:51:57.219
they just, they can't really think what they're doing

1063
00:51:57.229 --> 00:51:58.909
and then they'll start playing a video game or

1064
00:51:58.919 --> 00:52:00.000
something like that. And then the next thing, you

1065
00:52:00.010 --> 00:52:02.659
know, the day's gone and the first thing that

1066
00:52:02.669 --> 00:52:06.090
I ask them is show me your calendar for

1067
00:52:06.100 --> 00:52:09.120
like this past week. And most of the time

1068
00:52:09.129 --> 00:52:10.939
they'll say, I don't really use a calendar. But

1069
00:52:10.949 --> 00:52:12.469
when, when they do use one, I'll look at

1070
00:52:12.479 --> 00:52:15.580
it and there's just nothing in there. Right? And,

1071
00:52:15.879 --> 00:52:17.590
and then, and then the conversation turns to, well,

1072
00:52:17.600 --> 00:52:19.689
OK. Right. So say it's Tuesday and you said

1073
00:52:19.699 --> 00:52:21.389
to yourself that you were gonna do, you were

1074
00:52:21.399 --> 00:52:24.620
gonna work on your, your essay today, but you've

1075
00:52:24.629 --> 00:52:27.250
not planned your time. You've not got a goal,

1076
00:52:27.260 --> 00:52:29.669
you've not got a motivation there. You've just gone.

1077
00:52:29.679 --> 00:52:32.820
Tuesday is clear in my calendar. I'm gonna do

1078
00:52:32.830 --> 00:52:37.070
essay and it's a really stressful thing to do,

1079
00:52:37.080 --> 00:52:39.550
writing an essay. And it's not always the case

1080
00:52:39.560 --> 00:52:40.860
that you're writing an essay on something that you're

1081
00:52:40.870 --> 00:52:43.030
really passionate about. It's something that you're doing for

1082
00:52:43.040 --> 00:52:45.330
the purpose of passing a degree. So some of

1083
00:52:45.340 --> 00:52:48.860
those intrinsic motivations aren't there. You rely on extrinsic

1084
00:52:48.870 --> 00:52:52.290
ones. Um, BUT you kind of get to nine

1085
00:52:52.300 --> 00:52:53.909
o'clock in the morning. You've not prepped anything, you've

1086
00:52:53.919 --> 00:52:59.000
not planned anything. So, how have you been distracted

1087
00:52:59.010 --> 00:53:01.590
by something because you, you weren't doing anything to

1088
00:53:01.600 --> 00:53:04.929
be distracted from? Right. Um, AND of course, you

1089
00:53:04.939 --> 00:53:06.719
know, if I, I, I've been in that situation

1090
00:53:06.729 --> 00:53:09.020
literally writing this book where, you know, there are

1091
00:53:09.030 --> 00:53:11.389
days where I've gone. Oh, I've got an unexpectedly

1092
00:53:11.399 --> 00:53:14.270
clear day. I'll sit down and try writing and

1093
00:53:14.280 --> 00:53:16.820
I've had a terrible day because, you know, I've

1094
00:53:16.830 --> 00:53:19.310
not really, my mind's not been in the game.

1095
00:53:19.620 --> 00:53:21.110
Um, I wasn't really sure what I was gonna

1096
00:53:21.120 --> 00:53:22.929
write that day cos I haven't really planned it

1097
00:53:23.179 --> 00:53:26.479
and I find other things to distract me. Whereas

1098
00:53:26.489 --> 00:53:29.189
if you plan your time, you create those goals,

1099
00:53:29.379 --> 00:53:32.379
those sorts of top down factors will influence your

1100
00:53:32.389 --> 00:53:34.979
attention in that, you know, talk, talk a lot

1101
00:53:34.989 --> 00:53:37.659
with students about this, you know, block your time

1102
00:53:37.669 --> 00:53:40.330
out. Say, OK, I'm gonna work on, on Tuesday.

1103
00:53:40.340 --> 00:53:43.110
I'm gonna work on this essay for two hours,

1104
00:53:43.169 --> 00:53:45.820
but actually within that two hours, I'm gonna do

1105
00:53:45.830 --> 00:53:47.260
10 minutes of work and then have a bit

1106
00:53:47.270 --> 00:53:49.709
of a break and in those pockets of time,

1107
00:53:49.719 --> 00:53:51.139
I'm gonna plan what I do. So, you know,

1108
00:53:51.149 --> 00:53:54.540
I'm gonna start off by writing a structure for

1109
00:53:54.550 --> 00:53:56.879
my essay and you know what? They go away

1110
00:53:56.889 --> 00:53:58.899
and when they try that, when they do it,

1111
00:53:59.239 --> 00:54:03.850
they do really well. So we've become, and, and

1112
00:54:03.860 --> 00:54:06.350
ironically like technologies is a factor in that, that

1113
00:54:06.360 --> 00:54:08.500
I think we've become so reliant on technology as

1114
00:54:08.510 --> 00:54:11.330
this sort of convenience thing that very often we

1115
00:54:11.340 --> 00:54:12.850
don't see it for what it is, which is

1116
00:54:12.860 --> 00:54:15.889
a tool to help us. And if we use

1117
00:54:15.899 --> 00:54:19.760
those tools correctly, we're not distracted by tasks when

1118
00:54:19.770 --> 00:54:21.679
we need to get on with things. If we

1119
00:54:21.689 --> 00:54:24.179
use them incorrectly, we get massively distracted by them

1120
00:54:24.399 --> 00:54:27.280
because not only are they tools of convenience, they're

1121
00:54:27.290 --> 00:54:31.120
tools of entertainment as well. Um So a lot

1122
00:54:31.129 --> 00:54:33.550
of this is around being more mindful, more, more

1123
00:54:33.560 --> 00:54:35.659
reflective about, you know, what, what do you want

1124
00:54:35.669 --> 00:54:38.750
out of your time online? What are you doing

1125
00:54:38.760 --> 00:54:41.260
online? How is that? How are those two things

1126
00:54:41.270 --> 00:54:42.969
aligning? And if they're not, what, what can you

1127
00:54:42.979 --> 00:54:45.229
do about them? And again, you kind of see

1128
00:54:45.239 --> 00:54:46.820
that when we have this sort of conversation, we

1129
00:54:46.830 --> 00:54:49.979
move away from these very top line. Very scary

1130
00:54:49.989 --> 00:54:53.860
things around are our attention spans collapsing to thinking

1131
00:54:53.870 --> 00:54:55.989
about it in a more, what I feel is

1132
00:54:56.000 --> 00:54:58.189
a bit more of a productive and constructive, more

1133
00:54:58.199 --> 00:54:59.070
nuanced way.

1134
00:55:00.169 --> 00:55:02.489
OK. So in, in the interest of time, let

1135
00:55:02.500 --> 00:55:05.469
me ask you about just one more type of

1136
00:55:05.479 --> 00:55:08.800
claim that people make about screen based technologies. And

1137
00:55:08.810 --> 00:55:11.810
then I will have just one last more general

1138
00:55:11.820 --> 00:55:14.830
question to ask you about the book. So another

1139
00:55:14.840 --> 00:55:18.370
thing that sometimes people claim is that these types

1140
00:55:18.379 --> 00:55:23.080
of technologies can be addictive and there's, there would

1141
00:55:23.090 --> 00:55:25.729
be of course a lot one back here. But

1142
00:55:26.050 --> 00:55:30.239
uh what can we say about that? And by

1143
00:55:30.250 --> 00:55:32.560
the way, what does it mean to say that

1144
00:55:32.570 --> 00:55:34.439
something is addictive?

1145
00:55:35.629 --> 00:55:40.159
Oh Those are great questions. So um where, where

1146
00:55:40.169 --> 00:55:44.760
should we start with that? I think for as

1147
00:55:44.770 --> 00:55:49.139
long as digital technologies have been coming out, um

1148
00:55:49.850 --> 00:55:53.520
people have been worried about their potentially addictive uh

1149
00:55:53.530 --> 00:55:57.030
properties and you see that with, you know, there

1150
00:55:57.040 --> 00:55:59.300
were, there were concerns that, you know, when the

1151
00:55:59.310 --> 00:56:02.149
radio came out that uh people were gonna be

1152
00:56:02.159 --> 00:56:04.050
so hooked on it. You know, we weren't maybe

1153
00:56:04.060 --> 00:56:06.590
talking about it in specifically addiction terms. But you

1154
00:56:06.600 --> 00:56:07.949
can see these stories are, people are gonna be

1155
00:56:07.959 --> 00:56:09.510
so hooked on it. They're not gonna do anything

1156
00:56:09.520 --> 00:56:12.449
else and that certainly feels like an aspect of,

1157
00:56:12.459 --> 00:56:14.860
of addiction, right? That you start doing this thing

1158
00:56:14.870 --> 00:56:17.620
to the exclusion of other things that you, you,

1159
00:56:17.629 --> 00:56:21.560
you uh you did um previously. So there's always

1160
00:56:21.570 --> 00:56:25.110
been that element of these things. Um VIDEO games

1161
00:56:25.120 --> 00:56:32.030
has been the same and um internet addiction. You

1162
00:56:32.040 --> 00:56:34.360
know, there's a, there's a really interesting story there

1163
00:56:34.370 --> 00:56:37.750
around, I think it was about 1995 where there's

1164
00:56:37.760 --> 00:56:40.439
a psychiatrist in the USA guy called Ivan Goldberg

1165
00:56:40.449 --> 00:56:44.939
who ran a bulletin board system for other psychiatrists.

1166
00:56:45.189 --> 00:56:47.479
I was very frustrated at the time with um

1167
00:56:48.300 --> 00:56:51.270
the, the diagnostic and statistical manual, which is like

1168
00:56:51.280 --> 00:56:55.750
the American Psychiatric Association reference book for diagnosing and

1169
00:56:55.760 --> 00:56:59.350
listing mental health disorders. He was very kind of

1170
00:56:59.360 --> 00:57:02.419
frustrated that this th the way that things are

1171
00:57:02.429 --> 00:57:04.459
included in that the way that disorders are included

1172
00:57:04.469 --> 00:57:06.020
in it is, is not in a very kind

1173
00:57:06.030 --> 00:57:08.659
of critical thinking sort of way. And what you

1174
00:57:08.669 --> 00:57:10.820
can risk doing is like you can in an

1175
00:57:10.830 --> 00:57:13.360
unthinking way, come up with a list of symptoms

1176
00:57:13.370 --> 00:57:16.530
and create a disorder that just completely path pathologize

1177
00:57:16.540 --> 00:57:19.189
or medicalize as a normal everyday behavior. And he

1178
00:57:19.199 --> 00:57:22.520
created this thing called internet addiction disorder as a,

1179
00:57:22.530 --> 00:57:25.300
as a way of like s satirizing the uh

1180
00:57:25.310 --> 00:57:27.810
the DS M and uh and it kind of

1181
00:57:27.820 --> 00:57:30.540
backfired, right? Because people started getting in touch with

1182
00:57:30.550 --> 00:57:32.219
him and saying, oh God, I think, I think

1183
00:57:32.229 --> 00:57:34.649
I've got internet addiction, I've read, read through your

1184
00:57:34.659 --> 00:57:37.649
list and that's me. And then these like internet

1185
00:57:37.659 --> 00:57:42.090
addiction online forums. So these self help self organized

1186
00:57:42.100 --> 00:57:44.510
forums start cropping up. And then I think what

1187
00:57:44.520 --> 00:57:46.260
happens after that is that it becomes a little

1188
00:57:46.270 --> 00:57:48.360
bit more embedded in. I'm not saying that's like

1189
00:57:48.370 --> 00:57:51.439
the soul or root origination of internet addiction. But,

1190
00:57:51.550 --> 00:57:53.899
you know, you start to see people worrying about

1191
00:57:53.909 --> 00:57:57.760
this because whenever something like this comes along, there

1192
00:57:57.770 --> 00:57:59.689
are sort of two ways of referring to addiction,

1193
00:57:59.699 --> 00:58:03.219
right? One is the literal clinical sense. Um And

1194
00:58:03.229 --> 00:58:05.139
there's uh you asked about, you know, how do

1195
00:58:05.149 --> 00:58:07.709
we define that? Um I don't think there's a

1196
00:58:07.719 --> 00:58:12.060
good clear consensus definition around behavioral definitions, behavioral addiction.

1197
00:58:12.070 --> 00:58:16.580
Sorry. So, I think, um, there's, and, and there's

1198
00:58:16.590 --> 00:58:19.280
certainly not a definition that's like a single sentence,

1199
00:58:19.290 --> 00:58:21.169
right? There's lots of ways to think about it.

1200
00:58:21.179 --> 00:58:24.550
So generally think some of the best, um, definitions

1201
00:58:24.560 --> 00:58:27.870
of behavioral addictions, uh that I've seen are, you

1202
00:58:27.879 --> 00:58:31.750
know, engaging in, uh, a behavior repeatedly that leads

1203
00:58:31.760 --> 00:58:35.330
to sort of significant harm or significant distress, but

1204
00:58:35.340 --> 00:58:37.949
there have to be some exclusion factors in there.

1205
00:58:38.070 --> 00:58:41.449
So that behavior doesn't diminish over time. Uh It

1206
00:58:41.459 --> 00:58:47.030
persists. Um You can't explain that behavior due down

1207
00:58:47.040 --> 00:58:48.719
to other reasons. So it's not that it's a,

1208
00:58:48.729 --> 00:58:51.969
a coping mechanism for something else I think is

1209
00:58:51.979 --> 00:58:57.209
a really important uh aspect of considering a behavioral

1210
00:58:57.219 --> 00:59:02.899
addiction. Um HEAVY engagement or intense engagement in and

1211
00:59:02.909 --> 00:59:07.310
of itself is not a sufficient criterion. So if

1212
00:59:07.320 --> 00:59:09.189
you could have people who are heavily engaged in

1213
00:59:09.199 --> 00:59:12.060
that activity, but they don't always inevitably come to

1214
00:59:12.070 --> 00:59:14.860
harm that needs to be taken into a, into

1215
00:59:14.870 --> 00:59:19.340
account. And even if it is potentially harmful, is

1216
00:59:19.350 --> 00:59:22.739
it a result of intentional choice, deliberate choice on

1217
00:59:22.750 --> 00:59:25.510
behalf of the individual rather than this is something

1218
00:59:25.520 --> 00:59:27.419
that they feel like they have no control over.

1219
00:59:28.050 --> 00:59:30.350
So that's probably one of the better definitions that

1220
00:59:30.360 --> 00:59:32.750
I've seen of, you know, you could apply that

1221
00:59:32.760 --> 00:59:36.090
to internet or smartphone or whatever addiction. Uh And,

1222
00:59:36.100 --> 00:59:39.290
and in doing so what I argue in, in

1223
00:59:39.300 --> 00:59:42.969
the book is that um that we w you,

1224
00:59:42.979 --> 00:59:45.689
you're not addicted to your smartphone, you're not addicted

1225
00:59:45.699 --> 00:59:49.090
to social media. You can have bad relationships with

1226
00:59:49.100 --> 00:59:52.580
it. Problematic relationships with it, bad habits can be

1227
00:59:52.590 --> 00:59:56.500
developed but they're not addictions. So there's this whole

1228
00:59:56.510 --> 00:59:59.979
like clinical side of how do we define addiction.

1229
01:00:00.300 --> 01:00:02.489
But also we use the term addiction in an,

1230
01:00:02.500 --> 01:00:04.229
in an everyday sense, right? You know, we say

1231
01:00:04.239 --> 01:00:06.060
that we're addicted to things when we actually mean

1232
01:00:06.070 --> 01:00:09.000
we really like it or um we we do

1233
01:00:09.010 --> 01:00:10.919
that thing a lot and maybe there's a bit

1234
01:00:10.929 --> 01:00:13.260
of shame or guilt associated with that. It maybe

1235
01:00:13.270 --> 01:00:15.629
it feels like a bit of a vice, but

1236
01:00:15.639 --> 01:00:18.090
that is very different to full bone blow clinical

1237
01:00:18.100 --> 01:00:20.669
addiction, right? But I think what, what's happened in

1238
01:00:20.679 --> 01:00:23.750
the debate is that because we use those words

1239
01:00:24.010 --> 01:00:27.949
and in the absence of a good robust coherent

1240
01:00:28.189 --> 01:00:32.790
research base, it's inevitable then that you start to

1241
01:00:32.800 --> 01:00:36.310
think about this behavior. So let's go with smartphones

1242
01:00:36.320 --> 01:00:40.399
still. So smartphone addiction say you say we're addicted

1243
01:00:40.409 --> 01:00:42.550
to them, you start to think about that in

1244
01:00:42.560 --> 01:00:46.270
biological terms. So you start to see people say

1245
01:00:47.110 --> 01:00:52.860
smartphones are addictive by design because um whenever you

1246
01:00:53.229 --> 01:00:56.649
click like or click a button or press a

1247
01:00:56.659 --> 01:01:00.590
lever effectively, it's creating a hit of dopamine in

1248
01:01:00.600 --> 01:01:03.570
the head uh in the brain and dopamine is

1249
01:01:03.580 --> 01:01:07.229
involved in motivation and reward. It's involved in addiction.

1250
01:01:07.270 --> 01:01:11.340
So the more dopamine that something releases, the more

1251
01:01:11.350 --> 01:01:15.090
likely it is to be addictive. Now, that is

1252
01:01:16.060 --> 01:01:19.760
a a vast oversimplification of how dopamine works and

1253
01:01:19.770 --> 01:01:23.939
how, what the biochemical uh the bios, psychological uh

1254
01:01:23.949 --> 01:01:27.800
impacts and and mechanisms of, of addiction are, right?

1255
01:01:29.520 --> 01:01:32.709
And that all comes from research on like substance

1256
01:01:32.719 --> 01:01:35.500
use and abuse disorders. Uh And there are very

1257
01:01:35.510 --> 01:01:38.050
different things going on, like if you take cocaine

1258
01:01:38.060 --> 01:01:41.320
or heroin, that is very different in terms of

1259
01:01:41.330 --> 01:01:44.669
the biological effects, it's very different to being on

1260
01:01:44.679 --> 01:01:48.530
your phone or playing a game or eating or

1261
01:01:48.540 --> 01:01:52.080
driving, right? All of these things, dopamine will be

1262
01:01:52.090 --> 01:01:54.290
involved in some small way, doesn't mean that they're

1263
01:01:54.300 --> 01:01:58.639
addictive. Uh And, and you know, neuroscientists are kind

1264
01:01:58.649 --> 01:02:01.370
of clear on this, that, that addiction is complex.

1265
01:02:01.379 --> 01:02:04.739
There are really important social factors that um are,

1266
01:02:04.750 --> 01:02:07.889
are, are, are are crucial for uh for, for

1267
01:02:07.899 --> 01:02:12.020
the um introduction of maintenance of addiction. Um AND

1268
01:02:12.030 --> 01:02:14.679
that it's too simplistic to say, well, if you

1269
01:02:14.689 --> 01:02:19.120
like something, it releases dopamine and if you like

1270
01:02:19.129 --> 01:02:21.500
something too much, it's too much dopamine and dopamine

1271
01:02:21.510 --> 01:02:23.659
is involving an addiction. Therefore, anything can be addictive.

1272
01:02:23.669 --> 01:02:25.229
It's just sort of the wrong way of thinking

1273
01:02:25.239 --> 01:02:28.139
about it, but we've got stuck in that framing,

1274
01:02:28.149 --> 01:02:31.179
right? I see this people default to it all

1275
01:02:31.189 --> 01:02:34.310
the time like that, that social media is addictive.

1276
01:02:34.320 --> 01:02:36.639
Uh OR so social media is designed to be

1277
01:02:36.649 --> 01:02:40.780
addictive. Infinite scrolling is addictive, smartphones are addictive clinically,

1278
01:02:40.790 --> 01:02:44.570
they're not, there are no formalized, clinical definitions of

1279
01:02:44.580 --> 01:02:49.469
those things, the only formal clinical uh definition of

1280
01:02:49.479 --> 01:02:53.520
a, a behavioral digital addiction is gaming disorder. And

1281
01:02:53.530 --> 01:02:57.050
even there, there's no clear consensus on what it

1282
01:02:57.060 --> 01:02:58.780
is, right? If you look on the World Health

1283
01:02:58.790 --> 01:03:03.320
Organisation's own website for Gaming Disorder, they link to

1284
01:03:03.330 --> 01:03:07.790
a systematic review from 2020. And in that review,

1285
01:03:08.040 --> 01:03:10.179
they look at 100 and 60 studies that have

1286
01:03:10.189 --> 01:03:13.370
been published on this stuff. And across those studies,

1287
01:03:13.520 --> 01:03:16.669
there are 35 different ways in which gaming disorder

1288
01:03:16.679 --> 01:03:20.419
or gaming addiction is measured. 35 different ways, not

1289
01:03:20.429 --> 01:03:25.050
135. And in other studies, other systematic reviews, you

1290
01:03:25.060 --> 01:03:27.870
find similar numbers of studies but like even more

1291
01:03:27.879 --> 01:03:30.429
ways in which you determine a cut off criterion

1292
01:03:30.439 --> 01:03:33.949
for addiction within those ways of measuring it. So

1293
01:03:33.959 --> 01:03:37.120
35 different ways of measuring gaming disorder. And if

1294
01:03:37.129 --> 01:03:39.340
you look at the prevalence rates in that one

1295
01:03:39.350 --> 01:03:43.280
paper, uh they are anywhere from about 0.2% of

1296
01:03:43.290 --> 01:03:46.540
the gaming population meet the criteria for gaming disorder

1297
01:03:46.620 --> 01:03:50.800
up to just under 58% right? So what you're

1298
01:03:50.810 --> 01:03:53.620
saying, what you're saying there to me is that

1299
01:03:53.629 --> 01:03:58.540
either basically nobody has gaming addiction or pretty much

1300
01:03:58.550 --> 01:04:03.540
everybody has. Now. That's true. But yeah, the number

1301
01:04:03.550 --> 01:04:06.709
is somewhere between zero and 100 right? Not useful

1302
01:04:06.719 --> 01:04:08.949
to say that at all. Uh And what that's

1303
01:04:08.959 --> 01:04:10.860
very clearly signals to me is that we've not

1304
01:04:10.870 --> 01:04:13.149
got a handle on what that thing is yet,

1305
01:04:13.159 --> 01:04:15.899
even if it's a thing at all. It's very

1306
01:04:15.909 --> 01:04:19.770
unhelpful. And that's for the one formalized clinical digital

1307
01:04:19.780 --> 01:04:21.929
addiction. We're not there yet with the research on

1308
01:04:21.939 --> 01:04:26.610
smartphones and social media. So it's a mess, right?

1309
01:04:26.620 --> 01:04:30.179
And I think again, we, we've, we've fallen into

1310
01:04:30.189 --> 01:04:34.479
this trap of thinking about our digital technology, use

1311
01:04:34.489 --> 01:04:36.929
whatever aspect of it in terms of addiction because

1312
01:04:36.939 --> 01:04:38.669
we use it in that day to day usage.

1313
01:04:38.689 --> 01:04:42.889
And I should say, yeah, mobile games companies advertise

1314
01:04:42.899 --> 01:04:46.189
their games and they say this game is addictive.

1315
01:04:46.199 --> 01:04:48.100
It's great. You should. And I was like, that's

1316
01:04:48.110 --> 01:04:50.310
a weird thing to say, right? Why are you

1317
01:04:50.320 --> 01:04:52.500
say, like this thing is really harmful for your

1318
01:04:52.510 --> 01:04:55.209
behavior? You should do it. So there's absolutely responsibility

1319
01:04:55.219 --> 01:04:56.909
on the tech companies on this side of things.

1320
01:04:56.919 --> 01:04:59.610
You can't claim that games aren't addictive and then

1321
01:04:59.620 --> 01:05:01.800
say in an advert, this game is really addictive

1322
01:05:01.810 --> 01:05:03.570
when you mean it's great, you know, just come

1323
01:05:03.580 --> 01:05:07.989
on like do better. Um So yeah, there's the,

1324
01:05:08.030 --> 01:05:10.409
there's this like whole mess and we, we can't

1325
01:05:10.419 --> 01:05:12.129
help but think about it in this sort of

1326
01:05:12.139 --> 01:05:15.629
framework. But there are other ways of thinking about

1327
01:05:15.639 --> 01:05:19.639
how we develop relationships with digital technology uh that

1328
01:05:19.649 --> 01:05:21.919
are better. I think cos the problem with just

1329
01:05:21.929 --> 01:05:24.040
thinking about this in terms of addiction is that

1330
01:05:24.050 --> 01:05:26.590
you're left with a very limited set of solutions

1331
01:05:26.620 --> 01:05:30.110
which are basically stop using it various forms of

1332
01:05:30.120 --> 01:05:35.530
that digital detoxes, abstinence restrictions bans. Those are the

1333
01:05:35.540 --> 01:05:37.860
things that you're left with. Right. And we know

1334
01:05:37.870 --> 01:05:42.080
digital detoxes don't really work. Right. Um, BETTER ways

1335
01:05:42.090 --> 01:05:43.550
of thinking about it I think are in terms

1336
01:05:43.560 --> 01:05:46.439
of what we call technology habit formation. And again,

1337
01:05:46.449 --> 01:05:49.699
there's, there's lots of research, new research, like within

1338
01:05:49.709 --> 01:05:52.080
the past three or four years that is starting

1339
01:05:52.090 --> 01:05:54.469
to back this up and support this idea. Are

1340
01:05:54.479 --> 01:05:58.020
you addicted to your phone? Probably not. Have you

1341
01:05:58.030 --> 01:06:01.540
developed bad habits with your phone? Yeah, definitely. Everybody

1342
01:06:01.550 --> 01:06:03.879
has. I have, I know I have, I've also

1343
01:06:03.889 --> 01:06:06.139
developed some really good habits with my phone so

1344
01:06:06.149 --> 01:06:08.350
you can have good, good and bad habits. Habits

1345
01:06:08.360 --> 01:06:11.429
in and of themselves are neutral, whether they become

1346
01:06:11.439 --> 01:06:14.649
good or bad. Depends on other things. Right. So,

1347
01:06:14.659 --> 01:06:19.189
um, checking your phone, neutral habit, not inherently harmful.

1348
01:06:20.070 --> 01:06:22.209
Checking your phone at night when you're feeling really

1349
01:06:22.219 --> 01:06:23.929
lonely and you wanna catch up with some friends,

1350
01:06:23.939 --> 01:06:25.840
good thing to do. Probably a good impact on

1351
01:06:25.850 --> 01:06:28.560
your well-being, checking your phone just to see what

1352
01:06:28.570 --> 01:06:31.560
somebody's posted on Twitter while you're driving down the

1353
01:06:31.570 --> 01:06:35.649
road. Super bad habit to get into. Right. Definitely.

1354
01:06:35.659 --> 01:06:39.070
The more often you do that, the more likely

1355
01:06:39.080 --> 01:06:40.840
a bad thing is gonna happen, you're gonna crash

1356
01:06:40.850 --> 01:06:42.530
your car and you're gonna kill somebody. So don't

1357
01:06:42.540 --> 01:06:44.850
do that. But that's the thing, right? So, whether

1358
01:06:44.860 --> 01:06:47.219
a habit becomes good or bad or not, depends

1359
01:06:47.229 --> 01:06:50.439
on things like how frequently you engage in that

1360
01:06:50.449 --> 01:06:53.429
habit, but not just the intensity or frequency of

1361
01:06:53.439 --> 01:06:55.409
it. But how often you do that in a

1362
01:06:55.419 --> 01:06:57.899
way that doesn't align with your goals or your

1363
01:06:57.909 --> 01:07:00.870
motivations. So if you're doing it in an unthinking

1364
01:07:00.879 --> 01:07:05.620
way, then it can lead to bad outcomes. Um

1365
01:07:05.629 --> 01:07:09.939
AAA kind of more, perhaps more relatable uh argument

1366
01:07:09.949 --> 01:07:12.379
here. So let's say we're talking about scrolling through

1367
01:07:12.389 --> 01:07:15.290
Instagram inherently not good or bad. It's just inherently

1368
01:07:15.300 --> 01:07:18.879
neutral. I do that a lot and I do

1369
01:07:18.889 --> 01:07:21.770
that because I want to, because I want to

1370
01:07:21.780 --> 01:07:23.909
use that as a way of just winding down

1371
01:07:23.919 --> 01:07:26.179
at the end of the day. And some of

1372
01:07:26.189 --> 01:07:30.020
the, the uh moments in the past year where

1373
01:07:30.030 --> 01:07:32.540
I have laughed out loud to the points of

1374
01:07:32.550 --> 01:07:35.459
tears of laughter, uh where I've been scrolling through

1375
01:07:35.469 --> 01:07:37.250
Instagram and I've found a really funny video and

1376
01:07:37.260 --> 01:07:39.699
I've shared it with my wife, like literal tears

1377
01:07:39.709 --> 01:07:42.409
of laughter and it's been really good. And then

1378
01:07:42.419 --> 01:07:44.709
there are other times where I'm sat there scrolling

1379
01:07:44.719 --> 01:07:45.770
and I look at the clock and it's an

1380
01:07:45.780 --> 01:07:47.459
11 o'clock at night. And I said to myself

1381
01:07:47.469 --> 01:07:49.560
tonight, I wanted to go to bed at 10

1382
01:07:50.199 --> 01:07:52.810
and I feel deflated and I feel like I've

1383
01:07:52.820 --> 01:07:56.620
undermined myself. Now, is that because scrolling through Ins

1384
01:07:56.649 --> 01:08:00.419
Instagram is addictive. Now, what I try and do

1385
01:08:00.429 --> 01:08:03.209
in that situation is go what's happened here? What,

1386
01:08:03.219 --> 01:08:05.750
why did this happen to me tonight? And very

1387
01:08:05.760 --> 01:08:08.139
often what I find when I start reflecting on

1388
01:08:08.149 --> 01:08:11.340
that and start interrogating it is I go, do

1389
01:08:11.350 --> 01:08:13.070
you know what I've had a really stressful day

1390
01:08:14.179 --> 01:08:17.279
I, in and amongst doing work and stuff and

1391
01:08:17.290 --> 01:08:18.370
all the things that I need to do at

1392
01:08:18.379 --> 01:08:21.189
home and looking after the kids and everything. I

1393
01:08:21.200 --> 01:08:24.720
actually didn't sit down until quarter to 10. Like

1394
01:08:24.729 --> 01:08:26.009
that was the point at which I went, oh,

1395
01:08:26.020 --> 01:08:30.698
I could finally relax. So maybe it was never

1396
01:08:30.707 --> 01:08:32.108
on the cards that I was going to bed

1397
01:08:32.118 --> 01:08:35.368
at 10 o'clock because I'm not gonna magically wind

1398
01:08:35.377 --> 01:08:40.180
down in 15 minutes. So maybe it was the

1399
01:08:40.189 --> 01:08:42.439
case that, you know, if I wasn't scrolling on

1400
01:08:42.450 --> 01:08:44.419
Instagram for an hour or whatever, I would have

1401
01:08:44.430 --> 01:08:46.169
been doing something else. So I was always gonna

1402
01:08:46.180 --> 01:08:48.779
go to bed at 11 o'clock. But actually, you

1403
01:08:48.790 --> 01:08:51.700
know, in scrolling through Instagram, I didn't, I didn't,

1404
01:08:51.939 --> 01:08:54.189
I didn't find anything fun that time. I didn't

1405
01:08:54.200 --> 01:08:57.669
really get anything interesting out of it. So tomorrow,

1406
01:08:57.680 --> 01:08:59.549
if the same thing happens, can I do something

1407
01:08:59.560 --> 01:09:02.879
differently instead to, to help unwind? Uh And maybe

1408
01:09:02.890 --> 01:09:05.180
tomorrow that's reading a book on my Kindle which

1409
01:09:05.189 --> 01:09:07.279
is still screen time, but maybe it helped me

1410
01:09:07.290 --> 01:09:09.149
unwind a bit more or maybe it is scrolling

1411
01:09:09.160 --> 01:09:10.859
through Instagram, but it's just doing it with my

1412
01:09:10.870 --> 01:09:13.029
wife and like looking at things and chatting to

1413
01:09:13.040 --> 01:09:15.330
each other. Uh And when you start thinking about

1414
01:09:15.339 --> 01:09:16.770
it in that sort of sense, a few things

1415
01:09:16.779 --> 01:09:20.640
happen, I think one is you don't feel as

1416
01:09:20.649 --> 01:09:23.220
guilty or as ashamed anymore. And I think there's

1417
01:09:23.229 --> 01:09:26.770
a lot of shame in this conversation, you know,

1418
01:09:26.779 --> 01:09:30.330
the shame around people saying things like who are

1419
01:09:30.339 --> 01:09:32.890
these stupid parents that give their kids a phone

1420
01:09:32.899 --> 01:09:34.740
at whatever age? You know, they're sort of looking

1421
01:09:34.750 --> 01:09:37.930
down and sneering at people, the sort of personal

1422
01:09:37.939 --> 01:09:40.390
shame. You know, there are moments where I felt

1423
01:09:40.399 --> 01:09:43.220
guilty about being on my phone and I think

1424
01:09:43.229 --> 01:09:45.529
we need to cut ourselves a bit of slack.

1425
01:09:45.709 --> 01:09:48.069
I'm probably coming into your final question though. Right.

1426
01:09:48.080 --> 01:09:50.379
You know, what can we do? Um I think

1427
01:09:50.390 --> 01:09:52.000
we definitely need to cut ourselves a bit of

1428
01:09:52.009 --> 01:09:55.209
slack and just, you know, go, ok. You know,

1429
01:09:55.439 --> 01:09:59.149
we've all developed bad habits with our phones. The

1430
01:09:59.160 --> 01:10:00.669
answer to that is not get rid of them

1431
01:10:00.680 --> 01:10:02.640
because there's loads of good stuff that they do

1432
01:10:02.649 --> 01:10:05.410
for us as well. The answer to that is

1433
01:10:05.419 --> 01:10:07.819
how do we maximize the positive aspects of this

1434
01:10:07.830 --> 01:10:10.270
and mini minimize the, the, the negatives and the

1435
01:10:10.279 --> 01:10:12.979
potential harms. And the first step in that is

1436
01:10:12.990 --> 01:10:15.560
being more reflective, being more aware of what you're

1437
01:10:15.569 --> 01:10:19.500
doing on your phone. But feeling empowered to say

1438
01:10:19.870 --> 01:10:22.060
if there's something that I'm not getting anything good

1439
01:10:22.069 --> 01:10:24.959
out of, how do I get rid of that?

1440
01:10:26.069 --> 01:10:28.009
You know, I think sometimes we feel like, oh,

1441
01:10:28.020 --> 01:10:30.250
well, I don't want to delete my Twitter account

1442
01:10:30.259 --> 01:10:33.549
because sometimes I get really good stuff out of

1443
01:10:33.560 --> 01:10:34.779
that. And I feel as though I should be

1444
01:10:34.790 --> 01:10:37.830
kind of available there. Um, IF you're not getting

1445
01:10:37.839 --> 01:10:40.540
anything good out of Twitter, I'm just using that

1446
01:10:40.549 --> 01:10:41.810
because it's the first example that came out of

1447
01:10:41.819 --> 01:10:43.870
my head. Yeah, delete it and you won't lose

1448
01:10:43.879 --> 01:10:46.549
out. Right. But, you know, I know from my

1449
01:10:46.560 --> 01:10:49.509
personal experience I've had some really rubbish encounters with

1450
01:10:49.520 --> 01:10:51.589
people on, on Twitter that have left me feeling

1451
01:10:51.600 --> 01:10:54.250
really bad, but I have really good ones as

1452
01:10:54.259 --> 01:10:56.120
well and I still use it as a sort

1453
01:10:56.129 --> 01:10:58.479
of, despite everything that's happened with Twitter the Dumpster

1454
01:10:58.490 --> 01:10:59.910
fire that that's been over the past year or

1455
01:10:59.919 --> 01:11:02.390
more. I still find it a really good source

1456
01:11:02.399 --> 01:11:06.040
of finding new papers, finding new, um stories that

1457
01:11:06.049 --> 01:11:08.750
are relevant to my research and science communication interest.

1458
01:11:08.850 --> 01:11:10.359
So try and focus on how do I use

1459
01:11:10.370 --> 01:11:12.379
it more for that. And I say this in

1460
01:11:12.390 --> 01:11:14.100
the book, you know, part of what that's meant

1461
01:11:14.109 --> 01:11:16.359
for me, uh not just on Twitter, but online

1462
01:11:16.370 --> 01:11:19.120
generally is that I've thought really more about being

1463
01:11:19.129 --> 01:11:21.279
a nicer person online. You know, there have been

1464
01:11:21.290 --> 01:11:23.500
times in the past where I've said something really

1465
01:11:23.509 --> 01:11:27.029
snarky to somebody cos I'm frustrated with the conversation.

1466
01:11:27.109 --> 01:11:28.930
And do you know what? Never had a good

1467
01:11:28.939 --> 01:11:31.645
outcome? I never had something productive come out of

1468
01:11:31.654 --> 01:11:33.415
it. So I try really hard now to go,

1469
01:11:33.645 --> 01:11:35.254
you know, and sometimes I have that urge to

1470
01:11:35.265 --> 01:11:37.714
like dunk on somebody online, but I, I withhold

1471
01:11:37.725 --> 01:11:40.625
it and I post less online and I think

1472
01:11:40.634 --> 01:11:42.535
that's a good thing for me. You know, because

1473
01:11:42.544 --> 01:11:45.395
I'm making it work more for me, making, making

1474
01:11:45.404 --> 01:11:48.049
it more useful as a tool. And I think

1475
01:11:48.060 --> 01:11:49.759
if we can all do that a little bit

1476
01:11:49.779 --> 01:11:51.740
and it's not, it's not something that you just

1477
01:11:51.750 --> 01:11:53.879
do once, it's something that you have to do

1478
01:11:53.890 --> 01:11:56.919
repeatedly. You know, it's hard work, changing a habit

1479
01:11:56.930 --> 01:12:00.000
is not gonna happen overnight. It's like changing the

1480
01:12:00.009 --> 01:12:03.020
direction of a massive oil tanker. It takes ages,

1481
01:12:03.029 --> 01:12:04.819
right? Cos this is a massive, heavy thing to

1482
01:12:04.830 --> 01:12:08.979
turn around. You're not gonna get overnight results. But

1483
01:12:08.990 --> 01:12:12.529
what you will get is long term healthier, more

1484
01:12:12.540 --> 01:12:13.839
empowering results if you do that.

1485
01:12:14.870 --> 01:12:18.549
So you've already ended up at least partly answering

1486
01:12:18.560 --> 01:12:21.549
my last question. But uh let me still ask

1487
01:12:21.560 --> 01:12:26.379
you generally speaking, what would be your suggestions when

1488
01:12:26.390 --> 01:12:29.830
it comes to how we should think about and

1489
01:12:29.839 --> 01:12:34.504
approach screen based technology and even digital technologies more

1490
01:12:34.515 --> 01:12:39.064
generally without coming to the table with just an

1491
01:12:39.075 --> 01:12:43.064
assumption stemming from a moral panic about how supposedly

1492
01:12:43.075 --> 01:12:46.975
they are all negative and we should just drop

1493
01:12:46.984 --> 01:12:49.665
them all together and stuff like that.

1494
01:12:50.620 --> 01:12:53.279
So it's a great question. And I think the

1495
01:12:53.290 --> 01:12:55.020
thing that everybody has to remember is that the

1496
01:12:55.029 --> 01:12:58.229
these sorts of technologies they're not going away right?

1497
01:12:58.459 --> 01:13:02.330
There is no scenario here where we all stop

1498
01:13:02.339 --> 01:13:06.899
using smartphones or tablets or social media. What, what

1499
01:13:06.910 --> 01:13:09.089
the likely scenario is that the way that we

1500
01:13:09.100 --> 01:13:14.029
use them changes and then that drives technological changes

1501
01:13:14.040 --> 01:13:15.810
so that the kinds of tech that, that we'll

1502
01:13:15.819 --> 01:13:18.459
be using in 10 years time will look very

1503
01:13:18.470 --> 01:13:21.629
different to what we're using now. That's, that's just

1504
01:13:21.640 --> 01:13:25.919
how things change. Right. Um We, we've talked a

1505
01:13:25.930 --> 01:13:28.089
little bit about these sorts of individual things that

1506
01:13:28.100 --> 01:13:30.589
we can do, you know, re reframing how we

1507
01:13:30.600 --> 01:13:32.470
think about this, not in terms of addictions, but

1508
01:13:32.479 --> 01:13:35.750
in terms of habits, feeling more empowered to make

1509
01:13:35.759 --> 01:13:38.430
changes in that sense, I think is a really

1510
01:13:38.439 --> 01:13:41.600
useful thing to do. Um I think that's also

1511
01:13:41.609 --> 01:13:45.580
true of the public debate though more generally, the

1512
01:13:45.589 --> 01:13:49.479
thing that I'm, I'm really saddened about uh at

1513
01:13:49.490 --> 01:13:52.529
the minute is that, that, that public debate has

1514
01:13:52.540 --> 01:13:58.319
become very polarized. We're now in a position effectively

1515
01:13:58.419 --> 01:14:03.180
where people are so ideologically entrenched, either in terms

1516
01:14:03.189 --> 01:14:05.799
of the phones are bad or the phones aren't

1517
01:14:05.810 --> 01:14:11.149
bad. It's very difficult to talk across that barrier

1518
01:14:11.669 --> 01:14:14.310
that, you know, I find it very difficult as

1519
01:14:14.319 --> 01:14:18.229
I, I feel and I include myself in this.

1520
01:14:18.240 --> 01:14:20.209
I'm not saying that this is other people, although

1521
01:14:20.220 --> 01:14:23.500
it is other people as well collectively, I feel

1522
01:14:23.509 --> 01:14:25.620
as though there's been a failure of science communication

1523
01:14:25.790 --> 01:14:29.859
here that we've not taken lessons from the past

1524
01:14:30.100 --> 01:14:35.919
and adapted them here and admitted that there is

1525
01:14:35.930 --> 01:14:39.779
a lot of uncertainty around the research that we

1526
01:14:39.790 --> 01:14:42.459
don't know the answers yet. The science is not

1527
01:14:42.470 --> 01:14:47.049
there yet. We know what happens in other sorts

1528
01:14:47.060 --> 01:14:49.339
of moral planets. If you look at the, the

1529
01:14:49.350 --> 01:14:52.040
trajectory of research in things like the, the violent

1530
01:14:52.049 --> 01:14:57.109
video game debate, whatever, when those debates kick off,

1531
01:14:57.120 --> 01:14:59.779
they're very heated and very polarized. And the research

1532
01:14:59.790 --> 01:15:02.060
that comes out reflects that you see these very

1533
01:15:02.069 --> 01:15:05.790
big effects suggesting that video games are really bad

1534
01:15:06.629 --> 01:15:10.009
but using poor methodologies because everybody's rushing around trying

1535
01:15:10.020 --> 01:15:11.979
to do some research in it. And as the

1536
01:15:11.990 --> 01:15:14.919
research matures and as it gets better and our,

1537
01:15:15.020 --> 01:15:17.299
and the the methods that we develop improve over

1538
01:15:17.310 --> 01:15:21.479
time, the effects get smaller and sometimes they go

1539
01:15:21.490 --> 01:15:25.569
away and that sort of tracks with debates over

1540
01:15:25.580 --> 01:15:28.830
time where people get less and less vociferous about

1541
01:15:28.839 --> 01:15:30.979
this. You know, the best evidence that we've got

1542
01:15:30.990 --> 01:15:32.720
in the violent video game and aggression of debate

1543
01:15:32.729 --> 01:15:34.759
at the minute is that there are some small

1544
01:15:34.770 --> 01:15:36.919
associations there. You do feel a bit more aggressive,

1545
01:15:36.930 --> 01:15:39.200
playing a violent video game, but it's very, very

1546
01:15:39.209 --> 01:15:40.899
short lived and it doesn't have any impact on

1547
01:15:40.910 --> 01:15:42.919
anything else. It's not the thing to worry about,

1548
01:15:44.629 --> 01:15:46.640
but we're not really talking about violent video games

1549
01:15:46.649 --> 01:15:50.640
and aggression anymore, right? We've moved on. Um So,

1550
01:15:51.060 --> 01:15:52.979
and, and, but I feel like that's more of

1551
01:15:52.990 --> 01:15:54.890
an extreme thing at the minute that we've become

1552
01:15:54.899 --> 01:15:58.770
really polarized in this debate and that it's not

1553
01:15:58.779 --> 01:16:00.609
that people aren't talking to each other on, on

1554
01:16:00.620 --> 01:16:03.250
both sides. It's that they're, they're shouting, they're angry

1555
01:16:03.259 --> 01:16:05.930
like people are angry about it and I get

1556
01:16:05.939 --> 01:16:08.560
where that comes from, right? And again, it goes

1557
01:16:08.569 --> 01:16:10.109
back to that idea that I said earlier that

1558
01:16:10.120 --> 01:16:11.879
I think everybody in this debate is trying to

1559
01:16:11.890 --> 01:16:14.509
do the right thing, trying to get good outcomes.

1560
01:16:14.520 --> 01:16:19.589
Everybody wants everybody but kids in particular, but everybody

1561
01:16:19.600 --> 01:16:24.680
to develop healthier relationships with their tech like weird

1562
01:16:24.689 --> 01:16:27.160
if somebody didn't want that like they are, that

1563
01:16:27.169 --> 01:16:29.490
person is not part of that conversation. Whoever that

1564
01:16:29.500 --> 01:16:32.339
is, everybody's trying to get to the same outcome

1565
01:16:32.529 --> 01:16:35.299
where we all differ is on the journey and

1566
01:16:35.310 --> 01:16:37.689
on what the actual outcome looks like. So for

1567
01:16:37.700 --> 01:16:42.620
me, the journey shouldn't be about fear or shame

1568
01:16:42.629 --> 01:16:45.620
or guilt that uh because what happens there is

1569
01:16:45.629 --> 01:16:48.359
we start scaring people and this is literally what's

1570
01:16:48.370 --> 01:16:50.479
happening at the minute. People are going, this is

1571
01:16:50.490 --> 01:16:53.680
an emergency, this is definitely bad. We need to

1572
01:16:53.689 --> 01:16:56.959
do something. Now you put people into that panic

1573
01:16:56.970 --> 01:17:00.439
mode and they get scared and they lash out

1574
01:17:01.140 --> 01:17:03.209
and that's what happens in this debate, right? You

1575
01:17:03.220 --> 01:17:05.200
know, if you come along and say the the

1576
01:17:05.209 --> 01:17:08.899
research is complicated, people start attacking you. You know,

1577
01:17:08.910 --> 01:17:10.799
they'll start asking you well, who you're funded by,

1578
01:17:10.810 --> 01:17:13.220
you must be funded by big tech or Metro

1579
01:17:13.229 --> 01:17:15.040
or stuff like that. And just for the sake

1580
01:17:15.049 --> 01:17:16.890
of arguing, I am not funded by any of

1581
01:17:16.899 --> 01:17:19.310
those uh companies. I don't have any conflicts of

1582
01:17:19.319 --> 01:17:21.080
interest in that sense. Uh I don't have any

1583
01:17:21.089 --> 01:17:23.279
conflicts of interest. My conflict of interest is that

1584
01:17:23.290 --> 01:17:24.919
I wrote a book about this. So you could

1585
01:17:24.930 --> 01:17:27.379
perceive that as I've got a book to sell

1586
01:17:27.399 --> 01:17:29.000
fine if you wanna, if you wanna look at

1587
01:17:29.009 --> 01:17:32.189
it that way. But, um, so do other people

1588
01:17:32.200 --> 01:17:36.000
as well, right? Um, BUT they like, you know,

1589
01:17:36.009 --> 01:17:37.240
and, and I think also it's re it's a

1590
01:17:37.250 --> 01:17:38.990
reasonable question, right? It's a reasonable thing to say,

1591
01:17:39.000 --> 01:17:40.330
you know, who are you funded by? Because that's

1592
01:17:40.339 --> 01:17:42.060
part of the puzzle, right? So I'm not saying

1593
01:17:42.069 --> 01:17:44.629
that we shouldn't ask those questions. But I think

1594
01:17:44.640 --> 01:17:46.120
if we can approach this with a little bit

1595
01:17:46.129 --> 01:17:52.270
more compassionate, empathetic, critical curiosity, we would do the

1596
01:17:52.279 --> 01:17:55.490
debate a big service, right? And I, I think

1597
01:17:55.500 --> 01:17:57.049
this is the final point that I make in

1598
01:17:57.060 --> 01:18:01.290
my book that um it's very easy to be

1599
01:18:01.299 --> 01:18:03.850
very critical of something that you don't agree with

1600
01:18:03.859 --> 01:18:06.759
that. Something that doesn't fit with your worldview. It's

1601
01:18:06.770 --> 01:18:09.979
very hard to be critical of something that does

1602
01:18:09.990 --> 01:18:11.660
fit with your worldview. And I think we need

1603
01:18:11.669 --> 01:18:13.720
to be better at that when, when a headline

1604
01:18:13.729 --> 01:18:15.729
comes along or a new article, new p A

1605
01:18:15.740 --> 01:18:21.029
journal article comes along that says something about smartphones

1606
01:18:21.040 --> 01:18:23.430
and social media, whatever it says good or bad,

1607
01:18:23.680 --> 01:18:27.129
don't just take it at face value. Think about

1608
01:18:27.549 --> 01:18:29.959
what the evidence is to support that claim. Is

1609
01:18:29.970 --> 01:18:32.290
it a good paper or a bad paper? I

1610
01:18:32.299 --> 01:18:34.890
fully appreciate that. It's all well and good me

1611
01:18:34.899 --> 01:18:37.720
saying that people don't have time for this, right?

1612
01:18:37.759 --> 01:18:39.870
We don't have time. Every time somebody comes along

1613
01:18:39.879 --> 01:18:42.879
and says phones are bad because whatever we don't,

1614
01:18:42.890 --> 01:18:44.540
we don't literally don't have the time to go.

1615
01:18:44.549 --> 01:18:46.850
I'm gonna spend two hours of my day investigating

1616
01:18:46.859 --> 01:18:48.839
the science behind it. I'm sorry, I know we

1617
01:18:48.850 --> 01:18:51.049
don't have time for that. Um If you do

1618
01:18:51.060 --> 01:18:53.500
great, try and do it. Um, BUT if nothing

1619
01:18:53.509 --> 01:18:57.399
else just try and use those moments as a

1620
01:18:57.410 --> 01:18:59.410
chance to check in with yourself and go. Ok.

1621
01:18:59.419 --> 01:19:00.959
Well, I've seen this thing whether I agree with

1622
01:19:00.970 --> 01:19:03.100
it or not. Is it an opportunity for me

1623
01:19:03.109 --> 01:19:05.350
to think about the relationship that I've got with

1624
01:19:05.359 --> 01:19:08.040
my phone or my social media or use or

1625
01:19:08.049 --> 01:19:10.890
whatever it is just using it as those chances

1626
01:19:10.899 --> 01:19:13.810
to think about building better habits I think is

1627
01:19:13.819 --> 01:19:15.479
something that we could all kind of get on

1628
01:19:15.490 --> 01:19:15.959
board with.

1629
01:19:17.330 --> 01:19:20.790
Great. So the book is again unlocked the real

1630
01:19:20.799 --> 01:19:23.709
science of screen time and how to spend it

1631
01:19:23.720 --> 01:19:25.759
better. Of course, I'm leaving a link to it

1632
01:19:25.770 --> 01:19:28.750
in the description box of the interview and doctor

1633
01:19:29.149 --> 01:19:31.830
Els just before we go apart from the book,

1634
01:19:31.839 --> 01:19:34.270
would you like to let people know where they

1635
01:19:34.279 --> 01:19:36.589
can find you and the rest of your work

1636
01:19:36.600 --> 01:19:37.529
on the internet?

1637
01:19:38.180 --> 01:19:40.640
Sure. Uh Yeah, a anywhere really, I'm available on

1638
01:19:40.649 --> 01:19:44.450
most um social media platforms. I'm usually my handle

1639
01:19:44.459 --> 01:19:46.990
is usually at Peter Actuals uh or you can

1640
01:19:47.000 --> 01:19:49.430
contact me through my website, my website is Peter

1641
01:19:49.520 --> 01:19:53.640
actuals.com. Um Yeah, that, that that'll kind of come

1642
01:19:53.649 --> 01:19:55.640
through to my email. So, however, you want to

1643
01:19:55.649 --> 01:19:57.450
get in touch. I'm, I'm always happy to chat

1644
01:19:57.459 --> 01:19:58.399
to people about this stuff.

1645
01:19:59.140 --> 01:20:01.709
Great. So thank you so much for taking the

1646
01:20:01.720 --> 01:20:04.979
time to do the interview. I it was a

1647
01:20:04.990 --> 01:20:07.979
really informative one. So thank you so much.

1648
01:20:08.180 --> 01:20:09.399
Uh Thank you for having me.

1649
01:20:10.540 --> 01:20:13.270
Hi guys. Thank you for watching this interview. Until

1650
01:20:13.279 --> 01:20:15.439
the end. If you liked it, please share it.

1651
01:20:15.450 --> 01:20:18.250
Leave a like and hit the subscription button. The

1652
01:20:18.259 --> 01:20:20.359
show is brought to you by N Lights learning

1653
01:20:20.370 --> 01:20:23.379
and development. Then differently check the website at N

1654
01:20:23.390 --> 01:20:27.339
lights.com and also please consider supporting the show on

1655
01:20:27.350 --> 01:20:30.390
Patreon or paypal. I would also like to give

1656
01:20:30.399 --> 01:20:32.700
a huge thank you to my main patrons and

1657
01:20:32.709 --> 01:20:36.919
paypal supporters, Perera Larson, Jerry Muller and Frederick Suno,

1658
01:20:36.970 --> 01:20:40.040
Bernard Seche O of Alex Adam, Castle Matthew Whitting

1659
01:20:40.080 --> 01:20:43.319
Bear. No wolf, Tim Ho Erica LJ Connors, Philip

1660
01:20:43.330 --> 01:20:46.240
Forrest Connelly. Then the Met Robert Wine in Nai

1661
01:20:46.600 --> 01:20:49.990
Z Mark Nevs calling in Holbrook Field, Governor Mikel

1662
01:20:50.000 --> 01:20:53.830
Stormer Samuel Andre Francis for Agns Ferger, Ken Herz

1663
01:20:54.979 --> 01:20:58.609
and Lain Jung Y and the K Hes Mark

1664
01:20:58.620 --> 01:21:03.709
Smith J Tom Hummel Sran, David Wilson, Yasa dear,

1665
01:21:03.750 --> 01:21:08.529
Roman Roach Diego, Jan Punter, Romani Charlotte Bli Nicole

1666
01:21:08.589 --> 01:21:12.359
Barba, Adam Hunt, Pavlo, Stassi, Nale me, Gary G

1667
01:21:12.569 --> 01:21:17.220
Alman, Samos, Ari and YPJ Barboza Julian Price Edward

1668
01:21:17.229 --> 01:21:21.629
Hall, Eden, Broner Douglas Fry Franca, Lati Gilon Cortez

1669
01:21:21.870 --> 01:21:28.600
or Solis Scott. Zachary. Ftw Daniel Friedman, William Buckner,

1670
01:21:28.609 --> 01:21:32.649
Paul Giorgino, Luke Loki, Georgio Theophano Chris Williams and

1671
01:21:32.660 --> 01:21:37.279
Peter Wo David Williams Di Costa, Anton Erickson Charles

1672
01:21:37.290 --> 01:21:42.589
Murray, Alex Shaw, Marie Martinez, Coralie Chevalier, Bangalore Larry

1673
01:21:42.600 --> 01:21:47.370
Dey junior, Old Einon Starry Michael Bailey. Then Spur

1674
01:21:47.379 --> 01:21:51.669
by Robert Grassy Zorn, Jeff mcmahon, Jake Zul Barnabas

1675
01:21:51.689 --> 01:21:55.259
Radis Mark Kemple Thomas Dvor Luke Neeson Chris to

1676
01:21:55.419 --> 01:22:00.040
Kimberley Johnson, Benjamin Gilbert Jessica. No, Linda Brendan Nicholas

1677
01:22:00.049 --> 01:22:05.240
Carlson Ismael Bensley Man George Katis, Valentine Steinman, Perlis

1678
01:22:05.810 --> 01:22:11.509
Kate Von Goler, Alexander Abert Liam Dan Biar Masoud

1679
01:22:12.220 --> 01:22:16.919
Ali Mohammadi Perpendicular J Ner Urla. Good enough, Gregory

1680
01:22:16.930 --> 01:22:21.229
Hastings David Pins of Sean Nelson, Mike Levin and

1681
01:22:21.259 --> 01:22:24.500
Jos Net. A special thanks to my producers is

1682
01:22:24.509 --> 01:22:27.149
our web, Jim Frank Luca stuffin, Tom Vig and

1683
01:22:27.160 --> 01:22:31.609
Bernard N Cortes Dixon Benedikt Muller Thomas Trumbo, Catherine

1684
01:22:31.620 --> 01:22:34.779
and Patrick Tobin, John Carlman, Negro, Nick Ortiz and

1685
01:22:34.790 --> 01:22:38.069
Nick Golden. And to my executive producers Matthew lavender,

1686
01:22:38.200 --> 01:22:41.379
Si Adrian Bogdan Knits and Rosie. Thank you for

1687
01:22:41.390 --> 01:22:41.700
all

