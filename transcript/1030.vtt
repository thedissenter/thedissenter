WEBVTT

1
00:00:00.779 --> 00:00:03.430
Hello, everyone. Welcome to a new episode of the,

2
00:00:03.440 --> 00:00:06.179
the Center. I'm your host, Ricard Lobs. And today

3
00:00:06.190 --> 00:00:09.439
I'm joined by returned guest, Doctor Joel Mars, he's

4
00:00:09.449 --> 00:00:12.319
Professor Emeritus of Philosophy at the University of New

5
00:00:12.329 --> 00:00:17.090
Haven and Bioethics Center scholar at Yale University. Uh

6
00:00:17.100 --> 00:00:20.500
I'm also leaving a link to our first interview

7
00:00:20.510 --> 00:00:22.965
where we talked a lot about moral and the

8
00:00:23.045 --> 00:00:26.715
realism, desires and topics like that. And today we're

9
00:00:26.725 --> 00:00:30.545
going to focus mostly on his book, The Spread

10
00:00:30.565 --> 00:00:34.284
and other essays on moralism and Guilt. But we're

11
00:00:34.294 --> 00:00:38.075
also going to talk more generally about moral realism

12
00:00:38.084 --> 00:00:41.525
and anti realism desires and other related topics. So,

13
00:00:41.534 --> 00:00:43.915
Doctor Marks, welcome back to the show. It's always

14
00:00:43.924 --> 00:00:45.034
a pleasure to everyone.

15
00:00:45.645 --> 00:00:48.154
Thank you, Ricardo. Very nice to be back with

16
00:00:48.165 --> 00:00:48.435
you.

17
00:00:49.720 --> 00:00:53.130
So let me start by asking you then also

18
00:00:53.139 --> 00:00:56.610
because to try to contextualize the conversation here a

19
00:00:56.619 --> 00:01:00.450
little bit and talking about moral realism and anti

20
00:01:00.459 --> 00:01:05.580
realism. Where would you say morals stem from?

21
00:01:07.610 --> 00:01:10.370
Well, I, I think there must be several answers

22
00:01:10.379 --> 00:01:14.160
to that question. Uh FOR example, uh a biologist

23
00:01:14.169 --> 00:01:19.209
of course, or a social biologist uh might uh

24
00:01:19.220 --> 00:01:23.389
look for an evolutionary answer. A Darwinian answer, somehow

25
00:01:23.400 --> 00:01:27.480
morality has served a purpose uh contributing to the

26
00:01:27.489 --> 00:01:30.430
survival of our species or at the very least,

27
00:01:30.440 --> 00:01:35.550
has not inhibited the survival of our species. Uh

28
00:01:35.559 --> 00:01:39.879
I am not uh especially qualified to uh give

29
00:01:39.889 --> 00:01:44.900
a biological answer. Uh I'm currently much more interested

30
00:01:44.910 --> 00:01:46.160
uh in a and I guess I call it

31
00:01:46.169 --> 00:01:50.089
a psychological answer, even though I'm a philosopher by

32
00:01:50.099 --> 00:01:54.000
trade, I do have some background in psychology. That

33
00:01:54.010 --> 00:01:57.290
is where my studies actually began. And uh for,

34
00:01:58.379 --> 00:02:01.919
for probably all my career, even though I focused

35
00:02:01.930 --> 00:02:05.360
mainly on ethics, there's always been a strong psychological

36
00:02:05.370 --> 00:02:08.899
component of how I have done uh my ethical

37
00:02:08.910 --> 00:02:13.179
philosophizing. So, uh to get right to your question,

38
00:02:13.679 --> 00:02:19.910
uh My thinking is that desire is, is where

39
00:02:19.919 --> 00:02:24.750
it starts. Uh Now I'm, I'm totally speculating here

40
00:02:25.029 --> 00:02:28.509
uh informed by my studies, by my experience and

41
00:02:28.520 --> 00:02:31.429
so forth and so forth. But uh my, my

42
00:02:31.440 --> 00:02:36.500
strong hunch is that, uh it begins with all

43
00:02:36.509 --> 00:02:44.119
human beings desiring, well, I might say that things

44
00:02:44.130 --> 00:02:48.720
turn out well, even for the best, uh there's

45
00:02:48.729 --> 00:02:51.139
a bit of question begging there because then I'd

46
00:02:51.149 --> 00:02:52.740
have to say, well, what does that mean? What

47
00:02:52.750 --> 00:02:54.279
does it mean for things to be for the

48
00:02:54.289 --> 00:02:57.570
best and so forth? And ultimately, I would want

49
00:02:57.580 --> 00:03:01.050
to give a psychological answer to that as well

50
00:03:01.059 --> 00:03:03.479
in effect. I want to say we desire what

51
00:03:03.490 --> 00:03:08.080
we desire. Um And, and not hold out for

52
00:03:08.089 --> 00:03:11.240
there being some kind of objective good that we

53
00:03:11.250 --> 00:03:14.789
desire, no, something becomes good because we desire it.

54
00:03:15.440 --> 00:03:18.179
So perhaps the way I should put this is

55
00:03:18.190 --> 00:03:21.850
to say that uh we have a hierarchy of

56
00:03:21.860 --> 00:03:25.210
desires, obviously, some of our desires are stronger than

57
00:03:25.220 --> 00:03:28.130
others. So when it comes to the, the things

58
00:03:28.139 --> 00:03:31.949
that we desire most and here again, we, it

59
00:03:32.330 --> 00:03:35.080
doesn't have to imply that we all have the

60
00:03:35.089 --> 00:03:39.220
same desires. OK. Uh But whatever it is that

61
00:03:39.229 --> 00:03:43.929
we desire most uh we desire with such strength

62
00:03:45.289 --> 00:03:51.630
that we, we cannot tolerate the possibility that our

63
00:03:51.639 --> 00:03:57.350
desires might not be satisfied. And so I think

64
00:03:57.360 --> 00:04:01.220
that so by some kind of psychologic, which is

65
00:04:01.229 --> 00:04:08.039
not really logical in some purer sense, are desiring

66
00:04:08.050 --> 00:04:16.660
something so strongly induces in us a belief that

67
00:04:16.670 --> 00:04:21.869
the world ought to be a certain way. In

68
00:04:21.880 --> 00:04:25.209
other words, it ought to be the way we

69
00:04:25.220 --> 00:04:29.309
want it to be. And that I think is

70
00:04:29.320 --> 00:04:37.079
where morality begins, as soon as desiring something becomes

71
00:04:37.869 --> 00:04:42.109
believing that it ought to be that way. I

72
00:04:42.119 --> 00:04:47.339
think we introduce morality into the world. That's my

73
00:04:47.350 --> 00:04:47.760
short,

74
00:04:49.959 --> 00:04:54.019
that's fine. So, but getting more specifically into some

75
00:04:54.029 --> 00:04:57.140
of the topics that you explore in your book,

76
00:04:57.149 --> 00:05:00.220
the spread, what would you say are the roles

77
00:05:00.230 --> 00:05:05.809
played by behaviors like blaming emotions, like guilt and

78
00:05:05.820 --> 00:05:08.859
shame. What kind, what kind of roles do they

79
00:05:08.869 --> 00:05:10.459
play in morality?

80
00:05:11.700 --> 00:05:16.329
Well, see there's a whole uh bunch of concepts

81
00:05:16.630 --> 00:05:23.480
which go together and starting with, I suppose morality

82
00:05:23.489 --> 00:05:26.980
itself, the concept. All right. And, uh, and here

83
00:05:26.989 --> 00:05:30.329
again, given the diversity of the world, the diversity

84
00:05:30.339 --> 00:05:35.380
of human thinking desires and now beliefs, uh, there

85
00:05:35.390 --> 00:05:40.179
will be many different concepts of morality. Uh, BUT

86
00:05:40.190 --> 00:05:43.640
it does seem to be that well, at, at

87
00:05:43.649 --> 00:05:47.250
least in my knowledge and my experience, uh, the

88
00:05:47.260 --> 00:05:50.769
concept of morality will tend to bring along these

89
00:05:50.779 --> 00:05:56.570
other concepts that you've mentioned. So, for example, if,

90
00:05:56.579 --> 00:06:00.630
um, if you believe uh that things ought to

91
00:06:00.640 --> 00:06:06.209
be a certain way, then if somebody uh does

92
00:06:06.220 --> 00:06:11.839
something that goes against that, well, we might say

93
00:06:11.850 --> 00:06:16.480
that they've done something wrong. And then if we

94
00:06:16.489 --> 00:06:22.369
believe somebody has done something wrong, we might feel

95
00:06:22.380 --> 00:06:29.429
that they uh deserve. There's another moral concept, blame

96
00:06:29.440 --> 00:06:34.579
another moral concept. And if the person we believe

97
00:06:34.589 --> 00:06:40.190
has done something wrong happens to be oneself, then

98
00:06:40.200 --> 00:06:45.049
the self blaming is in effect guilt or the

99
00:06:45.059 --> 00:06:48.589
feeling of guilt. So that's where they all come

100
00:06:48.600 --> 00:06:53.679
in. And, you know, these are commonplace concepts, at

101
00:06:53.690 --> 00:06:58.459
least in what should I say, Western society. Uh

102
00:06:58.649 --> 00:07:02.170
You know, the societies that I'm most familiar with,

103
00:07:02.179 --> 00:07:06.040
although I do have some knowledge of uh non

104
00:07:06.049 --> 00:07:08.600
western philosophies, although I can't say that I have

105
00:07:08.609 --> 00:07:15.190
much knowledge of, of uh non western societies, their

106
00:07:15.200 --> 00:07:18.540
philosophies. I know something about the, the, the stuff

107
00:07:18.549 --> 00:07:21.690
you get from book learning. Uh But I have

108
00:07:21.700 --> 00:07:24.350
not lived in a non Western society and so

109
00:07:24.359 --> 00:07:27.179
forth. But I would imagine, I would imagine you'll

110
00:07:27.190 --> 00:07:30.950
find something similar, something analogous there as well.

111
00:07:31.989 --> 00:07:37.010
Mhm. But then emotions like guilt and shame, I

112
00:07:37.019 --> 00:07:41.750
mean, the way they play out psychologically they sort

113
00:07:41.760 --> 00:07:45.709
of, uh, bring about, uh, a form of self

114
00:07:45.899 --> 00:07:48.510
control. Right. I mean, people, they live in a

115
00:07:48.519 --> 00:07:54.195
particular kind of society that is where certain particular

116
00:07:54.204 --> 00:07:57.894
moral, moral values are dominant and then if they

117
00:07:57.904 --> 00:08:01.524
go against those moral values or against the norms

118
00:08:01.535 --> 00:08:04.964
that are prevalent in that society, then it's when

119
00:08:04.975 --> 00:08:09.825
they might feel guilt or shame. So those emotions

120
00:08:09.834 --> 00:08:14.255
uh serve a function of also self control or

121
00:08:14.265 --> 00:08:14.614
not.

122
00:08:15.559 --> 00:08:19.089
Uh Yeah. Yeah, I mean, that's exactly right. Uh

123
00:08:19.100 --> 00:08:21.519
Again, here, here, I would be going into yet

124
00:08:21.529 --> 00:08:24.690
another field, you know, the social sciences. Right. Again,

125
00:08:24.700 --> 00:08:28.480
that, that's not my field, but certainly just uh

126
00:08:28.489 --> 00:08:31.260
as a layperson. And also, of course, having read

127
00:08:31.269 --> 00:08:33.929
some things in those other fields, I, I had

128
00:08:33.940 --> 00:08:39.409
that sense that uh morality amounts to internalizing something

129
00:08:39.789 --> 00:08:43.260
that certainly for most, if not all of us,

130
00:08:43.270 --> 00:08:45.890
uh must come from the society in which we

131
00:08:45.900 --> 00:08:49.820
grow up. Um uh So the society can, of

132
00:08:49.830 --> 00:08:54.289
course, exert external forces upon us. Uh MOST obviously

133
00:08:54.299 --> 00:08:58.210
and explicitly in the statutory laws, criminal laws and

134
00:08:58.219 --> 00:09:02.400
so so forth. Um But uh yes, uh there

135
00:09:02.409 --> 00:09:06.039
are subtler ways, you know, uh society helps to

136
00:09:06.049 --> 00:09:10.309
constitute who we are, uh even as individuals, uh

137
00:09:10.320 --> 00:09:13.219
uh starting with parents, I suppose. But ultimately the

138
00:09:13.229 --> 00:09:16.659
entire society, the entire culture and certainly a huge

139
00:09:16.669 --> 00:09:19.380
part of that would be internalizing a morality that

140
00:09:19.390 --> 00:09:23.780
is somehow, uh you know, present uh all around

141
00:09:23.789 --> 00:09:25.780
us and, and this may be a good place

142
00:09:25.789 --> 00:09:30.239
for me to introduce another idea that um uh

143
00:09:30.250 --> 00:09:33.070
I spoke of there being different concepts of morality.

144
00:09:33.080 --> 00:09:37.070
One very basic kind of distinction. I think you

145
00:09:37.080 --> 00:09:42.349
would find is between uh uh morality as what

146
00:09:42.359 --> 00:09:48.309
I guess the sociologists would call ma uh whi

147
00:09:48.320 --> 00:09:50.739
which, you know, again, with, with the notions of,

148
00:09:50.750 --> 00:09:53.390
of right and wrong or, or proper behavior in

149
00:09:53.400 --> 00:09:56.349
a given society. Uh But then there would be

150
00:09:56.359 --> 00:09:59.900
something that is uh you might say more metaphysical

151
00:10:00.369 --> 00:10:04.219
uh what I tend to speak of as objective,

152
00:10:04.229 --> 00:10:11.020
right and wrong, objective, good and bad. Um And

153
00:10:11.030 --> 00:10:16.890
these are uh things speaking, you know, in the

154
00:10:16.900 --> 00:10:23.429
metaphysical sense now, metaphysical things, entities uh um that

155
00:10:23.440 --> 00:10:26.979
I don't believe in, but I, but I think

156
00:10:26.989 --> 00:10:28.859
that most people do believe and, and I think

157
00:10:28.869 --> 00:10:35.520
what constitutes uh the, the sociologists, Maury uh or

158
00:10:35.530 --> 00:10:41.229
what I sometimes call empirical morality would, would be

159
00:10:41.239 --> 00:10:47.130
based on a belief in that metaphysical idea or

160
00:10:47.140 --> 00:10:51.289
those metaphysical notions. So the belief of course, is

161
00:10:51.299 --> 00:10:56.090
something real. It's a real psychological phenomenon uh that

162
00:10:56.099 --> 00:11:00.659
somebody might have. But what they believe in would

163
00:11:00.669 --> 00:11:05.780
be, you know, this seemingly meta metaphysical notion of

164
00:11:05.789 --> 00:11:09.130
an objective, rightness and objective wrongness, et cetera.

165
00:11:10.690 --> 00:11:14.090
And so, is that the same, I, I mean,

166
00:11:14.099 --> 00:11:19.270
moral philosopher, moral philosophers themselves is that the way

167
00:11:19.280 --> 00:11:23.469
they also think about objective moral values, are they,

168
00:11:23.479 --> 00:11:26.880
when they use this expression, objective moral values, are

169
00:11:26.890 --> 00:11:30.825
they also viewing moral values with some kind of,

170
00:11:31.184 --> 00:11:36.434
uh, metaphysical reality to them and claiming that moral

171
00:11:36.445 --> 00:11:42.265
values exist even independent of human minds or other

172
00:11:42.275 --> 00:11:43.934
animal minds, for example.

173
00:11:44.244 --> 00:11:49.640
Yeah. Well, well, that's the big issue. Uh, THERE

174
00:11:49.650 --> 00:11:53.630
are certainly, uh, traditionally, I suppose, uh, it was

175
00:11:53.640 --> 00:11:58.059
a straightforward metaphysical belief, a belief in something metaphysical.

176
00:11:58.750 --> 00:12:01.049
Uh, ALTHOUGH I suppose some people might question that

177
00:12:01.059 --> 00:12:03.130
as well, but that, I think that's a, a

178
00:12:03.140 --> 00:12:08.169
common view but more recently, meaning in recent centuries,

179
00:12:08.179 --> 00:12:11.330
uh uh, and certainly in the kind of western

180
00:12:11.340 --> 00:12:15.950
philosophy and philo philosophical ethics, I'm acquainted with, uh,

181
00:12:15.960 --> 00:12:19.250
not, not so, not necessarily theological ethics. That's, that's

182
00:12:19.260 --> 00:12:22.150
again another whole other bag, right? But, but in,

183
00:12:22.159 --> 00:12:28.200
in philosophical ethics, the trend has certainly been, um

184
00:12:28.369 --> 00:12:34.349
to try to de metaphysical, uh the notions of

185
00:12:34.359 --> 00:12:38.380
right and wrong, uh, good and bad. Uh And

186
00:12:38.390 --> 00:12:42.369
here it gets very, very, very tricky, uh, because

187
00:12:43.539 --> 00:12:48.940
I would say most philosophical ethics do want to

188
00:12:48.950 --> 00:12:55.460
preserve the idea of a real morality. There really

189
00:12:55.469 --> 00:12:57.409
are things that are right and wrong and good

190
00:12:57.419 --> 00:13:03.950
and bad. Uh But since they're rejecting the metaphysical

191
00:13:03.960 --> 00:13:08.469
view, you know, what's, what's gonna take its place

192
00:13:08.479 --> 00:13:10.250
and, and, and so as I say, it becomes

193
00:13:10.260 --> 00:13:12.469
very tricky and, and I think you find a

194
00:13:12.479 --> 00:13:19.530
wide variety of attempts to secularize uh an objective,

195
00:13:19.539 --> 00:13:26.320
right and wrong, good and bad. Uh I don't

196
00:13:26.330 --> 00:13:29.000
particularly want to go through an inventory of, of

197
00:13:29.010 --> 00:13:32.679
the various attempts that have been made. Uh But,

198
00:13:32.690 --> 00:13:35.409
you know, to, to cut to the chase, uh

199
00:13:36.320 --> 00:13:41.830
I myself and a handful of others uh in,

200
00:13:42.109 --> 00:13:45.809
in modern philosophy again, meaning the last 100 or

201
00:13:45.820 --> 00:13:50.750
more years. Uh PEOPLE even with like Nietzsche and

202
00:13:50.760 --> 00:13:55.570
so forth. Um We, we have kind of rejected

203
00:13:56.179 --> 00:14:00.330
the whole notion of, of an objective, right and

204
00:14:00.340 --> 00:14:01.929
wrong and good and bad. I mean, that makes

205
00:14:01.940 --> 00:14:05.090
it so much simpler, you know, we, we, we

206
00:14:05.099 --> 00:14:08.609
don't have to come up with some elaborate, complex,

207
00:14:08.619 --> 00:14:11.260
subtle. I mean, I mean, you know, they're really,

208
00:14:11.369 --> 00:14:13.650
really have been good attempts. I mean, they're very,

209
00:14:13.659 --> 00:14:16.030
they can be very persuasive and I can't say

210
00:14:16.039 --> 00:14:20.729
that I myself completely reject all of them. Uh

211
00:14:20.739 --> 00:14:23.390
I, I have an open mind. I'm a philosopher.

212
00:14:23.400 --> 00:14:26.549
I, I can never really come to a final

213
00:14:26.559 --> 00:14:30.630
conclusion about anything. All right. But, but by and

214
00:14:30.640 --> 00:14:37.179
large, um I have become convinced uh that it

215
00:14:37.190 --> 00:14:40.280
just makes a lot more sense. Uh It would

216
00:14:40.289 --> 00:14:44.609
work out more to, I think the collective liking

217
00:14:44.619 --> 00:14:48.340
of human beings if we had a world in

218
00:14:48.349 --> 00:14:51.820
which we just didn't believe anymore in an object

219
00:14:51.830 --> 00:14:53.450
of right and wrong and good and bad.

220
00:14:54.590 --> 00:14:58.830
But then in terms of morality, if you reject

221
00:14:59.460 --> 00:15:04.760
the idea of objective moral values, um I mean,

222
00:15:04.770 --> 00:15:09.049
do you have a subjective approach to morality. Do

223
00:15:09.059 --> 00:15:11.570
you use terms like that or not?

224
00:15:12.349 --> 00:15:16.289
Well, again, uh uh that you'll find different points

225
00:15:16.299 --> 00:15:19.330
of view about this as well. Uh Because even

226
00:15:19.340 --> 00:15:25.679
among uh the philosophers who reject uh a, a

227
00:15:25.690 --> 00:15:29.500
moral realism in, in terms of, well, wait, wait,

228
00:15:29.510 --> 00:15:32.229
see again, everything is so complicated in philosophy because

229
00:15:32.469 --> 00:15:35.869
every term can be debated. All right. So I

230
00:15:35.880 --> 00:15:39.030
shouldn't even use the word real realism yet. OK,

231
00:15:39.039 --> 00:15:41.330
let me, let me just go back to metaphysical.

232
00:15:41.359 --> 00:15:46.250
Even those who would reject a metaphysical notion of

233
00:15:46.260 --> 00:15:51.289
morality uh might still want to hold on to

234
00:15:51.299 --> 00:15:56.219
the vocabulary of morality. Uh A lot of these

235
00:15:56.229 --> 00:16:01.390
people call themselves uh fictional lists. So they would

236
00:16:01.400 --> 00:16:04.400
hold on to a belief that they don't really

237
00:16:04.409 --> 00:16:06.739
believe they would, it would be a pretend belief

238
00:16:06.969 --> 00:16:09.900
so that they could continue to use the vocabulary

239
00:16:09.909 --> 00:16:13.169
of morality. And their argument would be because it's

240
00:16:13.179 --> 00:16:18.460
very useful. Um uh It obviously, again from an

241
00:16:18.469 --> 00:16:22.599
evolutionary standpoint, believing in morality in a very strong

242
00:16:22.609 --> 00:16:26.789
sense must have served a purpose. Uh But these

243
00:16:26.799 --> 00:16:31.630
fictional lists would argue that well, now that those

244
00:16:31.640 --> 00:16:34.830
of us who really think about this, don't, don't

245
00:16:34.840 --> 00:16:39.590
believe in that hardcore kind of morality. Nonetheless, we,

246
00:16:39.599 --> 00:16:43.750
we can still see the benefit in, in retaining

247
00:16:44.030 --> 00:16:47.469
some of the gestures as it were of morality,

248
00:16:47.520 --> 00:16:50.909
some of the psychological attitudes that go around with

249
00:16:50.919 --> 00:16:56.640
morality. Um And, and it would just be too

250
00:16:56.969 --> 00:17:00.340
dangerous and ultimately just wouldn't work out well for

251
00:17:00.349 --> 00:17:03.500
the individual or society if we got rid of

252
00:17:03.510 --> 00:17:10.939
the terminology. But, uh, then others of us like

253
00:17:10.949 --> 00:17:15.040
myself, uh, who tend to go by the name

254
00:17:15.050 --> 00:17:20.989
of, um, moral abolitionists as opposed to moral fictional

255
00:17:21.000 --> 00:17:26.130
lists. Uh, WE feel it would be better even

256
00:17:26.140 --> 00:17:29.369
to get rid of the terminology not to be,

257
00:17:29.380 --> 00:17:31.810
uh, not to be a school mom about it.

258
00:17:32.089 --> 00:17:34.079
Uh, AND say no, no, no, no, no, you

259
00:17:34.089 --> 00:17:35.430
shouldn't do it, da da da da. Because if

260
00:17:35.439 --> 00:17:36.959
we, if we say you shouldn't do that, we're

261
00:17:36.969 --> 00:17:39.699
gonna be moralists again. Right? So we don't want

262
00:17:39.709 --> 00:17:41.930
to go that far, but we do want to

263
00:17:41.939 --> 00:17:47.189
encourage people to uh to go easy to go

264
00:17:47.199 --> 00:17:50.609
light on the use of right, wrong, good, bad.

265
00:17:51.359 --> 00:17:55.219
Uh And, and more recently, I myself in this

266
00:17:55.229 --> 00:17:59.010
latest book that we're talking about, I even go

267
00:17:59.020 --> 00:18:02.369
so far as to say, let's stop talking about

268
00:18:02.630 --> 00:18:05.150
truth and falsity. I know you wanna ask me

269
00:18:05.160 --> 00:18:07.989
about that later in the conversation, but I'll just

270
00:18:08.000 --> 00:18:10.510
throw that in now but just talking about right

271
00:18:10.520 --> 00:18:14.510
and wrong, good and bad now. Um Yeah, I,

272
00:18:14.520 --> 00:18:18.329
I think it would be better. Uh And um

273
00:18:18.469 --> 00:18:22.550
why, because I think if we continue to use

274
00:18:22.560 --> 00:18:27.689
moral terminology, I think inevitably it's gonna bring along

275
00:18:27.699 --> 00:18:31.569
with it, the attitudes, the metaphysical attitudes, it's, it's

276
00:18:31.579 --> 00:18:35.339
just, you know, it's so ingrained in us that

277
00:18:36.489 --> 00:18:38.229
if we really want to get rid of the

278
00:18:38.239 --> 00:18:41.780
metaphysical attitudes, I think ultimately, we have to get

279
00:18:41.790 --> 00:18:43.069
rid of the language as well.

280
00:18:44.989 --> 00:18:49.219
So going back to perhaps the first question I

281
00:18:49.229 --> 00:18:53.050
asked you when I asked you, um where moral

282
00:18:53.060 --> 00:18:57.010
values stem from? I mean, there's another particular point

283
00:18:57.020 --> 00:18:59.339
there that I would like to address because there

284
00:18:59.349 --> 00:19:02.859
are some moral philosophers that I would imagine would

285
00:19:02.869 --> 00:19:07.885
place themselves on the moral realist camp that uh

286
00:19:07.895 --> 00:19:13.175
in reaction or in response to some scientific accounts

287
00:19:13.185 --> 00:19:16.935
of morality and moral values. I mean, whether, whether

288
00:19:16.944 --> 00:19:22.744
it is evolutionary approaches, soc social learning, anthropology, game

289
00:19:22.755 --> 00:19:27.395
theory, behavior, economics or any of those approaches, they

290
00:19:27.405 --> 00:19:32.150
say that uh we do not have a full

291
00:19:32.160 --> 00:19:36.949
account of morality or where moral values stem from.

292
00:19:36.959 --> 00:19:40.589
But uh I'm not claiming here that we already

293
00:19:40.599 --> 00:19:44.390
have that full account through science or any of

294
00:19:44.400 --> 00:19:49.859
these sciences. But if science would not account or,

295
00:19:49.880 --> 00:19:53.540
or, or would not fully account for where moral

296
00:19:53.550 --> 00:19:58.469
values stem from, then what would uh account for

297
00:19:58.479 --> 00:20:00.839
it? I mean, because uh it's, it's hard for

298
00:20:00.849 --> 00:20:04.489
me to understand people when they make that kind

299
00:20:04.500 --> 00:20:09.540
of claim that science itself cannot fully explain where

300
00:20:09.550 --> 00:20:10.910
moral values stem from.

301
00:20:12.729 --> 00:20:16.010
What can I say? I mean, you know, there'll

302
00:20:16.020 --> 00:20:18.979
be so many different responses to that. So some,

303
00:20:18.989 --> 00:20:22.140
you know, are gonna just say, well, they came

304
00:20:22.150 --> 00:20:26.319
from God, right, going back to the metaphysical notions.

305
00:20:26.660 --> 00:20:30.739
Um AND I gave you my own psychological account,

306
00:20:30.880 --> 00:20:33.260
you know, of, of where they could come from.

307
00:20:33.449 --> 00:20:38.079
I, I would say one, I, I imagine that

308
00:20:38.089 --> 00:20:40.069
uh and, and this is gonna overlap with some

309
00:20:40.079 --> 00:20:43.239
of this, the more scientific versions you already alluded

310
00:20:43.250 --> 00:20:47.750
to. I would imagine that uh playing the moral

311
00:20:47.760 --> 00:20:52.949
card must give people or must make people think

312
00:20:52.959 --> 00:20:57.630
it gives them a certain advantage. Uh For example,

313
00:20:57.640 --> 00:21:03.250
when I, when I hear uh politicians uh use

314
00:21:03.260 --> 00:21:08.130
morality, it strikes me that they're, they're just trying

315
00:21:08.140 --> 00:21:12.310
to, to buttress their, their arguments, whatever they are.

316
00:21:12.319 --> 00:21:15.339
So they'll say, you know, well, I am supporting

317
00:21:15.349 --> 00:21:18.719
this policy or you should vote for me, you

318
00:21:18.729 --> 00:21:20.949
know, because blah, blah, blah, blah, blah, and then

319
00:21:20.959 --> 00:21:23.000
they end it with and it's the right thing

320
00:21:23.010 --> 00:21:27.390
to do, you know, I call it moral punctuation.

321
00:21:27.400 --> 00:21:29.500
I'm probably repeating some things I said in our

322
00:21:29.510 --> 00:21:32.589
first interview. But um it does seem to me

323
00:21:32.599 --> 00:21:35.680
that people feel there's an advantage to doing it.

324
00:21:35.810 --> 00:21:38.869
And I acknowledge that, I mean, uh I, if

325
00:21:38.880 --> 00:21:44.420
I'm debating somebody who is very confident about their

326
00:21:45.189 --> 00:21:48.229
position on whatever it is a practical matter, a

327
00:21:48.239 --> 00:21:55.569
philosophical matter, whatever it is, um their confidence might

328
00:21:55.579 --> 00:21:59.689
derive from their believing that morality is on their

329
00:21:59.699 --> 00:22:04.500
side. Uh Especially if we're talking about some extreme

330
00:22:04.510 --> 00:22:07.869
situation like something which in, you know, I, I

331
00:22:07.880 --> 00:22:11.109
is, is, is, is atrocity a moral term. It

332
00:22:11.119 --> 00:22:14.670
probably is. And there's certainly many times when I

333
00:22:14.680 --> 00:22:18.140
view something which, you know, I am just naturally

334
00:22:18.150 --> 00:22:22.750
gonna wanna label it an atrocity and then if,

335
00:22:22.760 --> 00:22:26.770
if, II, I, if somebody then says to me

336
00:22:26.780 --> 00:22:30.339
in a debate about morality and amorality, now, what

337
00:22:30.349 --> 00:22:32.250
do you think about that? You know, wasn't that

338
00:22:32.260 --> 00:22:35.199
an atrocity? Are you telling me it's not wrong?

339
00:22:35.209 --> 00:22:37.689
The person who did that didn't do anything wrong?

340
00:22:37.839 --> 00:22:42.880
Well, you know, I'm at an extreme rhetorical disadvantage.

341
00:22:42.890 --> 00:22:47.199
Right. It sounds terrible for me to say no,

342
00:22:47.209 --> 00:22:49.050
I don't think they did anything wrong. You know,

343
00:22:49.060 --> 00:22:52.270
it, it's absurd. Right. It's absurd. Well, well, how

344
00:22:52.280 --> 00:22:55.719
do I get around that? Uh, WELL, I mean,

345
00:22:55.729 --> 00:22:59.849
theoretically how do I get around that? Theoretically I'm

346
00:22:59.859 --> 00:23:04.619
going to say, yeah. But I really, you know,

347
00:23:04.630 --> 00:23:08.489
I'm, I'm repelled by it. I'm iii, I really

348
00:23:08.500 --> 00:23:10.770
don't want things like that. I, I, I'm very

349
00:23:10.780 --> 00:23:13.619
motivated to try to prevent things like that and,

350
00:23:13.819 --> 00:23:16.739
and discourage things like that from happening and I

351
00:23:16.750 --> 00:23:19.959
would certainly want to stop somebody from doing something

352
00:23:19.969 --> 00:23:21.670
like that, you know. So I would give all

353
00:23:21.680 --> 00:23:27.150
these psychological answers, not moral answers. I would talk

354
00:23:27.160 --> 00:23:32.869
about, um, what I feel. Um, AND I think

355
00:23:32.880 --> 00:23:38.630
that's enough, uh, to, to, uh, to make, to

356
00:23:38.640 --> 00:23:40.469
help make the world be the way I'd like

357
00:23:40.479 --> 00:23:44.150
it to be. If everybody shared the feelings I

358
00:23:44.160 --> 00:23:48.520
have, I certainly would, would think it would be

359
00:23:48.530 --> 00:23:50.920
a better world as a result. It would just

360
00:23:50.930 --> 00:23:55.040
be superfluous to add on top of that. Oh,

361
00:23:55.050 --> 00:23:56.390
and by the way, it's the right thing to

362
00:23:56.400 --> 00:24:02.400
do, you know? But as I say rhetorically, somebody

363
00:24:02.410 --> 00:24:06.180
can use that right, wrong language to put an

364
00:24:06.189 --> 00:24:08.839
Amoral list at a disadvantage.

365
00:24:10.469 --> 00:24:12.969
So let me ask you this question that is

366
00:24:12.979 --> 00:24:17.550
also sort of related to the previous one. So

367
00:24:17.689 --> 00:24:21.729
let's say that someone is an atheist and they

368
00:24:21.739 --> 00:24:27.329
go around condemning religious people for believing in things

369
00:24:27.339 --> 00:24:30.319
that they think do not exist gods or some

370
00:24:30.329 --> 00:24:34.050
other kind of divine entity. But at the same

371
00:24:34.060 --> 00:24:39.310
time, they believe that moral values are objective. And

372
00:24:39.319 --> 00:24:43.969
so they think that moral values are as real

373
00:24:43.979 --> 00:24:46.969
and as far as the universe as atoms, for

374
00:24:46.979 --> 00:24:51.869
example, and we can find them through whatever means.

375
00:24:52.209 --> 00:24:55.390
Uh I mean, does that make sense? Does it

376
00:24:55.400 --> 00:25:00.380
make sense sense to condemn someone for believing in

377
00:25:00.390 --> 00:25:04.400
something gods in this case that uh an atheist

378
00:25:04.619 --> 00:25:09.810
thinks does not, does not exist but then believing

379
00:25:09.819 --> 00:25:13.569
uh also holding these sort of beliefs that sound

380
00:25:13.579 --> 00:25:17.109
very similar to that. But in another camp, in

381
00:25:17.119 --> 00:25:18.290
this case, morality,

382
00:25:19.319 --> 00:25:22.040
well, you're giving me a leading question. I mean,

383
00:25:22.050 --> 00:25:26.030
thank you. Of course, I, I think it's ridiculous

384
00:25:26.040 --> 00:25:30.170
to, to uh to try to reconcile those uh

385
00:25:31.119 --> 00:25:36.869
those beliefs and attitudes. Um And, and, and I,

386
00:25:37.390 --> 00:25:40.829
you know, I think the so called new atheists

387
00:25:40.839 --> 00:25:46.609
who aren't so new anymore. Uh And, and who

388
00:25:46.619 --> 00:25:50.510
might, you know, certainly respect a great deal. I

389
00:25:50.520 --> 00:25:53.689
mean, I'm certainly in sync with a lot of

390
00:25:53.699 --> 00:25:58.729
their attitudes, but I think they just are mistaken

391
00:25:58.739 --> 00:26:02.380
in this case. Um uh THEY are at cross

392
00:26:02.390 --> 00:26:08.459
purposes. Uh uh And uh I would, I would,

393
00:26:08.469 --> 00:26:10.589
you know, just as you suggested, I would s

394
00:26:10.680 --> 00:26:12.969
say they, they still believe in a kind of

395
00:26:12.979 --> 00:26:17.609
God uh that, that objective, right and r right

396
00:26:17.619 --> 00:26:20.050
and wrong and good and evil or good and

397
00:26:20.060 --> 00:26:26.719
bad um amount to believing in God uh to

398
00:26:26.729 --> 00:26:28.890
think that uh you know, God's in His heaven

399
00:26:28.900 --> 00:26:30.729
and all is right with the world And there

400
00:26:30.739 --> 00:26:33.569
is somebody who will, who will make it all

401
00:26:33.579 --> 00:26:35.630
right in the end and all that, that, that

402
00:26:35.640 --> 00:26:38.000
relates to the desire I spoke to at the

403
00:26:38.010 --> 00:26:41.739
beginning of our discussion today that we desire something

404
00:26:41.750 --> 00:26:45.069
so strongly. It's not only a case that we

405
00:26:45.079 --> 00:26:48.920
bring the ought into existence, the world ought to

406
00:26:48.930 --> 00:26:52.260
be a certain way. But, you know, of course,

407
00:26:52.270 --> 00:26:56.349
we bring God or Gods into existence, the ones

408
00:26:56.510 --> 00:27:00.160
who are going to make it be the way

409
00:27:00.170 --> 00:27:02.060
we want it to be and think it ought

410
00:27:02.069 --> 00:27:04.459
to be or even tell us how it ought

411
00:27:04.469 --> 00:27:07.180
to be and establish that. That is why it

412
00:27:07.189 --> 00:27:09.540
ought to be that way just because the gods

413
00:27:09.550 --> 00:27:11.329
think it ought to be that way or want

414
00:27:11.339 --> 00:27:14.260
it to be that way. Uh Yeah, they're all

415
00:27:14.270 --> 00:27:17.150
of a piece. It seems to me. So that's

416
00:27:17.160 --> 00:27:23.180
why I coined the term hard atheism um uh

417
00:27:23.189 --> 00:27:27.930
for an atheism that is thoroughly uh honest or

418
00:27:27.939 --> 00:27:31.729
at least self-aware uh that is going to give

419
00:27:31.739 --> 00:27:36.719
up. Not only an explicit God but the implicit

420
00:27:36.729 --> 00:27:41.300
God of an objective morality. Mhm.

421
00:27:41.790 --> 00:27:45.750
So, in our previous conversation we touched a little

422
00:27:45.760 --> 00:27:49.069
bit on desires and today I would like to

423
00:27:49.079 --> 00:27:53.430
get into more detail on that topic. So, first

424
00:27:53.439 --> 00:27:57.170
of all, just to recap, would, would you like

425
00:27:57.180 --> 00:28:02.069
to tell us again, uh, uh, what desires is

426
00:28:02.079 --> 00:28:02.589
about?

427
00:28:03.959 --> 00:28:07.540
Uh, SURE. Uh, I, if we, if we decide

428
00:28:07.550 --> 00:28:11.680
to get rid of morality as it were, yeah.

429
00:28:11.689 --> 00:28:14.140
I, I don't really like the term abolitionism either.

430
00:28:14.150 --> 00:28:17.099
You know, because again, I'm not gonna abolish, right?

431
00:28:17.109 --> 00:28:21.079
But that's the term that I inherited from whoever

432
00:28:21.089 --> 00:28:24.260
coined it. All right. Uh, BUT if I, if

433
00:28:24.270 --> 00:28:26.479
we get rid of, of, of morality, we don't

434
00:28:26.489 --> 00:28:29.819
believe there's metaphysical, uh, morality and we just kind

435
00:28:29.829 --> 00:28:33.689
of stop using the language and so forth and

436
00:28:33.699 --> 00:28:39.439
so forth. Um, WELL, the, the people who don't

437
00:28:39.449 --> 00:28:44.010
like amorality often say, well, you'd end up with

438
00:28:44.020 --> 00:28:49.280
chaos or people, you know, raping and pillaging and

439
00:28:49.290 --> 00:28:51.880
murdering and blah, blah, blah, the same kinds of

440
00:28:51.890 --> 00:28:56.060
arguments that theists make a, about, about atheists. You

441
00:28:56.069 --> 00:28:59.479
know, again, there's another parallel right there, you know.

442
00:28:59.869 --> 00:29:05.829
Um, AND just as, you know, atheists think that's

443
00:29:05.839 --> 00:29:11.349
ridiculous. You know, there are certainly lots of atheists

444
00:29:11.359 --> 00:29:18.900
who are upstanding, wonderful compassionate individuals. Uh, JUST as

445
00:29:18.910 --> 00:29:23.180
much as there are theists like that and we

446
00:29:23.189 --> 00:29:26.439
certainly know many theists who aren't like that. Right.

447
00:29:26.449 --> 00:29:30.150
So, you know, the same thing with moralism and

448
00:29:30.160 --> 00:29:34.229
Amoral, you're gonna have some, you know, real stinkers

449
00:29:34.239 --> 00:29:36.630
who are moralists and real thinkers who are am

450
00:29:36.640 --> 00:29:40.859
moralists, no doubt. Uh, BUT in an attempt to

451
00:29:40.869 --> 00:29:44.770
come up with a, a form of a morality,

452
00:29:45.699 --> 00:29:52.969
uh, that will satisfy even a moralist who was

453
00:29:52.979 --> 00:29:59.310
a very nice upstanding person. Uh, I wanted to

454
00:29:59.319 --> 00:30:02.359
suggest some things, you know, I, I don't just

455
00:30:02.369 --> 00:30:04.310
wanna say, get rid of morality. Right. I wanna

456
00:30:04.319 --> 00:30:08.290
offer some kind of alternative. Uh, NOW getting rid

457
00:30:08.300 --> 00:30:10.109
of morality is a big part of it. All

458
00:30:10.119 --> 00:30:13.069
right. Uh, I, I do think that will help

459
00:30:13.079 --> 00:30:15.900
to make the, the, the, the, the, the way

460
00:30:15.910 --> 00:30:19.349
smoother. Uh, IT'S gonna get rid of a lot

461
00:30:19.359 --> 00:30:22.119
of things that can cause unnecessary conflicts and so

462
00:30:22.130 --> 00:30:26.359
forth. That that's a big argument that, uh, many

463
00:30:26.369 --> 00:30:28.750
of the, uh, Amoral lists will make, will have,

464
00:30:28.760 --> 00:30:31.969
will reduce the number of, of conflicts and the

465
00:30:31.979 --> 00:30:35.390
intensity of conflicts in the world. But desir is

466
00:30:35.410 --> 00:30:40.180
specifically, is the idea that, uh, we're left with,

467
00:30:40.189 --> 00:30:44.609
um, what we want or what we like or

468
00:30:44.619 --> 00:30:46.640
what we don't want or what we don't like.

469
00:30:46.650 --> 00:30:48.949
You know, that's, that's pretty much the basis of

470
00:30:49.199 --> 00:30:52.540
what will motivate us to do things or not

471
00:30:52.550 --> 00:30:55.569
do things in lieu of, you know, thinking it's

472
00:30:55.579 --> 00:30:58.630
right or wrong, good or bad. Uh, AND then

473
00:30:58.640 --> 00:31:04.060
I add one more wrinkle to that. So, uh,

474
00:31:04.069 --> 00:31:06.959
choosing the name Desir is might, I, I, it's,

475
00:31:06.969 --> 00:31:10.619
it's not the happiest of names for what my

476
00:31:10.630 --> 00:31:12.900
my view is, and th this is a constant

477
00:31:12.910 --> 00:31:16.660
problem for any theory. You know, the name never

478
00:31:16.670 --> 00:31:20.099
really fully captures what a theory is. Cause theories

479
00:31:20.109 --> 00:31:24.000
tend to be rich entities. OK? But the other

480
00:31:24.010 --> 00:31:27.599
wrinkle I'll throw in that I've thrown in is

481
00:31:27.609 --> 00:31:37.079
that you want to rationalize your desires. Um And

482
00:31:37.560 --> 00:31:41.109
now this, this raises a whole, this opens a

483
00:31:41.119 --> 00:31:43.089
whole new can of worms. All right, because then

484
00:31:43.099 --> 00:31:44.949
we have to talk about rationality, which I want

485
00:31:44.959 --> 00:31:47.839
to do with you. Uh But that, that would

486
00:31:47.849 --> 00:31:51.689
be the essence of desires, desires, says, instead of

487
00:31:51.699 --> 00:31:54.380
trying to figure out what's the right or wrong

488
00:31:54.390 --> 00:31:58.719
thing to do, ask yourself what is the rational

489
00:31:58.729 --> 00:32:04.680
thing to desire or like that's it. That's rationalism

490
00:32:04.689 --> 00:32:05.390
in a nutshell.

491
00:32:06.099 --> 00:32:09.400
OK. But then we have to get a bit

492
00:32:09.410 --> 00:32:15.880
deeper into that bit about rationality. So iiiii, I

493
00:32:15.890 --> 00:32:17.979
mean, because I have to ask you at this

494
00:32:17.989 --> 00:32:22.119
point, what does rationality mean in the context of

495
00:32:22.130 --> 00:32:23.650
desires specifically

496
00:32:24.650 --> 00:32:27.979
that that turned out to be the real, the

497
00:32:27.989 --> 00:32:32.890
real kicker? Uh And I've been struggling with that

498
00:32:33.030 --> 00:32:37.290
for the whole time. I've been thinking about desires

499
00:32:37.339 --> 00:32:39.949
and immorality, which is how long now going on,

500
00:32:39.959 --> 00:32:43.069
maybe 18 years or something since I had my

501
00:32:43.459 --> 00:32:48.780
so called anti epiphany that morality doesn't exist. That

502
00:32:48.790 --> 00:32:50.939
was a real shocker to me, a real shocker.

503
00:32:51.270 --> 00:32:59.180
Um OK. Uh Again, morality, well, that's a favorite,

504
00:32:59.609 --> 00:33:01.780
I mean, excuse me, rationality. And that, that's a

505
00:33:01.790 --> 00:33:06.119
favorite concept. Of philosophers in particular. I mean, you

506
00:33:06.130 --> 00:33:10.359
know, some would say precisely what distinguishes philosophy from,

507
00:33:10.609 --> 00:33:15.479
well, religion is that it insists upon a strictly

508
00:33:15.489 --> 00:33:19.880
rational approach to the big questions. Whereas theology not,

509
00:33:19.890 --> 00:33:22.699
I shouldn't say religion, theology. Theo well, no, I

510
00:33:22.709 --> 00:33:26.390
shouldn't say theology, I think is so complicated. Ricardo,

511
00:33:26.400 --> 00:33:29.969
I'm sorry, this is, this is what happens when

512
00:33:29.979 --> 00:33:33.219
you're a philosopher, you can hardly get one sentence

513
00:33:33.229 --> 00:33:38.130
out before, you know, contradicting yourself. Uh How can

514
00:33:38.140 --> 00:33:42.250
I just say calm down, Joel, calm down? Um

515
00:33:43.989 --> 00:33:47.329
OK, there are many things that one can mean

516
00:33:47.339 --> 00:33:53.250
by rationality. So when I first started using the

517
00:33:53.260 --> 00:33:58.680
term, I meant what is pretty standard in philosophy

518
00:34:00.020 --> 00:34:04.410
is that you, you think logically, you know, you,

519
00:34:04.420 --> 00:34:07.459
you uh you vet we say you vet your

520
00:34:07.469 --> 00:34:12.050
beliefs with logic. Uh And there are, there are

521
00:34:12.060 --> 00:34:15.889
strict laws of logic, you know, even Aristotle came

522
00:34:15.899 --> 00:34:18.520
up with a lot of them. Uh This goes

523
00:34:18.530 --> 00:34:21.969
back a long way and uh I mean, my

524
00:34:21.978 --> 00:34:23.580
God, he was such a genius to come up

525
00:34:23.590 --> 00:34:27.500
with these things. Uh And, and, and they, you

526
00:34:27.510 --> 00:34:29.139
know, many of them have stood the test of

527
00:34:29.149 --> 00:34:35.070
time. Uh So you wanna think logically and you

528
00:34:35.080 --> 00:34:38.489
know, we would also in a more modern sense,

529
00:34:38.500 --> 00:34:41.168
say you wanna think in uh scientifically or we

530
00:34:41.179 --> 00:34:46.270
wanna think in terms of um uh well, inductive

531
00:34:46.280 --> 00:34:50.469
logic. Some people talk about uh uh things that

532
00:34:50.478 --> 00:34:56.188
conform to um uh probability theory and scientific theories.

533
00:34:56.199 --> 00:34:58.909
And all that kind of thing. So uh not

534
00:34:58.919 --> 00:35:02.750
certainly we, we don't expect that you have to

535
00:35:02.760 --> 00:35:07.969
um uh resolve everything deductively. Uh You can't, I

536
00:35:07.979 --> 00:35:10.850
mean, nothing in the real world um in the

537
00:35:10.860 --> 00:35:15.229
world of experience, even scientific laws, none of these

538
00:35:15.239 --> 00:35:20.610
can be established deductively. Even scientists fully recognize that

539
00:35:20.620 --> 00:35:27.149
now. Uh BUT nonetheless, there are standards um and

540
00:35:27.159 --> 00:35:31.040
they are considered objective standards. OK? Of, of what

541
00:35:31.050 --> 00:35:37.600
constitutes a rational belief or accordingly, uh an irrational

542
00:35:37.610 --> 00:35:40.830
belief or a fallacious belief, which is another way

543
00:35:40.840 --> 00:35:45.010
of speaking of something that's not strictly uh rational.

544
00:35:45.020 --> 00:35:48.280
So that's what I had in mind originally when

545
00:35:48.290 --> 00:35:54.850
I suggested that we uh vet or rationalize our

546
00:35:54.860 --> 00:35:59.209
beliefs. But more recently, uh with my book on

547
00:35:59.219 --> 00:36:03.840
uh uh ethics and reason or was it reason

548
00:36:03.850 --> 00:36:06.100
and ethics? I think it was reason that I've

549
00:36:06.110 --> 00:36:09.080
written so many books now, Reason and Ethics. Um

550
00:36:09.090 --> 00:36:11.510
And then, and then the spread the, the most

551
00:36:11.520 --> 00:36:15.229
recent and now I've got another one being considered

552
00:36:15.239 --> 00:36:19.659
for publication in these more recent books. I'm taking

553
00:36:19.669 --> 00:36:28.719
a more relaxed view of rationality because uh I,

554
00:36:28.729 --> 00:36:35.969
I familiarized myself with um uh the objection to

555
00:36:36.580 --> 00:36:41.090
a moralism that goes by the name of companions

556
00:36:41.100 --> 00:36:47.239
in guilt, which argues it has many forms of

557
00:36:47.250 --> 00:36:54.879
course. But one form is that rationality is just

558
00:36:54.889 --> 00:37:00.040
as much uh a uh an, a, a um

559
00:37:00.050 --> 00:37:09.489
um they might say a metaphysically based uh attitude

560
00:37:10.979 --> 00:37:20.550
as morality can be. Um And I thought about

561
00:37:20.560 --> 00:37:24.459
that and, and I became convinced that that was

562
00:37:24.469 --> 00:37:28.530
true too. Uh, OH, look what I just said

563
00:37:28.540 --> 00:37:30.270
that that was true too. I'm gonna have to

564
00:37:30.280 --> 00:37:35.840
qualify that in a minute too. Uh, YEAH, because,

565
00:37:35.870 --> 00:37:39.209
you know, where do we get this idea that,

566
00:37:39.219 --> 00:37:43.919
um, you can only think in these terms, there

567
00:37:43.929 --> 00:37:48.260
are these laws of inference, deductive or inductive, et

568
00:37:48.270 --> 00:37:52.560
cetera, et cetera who says, you know, I mean,

569
00:37:52.570 --> 00:37:56.879
yeah, they intuitively grab us at least some of

570
00:37:56.889 --> 00:38:00.439
us, you know, those of us who think that

571
00:38:00.449 --> 00:38:04.610
we're very logical, we say of course, just like

572
00:38:04.620 --> 00:38:08.850
two plus two equals four, of course. But then

573
00:38:09.270 --> 00:38:12.629
isn't it just like, well, you don't wanna go

574
00:38:12.639 --> 00:38:17.669
around torturing babies, do you? Of course not, you

575
00:38:17.679 --> 00:38:24.159
know, it's the same kind of intuition that comes

576
00:38:24.169 --> 00:38:27.719
very naturally and that it seems absurd to deny,

577
00:38:29.280 --> 00:38:34.909
but having become convinced that it can be denied

578
00:38:35.800 --> 00:38:39.510
in the realm of morality, why should I be

579
00:38:39.520 --> 00:38:43.439
so confident that it can't be denied in the

580
00:38:43.449 --> 00:38:46.979
realm of rationality? And so finally, as I say,

581
00:38:46.989 --> 00:38:49.820
I, I did become convinced that it can be

582
00:38:49.830 --> 00:38:52.399
denied there as well. So what do I do

583
00:38:52.409 --> 00:38:55.699
then? Do I become an a rationalist the way

584
00:38:55.709 --> 00:38:59.360
I think of myself as an a moralist? Well,

585
00:38:59.649 --> 00:39:03.090
yes, but now I'm gonna say something which has

586
00:39:03.100 --> 00:39:07.429
become my, my ultimate meta, meta, meta philosophy of

587
00:39:07.439 --> 00:39:12.610
life. Yes and no, you know, I speak now

588
00:39:12.620 --> 00:39:16.850
of the philosophy of yes and no. Uh BECAUSE

589
00:39:16.860 --> 00:39:18.379
I kind of think you can give a yes

590
00:39:18.389 --> 00:39:21.709
AAA plausible, yes or no answer to every question.

591
00:39:21.989 --> 00:39:25.389
So yes, I have now become an a rationalist.

592
00:39:25.399 --> 00:39:30.449
But no, I can also give you a concept

593
00:39:30.459 --> 00:39:36.550
of rationality which I can live with. Uh It's

594
00:39:36.560 --> 00:39:39.080
a laid back concept as I think I said,

595
00:39:39.500 --> 00:39:42.780
uh where all I mean by it is that

596
00:39:42.790 --> 00:39:49.070
I'm asking you to reflect on your beliefs before

597
00:39:49.080 --> 00:39:52.389
you act upon them. So, now a desir is,

598
00:39:52.399 --> 00:39:59.320
is someone who, who reflects on their beliefs before

599
00:39:59.330 --> 00:40:02.310
acting on them? It's that simple.

600
00:40:04.699 --> 00:40:07.830
But then let me just ask you this question

601
00:40:07.840 --> 00:40:15.459
more directly here. Are you against morality or just

602
00:40:15.469 --> 00:40:17.479
against moralism?

603
00:40:18.489 --> 00:40:24.030
Ah, good. Uh, BECAUSE in line with what I

604
00:40:24.040 --> 00:40:26.389
had just said about how I can live with

605
00:40:26.399 --> 00:40:32.469
a laid back sense of rationality, one might suppose

606
00:40:32.479 --> 00:40:35.790
if I'm going to be consistent, which I like

607
00:40:35.800 --> 00:40:41.060
to be because I like rationality. I like it.

608
00:40:41.209 --> 00:40:46.520
Right. So, if I'm going to be consistent, then

609
00:40:46.530 --> 00:40:49.590
why not have a laid back sense of morality?

610
00:40:50.040 --> 00:40:53.429
Two, you know, and, and that I think is

611
00:40:53.439 --> 00:40:59.790
what many of the, um, the, the non metaphysical

612
00:40:59.800 --> 00:41:04.070
moralists have been attempting to develop all these for

613
00:41:04.080 --> 00:41:08.699
the last couple of centuries, you know. Um, AND

614
00:41:08.709 --> 00:41:10.189
as I said, well, I just found it simpler

615
00:41:10.199 --> 00:41:14.739
to get rid of morality, you know, but, hey,

616
00:41:14.850 --> 00:41:17.679
you know, arguments pro and con, all right. Uh,

617
00:41:17.689 --> 00:41:20.219
AND, and, and and it's also important to note

618
00:41:21.239 --> 00:41:24.969
that these are empirical questions. All right. I am

619
00:41:24.979 --> 00:41:32.750
very much an empirical minded philosopher. And I recognize

620
00:41:32.760 --> 00:41:37.000
that, you know, because these are empirical questions, I

621
00:41:37.010 --> 00:41:40.810
cannot claim to have definitive answers. And that's not

622
00:41:40.820 --> 00:41:45.370
only because I'm not a scientist in the formal

623
00:41:45.379 --> 00:41:48.889
sense because I think that a non scientist can

624
00:41:48.899 --> 00:41:52.030
be very empirical in their studies as well. I

625
00:41:52.040 --> 00:41:55.270
think I am being very empirical when I'm in

626
00:41:55.280 --> 00:41:59.959
my armchair doing philosophy because I'm thinking about daily

627
00:41:59.969 --> 00:42:03.340
life, which is an empirical phenomenon. What would I

628
00:42:03.350 --> 00:42:05.510
do? What would other people do? What do other

629
00:42:05.520 --> 00:42:07.050
people I know do or say da, da, da,

630
00:42:07.139 --> 00:42:10.550
da, da, that's an empirical study too. All right.

631
00:42:11.209 --> 00:42:19.209
Mhm. Uh But even science recognizes, as I pointed

632
00:42:19.219 --> 00:42:22.620
out when saying science doesn't look for deductive answers.

633
00:42:22.860 --> 00:42:27.719
Even science recognizes that it's not necessarily now or

634
00:42:27.729 --> 00:42:31.530
ever in possession of the absolute truth. So, in

635
00:42:31.540 --> 00:42:36.080
that sense, you know, I'm open, very open to

636
00:42:36.090 --> 00:42:40.320
the possibility that uh it, it might make more

637
00:42:40.330 --> 00:42:43.000
sense in the end to live with a laid

638
00:42:43.010 --> 00:42:47.399
back morality instead of having an out and out

639
00:42:47.629 --> 00:42:52.810
amorality. OK. Uh But you had asked me about

640
00:42:52.820 --> 00:42:55.709
moralism. So one way I can split the difference

641
00:42:55.719 --> 00:42:58.389
here is to say, well, OK, I do still

642
00:42:58.399 --> 00:43:01.590
want to get rid of morality for various reasons,

643
00:43:01.600 --> 00:43:04.810
blah, blah, blah. Uh But let's, let's have this

644
00:43:04.820 --> 00:43:10.379
conceptual distinction between morality and uh moralism. Um OH,

645
00:43:10.389 --> 00:43:13.330
wait Excuse me? Iii I just, I just misspoke,

646
00:43:13.340 --> 00:43:16.739
I just misspoke. I could say yes, let us

647
00:43:16.750 --> 00:43:21.909
hold on to a laid back morality. OK. But

648
00:43:21.919 --> 00:43:24.030
let, what we want to get rid of is

649
00:43:24.040 --> 00:43:28.010
moralism. And in fact, we already have in, in

650
00:43:28.020 --> 00:43:32.280
everyday speech in English. Uh You can tell me

651
00:43:32.290 --> 00:43:35.250
if that's the case in, in some other languages.

652
00:43:35.260 --> 00:43:39.030
Ricardo. Uh CAUSE I know you're probably a linguist.

653
00:43:39.040 --> 00:43:43.330
Uh I'm not, uh but certainly in English, um

654
00:43:44.179 --> 00:43:50.040
with the term moralism usually connotes something not so

655
00:43:50.050 --> 00:43:54.030
nice. Uh You know, you're just getting carried away

656
00:43:54.040 --> 00:43:58.280
with your morality there. Uh You know, relax a

657
00:43:58.290 --> 00:44:00.909
little bit in other words, right? Uh And, and

658
00:44:00.919 --> 00:44:03.790
it's also suggests a kind of arrogance, somebody who's

659
00:44:03.800 --> 00:44:06.770
very uh moralistic in the sense of moralism as

660
00:44:06.780 --> 00:44:09.110
opposed to moralistic in the sense of just moral.

661
00:44:09.989 --> 00:44:15.330
Uh So, yeah, that, that's definitely something I've become

662
00:44:15.340 --> 00:44:20.340
more open to after 18 years of being an,

663
00:44:20.350 --> 00:44:23.739
a moralist. I, I'm a little more moderate now

664
00:44:23.750 --> 00:44:26.750
in my Amoral. Is it, it can be, you

665
00:44:26.760 --> 00:44:29.020
see the term itself, it can be an a

666
00:44:29.350 --> 00:44:33.260
moralism instead of an a morality. I, I could

667
00:44:33.270 --> 00:44:34.489
maybe live with that.

668
00:44:36.300 --> 00:44:39.520
OK. But then in your work, one of the

669
00:44:39.530 --> 00:44:42.659
things that you say we should try to do

670
00:44:42.669 --> 00:44:47.360
is to abolish moral language. But if we were

671
00:44:47.370 --> 00:44:50.600
able to do that, then how would we refer

672
00:44:50.610 --> 00:44:54.889
to uh values that are then part of that

673
00:44:54.899 --> 00:44:58.679
more laid back morality would we refer to them

674
00:44:58.689 --> 00:45:02.449
as, I don't know, personal preferences or something like

675
00:45:02.459 --> 00:45:02.719
that?

676
00:45:03.899 --> 00:45:06.409
Yeah. Ok. Well, see, now, now we're, now we've

677
00:45:06.419 --> 00:45:10.629
got two paths here. Ok. So if, if we're

678
00:45:10.639 --> 00:45:15.340
gonna be laid back moralists, then we don't have

679
00:45:15.350 --> 00:45:20.959
to get rid of the moral language, uh, necessarily,

680
00:45:20.969 --> 00:45:22.959
or we can use it from time to time

681
00:45:22.969 --> 00:45:29.959
as appropriate, you know. Uh, uh, SO I, but,

682
00:45:29.969 --> 00:45:33.479
but we might also still choose and I kind

683
00:45:33.489 --> 00:45:36.780
of, you know, I, I still have a preference

684
00:45:36.790 --> 00:45:42.270
to be a AAA thorough going Amoral list, you

685
00:45:42.280 --> 00:45:46.139
know. Uh So, so as I said, we have

686
00:45:46.149 --> 00:45:48.770
two pairs. So, so one way I could answer

687
00:45:48.780 --> 00:45:50.389
your question is to say, well, we don't have

688
00:45:50.399 --> 00:45:53.760
to really change her language so much. OK. But

689
00:45:53.770 --> 00:45:55.969
the other way I would answer it as a,

690
00:45:55.979 --> 00:45:58.600
as a thoroughgoing Amoral list and perhaps I should

691
00:45:58.610 --> 00:46:01.010
argue in those terms, you know, for most of

692
00:46:01.020 --> 00:46:04.370
our discussion because, you know, that, that's, that's my

693
00:46:04.379 --> 00:46:07.070
claim to fame. I'm an Amoral list and, and

694
00:46:07.080 --> 00:46:10.739
I've, I've been considered really perhaps even the most

695
00:46:10.750 --> 00:46:16.899
extreme one in, in philosophical circles recently. And I'm

696
00:46:16.909 --> 00:46:18.370
kind of proud of that and I don't mind,

697
00:46:18.379 --> 00:46:21.310
you know, even playing as it were a devil's

698
00:46:21.320 --> 00:46:23.770
advocate. But, but, you know, I personally like it.

699
00:46:23.780 --> 00:46:26.770
All right. So, so I'll, I'll, I'll go that

700
00:46:26.780 --> 00:46:30.830
route. So how would I, what language would I

701
00:46:30.840 --> 00:46:37.600
use to talk about something? Uh Which a moralist,

702
00:46:37.610 --> 00:46:41.520
you know, would say is just bad, evil, atrocious,

703
00:46:41.530 --> 00:46:43.659
horrible, outrageous da, da, da, da, da, da, da.

704
00:46:44.719 --> 00:46:47.350
Well, you know, I think we have lots of

705
00:46:47.360 --> 00:46:53.149
language for that. Um uh YOU know, another way

706
00:46:53.159 --> 00:46:55.120
I could go by the way, you know, actually

707
00:46:55.129 --> 00:46:57.320
this is a third path that occurs to me

708
00:46:57.330 --> 00:47:00.580
now and your questions as always are spurring me

709
00:47:00.590 --> 00:47:03.389
on to think, which is what the beauty of

710
00:47:03.399 --> 00:47:06.719
philosophical dialogue, right? That, that's why, you know, the

711
00:47:06.729 --> 00:47:09.260
best possible way to do philosophy is in a

712
00:47:09.270 --> 00:47:12.419
dialogue because another person is gonna point out to

713
00:47:12.429 --> 00:47:14.909
things you that you and your own little head

714
00:47:14.919 --> 00:47:17.179
just didn't think of. All right. Yeah. So now

715
00:47:17.189 --> 00:47:18.580
it occurs to me that it's really a third

716
00:47:18.590 --> 00:47:23.919
path, the relativistic path because one thing I could

717
00:47:23.929 --> 00:47:28.500
say is uh in a subjective this tone, I

718
00:47:28.510 --> 00:47:35.050
could say, well, I think it's wrong, you know,

719
00:47:35.159 --> 00:47:38.350
so instead of saying it's wrong, if I were

720
00:47:38.360 --> 00:47:43.139
just speaking as either a hardcore moralist or even

721
00:47:43.149 --> 00:47:47.020
a laid back moralist, you know, it's wrong. I

722
00:47:47.030 --> 00:47:50.469
could be a relativistic moralist and say, well, I

723
00:47:50.479 --> 00:47:53.209
think it's wrong, you know, I recognize that you

724
00:47:53.219 --> 00:47:55.830
may consider it not wrong. See, I could go

725
00:47:55.840 --> 00:48:00.719
that route or as a pure am moralist, I

726
00:48:00.729 --> 00:48:06.530
could say, uh well, moderately I could say, I

727
00:48:06.540 --> 00:48:10.850
don't like that, you know, in, in an aesthetic,

728
00:48:10.860 --> 00:48:12.649
you know, if we're talking aesthetics, which I think

729
00:48:12.659 --> 00:48:15.350
again is another realm just like morality where you

730
00:48:15.360 --> 00:48:17.050
have the same kinds of issues I could say.

731
00:48:17.290 --> 00:48:23.469
Well, you know, I, uh, Beet, well, Beethoven, I

732
00:48:23.479 --> 00:48:27.350
love, you know, I love Beethoven. Uh, BUT, uh,

733
00:48:27.360 --> 00:48:30.350
you know, oh, the Beatles aren't my cup of

734
00:48:30.360 --> 00:48:32.870
tea actually. I love the Beatles too. But, yeah,

735
00:48:32.959 --> 00:48:35.070
I, I could use like, and don't like, or,

736
00:48:35.639 --> 00:48:40.989
you know, love the subjective, obviously subjective terms without

737
00:48:41.000 --> 00:48:43.070
making an issue of it. Oh, Beethoven is the

738
00:48:43.080 --> 00:48:46.199
best composer there ever was objective fact, you know,

739
00:48:47.360 --> 00:48:50.389
in my objectivist mode, I sure think that, right.

740
00:48:50.800 --> 00:48:54.050
But in my purely subjectivism and a and even

741
00:48:54.060 --> 00:48:57.229
amorist mode, no, I can just say, um I

742
00:48:57.239 --> 00:49:01.020
just, I love him every opportunity I have, you

743
00:49:01.030 --> 00:49:02.989
know, I'm gonna go listen to Beethoven and I,

744
00:49:03.000 --> 00:49:05.070
and I'd like to encourage you to listen, you

745
00:49:05.080 --> 00:49:06.689
know, I know you don't care so much for

746
00:49:06.699 --> 00:49:09.689
Beethoven now. But maybe if you, you know, listen

747
00:49:09.699 --> 00:49:11.620
to this particular piece, you know, we could start

748
00:49:11.629 --> 00:49:13.169
with that or, you know, or let me point

749
00:49:13.179 --> 00:49:15.750
out some things about the, the melodic structure of

750
00:49:15.760 --> 00:49:18.139
this piece that maybe you didn't pay attention to

751
00:49:18.149 --> 00:49:19.870
before, you know, that, you know, so there are

752
00:49:19.879 --> 00:49:22.070
all sorts of ways I can subjectively try to

753
00:49:22.080 --> 00:49:25.750
convince people to come along to my way of

754
00:49:25.760 --> 00:49:30.770
seeing things and remember one part of desires and

755
00:49:30.780 --> 00:49:32.550
that I used to refer to as rational and,

756
00:49:32.560 --> 00:49:34.199
and again, I could still do that in a,

757
00:49:34.209 --> 00:49:36.360
in a, in an easy going way I could

758
00:49:36.370 --> 00:49:39.429
still say, look, let me give you reasons because

759
00:49:39.439 --> 00:49:41.649
the whole notion of rationality comes from the idea

760
00:49:41.659 --> 00:49:44.290
of giving reasons, you know, having reasons for things.

761
00:49:44.500 --> 00:49:46.969
I would still, I would always want to say,

762
00:49:47.270 --> 00:49:50.250
uh, not just that. Oh, well, I like this

763
00:49:50.260 --> 00:49:52.709
or I really hate that or I really don't

764
00:49:52.719 --> 00:49:54.929
want you to do that. I could, I could

765
00:49:54.939 --> 00:49:57.760
also give reason, like I say, and here's why,

766
00:49:58.050 --> 00:50:01.090
right? That's to be rational. I want people to

767
00:50:01.100 --> 00:50:04.610
reflect upon uh what they're doing or what they're

768
00:50:04.620 --> 00:50:06.949
not doing, to encourage them to do something or

769
00:50:06.959 --> 00:50:10.050
to discourage them from doing something right? In a

770
00:50:10.060 --> 00:50:14.229
critical situation, of course, you know, call the building's

771
00:50:14.239 --> 00:50:16.090
on fire. Well, of course, I want to say,

772
00:50:16.100 --> 00:50:18.689
hey, because the building's on fire, let's get out

773
00:50:18.699 --> 00:50:21.060
of here, right? You know, so even in a,

774
00:50:21.070 --> 00:50:24.610
in a critical urgent situations, we give reasons, right?

775
00:50:24.780 --> 00:50:29.610
So they're just about always appropriate. And so, so

776
00:50:29.620 --> 00:50:32.729
what it wouldn't just be a case of substituting

777
00:50:32.959 --> 00:50:36.129
the language of like, don't like, you know, want,

778
00:50:36.139 --> 00:50:39.429
don't want and, and again, we have various gradations

779
00:50:39.439 --> 00:50:42.860
of that obviously in, in English and other languages

780
00:50:42.989 --> 00:50:45.060
uh for the language of right, wrong, good, bad.

781
00:50:45.070 --> 00:50:48.149
No, it would also be involve giving reasons, giving

782
00:50:48.159 --> 00:50:53.629
reasons uh AAA as part of uh explaining why

783
00:50:53.639 --> 00:50:56.110
I have the likes and tastes that I do

784
00:50:56.300 --> 00:50:59.030
and why I want you to have the, the

785
00:50:59.040 --> 00:51:01.080
likes and dislikes and so forth that I want

786
00:51:01.090 --> 00:51:04.810
you to have. Um Yeah, so there, there's a

787
00:51:04.820 --> 00:51:08.350
lot more to it than just substituting one word

788
00:51:08.360 --> 00:51:09.050
for another.

789
00:51:09.979 --> 00:51:13.879
Mhm. So let's take one step further now because

790
00:51:13.889 --> 00:51:18.820
we were still in the realm exclusively of morality.

791
00:51:18.830 --> 00:51:23.060
But then last time I think you just alluded

792
00:51:23.070 --> 00:51:27.520
very briefly to uh another thing that I would

793
00:51:27.530 --> 00:51:30.250
like to ask you about. Now, what is Anna

794
00:51:30.350 --> 00:51:33.699
lits or lit is? And I hope I'm pronouncing

795
00:51:33.709 --> 00:51:36.010
the word correctly, but tell us about

796
00:51:36.020 --> 00:51:41.280
it. Don't ask me. It's great today. That's a

797
00:51:41.290 --> 00:51:47.179
pun folks. Um Yeah, II, I think other people

798
00:51:47.189 --> 00:51:51.439
have used that term or maybe one person and

799
00:51:51.449 --> 00:51:52.850
I don't know if they're using it the way

800
00:51:52.860 --> 00:51:55.600
I did. I, you know, all philosophers use terms

801
00:51:55.610 --> 00:51:59.610
in their own ways. But uh the lee, you

802
00:51:59.620 --> 00:52:03.840
know, was the, the Greek uh river, the river

803
00:52:04.229 --> 00:52:07.979
between the world and Hades the uh the above

804
00:52:07.989 --> 00:52:10.409
ground and the below ground I think, let the

805
00:52:10.419 --> 00:52:14.080
river of forgetfulness. So when you die, you know,

806
00:52:14.090 --> 00:52:17.360
you forget everything and, and go to the underworld

807
00:52:17.620 --> 00:52:20.919
for whatever purpose. Uh I hope I got that

808
00:52:20.929 --> 00:52:27.070
right. My Greek mythology there. Um And so all

809
00:52:27.129 --> 00:52:34.989
atheism is a philosophical term of art, uh which

810
00:52:35.000 --> 00:52:38.699
is, you know, ah or an is a negation

811
00:52:38.870 --> 00:52:44.120
prefix. So a lithium, Aletheia, Aletheia, just like a

812
00:52:44.129 --> 00:52:49.879
morality, a rationality. Da da da. Aletheia would be

813
00:52:49.889 --> 00:52:54.379
not forgetting, which is so Ali Aletheia would be

814
00:52:54.389 --> 00:52:58.919
a theory of truth, you see forgetting somehow it's

815
00:52:58.929 --> 00:53:05.489
become in, in p in philosophical lingo referring to

816
00:53:05.500 --> 00:53:08.810
a theory of truth. So then I now wanna

817
00:53:08.820 --> 00:53:10.850
deny truth that there is such a thing as

818
00:53:10.860 --> 00:53:14.179
truth. And so I came up with the term

819
00:53:14.189 --> 00:53:19.080
Anna atheism, the denial of the denial of forgetfulness.

820
00:53:19.280 --> 00:53:21.800
And we could just shorten that because two negatives

821
00:53:21.810 --> 00:53:28.340
are positive, right? Uh OR is m Morgenthau once

822
00:53:28.350 --> 00:53:31.939
said, well, no, and that's a a jo I

823
00:53:31.949 --> 00:53:34.739
won't bring in that joke. It's a tangent. But

824
00:53:34.750 --> 00:53:37.969
um uh we can get rid of the two

825
00:53:37.979 --> 00:53:41.090
negatives and just say lethe is for sure, you

826
00:53:41.100 --> 00:53:45.969
know, getting back to forgetfulness here. So, so it's

827
00:53:45.979 --> 00:53:50.649
just a fancy term, my fancy term for saying

828
00:53:50.830 --> 00:53:54.699
that uh there's no such thing as truth. And

829
00:53:54.709 --> 00:53:59.699
here again, we can go to different routes, right?

830
00:53:59.709 --> 00:54:03.020
We can say, well, let's just be laid back

831
00:54:03.030 --> 00:54:06.050
about it and say, you know, there's truth, but

832
00:54:06.199 --> 00:54:09.419
we don't need it in any metaphysical sense or

833
00:54:09.429 --> 00:54:12.340
we can be relativistic about it and say, well,

834
00:54:12.350 --> 00:54:14.520
I believe this is true or I believe this

835
00:54:14.530 --> 00:54:18.580
is false or we can go all out lethe

836
00:54:18.699 --> 00:54:23.639
is about it and say there's no truth. Now,

837
00:54:23.649 --> 00:54:27.070
how did I get around to, to that idea

838
00:54:27.370 --> 00:54:31.090
which you know, is to some people just so

839
00:54:31.550 --> 00:54:38.520
obviously self defeating. Because if I'm claiming that there

840
00:54:38.530 --> 00:54:40.790
is no truth, am I not claiming that? That's

841
00:54:40.800 --> 00:54:46.709
true? Yeah. Well, the way I get around it?

842
00:54:47.870 --> 00:54:51.409
Is, is, is analogous to how I get around

843
00:54:51.419 --> 00:54:53.409
using right and wrong and good and bad. And

844
00:54:53.510 --> 00:54:55.479
because some people would make the same kind of

845
00:54:55.489 --> 00:54:58.479
argument to a moral, a morality they'd say, oh,

846
00:54:58.489 --> 00:55:01.939
are you saying it's wrong to be moralistic? You

847
00:55:01.949 --> 00:55:05.939
know, and the way I got around that was

848
00:55:05.949 --> 00:55:07.179
just to say, no, I'm just talking in terms

849
00:55:07.189 --> 00:55:10.090
of what I like and don't like so analogously

850
00:55:10.100 --> 00:55:14.239
or desire and don't desire. So analogously, I would

851
00:55:14.250 --> 00:55:16.909
say, well, I'm gonna get around if I wanna

852
00:55:16.919 --> 00:55:20.679
go hardcore, let theist, I'm just gonna go around

853
00:55:20.689 --> 00:55:23.080
saying, well, I believe this and I don't believe

854
00:55:23.090 --> 00:55:26.739
that that's all very simple. Well, some people would

855
00:55:26.750 --> 00:55:28.500
say, hey, but you know, if you believe something

856
00:55:28.510 --> 00:55:30.479
you believe it's true. I mean, that's, that's the

857
00:55:30.489 --> 00:55:35.479
very concept of believing. And I say, well, that's

858
00:55:35.489 --> 00:55:42.000
your analysis of belief. It's not mine. Philosophy is

859
00:55:42.010 --> 00:55:50.840
fun. Philosophy can be pure wish fulfillment. Um But,

860
00:55:50.919 --> 00:55:53.659
you know, I, I take these things very seriously.

861
00:55:53.669 --> 00:55:56.360
Uh uh There's no problem in having fun with

862
00:55:56.370 --> 00:55:58.939
the work you do, but it can have very

863
00:55:58.949 --> 00:56:02.320
serious uh implications. And we did talk a lot

864
00:56:02.330 --> 00:56:05.280
about those in our first interview. Uh And if

865
00:56:05.290 --> 00:56:07.040
we have time, we'll do that this time too.

866
00:56:07.050 --> 00:56:09.899
But, you know, I, I do mean this quite

867
00:56:09.909 --> 00:56:12.760
seriously a a again, this relates to what I

868
00:56:12.770 --> 00:56:15.719
had mentioned a little earlier about my answering every

869
00:56:15.729 --> 00:56:18.879
question. Yes and no. Now that's another way of

870
00:56:18.889 --> 00:56:22.939
saying, uh, you know, well, there's, you know, we

871
00:56:22.949 --> 00:56:25.840
can have, this could be true, it could be

872
00:56:25.850 --> 00:56:28.600
false. We can even say that about truth itself.

873
00:56:28.610 --> 00:56:31.120
Yes. Every, there, there has to be such a

874
00:56:31.129 --> 00:56:32.889
thing as truth. I know there doesn't have to

875
00:56:32.899 --> 00:56:35.370
be such a thing as truth and belief you

876
00:56:35.379 --> 00:56:37.260
have on and on and on and on and

877
00:56:37.270 --> 00:56:41.929
on. You know, now will, will I ultimately trip

878
00:56:41.939 --> 00:56:44.959
myself up? You know, I, if I, if I

879
00:56:44.969 --> 00:56:48.560
really, really, really try to make this a completely

880
00:56:48.570 --> 00:56:53.050
coherent uh theory that makes sense and actually relates

881
00:56:53.060 --> 00:56:55.719
somehow or other to reality, am I ultimately gonna

882
00:56:55.729 --> 00:56:59.510
be, you know, tripping on my own feet here?

883
00:57:00.199 --> 00:57:03.719
I don't know, you know, time will tell. But

884
00:57:04.360 --> 00:57:07.429
this is just another of those perennial issues which

885
00:57:07.439 --> 00:57:10.300
is never gonna be resolved. That's, that's why there's

886
00:57:10.310 --> 00:57:13.040
always gonna be philosophy because these issues will never

887
00:57:13.050 --> 00:57:15.780
be resolved. So, so, you know, I stake my

888
00:57:15.790 --> 00:57:19.500
position, you know, I defend my position. I take

889
00:57:19.510 --> 00:57:22.169
it with somewhat of a grain of salt. But

890
00:57:22.699 --> 00:57:25.260
I, I have to say, I'm, I'm very deeply

891
00:57:25.270 --> 00:57:28.169
convinced of all these things. Now I'm, I probably

892
00:57:28.179 --> 00:57:32.979
have veered from your question. Uh Yeah. Were you

893
00:57:32.989 --> 00:57:35.889
asking me, how, how did I come about believing

894
00:57:35.899 --> 00:57:37.770
this in the first place? This lethe is

895
00:57:38.550 --> 00:57:41.899
uh no, actually that was the question I was

896
00:57:41.909 --> 00:57:43.479
going to ask now. So

897
00:57:44.830 --> 00:57:45.169
OK,

898
00:57:46.239 --> 00:57:49.629
but, but, but do I, I mean, how did

899
00:57:49.639 --> 00:57:52.850
you arrive at li is, uh uh, do you

900
00:57:52.860 --> 00:57:56.219
see it, for example, as a continuation, just a

901
00:57:56.229 --> 00:58:00.030
normal continuation from, uh, a moralism?

902
00:58:01.350 --> 00:58:04.169
Yes. Yeah. And, and again, I think it came

903
00:58:04.179 --> 00:58:10.840
from dealing with the companions in guilt objection. Uh,

904
00:58:10.850 --> 00:58:14.239
IT'S, it's called companions and guilt. You know, just

905
00:58:14.250 --> 00:58:19.830
because it's claiming that morality and rationality are equally

906
00:58:19.840 --> 00:58:24.250
guilty. Uh So if you think morality is guilty

907
00:58:24.260 --> 00:58:26.979
of something, you ought to think rationality is guilty

908
00:58:26.989 --> 00:58:29.260
of something. And, and, you know, I told you,

909
00:58:29.270 --> 00:58:32.550
my response was to, to seize the bull by

910
00:58:32.560 --> 00:58:39.189
both horns and say, OK, yeah, uh they're both

911
00:58:39.199 --> 00:58:42.340
guilty. So I'm giving up rationality too. If I'm

912
00:58:42.350 --> 00:58:50.189
going the pure a rationality, amorality root. Um So

913
00:58:50.629 --> 00:58:53.030
when I thought, you know, more and more about

914
00:58:53.360 --> 00:58:56.699
giving up rationality in particular, it did seem to

915
00:58:56.709 --> 00:59:02.610
me that rationality uh in its, in its pure

916
00:59:02.620 --> 00:59:05.929
extreme metaphysical sense goes hand in hand with the

917
00:59:05.939 --> 00:59:09.949
idea of their, of, of truth and falsity. Um

918
00:59:10.870 --> 00:59:13.000
And so if you give up one, you kind

919
00:59:13.010 --> 00:59:14.969
of give up the other. That, that was my

920
00:59:15.370 --> 00:59:16.870
general thinking about it.

921
00:59:17.800 --> 00:59:21.530
Mhm OK. So then let me ask you because

922
00:59:21.540 --> 00:59:25.699
this is something that probably many people will be

923
00:59:25.709 --> 00:59:30.030
wondering about. So, um when it comes to the

924
00:59:30.040 --> 00:59:36.239
idea of abolishing morality or people becoming a moralists

925
00:59:36.439 --> 00:59:39.899
in your estimation, do you think that it is

926
00:59:39.909 --> 00:59:42.739
feasible for us as humans to do that?

927
00:59:43.469 --> 00:59:48.409
That is the question that uh I've I've certainly

928
00:59:48.419 --> 00:59:52.979
grappled with all along but that, that has central

929
00:59:52.989 --> 00:59:57.239
place in the book I've just written, uh, which

930
00:59:57.250 --> 01:00:03.610
has the tentative title, Ethical Health. Uh, AND the

931
01:00:03.620 --> 01:00:12.050
subtitle is, uh, managing our moral impulses because, uh,

932
01:00:12.250 --> 01:00:15.500
you know, I have tried to walk the talk.

933
01:00:15.510 --> 01:00:23.260
Uh, I don't just wanna be philosophizing theoretically, as

934
01:00:23.270 --> 01:00:27.649
I said, I'm a very empirically minded philosopher and

935
01:00:27.659 --> 01:00:29.770
so I wanted to know if it was really

936
01:00:29.780 --> 01:00:35.969
possible to become an aorist. Uh I'm not sure,

937
01:00:36.830 --> 01:00:40.429
you know, and considering the people I know personally

938
01:00:41.189 --> 01:00:44.340
or the people I know, you know, just via

939
01:00:44.350 --> 01:00:49.159
reading or media or whatever, I'm not sure if

940
01:00:49.169 --> 01:00:55.260
there are folks out there who would be full-fledged,

941
01:00:55.290 --> 01:00:59.360
Amoral in the sense that I theorize about and

942
01:00:59.370 --> 01:01:03.370
promote. I kind of think some are, I actually

943
01:01:03.379 --> 01:01:07.169
toy with the idea that Jesus might have been

944
01:01:07.620 --> 01:01:10.719
an am moralist. I mean, you know, who knows

945
01:01:10.729 --> 01:01:13.659
what Jesus was actually like? Right. We all have

946
01:01:13.669 --> 01:01:17.979
our favorite Jesus. Uh, BUT, you know, there are

947
01:01:17.989 --> 01:01:22.090
many aspects of, of jesus' life as I know

948
01:01:22.100 --> 01:01:26.669
about it through secondary tertiary et cetera. So sources,

949
01:01:26.889 --> 01:01:29.419
um, that make me think that he might have

950
01:01:29.429 --> 01:01:32.399
been uh an Amoral list. But anyway, I wanted

951
01:01:32.409 --> 01:01:34.850
to see if I could become an Amoral list

952
01:01:34.989 --> 01:01:38.409
and that's simply to test my theory. But because

953
01:01:38.419 --> 01:01:40.560
I really thought it would make my life better.

954
01:01:41.489 --> 01:01:47.050
Uh So I've been struggling to do that and

955
01:01:47.060 --> 01:01:50.439
I've through the years I've had my ups and

956
01:01:50.449 --> 01:01:55.139
downs both in terms of thinking that I was

957
01:01:55.149 --> 01:01:58.639
able to be Amoral and in terms of whether

958
01:01:58.649 --> 01:02:01.840
I thought it was making my life better to

959
01:02:01.850 --> 01:02:05.550
be, to be this way. Uh, I can say

960
01:02:05.560 --> 01:02:10.739
now after 18 years, I am more convinced than

961
01:02:10.750 --> 01:02:20.139
ever that my personal experience bears out the possibility

962
01:02:20.149 --> 01:02:26.219
of becoming a moral and that it makes things

963
01:02:26.229 --> 01:02:29.560
more, I don't wanna say better to objective, more

964
01:02:29.570 --> 01:02:33.020
to my liking. And I think the liking of

965
01:02:33.030 --> 01:02:37.239
people who know me, who have to interact with

966
01:02:37.250 --> 01:02:42.159
me. All right. Um And oh, by the way,

967
01:02:42.169 --> 01:02:47.169
11 probable repercussion of this is how I've gotten

968
01:02:47.179 --> 01:02:50.909
more laid back about morality as, as we've been

969
01:02:50.919 --> 01:02:54.250
discussing, right? Because it does make me more laid

970
01:02:54.260 --> 01:02:59.290
back, I think to become more, you know, tolerant

971
01:02:59.300 --> 01:03:04.320
is a word we use. Uh And, but so

972
01:03:04.330 --> 01:03:08.280
many would remark about me that I'm a very

973
01:03:08.290 --> 01:03:11.709
emotional person. I would remark that about me. I

974
01:03:11.719 --> 01:03:17.629
am a very emotional person. Uh I feel everything

975
01:03:17.639 --> 01:03:21.909
very deeply. And so it has certainly been a,

976
01:03:21.919 --> 01:03:26.870
a very great struggle for me to tone it

977
01:03:26.879 --> 01:03:33.669
down. Uh BECAUSE, and, and that's why IE even

978
01:03:33.679 --> 01:03:40.929
when I became an, a moralist, uh became a,

979
01:03:40.939 --> 01:03:44.429
a AAA uh a very emotional, a moralist which,

980
01:03:44.439 --> 01:03:50.649
which made me very liable to being moralistic, right?

981
01:03:50.659 --> 01:03:53.469
Because moralists tend to be, well, I, I shouldn't

982
01:03:53.479 --> 01:03:55.169
say that either. See, I have to, this is

983
01:03:55.179 --> 01:04:00.129
why I'm qualifying everything. Right. No, moralists don't have

984
01:04:00.139 --> 01:04:05.750
to be absolutely confident about everything. Only the, the

985
01:04:05.760 --> 01:04:10.229
ones who embrace moralism. Right. But you can certainly

986
01:04:10.239 --> 01:04:16.459
be a very reasonable, rational moralist who, who always

987
01:04:16.469 --> 01:04:20.610
recognizes that, you know, they might be mistaken in

988
01:04:20.620 --> 01:04:24.280
their moral convictions even though they feel them deeply.

989
01:04:24.290 --> 01:04:27.520
Yes, of course, there are moralists like that too.

990
01:04:27.699 --> 01:04:29.449
All right. So, I'm willing to grant all that.

991
01:04:29.699 --> 01:04:35.850
I just wanna say now that I think an,

992
01:04:36.179 --> 01:04:40.459
uh, an Amoral list has the potential to do

993
01:04:40.469 --> 01:04:47.870
it even more effectively. Uh, DOES that answer your

994
01:04:47.879 --> 01:04:48.330
question?

995
01:04:49.270 --> 01:04:52.280
Mhm. Yeah. But, uh, let me just ask you

996
01:04:52.290 --> 01:04:55.290
a follow up to that reps because of an

997
01:04:55.300 --> 01:04:58.979
objection that some people might have to your, a

998
01:04:58.989 --> 01:05:02.969
moralist position here. And this will also be my

999
01:05:02.979 --> 01:05:06.919
last question for today. So, uh, I mean, some

1000
01:05:06.929 --> 01:05:12.219
people would probably object to you. I, in this

1001
01:05:12.229 --> 01:05:16.409
way, they would probably say that if people became

1002
01:05:16.419 --> 01:05:20.610
am moralists, then there would be a big risk

1003
01:05:20.620 --> 01:05:26.530
of them behaving in ways that we would consider

1004
01:05:26.540 --> 01:05:31.260
immoral all the time that is mistreating other people

1005
01:05:31.270 --> 01:05:33.310
and stuff like that. I mean, do you think

1006
01:05:33.320 --> 01:05:37.610
that there's any merit to that objection or

1007
01:05:37.620 --> 01:05:42.449
not? Of course, there is, uh, just as there's

1008
01:05:42.459 --> 01:05:45.830
merit to the theist objection that an atheist might

1009
01:05:45.840 --> 01:05:50.520
start raping and pillaging and murdering. But the question

1010
01:05:50.530 --> 01:05:54.290
for somebody in reality, in the empirical world of

1011
01:05:54.300 --> 01:05:57.489
experience that we know is, yes. But what are

1012
01:05:57.500 --> 01:06:00.679
the likelihoods, what are the tendencies? It's always a

1013
01:06:00.689 --> 01:06:03.379
matter of more and less in the real world.

1014
01:06:03.989 --> 01:06:07.699
Uh When, you know, anytime somebody institutes any policy,

1015
01:06:07.709 --> 01:06:12.229
even if you're a, a social scientist pro promoting

1016
01:06:12.239 --> 01:06:16.320
an economic policy, uh you know, social political policy,

1017
01:06:16.330 --> 01:06:20.729
whatever, obviously, some people are gonna get it wrong.

1018
01:06:20.939 --> 01:06:23.729
It's not gonna work for everybody because you're in,

1019
01:06:23.739 --> 01:06:27.429
you're always dealing with people who are totally diverse

1020
01:06:27.439 --> 01:06:30.669
in terms of their be police, their genes, their

1021
01:06:30.679 --> 01:06:34.800
cultures, their everything, or they might even have misheard

1022
01:06:34.810 --> 01:06:38.600
what you said or just so many ways things

1023
01:06:38.610 --> 01:06:41.469
can go wrong, excuse me, you know, go to

1024
01:06:41.479 --> 01:06:44.580
the way you weren't anticipating, right? See, I'm constantly

1025
01:06:44.590 --> 01:06:47.489
looking for different ways to say things now. Um

1026
01:06:47.610 --> 01:06:50.000
AND there are always ways to do it. Um

1027
01:06:50.570 --> 01:06:54.159
So yes, of course. But you know what's, what's

1028
01:06:54.169 --> 01:07:00.659
not logical or rational is to leap on those

1029
01:07:00.739 --> 01:07:04.189
exceptions of what you hope are exceptions. And so

1030
01:07:04.199 --> 01:07:06.000
you see, you see, you see, I mean, this

1031
01:07:06.010 --> 01:07:09.770
is what demagogues do. This is what a certain

1032
01:07:10.090 --> 01:07:15.189
presidential candidate in my country has been doing for

1033
01:07:15.199 --> 01:07:23.300
years. He takes one case where an immigrant may

1034
01:07:23.310 --> 01:07:27.790
have done something, you know, not guilty till proven

1035
01:07:27.800 --> 01:07:29.989
in a case of in, in, in a court

1036
01:07:30.000 --> 01:07:32.379
of law. And even then there's doubt, even then

1037
01:07:32.389 --> 01:07:34.659
there's doubt, right? But some immigrant, it looks like

1038
01:07:34.669 --> 01:07:38.229
some immigrant has done something, you know, that a

1039
01:07:38.239 --> 01:07:41.010
moral pers moralist would say is, is horrible. And

1040
01:07:41.020 --> 01:07:43.070
then I would say that's, I don't like that.

1041
01:07:43.530 --> 01:07:44.929
I don't want that to be done ever by

1042
01:07:44.939 --> 01:07:48.570
anybody. And that person needs to somehow, you know.

1043
01:07:48.870 --> 01:07:52.550
Um, YEAH. So he, he, he'll leap on one,

1044
01:07:53.350 --> 01:07:57.860
you know, awful case like that and say, you

1045
01:07:57.870 --> 01:08:00.879
see, we shouldn't allow any immigrants into the country

1046
01:08:00.889 --> 01:08:05.479
or illegal immigrants or, and even dramatically cut down

1047
01:08:05.679 --> 01:08:10.600
the percentage of, of a number of illegal immigrants

1048
01:08:10.610 --> 01:08:12.959
coming here legally. He even cut down. All right,

1049
01:08:12.969 --> 01:08:15.139
we don't want them, we don't want them. Yeah,

1050
01:08:15.149 --> 01:08:18.109
this is, I hate, I hate this. I really

1051
01:08:18.120 --> 01:08:23.479
don't like it. Right. Um So yeah, that's always,

1052
01:08:23.490 --> 01:08:25.720
there's no way to get around that if you're

1053
01:08:25.729 --> 01:08:28.799
dealing with somebody who's gonna to argue in that

1054
01:08:28.810 --> 01:08:33.950
way. Uh And, and rhetorically, they may get away

1055
01:08:33.959 --> 01:08:37.549
with it, you know, who knows? All right, all

1056
01:08:37.560 --> 01:08:39.750
I'm suggesting is the most that I think anybody

1057
01:08:39.759 --> 01:08:43.109
can ever suggest when they think they've come upon

1058
01:08:43.120 --> 01:08:47.200
something that might help to make the world more

1059
01:08:47.209 --> 01:08:49.560
the way we'd all like the world to be

1060
01:08:49.629 --> 01:08:54.629
is here are my reasons for thinking. So if

1061
01:08:54.640 --> 01:08:58.410
you agree, join the crowd, let's together, help, try

1062
01:08:58.419 --> 01:09:01.359
to change the world in this way and that's

1063
01:09:01.370 --> 01:09:04.439
it. I cannot promise you a rose garden.

1064
01:09:06.288 --> 01:09:10.398
Great. So the book that we've been focusing on

1065
01:09:10.408 --> 01:09:14.358
today is again, the spread and other essays on

1066
01:09:14.368 --> 01:09:17.688
moralism and guilt. Uh Just before we go, Doctor

1067
01:09:17.698 --> 01:09:20.148
Marks, would you like to tell people where they

1068
01:09:20.158 --> 01:09:23.488
can find your work on the internet and also

1069
01:09:23.499 --> 01:09:27.749
particularly about your upcoming book that you mentioned earlier.

1070
01:09:29.207 --> 01:09:33.099
Oh, thank you. Uh Well, you know, Amazon Place

1071
01:09:33.108 --> 01:09:35.778
to go or any place else. I don't know.

1072
01:09:35.788 --> 01:09:37.207
You know, you're, you're as good as I was

1073
01:09:37.219 --> 01:09:40.877
figuring out where my books can be purchased or

1074
01:09:40.889 --> 01:09:43.738
read and so forth. Also, of course, anybody is

1075
01:09:43.749 --> 01:09:46.809
always welcome to contact me by email for a

1076
01:09:46.818 --> 01:09:50.337
discussion. I love to talk uh to dialogue with,

1077
01:09:50.349 --> 01:09:54.349
with, with people. Uh And uh yeah, the book

1078
01:09:54.358 --> 01:09:56.278
that I hope will be the next one out

1079
01:09:56.289 --> 01:09:59.149
is um you know, as I, I said, I

1080
01:09:59.160 --> 01:10:01.740
think the title will be uh Ethical Health where

1081
01:10:01.750 --> 01:10:03.939
I'm really getting down to the real, real, real

1082
01:10:03.950 --> 01:10:07.740
nitty gritty on, on how to go about uh

1083
01:10:07.750 --> 01:10:12.080
transforming yourself uh into the sort of person. You

1084
01:10:12.089 --> 01:10:14.439
know, I've been talking about here, a so-called uh

1085
01:10:14.870 --> 01:10:20.399
Amorist. Um I, I'd also like to uh just

1086
01:10:20.410 --> 01:10:24.140
mention some names of other philosophers to you Ricardo

1087
01:10:24.500 --> 01:10:27.439
uh a and to our listeners, but people I'd

1088
01:10:27.450 --> 01:10:30.140
love for you to invite to your program if

1089
01:10:30.149 --> 01:10:32.470
you haven't already done. So it's hard for me

1090
01:10:32.479 --> 01:10:35.520
to believe that you haven't invited them since you've

1091
01:10:35.529 --> 01:10:40.250
invited everybody in the universe. It seems, but I

1092
01:10:40.259 --> 01:10:42.529
did a quick search on your site and I

1093
01:10:42.540 --> 01:10:45.950
didn't, I didn't find these names there. So if

1094
01:10:45.959 --> 01:10:48.439
your search function is working correctly, I don't think

1095
01:10:48.450 --> 01:10:50.859
you have them, but I would say uh uh

1096
01:10:50.870 --> 01:10:57.620
Martha Nussbaum uh Steven Morris, uh Ronnie de Sousa,

1097
01:10:58.089 --> 01:11:05.279
uh Lucia Schwarz, Eric Campbell, Steven Ingram Russell. Oh,

1098
01:11:05.290 --> 01:11:10.990
you've had Russell Blackford um uh Thomas uh Pelzer.

1099
01:11:11.100 --> 01:11:13.930
Uh THESE are marvelous people and you know why?

1100
01:11:13.939 --> 01:11:16.299
I think they're marvelous because they agree with a

1101
01:11:16.310 --> 01:11:24.490
lot of my idea. Uh But besides that, speaking

1102
01:11:24.500 --> 01:11:29.569
purely objectively, these are really wonderful, excellent philosophers. So

1103
01:11:29.580 --> 01:11:31.930
I hope someday to see them on your program

1104
01:11:31.939 --> 01:11:34.810
as well. Thank you again so much for inviting

1105
01:11:34.819 --> 01:11:37.930
me for another opportunity to talk about my thoughts,

1106
01:11:38.750 --> 01:11:41.209
of course. And thank you so much for the

1107
01:11:41.220 --> 01:11:44.229
suggestions and for coming on the show again, it's

1108
01:11:44.240 --> 01:11:48.029
always an immense pleasure to talk with you. Hi

1109
01:11:48.040 --> 01:11:50.370
guys. Thank you for watching this interview. Until the

1110
01:11:50.379 --> 01:11:52.839
end. If you liked it, please share it. Leave

1111
01:11:52.850 --> 01:11:55.459
a like and hit the subscription button. The show

1112
01:11:55.470 --> 01:11:57.379
is brought to you by the N Lights learning

1113
01:11:57.390 --> 01:12:00.410
and development. Then differently check the website at N

1114
01:12:00.419 --> 01:12:04.379
lights.com and also please consider supporting the show on

1115
01:12:04.390 --> 01:12:07.430
Patreon or paypal. I would also like to give

1116
01:12:07.439 --> 01:12:09.720
a huge thank you to my main patrons and

1117
01:12:09.729 --> 01:12:13.959
paypal supporters, Perego Larson, Jerry Muller and Frederick Suno,

1118
01:12:14.009 --> 01:12:17.040
Bernard Seche O of Alex Adam Kel Matthew Whitten.

1119
01:12:17.120 --> 01:12:20.479
B are no wt ho Ealj con Phil for

1120
01:12:20.620 --> 01:12:23.649
con. Then the Met Robert Wine in NAI Z

1121
01:12:24.089 --> 01:12:27.870
Mark Nevs calling Hol Brookfield, Governor Mikel Stormer Samuel

1122
01:12:28.069 --> 01:12:31.720
Andre Francis for Agns Ferus and H her meal

1123
01:12:32.020 --> 01:12:35.290
and Lain Jung Y and the Samuel K Hes

1124
01:12:35.299 --> 01:12:39.009
Mark Smith J Tom Hummel s friends, David Sloan

1125
01:12:39.020 --> 01:12:44.350
Wilson. Ya dear, Roman Roach Diego, Jan Punter, Romani

1126
01:12:44.600 --> 01:12:47.819
Charlotte, Bli Nicole Barba, Adam Hunt Pavlo Stassi Na

1127
01:12:48.500 --> 01:12:52.299
Me, Gary G Alman Sam of Zed YPJ Barboa,

1128
01:12:52.720 --> 01:12:57.089
Julian Price Edward Hall, Eden Broner Douglas Fry Franca,

1129
01:12:57.529 --> 01:13:04.120
Beto Lati Gilon Cortez Solis Scott Zachary ftdw Daniel

1130
01:13:04.129 --> 01:13:08.520
Friedman, William Buckner, Paul Giorgio, Luke Loki, Georgio, Theophano

1131
01:13:08.669 --> 01:13:12.350
Chris Williams and Peter Wo David Williams, the Ausa

1132
01:13:12.939 --> 01:13:17.160
Anton Erickson Charles Murray, Alex Shaw, Marie Martinez, Coralie

1133
01:13:17.169 --> 01:13:23.319
Chevalier, Bangalore Larry Dey, Junior, Old Ebon, Starry Michael

1134
01:13:23.330 --> 01:13:27.020
Bailey then Spur by Robert Grassy Zorn, Jeff mcmahon,

1135
01:13:27.049 --> 01:13:31.270
Jake Zul Barnabas Radick, Mark Kempel, Thomas Dvor Luke

1136
01:13:31.279 --> 01:13:35.549
Neeson, Chris Tory Kimberley Johnson Benjamin Gilbert Jessica. No

1137
01:13:35.560 --> 01:13:40.350
week. Linda Brendan Nicholas Carlson, Ismael Bensley Man George

1138
01:13:40.939 --> 01:13:45.839
Katis, Valentine Steinman Perras, Kate Von Goler, Alexander Abert

1139
01:13:45.910 --> 01:13:52.430
Liam Dan Biar Masoud Ali Mohammadi Perpendicular Jer Urla.

1140
01:13:52.830 --> 01:13:56.810
Good enough, Gregory Hastings David Pins of Sean Nelson,

1141
01:13:56.819 --> 01:14:00.470
Mike Levin and Jos Net. A special thanks to

1142
01:14:00.479 --> 01:14:03.339
my producers is our web, Jim Frank Luca Stina,

1143
01:14:03.390 --> 01:14:07.069
Tom Vig and Bernard N Cortes Dixon, Bendik Muller,

1144
01:14:07.080 --> 01:14:10.810
Thomas Trumble, Catherine and Patrick Tobin, John Carlman, Negro,

1145
01:14:11.020 --> 01:14:13.720
Nick Ortiz and Nick Golden. And to my executive

1146
01:14:13.729 --> 01:14:17.770
producers, Matthew lavender, Sergi Adrian Bogdan Knits and Rosie.

1147
01:14:17.779 --> 01:14:18.729
Thank you for all.

