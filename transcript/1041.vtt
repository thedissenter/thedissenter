WEBVTT

1
00:00:00.360 --> 00:00:03.339
Hello, everyone. Welcome to a new episode of the

2
00:00:03.339 --> 00:00:06.150
Center. I'm your host, Ricardo Lopez, and today I'm

3
00:00:06.150 --> 00:00:09.270
Jane by Doctor Anna Puzio. She's a researcher in

4
00:00:09.270 --> 00:00:13.590
the ethics of socially disruptive Technologies research program at

5
00:00:13.590 --> 00:00:17.909
the University of Puente. She's a philosopher, theologian, and

6
00:00:17.985 --> 00:00:22.774
Ethicist and her research areas include anthropology, anthropology, and

7
00:00:22.774 --> 00:00:27.045
ethics of technology and environmental ethics, and today we're

8
00:00:27.045 --> 00:00:32.904
talking about topics like social robots, anthropology, religious robots,

9
00:00:33.064 --> 00:00:38.014
posthumanism and transhumanism, and some other related topics. So,

10
00:00:38.224 --> 00:00:40.584
Doctor Puzzio, welcome to the show. It's a huge

11
00:00:40.584 --> 00:00:41.805
pleasure to everyone.

12
00:00:42.345 --> 00:00:43.615
Hi, thank you.

13
00:00:45.099 --> 00:00:49.659
So, just to introduce the topic and to clarify

14
00:00:49.659 --> 00:00:54.139
some definitions before we start getting more specifically into

15
00:00:54.139 --> 00:00:58.540
some aspects of social robots, how we respond to

16
00:00:58.540 --> 00:01:02.669
them and so on. What is a social robot?

17
00:01:03.139 --> 00:01:07.099
Yeah. So social robots are robots designed for social

18
00:01:07.099 --> 00:01:10.980
interactions, so they are displaying social behaviors even if

19
00:01:10.980 --> 00:01:14.569
we do not necessarily have to call them social.

20
00:01:16.849 --> 00:01:20.129
Uh, AND so among the social robots, there are

21
00:01:20.129 --> 00:01:23.809
those who are humanoid, right? I, I mean, what

22
00:01:23.809 --> 00:01:26.440
counts as a humanoid robot.

23
00:01:26.680 --> 00:01:31.440
Yeah. So humanoid robots are robots that have human-like

24
00:01:31.440 --> 00:01:34.529
design. So they are, but there are also many

25
00:01:34.529 --> 00:01:37.529
social robots that look like more like uh animals

26
00:01:37.529 --> 00:01:41.610
or toys, and it often happens that we anthropomorphize

27
00:01:41.610 --> 00:01:42.440
robots.

28
00:01:43.919 --> 00:01:46.839
Uh, SURE, I'm going to ask you more specifically

29
00:01:46.839 --> 00:01:50.360
about the issue of anthropomorphization. I mean, the issue,

30
00:01:50.489 --> 00:01:52.610
I am not sure if it's always an issue

31
00:01:52.610 --> 00:01:57.160
or not, but what are the issues specifically with

32
00:01:57.569 --> 00:02:01.569
when we think about and relate to robots as

33
00:02:01.569 --> 00:02:04.330
if they were human? And I would imagine that

34
00:02:04.330 --> 00:02:09.038
this happens more when they are similar to humans,

35
00:02:09.130 --> 00:02:09.449
right?

36
00:02:10.179 --> 00:02:13.570
Yeah, so what concerns people is, for example, um,

37
00:02:13.580 --> 00:02:17.779
whether robots can also have consciousness, whether they can

38
00:02:17.779 --> 00:02:21.860
be intelligent, whether they can feel pain, or whether

39
00:02:21.860 --> 00:02:26.119
they can have emotions. And our behavior towards humans

40
00:02:26.119 --> 00:02:29.990
is guided by certain rules, so humans have dignity.

41
00:02:30.160 --> 00:02:32.800
We must not kill or cause some suffering. We

42
00:02:32.800 --> 00:02:36.580
must protect them, and humans have rights and duties

43
00:02:36.580 --> 00:02:39.949
and we are now questioning how to behave towards

44
00:02:39.949 --> 00:02:40.759
robots.

45
00:02:42.240 --> 00:02:48.720
And is anthropomorphization uh a mistake? Is it always

46
00:02:48.720 --> 00:02:52.919
problematic or are there perhaps instances where it might

47
00:02:52.919 --> 00:02:53.589
not be?

48
00:02:54.270 --> 00:02:58.449
Mhm. So anthropomorphizing robots means that we are thinking

49
00:02:58.449 --> 00:03:01.660
of robots and treating them as if they were

50
00:03:01.660 --> 00:03:06.539
humans. And research, there's a big debate whether anthropomorphizing

51
00:03:06.539 --> 00:03:11.259
robots is acceptable or should be avoided. And we

52
00:03:11.259 --> 00:03:14.139
do not only do the um with robots, we

53
00:03:14.139 --> 00:03:18.149
also do this with animals, with gods and objects.

54
00:03:18.300 --> 00:03:21.380
So um look at how we treat our dogs

55
00:03:21.380 --> 00:03:26.270
or cats. And this anthropomorphizing seems to be deeply

56
00:03:26.270 --> 00:03:30.850
rooted in our psychology and behavior, but the question

57
00:03:30.850 --> 00:03:34.580
indeed is whether we can unlearn it, and I

58
00:03:34.580 --> 00:03:37.309
think that it could be very beneficial for us

59
00:03:37.309 --> 00:03:40.910
humans to sometimes train ourselves to move away from

60
00:03:40.910 --> 00:03:44.300
this anthropocentric perspective that we have.

61
00:03:46.000 --> 00:03:49.610
So earlier when I asked you about what might

62
00:03:49.610 --> 00:03:52.580
be some of the issues with this thinking about

63
00:03:52.580 --> 00:03:55.130
and relating to robots as if they were human,

64
00:03:55.179 --> 00:03:59.360
you raise some issues there that if they were

65
00:03:59.360 --> 00:04:03.979
true, like, for example, if robots were conscious, they

66
00:04:03.979 --> 00:04:08.100
might raise some ethical or moral questions here, but

67
00:04:08.380 --> 00:04:12.699
can social robots be moral patients or not?

68
00:04:13.320 --> 00:04:18.160
Mhm. So more patients are entities who are subjects

69
00:04:18.160 --> 00:04:22.480
of moral consideration or who matter morally. So the

70
00:04:22.480 --> 00:04:26.119
question is now, do we have moral obligations and

71
00:04:26.119 --> 00:04:30.760
responsibilities towards robots? And in research, again, there's a

72
00:04:30.760 --> 00:04:33.880
big debate about robots' rights and more patients, and

73
00:04:33.880 --> 00:04:37.429
there's a wide range of positions taken from, yes,

74
00:04:37.559 --> 00:04:41.200
robots should have rights to no robots should be

75
00:04:41.200 --> 00:04:45.309
treated like slaves. And yeah, to new ways also

76
00:04:45.309 --> 00:04:47.820
to new ways of thinking about robots as something

77
00:04:47.820 --> 00:04:51.019
different, um, that cannot be forced into our old

78
00:04:51.019 --> 00:04:55.179
categories, and I particularly prefer, uh, yeah, the latter.

79
00:04:56.000 --> 00:04:59.420
So, um, yeah, and the major factor in this

80
00:04:59.420 --> 00:05:02.980
question is that we form close relationships with robots.

81
00:05:03.059 --> 00:05:06.059
So people fall in love with robots. There are

82
00:05:06.059 --> 00:05:09.579
funerals for robots, and it doesn't feel right to

83
00:05:09.579 --> 00:05:11.459
us when robots are harmed.

84
00:05:13.720 --> 00:05:15.739
But, uh, I mean, of course, I would imagine

85
00:05:15.739 --> 00:05:19.299
that there are people with different opinions when it

86
00:05:19.299 --> 00:05:22.579
comes to this matter and they would consider different

87
00:05:22.579 --> 00:05:26.980
criteria when it comes to us, uh, considering social

88
00:05:26.980 --> 00:05:30.450
robots as moral patients or treating them as such.

89
00:05:31.019 --> 00:05:35.083
But what are perhaps some of the main Criteria

90
00:05:35.083 --> 00:05:37.763
that people point to. I mean, I mean, in

91
00:05:37.763 --> 00:05:40.763
terms of perhaps some of the traits that they

92
00:05:40.763 --> 00:05:45.162
would need to have, particularly psychological traits for them

93
00:05:45.162 --> 00:05:49.842
to be considered moral patients, or does it have

94
00:05:49.842 --> 00:05:54.523
perhaps something to do with just how we relate

95
00:05:54.523 --> 00:05:57.585
to them. Since you mentioned there that certain people

96
00:05:57.585 --> 00:06:00.545
can fall in love with them or might think

97
00:06:00.545 --> 00:06:04.955
of them as having some sort of relationship, not

98
00:06:04.955 --> 00:06:08.316
necessarily romantic with them, but other kind, another kind

99
00:06:08.316 --> 00:06:12.506
of social relationship. I mean, what are basically the

100
00:06:12.506 --> 00:06:15.385
criteria that people usually point to?

101
00:06:16.019 --> 00:06:19.380
Yeah, so there's a like a big catalog of

102
00:06:19.380 --> 00:06:23.339
different criteria, and people often prefer like um this

103
00:06:23.339 --> 00:06:27.059
criterion of consciousness, for example, there is the robot

104
00:06:27.059 --> 00:06:33.420
conscious or not, um, intelligence, emotions, um, then also

105
00:06:33.420 --> 00:06:38.700
different behaviors or capacities, abilities, and, um, yeah, like

106
00:06:38.700 --> 00:06:42.019
I mentioned, the, the relationship we can have with

107
00:06:42.019 --> 00:06:45.299
those entities because there's also this big argument that

108
00:06:45.299 --> 00:06:50.500
um No matter whether they have consciousness or intelligence,

109
00:06:50.700 --> 00:06:54.450
emotions, etc. BECAUSE it's also very difficult to identify

110
00:06:54.700 --> 00:06:57.940
um these criteria also in other human and non-human

111
00:06:57.940 --> 00:07:01.540
beings. We say that, yeah, but in our everyday

112
00:07:01.540 --> 00:07:05.459
actions, um we still um have like this deep

113
00:07:05.459 --> 00:07:11.329
relationships with those entities and um. Yeah, I like

114
00:07:11.329 --> 00:07:13.970
to compare this, the debate that we have um

115
00:07:13.970 --> 00:07:16.640
to um in the robotic context with the debate

116
00:07:16.640 --> 00:07:20.250
that we have in environmental ethics, so our relationship

117
00:07:20.250 --> 00:07:23.809
towards animals and um this debate is even longer,

118
00:07:23.890 --> 00:07:28.769
so how to um whether um Whether animals are

119
00:07:28.769 --> 00:07:31.489
sentient beings and what is an animal or what

120
00:07:31.489 --> 00:07:35.170
is uh what is not alive or whether um

121
00:07:35.170 --> 00:07:38.690
they are intelligent, whether they have emotions center. So

122
00:07:38.690 --> 00:07:42.489
my, my key thesis is that um with those

123
00:07:42.489 --> 00:07:46.720
kind of new relationships and those new um technologies,

124
00:07:47.250 --> 00:07:52.320
um, our old ethical concepts are transformed or disrupted,

125
00:07:52.350 --> 00:07:55.489
and we need new criteria or new ways or

126
00:07:55.489 --> 00:07:56.690
new perspectives.

127
00:07:57.510 --> 00:08:01.640
It's interesting that you establish there are parallel with

128
00:08:01.640 --> 00:08:06.440
animal or environmental ethics because actually, some of these

129
00:08:06.440 --> 00:08:10.119
questions when it comes to the criteria for someone

130
00:08:10.119 --> 00:08:12.880
or something to be considered a moral patient, are

131
00:08:12.880 --> 00:08:16.600
really hard to determine, right? Because we could say

132
00:08:16.600 --> 00:08:20.390
that, oh, I mean, it's perhaps silly for us

133
00:08:20.390 --> 00:08:23.399
to try to really figure out if a particular

134
00:08:23.399 --> 00:08:28.559
robot. Or artificial artificial intelligence system is conscious or

135
00:08:28.559 --> 00:08:31.760
not because we can never be certain of that.

136
00:08:31.799 --> 00:08:35.919
But the same could be said about uh non-human

137
00:08:35.919 --> 00:08:39.030
animals and even other humans,

138
00:08:39.239 --> 00:08:42.039
right? Yeah, indeed, yeah. So this is something that

139
00:08:42.039 --> 00:08:45.590
also um Um, it is argued by, for example,

140
00:08:45.710 --> 00:08:48.719
David Ganko and Mark Huuckerberg, so they show this,

141
00:08:48.729 --> 00:08:52.190
um, this criticism of the properties approach. So it's

142
00:08:52.190 --> 00:08:55.590
difficult to identify those properties, to have to define

143
00:08:55.590 --> 00:08:59.429
those properties, and it's always difficult even in human

144
00:08:59.429 --> 00:09:02.049
beings and um yeah. We have to reflect on

145
00:09:02.049 --> 00:09:06.090
how our behavior and social interactions really function in

146
00:09:06.090 --> 00:09:09.650
our everyday life and not what philosophers like to

147
00:09:09.650 --> 00:09:12.489
think about. So because as philosophers, we like to

148
00:09:12.489 --> 00:09:15.250
think about the metaphysical questions and oh, will there

149
00:09:15.250 --> 00:09:19.559
be super intelligence and conscious AI and Everything but

150
00:09:19.559 --> 00:09:22.809
uh does it really matter in our everyday life.

151
00:09:23.440 --> 00:09:26.520
Yeah, maybe at the end of the day, people

152
00:09:26.520 --> 00:09:29.760
will just decide to treat robots one way or

153
00:09:29.760 --> 00:09:34.280
the other depending on how much they resemble humans

154
00:09:34.280 --> 00:09:39.820
or at least resemble beings that are deserving of

155
00:09:39.820 --> 00:09:46.619
moral consideration. Mhm. So, um, let me ask you

156
00:09:46.619 --> 00:09:50.140
another kind of question, and this is uh uh

157
00:09:50.140 --> 00:09:53.179
related to another aspect of your research. In what

158
00:09:53.179 --> 00:09:59.460
ways does technological change affect or might affect understandings

159
00:09:59.460 --> 00:10:02.960
of human beings and their bodies?

160
00:10:03.770 --> 00:10:06.789
Yes, so with technology, we can now almost completely

161
00:10:06.789 --> 00:10:09.309
change the human body every part, and we can

162
00:10:09.309 --> 00:10:12.710
modify various parts of the human, uh, with this

163
00:10:12.710 --> 00:10:17.219
particular medicine or pharmaceutica, or we can implement technology

164
00:10:17.219 --> 00:10:21.099
into the body. And this raises the fascinating question

165
00:10:21.099 --> 00:10:24.309
of where the boundary between technology and the body

166
00:10:24.309 --> 00:10:28.349
lies. So can we understand technology as part of

167
00:10:28.349 --> 00:10:32.590
the human body? For example, if technology is implemented

168
00:10:32.590 --> 00:10:35.919
in the body from an early age or um

169
00:10:35.919 --> 00:10:38.400
yeah, becomes part of us and we need it

170
00:10:38.400 --> 00:10:43.840
to survive. And additionally, we have um many self

171
00:10:43.840 --> 00:10:47.479
tracking systems like smart watches or we have medical

172
00:10:47.479 --> 00:10:51.479
technologies that collect a lot of data about us,

173
00:10:51.520 --> 00:10:55.280
and they're with um they're creating a specific image

174
00:10:55.280 --> 00:10:58.880
of us and our bodies. So we only um

175
00:10:58.880 --> 00:11:02.599
collect certain data and not others, and this changes

176
00:11:02.599 --> 00:11:04.549
how we understand ourselves.

177
00:11:05.710 --> 00:11:08.549
Uh, THAT is sort of related to that idea

178
00:11:08.549 --> 00:11:12.700
that some people nowadays talk about of the quantified

179
00:11:12.950 --> 00:11:16.549
uh self, for example, right, because we now have

180
00:11:16.549 --> 00:11:20.179
access to Uh, if we want, of course, uh,

181
00:11:20.190 --> 00:11:24.750
data from several different, uh, aspects of, for example,

182
00:11:24.869 --> 00:11:29.909
our physiology or even our own personality, like, for

183
00:11:29.909 --> 00:11:33.950
example, in terms of how we behave online and

184
00:11:33.950 --> 00:11:37.500
stuff like that. So it's sort of related to

185
00:11:37.500 --> 00:11:39.390
that kind of idea, right?

186
00:11:39.469 --> 00:11:40.590
Yeah, yeah, sure.

187
00:11:41.789 --> 00:11:45.570
And and talking about how we behave online, you've

188
00:11:45.570 --> 00:11:50.570
also done some work on digital identity. So what

189
00:11:50.570 --> 00:11:52.599
is a digital identity?

190
00:11:53.400 --> 00:11:56.609
Yes, so what's fascinating about this is that our

191
00:11:56.609 --> 00:12:01.780
identity um is kind of extended by digital and

192
00:12:01.780 --> 00:12:05.340
social media or maybe not extended but also transformed.

193
00:12:05.380 --> 00:12:08.940
So in the digital realm, we now have entirely

194
00:12:08.940 --> 00:12:13.210
new possibilities to construct identity. And at the same

195
00:12:13.210 --> 00:12:16.849
time, what happens in the digital world always affects

196
00:12:16.849 --> 00:12:20.039
our identity and our physical bodies. So in fact,

197
00:12:20.090 --> 00:12:23.080
there isn't a digital wheel or a digital world,

198
00:12:23.369 --> 00:12:27.210
and there's not, there's no separation between the identity

199
00:12:27.210 --> 00:12:29.809
that we have in the non-digital world and the

200
00:12:29.809 --> 00:12:32.590
identity. That we have in the digital world, they

201
00:12:32.590 --> 00:12:36.469
are always interconnected because everything that happens on social

202
00:12:36.469 --> 00:12:41.150
media, um, with VR or augmented reality, we, we

203
00:12:41.150 --> 00:12:44.349
need our body for this and it affects our

204
00:12:44.349 --> 00:12:48.070
body and our identity. So for example, um, also

205
00:12:48.070 --> 00:12:53.039
harm. Um, IF something happens to me online, um,

206
00:12:53.289 --> 00:12:56.049
some kind of mistreatment, then it's also harm to

207
00:12:56.049 --> 00:12:59.219
my physical uh body or my identity also in

208
00:12:59.219 --> 00:13:01.000
the, in the non-digital sphere.

209
00:13:02.299 --> 00:13:07.479
And does this point to any form of posthumanism

210
00:13:07.479 --> 00:13:11.000
since in your own words, you said that, uh,

211
00:13:11.169 --> 00:13:15.409
I mean, as having digital identities points to an

212
00:13:15.409 --> 00:13:20.440
extended or a transformed identity that we have online,

213
00:13:21.530 --> 00:13:25.450
should we consider that some form of posthumanism or

214
00:13:25.450 --> 00:13:25.780
not?

215
00:13:26.409 --> 00:13:30.570
Yeah, so I distinguish between the developments, um, so

216
00:13:30.570 --> 00:13:34.169
I strictly, uh, uh, differentiate them. So the developments

217
00:13:34.169 --> 00:13:36.919
that we have in the digital realm, social media,

218
00:13:37.090 --> 00:13:42.109
um, And our optimization enhancement in our society today

219
00:13:42.440 --> 00:13:47.799
and then um transhumanism, transhumanist goals or posthumanism because

220
00:13:47.799 --> 00:13:51.630
transhumanism for me is a movement with its own

221
00:13:51.630 --> 00:13:56.989
institutions, organization, and agenda and the normal efforts for

222
00:13:56.989 --> 00:14:00.320
optimization that we have in our society today, they

223
00:14:00.320 --> 00:14:04.150
still differ from what transhumanism aims to achieve.

224
00:14:05.320 --> 00:14:10.640
Uh, BUT then posthumanism is a synonym of transhumanism

225
00:14:10.640 --> 00:14:12.710
or are they slightly different?

226
00:14:12.929 --> 00:14:15.619
And they are slightly different, so it's uh difficult

227
00:14:15.619 --> 00:14:20.349
to, to strictly separate them, so, um. I, um,

228
00:14:20.599 --> 00:14:23.460
I built on the research from, from Yanina Lu,

229
00:14:23.590 --> 00:14:26.909
and, um, so the difference would be then that

230
00:14:26.909 --> 00:14:30.229
transhumanism wants to transform the human being, so like

231
00:14:30.229 --> 00:14:35.150
the trans um and posthumanism, the past, um, wants

232
00:14:35.150 --> 00:14:38.760
to have, um, yeah, wants to overcome the human

233
00:14:38.760 --> 00:14:42.390
beings, so they rather want to have this artificial

234
00:14:42.390 --> 00:14:45.429
superintelligence rather than the human being, but it's um

235
00:14:45.429 --> 00:14:47.099
difficult to separate them.

236
00:14:48.289 --> 00:14:53.820
And of course, all of these possibly as implications

237
00:14:53.820 --> 00:14:57.590
for, uh, anthropology. I mean, the study of humans,

238
00:14:57.599 --> 00:15:02.179
because since we introduced these new kinds of technologies

239
00:15:02.179 --> 00:15:05.619
and that are also evolving and progressing, they will.

240
00:15:05.724 --> 00:15:10.385
Transform me to even more complex kinds of technologies

241
00:15:10.385 --> 00:15:12.585
and also the way we relate to them. So

242
00:15:12.585 --> 00:15:16.505
do you think that this might also have implications

243
00:15:16.505 --> 00:15:21.424
for anthropology and how anthropologists study humans in this

244
00:15:21.424 --> 00:15:22.974
sort of new environment?

245
00:15:23.594 --> 00:15:27.145
Yes, of course. So anthropology uh changes with new

246
00:15:27.145 --> 00:15:31.945
technologies. So when new technological inventions emerge, we examine

247
00:15:31.945 --> 00:15:34.880
and use them. And then we ask ourselves what

248
00:15:34.880 --> 00:15:38.080
it means to be human. So for example, with

249
00:15:38.080 --> 00:15:42.440
robots, we ask what distinguishes robots from humans and

250
00:15:42.440 --> 00:15:46.530
what will distinguish them in the future. And um

251
00:15:46.530 --> 00:15:51.409
this leads us to reconsider what emotions, consciousness, intelligence

252
00:15:51.409 --> 00:15:55.070
are. So technology always changes how we understand ourselves

253
00:15:55.070 --> 00:15:57.729
and we can, we can already see this in

254
00:15:57.729 --> 00:16:00.080
history with the invention of the clock, for example,

255
00:16:00.250 --> 00:16:02.859
then people ask, oh, OK, what's the human body,

256
00:16:02.890 --> 00:16:05.690
and then we compare the the uh the human

257
00:16:05.690 --> 00:16:08.169
body to the clock, and the same was then

258
00:16:08.169 --> 00:16:11.210
with the computer because we have different computer models

259
00:16:11.210 --> 00:16:14.409
of the brain also or the. Um, THE mind

260
00:16:14.409 --> 00:16:17.809
and, um, so that's we, so human beings and

261
00:16:17.809 --> 00:16:20.409
technology are very, uh, much connected.

262
00:16:21.340 --> 00:16:25.049
So it's not only the ways by which we

263
00:16:25.049 --> 00:16:29.900
theorize about uh humanity because as you mentioned, there

264
00:16:29.900 --> 00:16:33.789
are perhaps different kinds of technologies, influences in thinking

265
00:16:33.789 --> 00:16:37.164
about, for example, the human. Mind or the human

266
00:16:37.164 --> 00:16:40.835
brain in particular ways like a computer, for example,

267
00:16:41.205 --> 00:16:44.515
something like that. But, uh, uh, but it's also

268
00:16:44.515 --> 00:16:49.034
about how these kinds, these new kinds of technologies

269
00:16:49.405 --> 00:16:55.614
might affect, uh, our, let's say, reality as human

270
00:16:55.614 --> 00:17:00.109
beings. I mean, it can. Uh, AFFECT directly, even

271
00:17:00.260 --> 00:17:05.020
phenomenologically, let's say how we think about ourselves and

272
00:17:05.020 --> 00:17:08.118
how we relate to one another as human beings,

273
00:17:08.280 --> 00:17:08.660
right.

274
00:17:09.339 --> 00:17:11.780
Yes, yes, and we can see this with medical

275
00:17:11.780 --> 00:17:15.939
technologies. They are implemented into our body or influencing

276
00:17:16.339 --> 00:17:21.368
medical decisions, our health, um, yeah, and also, um,

277
00:17:21.500 --> 00:17:25.020
yeah, it's, it's like this, uh, they, it's the

278
00:17:25.020 --> 00:17:30.339
connection between technology and um And uh human beings,

279
00:17:30.400 --> 00:17:33.189
it's, it's so close because they are shaping us

280
00:17:33.189 --> 00:17:36.839
and we are um giving data for them, so

281
00:17:36.839 --> 00:17:39.520
they are having our data and then they, yeah,

282
00:17:39.680 --> 00:17:43.140
so we are both we are shaping us constantly,

283
00:17:43.199 --> 00:17:43.550
yeah.

284
00:17:44.530 --> 00:17:49.560
So, you're also a theologian and you've written actually

285
00:17:49.560 --> 00:17:53.810
about religious robots. So what is this idea of

286
00:17:53.810 --> 00:17:56.930
a religious robot? What is a religious robot?

287
00:17:57.839 --> 00:18:01.260
Yeah, so the question of whether robots can be

288
00:18:01.260 --> 00:18:04.420
religious is a difficult one. So because we first

289
00:18:04.420 --> 00:18:08.420
must ask um how can we identify whether someone

290
00:18:08.420 --> 00:18:13.300
is religious. For example, can animals be religious? And

291
00:18:13.300 --> 00:18:16.569
what about humans who lack certain capabilities? So I

292
00:18:16.569 --> 00:18:19.410
think that in the context of robotics, the questions

293
00:18:19.410 --> 00:18:22.859
are not highly relevant to ethics, but rather fascinates

294
00:18:22.859 --> 00:18:26.680
us. So again, And therefore I focus more on

295
00:18:26.680 --> 00:18:30.400
the question whether robots can perform religious practices.

296
00:18:32.079 --> 00:18:35.800
Uh, AND can they perform religious practices? I, I

297
00:18:35.800 --> 00:18:37.949
mean, what does that mean

298
00:18:37.949 --> 00:18:38.439
exactly

299
00:18:38.439 --> 00:18:40.760
to perform a religious practice?

300
00:18:41.160 --> 00:18:45.119
Yes, so religious robots can accompany prayers. They can

301
00:18:45.119 --> 00:18:49.319
lead to religious ceremonies, create or generate or read

302
00:18:49.319 --> 00:18:53.770
religious texts and music, and then can provide um

303
00:18:54.050 --> 00:18:58.869
guiding tours of religious uh buildings. Yeah, and whether

304
00:18:58.869 --> 00:19:04.579
they um Should have religious functions is a really

305
00:19:04.579 --> 00:19:08.099
good question. So there are many social robots for

306
00:19:08.099 --> 00:19:12.219
hospitals, for care and for education, and they achieve

307
00:19:12.219 --> 00:19:15.140
great results in these areas. So for example, in

308
00:19:15.140 --> 00:19:21.810
therapy, autism, um, motivation and education, creativity. And they

309
00:19:21.810 --> 00:19:25.050
can sometimes do things that humans cannot, and I

310
00:19:25.050 --> 00:19:28.410
wonder if we use social robots in these fields,

311
00:19:28.609 --> 00:19:32.530
should they also perform religious practices, or should they

312
00:19:32.530 --> 00:19:37.079
remain agnostic atheistic. So I would argue that they

313
00:19:37.079 --> 00:19:41.479
should also have religious functions or perform religious practices,

314
00:19:42.040 --> 00:19:45.959
especially in the hospital context, um, you have so

315
00:19:45.959 --> 00:19:49.800
spiritual needs and existential questions arise there. So I

316
00:19:49.800 --> 00:19:53.760
think it would be, yeah, a good opportunity to

317
00:19:53.760 --> 00:19:57.069
also have like this religious functions and these robots.

318
00:19:57.560 --> 00:20:01.000
Could there also be somewhere in the future, a

319
00:20:01.000 --> 00:20:04.670
priest robots or is that too much?

320
00:20:05.660 --> 00:20:09.719
It could be, um, I prefer to, to change

321
00:20:09.719 --> 00:20:12.920
current structures because, um, for example, um, I'm a

322
00:20:12.920 --> 00:20:17.119
Catholic theologian and in Christianity we um question a

323
00:20:17.119 --> 00:20:20.719
lot of structures that we now have. For example,

324
00:20:20.800 --> 00:20:25.239
this um high authority or um hierarchy or for

325
00:20:25.239 --> 00:20:29.750
example, gender because only. Uh, MALE, um, persons can

326
00:20:29.750 --> 00:20:33.270
be, um, priests, so it would be better to

327
00:20:33.270 --> 00:20:38.069
not um replace human, human relationships with robots, but

328
00:20:38.069 --> 00:20:42.270
to, to use them for, um, in what they

329
00:20:42.270 --> 00:20:45.630
can do best, so extend our relationships and to

330
00:20:45.630 --> 00:20:47.790
use them for better things. So for example, it

331
00:20:47.790 --> 00:20:50.030
would be great if we could use them to

332
00:20:50.030 --> 00:20:54.520
um To increase the interaction in the religious community

333
00:20:54.520 --> 00:20:59.199
and to not um just simulate this hierarchy again,

334
00:20:59.410 --> 00:21:03.369
but to um yeah, to, to promote this community

335
00:21:03.369 --> 00:21:04.130
that we have.

336
00:21:05.219 --> 00:21:09.400
Do you think that this idea of religious robots

337
00:21:09.400 --> 00:21:13.510
and this issue could raise questions as to whether

338
00:21:13.930 --> 00:21:21.270
robots could actually believe in something religious and if,

339
00:21:21.569 --> 00:21:23.750
uh, I, I mean, and is that a question

340
00:21:23.750 --> 00:21:27.699
that We would even need to be answered because

341
00:21:27.699 --> 00:21:32.099
even talking about humans, for example, the, is it

342
00:21:32.099 --> 00:21:36.959
really necessary for someone, like, for example, a priest

343
00:21:36.959 --> 00:21:42.109
to actually believe in what they're preaching to be,

344
00:21:42.339 --> 00:21:45.780
uh, a priest or not, I mean, I'm not

345
00:21:45.780 --> 00:21:47.810
sure if you understand the question or.

346
00:21:49.099 --> 00:21:53.780
Yeah, um, um, I could have, um, yeah, maybe

347
00:21:53.780 --> 00:21:59.540
two possible answers to this. Um, FIRST, it's important

348
00:21:59.540 --> 00:22:03.699
what kind of teaching, um, they have implemented because,

349
00:22:03.819 --> 00:22:07.060
um, so it would be not good if they

350
00:22:07.060 --> 00:22:10.979
would just be used for fundamentalism or just to

351
00:22:10.979 --> 00:22:15.829
promote the one theological position of the church. Um,

352
00:22:15.859 --> 00:22:20.420
BECAUSE there's also this diversity of practiced faith, and

353
00:22:20.420 --> 00:22:23.859
so many religious practices, and they're still neglected by

354
00:22:23.859 --> 00:22:26.300
the church that they, and they are also valid

355
00:22:26.300 --> 00:22:30.380
and, of course, right? So, um, this kind of

356
00:22:30.380 --> 00:22:35.369
robots, um, should have, um, like this um. Some

357
00:22:35.369 --> 00:22:40.130
interaction modules and some flexibility and dynamic understanding and

358
00:22:40.130 --> 00:22:42.449
I think that we wouldn't need a robot who's

359
00:22:42.449 --> 00:22:44.640
um that says to us what we should do,

360
00:22:44.650 --> 00:22:48.689
but rather that increases our autonomy, our needs and

361
00:22:48.689 --> 00:22:51.449
our thinking, so like more or less sparing partner

362
00:22:51.449 --> 00:22:54.829
or something and not uh a new boss.

363
00:22:55.939 --> 00:22:59.189
Yeah, but when it comes to the more, let's

364
00:22:59.189 --> 00:23:04.670
say, side related to religious beliefs, the more doxastic

365
00:23:04.670 --> 00:23:09.380
side, I guess. Uh, I, I mean, if we,

366
00:23:09.660 --> 00:23:15.290
um, discovered that robots could also Themselves and hold

367
00:23:15.290 --> 00:23:19.609
religious beliefs. Do you think that would also tell

368
00:23:19.609 --> 00:23:26.180
us something about our own religious beliefs as humans

369
00:23:26.180 --> 00:23:31.160
and how they work, uh, cognitively speaking or not?

370
00:23:31.979 --> 00:23:34.140
I'm not sure about this. I, I have to

371
00:23:34.140 --> 00:23:38.219
think about this. I, but I agree that um.

372
00:23:39.140 --> 00:23:41.300
You know, there's the same like how we are

373
00:23:41.300 --> 00:23:46.050
now with emotional artificial intelligence, we are exploring how

374
00:23:46.050 --> 00:23:49.540
emotions work and um so this could also be

375
00:23:49.540 --> 00:23:53.060
a chance to um I'm skeptical whether they could

376
00:23:53.060 --> 00:23:56.849
be religious, but um I, we could just, um,

377
00:23:57.020 --> 00:24:00.689
yeah, we could reflect what religious experiences are.

378
00:24:02.199 --> 00:24:05.380
But on the more, let's say ethical side of

379
00:24:05.380 --> 00:24:10.160
things, what kinds of ethical questions uh can uh

380
00:24:10.160 --> 00:24:14.500
the idea of robots serving religious functions or raise?

381
00:24:14.680 --> 00:24:18.219
I mean, what, what considerations do people make here?

382
00:24:18.229 --> 00:24:21.920
Mhm. Yeah, so this whole topic of religious robots

383
00:24:21.920 --> 00:24:25.900
is still really new. And um that's why recently

384
00:24:25.900 --> 00:24:28.589
at the Catolientag an airport in Germany, so this

385
00:24:28.589 --> 00:24:31.670
is a big church event, um, last week, I

386
00:24:31.670 --> 00:24:35.069
conducted a study, um, to find out whether religious

387
00:24:35.069 --> 00:24:38.989
people want religious robots, and if so, what they

388
00:24:38.989 --> 00:24:42.819
should be like, because, um, I don't think that,

389
00:24:42.829 --> 00:24:47.060
um, philosophers or theologians or the church alone should

390
00:24:47.060 --> 00:24:49.349
decide whether we need them and how to design

391
00:24:49.349 --> 00:24:53.219
them. Um, BECAUSE also philosophers like to think about

392
00:24:53.219 --> 00:24:56.949
some fancy problems that uh don't matter for for

393
00:24:56.949 --> 00:25:00.949
other people sometimes. And, um, yeah, I found it

394
00:25:00.949 --> 00:25:04.459
really fascinating how open um people were to robots,

395
00:25:04.469 --> 00:25:08.099
and they were very interested and forward thinking and

396
00:25:08.099 --> 00:25:12.420
had um really great suggestions, um. So for example,

397
00:25:12.479 --> 00:25:17.040
that robots could um provide many ideas for religions

398
00:25:17.040 --> 00:25:20.959
and um for religions and churches, so generate text

399
00:25:20.959 --> 00:25:25.119
or they could answer some questions about religions in

400
00:25:25.119 --> 00:25:27.839
an entertaining way because I also saw it um

401
00:25:27.839 --> 00:25:30.599
in my workshop that People were so interested in

402
00:25:30.599 --> 00:25:34.439
religious robots, they um always had their, I, I

403
00:25:34.439 --> 00:25:37.660
brought a a small N robot, so the um

404
00:25:37.660 --> 00:25:40.400
social robot now with me, and they just uh

405
00:25:40.969 --> 00:25:43.119
did make so many photos with this now robots,

406
00:25:43.160 --> 00:25:46.239
so they were like uh very entertained by it.

407
00:25:47.130 --> 00:25:51.010
And um yeah, also they thought about promoting community

408
00:25:51.010 --> 00:25:53.380
and interaction so they don't want to have this

409
00:25:53.380 --> 00:25:58.880
religious hierarchy um established. And other topics were, were

410
00:25:58.880 --> 00:26:04.719
like um gender, data protection and manipulation um were

411
00:26:04.719 --> 00:26:06.959
also very important. I also had a child in

412
00:26:06.959 --> 00:26:09.910
one of these workshops who was also very interested

413
00:26:09.910 --> 00:26:13.239
in how this um robot works because I, what

414
00:26:13.239 --> 00:26:16.410
I tried, um, is that now also imitates like

415
00:26:16.410 --> 00:26:20.319
um a monkey or a mouse or these animations

416
00:26:20.319 --> 00:26:22.880
that are in, in the now robot and um

417
00:26:22.880 --> 00:26:26.229
people found it very funny and The children wanted

418
00:26:26.229 --> 00:26:29.380
to have them also in their um religious education

419
00:26:29.390 --> 00:26:31.510
to show them how to to knee or to

420
00:26:31.510 --> 00:26:33.109
pray or um yeah.

421
00:26:34.859 --> 00:26:38.319
Yeah, no, I, I was just thinking that particularly

422
00:26:38.319 --> 00:26:43.430
in Catholicism, since people have confessions, uh, I was

423
00:26:43.430 --> 00:26:46.459
wondering in terms of data privacy if there was

424
00:26:46.459 --> 00:26:51.260
a robot there, uh, acting as a confessor. Uh,

425
00:26:51.310 --> 00:26:54.949
I mean, of course, that those kinds of data

426
00:26:54.949 --> 00:26:57.670
would have to be made private, right?

427
00:26:58.430 --> 00:27:01.869
Yeah, yeah, of course. So, um, also with the

428
00:27:01.869 --> 00:27:05.869
relationship, uh, the problematic relationship between church, um, with

429
00:27:05.869 --> 00:27:09.670
the church now, um, it's very necessary that this

430
00:27:09.670 --> 00:27:12.630
would be completely anonymous and that, um, church or

431
00:27:12.630 --> 00:27:15.469
the state wouldn't have any access to the data

432
00:27:15.469 --> 00:27:18.910
and those kinds of robots. Yeah, that's, yeah, that's

433
00:27:18.910 --> 00:27:21.479
why data protection is so important, yeah. Mm.

434
00:27:22.979 --> 00:27:25.619
So, let me ask you about one last topic,

435
00:27:25.660 --> 00:27:28.900
then we've already touched on it earlier when we

436
00:27:28.900 --> 00:27:35.260
talked about posthumanism, transhumanism, but transhumanism specifically, what kinds

437
00:27:35.260 --> 00:27:38.699
of ethical questions does it raise? Uh AND by

438
00:27:38.699 --> 00:27:42.300
the way, uh, even before that question, what different

439
00:27:42.300 --> 00:27:45.099
kinds of transhumanism are there?

440
00:27:46.170 --> 00:27:50.920
Yeah, so, um, transhumanism is a philosophical technological movement

441
00:27:50.920 --> 00:27:55.770
of the late 20s or 21st centuries, um, primarily

442
00:27:55.770 --> 00:27:59.810
in the UK and the US, and transhumanism wants

443
00:27:59.810 --> 00:28:04.369
to transform human through new tech. AND visions of

444
00:28:04.369 --> 00:28:10.050
transhumanism are, for example, um enhancement technologies, cryonics, so

445
00:28:10.050 --> 00:28:14.839
freezing the human body, radical life extension from several

446
00:28:14.839 --> 00:28:19.479
100 years up to uh immortality. Or infinite life

447
00:28:19.479 --> 00:28:24.250
spans and then the elimination of suffering and diseases

448
00:28:24.479 --> 00:28:27.579
and posthumanism like we mentioned before, is rather they

449
00:28:27.579 --> 00:28:30.569
um they want to overcome the human being and

450
00:28:30.569 --> 00:28:35.959
to have this um artificial intelligence, um. And what

451
00:28:35.959 --> 00:28:40.439
kinds of ethical question arise? Um, SO in my

452
00:28:40.439 --> 00:28:46.560
research, I've primarily um examined the argumentativeative structures of

453
00:28:46.560 --> 00:28:51.859
transhumanism. And I noticed um that the way transhumanism

454
00:28:51.859 --> 00:28:56.069
argues, um it's ideas, they do not make sense.

455
00:28:56.319 --> 00:29:02.400
So um they employ manipulative augmentative, augmentation structures, they

456
00:29:02.400 --> 00:29:05.920
pretend to rely on natural sciences, for example, but

457
00:29:05.920 --> 00:29:11.060
actually they are contradicting scientific knowledge. For example, um

458
00:29:11.060 --> 00:29:14.739
transhumanism argues that they are good genes and bad

459
00:29:14.739 --> 00:29:17.569
genes in the human being and that we can

460
00:29:17.569 --> 00:29:20.180
simply cut out the bad ones and then we

461
00:29:20.180 --> 00:29:24.030
have to improved human being and no, um, yeah.

462
00:29:24.829 --> 00:29:28.410
No one from science would agree with this. And

463
00:29:28.410 --> 00:29:30.380
um yeah, I have also looked at the goals

464
00:29:30.380 --> 00:29:34.140
of transhumanism, so it's um it aims to overcome

465
00:29:34.140 --> 00:29:39.449
contingency and vulnerability. It focuses a lot on controlling

466
00:29:39.449 --> 00:29:43.209
the human body, also reproduction, for example, it's always

467
00:29:43.209 --> 00:29:48.699
controlling our bodies. And transhumanism seeks to improve human

468
00:29:48.699 --> 00:29:54.829
beings. Which initially sounds um attractive and unproblematic for

469
00:29:54.829 --> 00:30:00.109
us, but um the improvements um are always based

470
00:30:00.109 --> 00:30:06.219
on what transhumanism considers desirable and worth improving. So

471
00:30:06.219 --> 00:30:09.579
you can see that for example transhumanism and discriminates

472
00:30:09.579 --> 00:30:14.609
against women or people with disabilities and generally um

473
00:30:15.839 --> 00:30:20.599
Aging, um. Ill individuals, yeah.

474
00:30:22.030 --> 00:30:25.630
So do you think that the genetics part that

475
00:30:25.630 --> 00:30:29.510
you mentioned there related to how they classify perhaps

476
00:30:29.510 --> 00:30:32.250
certain genes as good and others bad. And I

477
00:30:32.250 --> 00:30:35.150
mean, this is actually something that I've talked on

478
00:30:35.150 --> 00:30:40.150
the show with particularly geneticists and biologists and it

479
00:30:40.150 --> 00:30:45.030
doesn't make much sense. At least in 99% of

480
00:30:45.030 --> 00:30:48.229
the cases to talk about good genes or bad

481
00:30:48.229 --> 00:30:51.339
genes. So that's already problematic. But do you think

482
00:30:51.339 --> 00:30:55.219
that perhaps certain branches of transhumanism where people are

483
00:30:55.670 --> 00:31:01.109
interested in genetics might also connect at least in

484
00:31:01.109 --> 00:31:03.189
some ways to eugenics?

485
00:31:04.800 --> 00:31:08.829
Um, YEAH, it's connected with eugenics, um, of course,

486
00:31:08.989 --> 00:31:12.910
because of the discrimination of, um, people with disability,

487
00:31:13.109 --> 00:31:15.750
so there is this, uh, very close connection, I

488
00:31:15.750 --> 00:31:20.760
think. Um, YEAH, and they're talking about good genes

489
00:31:20.760 --> 00:31:24.079
and bad genes, so there's, um, a lot of

490
00:31:24.079 --> 00:31:27.000
research in genetics about what a gene is and

491
00:31:27.000 --> 00:31:30.239
that our understanding of a, a gene as a

492
00:31:30.239 --> 00:31:33.930
physical ontological entity is already wrong and then Um,

493
00:31:33.939 --> 00:31:37.260
with bad genes and good genes or healthy genes

494
00:31:37.260 --> 00:31:39.750
or old genes, um, this is then a topic

495
00:31:39.750 --> 00:31:43.420
of, uh, ethics, of course, because they're, uh, moralizing

496
00:31:43.420 --> 00:31:46.500
genes or, um, they are, um, yeah, so there

497
00:31:46.500 --> 00:31:50.260
are normative implications and we cannot, um, even if

498
00:31:50.260 --> 00:31:52.979
they were traits that were just inscribed in an

499
00:31:52.979 --> 00:31:55.939
ontological entity, we couldn't just discuss whether they are

500
00:31:55.939 --> 00:31:58.780
good or bad or because it's way too complex

501
00:31:58.780 --> 00:32:01.260
to. To do this this way, um.

502
00:32:02.310 --> 00:32:05.910
So another interesting aspect here and that you pointed

503
00:32:05.910 --> 00:32:10.630
to earlier is that by talking, by using terms

504
00:32:10.630 --> 00:32:14.109
like good, bad, for example, good genes, bad genes

505
00:32:14.109 --> 00:32:16.949
or good traits, bad traits and so on, or

506
00:32:16.949 --> 00:32:22.729
better traits, for example. They are already applying language

507
00:32:22.729 --> 00:32:27.250
that is non-scientific, right, even though some of them

508
00:32:27.250 --> 00:32:32.910
present themselves as being a scientists or science enthusiasts

509
00:32:32.910 --> 00:32:37.569
and they say that whatever they are proposing is

510
00:32:38.099 --> 00:32:41.640
just science or objective or something like that.

511
00:32:42.020 --> 00:32:45.050
Yeah, I think that a big problem with transhumanism

512
00:32:45.050 --> 00:32:49.569
is also um. That their ideas are so fascinating

513
00:32:49.569 --> 00:32:52.510
and they are connected also with the AI hype

514
00:32:52.939 --> 00:32:57.439
and how media um discuss transhumanism, they are also,

515
00:32:57.699 --> 00:33:02.160
um, yeah, they're scared. Of transhumanism or they are

516
00:33:02.160 --> 00:33:07.119
very, um, also enthusiastic about transhumanism, but, um, people

517
00:33:07.119 --> 00:33:09.800
don't look into the text of transhumanism, and I

518
00:33:09.800 --> 00:33:13.520
think that maybe you don't need a philosophical background

519
00:33:13.520 --> 00:33:16.359
to just read one of those texts and to

520
00:33:16.359 --> 00:33:21.069
understand that what they are proposing is, uh, manipulative

521
00:33:21.069 --> 00:33:26.079
and scientific, not scientifically not correct. So, um, Yeah,

522
00:33:26.160 --> 00:33:27.859
I think that this is a big problem that

523
00:33:27.859 --> 00:33:30.959
we don't uh scientifically engage with um those kind

524
00:33:30.959 --> 00:33:33.719
of movements and that they are so powerful because

525
00:33:33.719 --> 00:33:36.550
also, because of the AI developments.

526
00:33:37.750 --> 00:33:42.229
So do you think that looking at transhumanism in

527
00:33:42.229 --> 00:33:47.829
general, perhaps it has more negative than positive aspects

528
00:33:47.829 --> 00:33:48.420
to it,

529
00:33:48.829 --> 00:33:52.780
or? Yeah, yeah, I think I, um, so in

530
00:33:52.780 --> 00:33:56.800
my book, I describe tech um transhumanism also as

531
00:33:56.800 --> 00:34:01.630
an ideology and um yeah, I see they're very

532
00:34:01.630 --> 00:34:06.410
dangerous um structures because um Like we mentioned in

533
00:34:06.410 --> 00:34:09.159
our um talk now, there are so many questions

534
00:34:09.159 --> 00:34:12.168
people now have, so anthropological questions, what is a

535
00:34:12.168 --> 00:34:15.290
human being, um, what diff um what is the

536
00:34:15.290 --> 00:34:18.649
difference now between us and robots and AI and

537
00:34:18.929 --> 00:34:20.969
what will the future look like, and we, we

538
00:34:20.969 --> 00:34:23.929
have this need for orientation in society. We need

539
00:34:23.929 --> 00:34:27.319
answers and we need good explanations and If then

540
00:34:27.319 --> 00:34:32.358
transhumanism comes and produces this fear or this euphoria,

541
00:34:32.478 --> 00:34:34.739
this is um this is not the way that

542
00:34:34.739 --> 00:34:36.998
we need in this uh AI discourse.

543
00:34:38.360 --> 00:34:42.139
So let me ask you then one last uh

544
00:34:42.139 --> 00:34:45.699
question or one about one last topic. So earlier

545
00:34:45.699 --> 00:34:50.780
we talked about digi digital identities. What are some

546
00:34:50.780 --> 00:34:55.739
of the ethical implications of digital afterlives because we've

547
00:34:55.739 --> 00:35:00.550
already, we, we're already seeing. Uh, I mean, not

548
00:35:00.550 --> 00:35:04.239
necessarily, perhaps a digital afterlife, but some people who

549
00:35:04.239 --> 00:35:08.149
die and they have social media and they were

550
00:35:08.149 --> 00:35:11.550
on other places on throughout the internet. And so

551
00:35:11.550 --> 00:35:15.479
they leave, for example, I don't know, Facebook or

552
00:35:15.479 --> 00:35:20.310
Twitter behind and they might have, uh, and, and

553
00:35:20.310 --> 00:35:24.820
after. A digital afterlife, uh, or, or not, but,

554
00:35:24.949 --> 00:35:27.830
uh, what are some of the ethical implications of

555
00:35:27.830 --> 00:35:28.139
that?

556
00:35:29.229 --> 00:35:32.669
Maybe first, um, there's also this connection between transhumanism

557
00:35:32.669 --> 00:35:35.350
and digital afterlife, uh, in the vision of mind

558
00:35:35.350 --> 00:35:38.149
uploading. So this is the vision, um, so they

559
00:35:38.149 --> 00:35:41.949
believe that we can upload our brains and continue

560
00:35:41.949 --> 00:35:44.580
to exist on A hard drive or a computer.

561
00:35:44.989 --> 00:35:47.399
And as I already mentioned, I'm a bit skeptical

562
00:35:47.399 --> 00:35:51.590
about their, their visions because, um, yeah, this overlaps

563
00:35:51.590 --> 00:35:53.989
the unity, the connection of the brain with the

564
00:35:53.989 --> 00:35:58.870
entire organism, with relationships, environment, and we cannot just

565
00:35:58.870 --> 00:36:01.469
not, we are not just our brains, um, and

566
00:36:01.469 --> 00:36:05.790
the brain cannot just exist independently. But there's also

567
00:36:05.790 --> 00:36:09.070
this, um, second area that you're mentioning, um, That's

568
00:36:09.070 --> 00:36:12.610
called death technology. So all the kinds of technologies

569
00:36:12.610 --> 00:36:16.409
are related to the topic of death. And um

570
00:36:16.409 --> 00:36:19.929
so it's possible to to um create avatars of

571
00:36:19.929 --> 00:36:23.000
ourselves that continue to live after we are dead.

572
00:36:23.770 --> 00:36:27.090
And um these avatars, they look like us, they

573
00:36:27.090 --> 00:36:30.969
speak like us, they can communicate with um with

574
00:36:30.969 --> 00:36:34.810
our dearest ones, even though we have long passed

575
00:36:34.810 --> 00:36:38.340
already passed away. Yeah. And this is already possible

576
00:36:38.340 --> 00:36:42.290
today. And of course this has great opportunities but

577
00:36:42.290 --> 00:36:47.350
also ethical challenges such as um for example, consent.

578
00:36:47.530 --> 00:36:51.169
So um did the deceased people want um the

579
00:36:51.169 --> 00:36:54.169
data to be used in this way after their

580
00:36:54.169 --> 00:36:59.669
death? Um, OF course, it's already a topic today

581
00:36:59.669 --> 00:37:03.949
because, um, we also have like VIPs and important,

582
00:37:04.030 --> 00:37:08.439
um, um, people in publicity and we use their

583
00:37:08.439 --> 00:37:13.919
data or material from them uh already. Yes. Um.

584
00:37:14.199 --> 00:37:17.600
Then there are questions of manipulation, especially if young

585
00:37:17.600 --> 00:37:21.479
children use these avatars. Um, SO for example, Amazon,

586
00:37:21.520 --> 00:37:26.479
I think, um, advertised, um, one or two years

587
00:37:26.479 --> 00:37:30.189
ago that they want to have Alexa reading, um,

588
00:37:30.199 --> 00:37:34.895
grandchildren, um. They want to have an Alexa that's

589
00:37:34.895 --> 00:37:37.094
uh that has the voice of a grandmother that

590
00:37:37.094 --> 00:37:40.215
reads the text to their grandchildren, even if the

591
00:37:40.215 --> 00:37:43.925
grandmother passed away. So this could be manipulative because

592
00:37:43.925 --> 00:37:48.645
um children don't understand that uh this grandmother passed

593
00:37:48.645 --> 00:37:51.284
away and it's not reading the text to them.

594
00:37:52.129 --> 00:37:55.530
And um a big question is that of um

595
00:37:55.739 --> 00:38:00.780
well-being and um psychological functions. So does um death

596
00:38:00.780 --> 00:38:03.899
tech or um do these technologies help us in

597
00:38:03.899 --> 00:38:08.195
the grieving process or do they hinder? Uh, SO

598
00:38:08.195 --> 00:38:11.754
I think that we have to explore ways um

599
00:38:12.114 --> 00:38:14.604
where we can, yeah, use them, where they benefit

600
00:38:14.604 --> 00:38:17.554
us, and maybe there's some functions task um where

601
00:38:17.554 --> 00:38:20.235
we don't need them and it's uh maybe too

602
00:38:20.235 --> 00:38:22.554
um too problematic to use them.

603
00:38:23.409 --> 00:38:27.919
Uh, BUT what specific kinds of technologies like that

604
00:38:28.290 --> 00:38:31.290
do we already have available or are in the

605
00:38:31.290 --> 00:38:34.610
process of developing? I mean, I mean, do, because

606
00:38:34.610 --> 00:38:36.810
I, I don't know much about it. Is it

607
00:38:36.810 --> 00:38:42.010
like a chatbots or perhaps even robots themselves that

608
00:38:42.010 --> 00:38:46.570
are fed uh information, I don't know, like messages

609
00:38:46.570 --> 00:38:50.169
or something like that, uh, written by the diseased

610
00:38:50.169 --> 00:38:54.530
person and then it sort of reproduces their kinds

611
00:38:54.530 --> 00:38:58.459
of speech patterns or some of their behaviors, what

612
00:38:58.459 --> 00:38:59.850
do we have available.

613
00:39:00.250 --> 00:39:03.120
Yeah. So what we already have is chatbot, so

614
00:39:03.120 --> 00:39:05.810
this was also a big debate with LGBT in

615
00:39:05.810 --> 00:39:09.600
the beginning. So, um, some years ago that um

616
00:39:09.810 --> 00:39:12.409
it was possible to have this uh large language

617
00:39:12.409 --> 00:39:16.120
models uh personalized and to um To have them

618
00:39:16.120 --> 00:39:19.250
like uh to create chatbots like a deceased person.

619
00:39:19.479 --> 00:39:23.790
It's also possible now with um all the image

620
00:39:23.790 --> 00:39:26.199
material that we have to have an avatar that

621
00:39:26.199 --> 00:39:29.439
really looks like you and behaves like you and

622
00:39:29.439 --> 00:39:32.094
that um for example, I would Records uh for

623
00:39:32.094 --> 00:39:35.014
my, for my dad, um, I would record some

624
00:39:35.014 --> 00:39:38.135
questions, um give some answers to some questions, and

625
00:39:38.135 --> 00:39:41.935
then my family could ask um when I, when

626
00:39:41.935 --> 00:39:44.375
I'm dead, um, some questions to me that I

627
00:39:44.375 --> 00:39:48.399
would just, uh, reply to. It's a bit spooky,

628
00:39:49.330 --> 00:39:52.250
um, yeah, and, um, I think that the next

629
00:39:52.250 --> 00:39:56.360
step um to transfer this to robots isn't actually

630
00:39:56.360 --> 00:39:58.860
a big step then because so from an ethical

631
00:39:58.860 --> 00:40:02.530
perspective, yes, but from a technological perspective, it's already

632
00:40:02.530 --> 00:40:06.429
possible to have JGBT and this LLMs and to

633
00:40:06.429 --> 00:40:09.419
um to transfer this, uh, connect them with robots.

634
00:40:09.449 --> 00:40:12.330
So it's not difficult to, to connect those chatbots

635
00:40:12.330 --> 00:40:14.199
also to, to robots as well.

636
00:40:16.139 --> 00:40:18.389
When it comes, uh, because at a certain point

637
00:40:18.389 --> 00:40:23.580
you mentioned there, people question if, uh, I mean,

638
00:40:23.669 --> 00:40:26.979
as dealing with the death of loved ones in

639
00:40:26.979 --> 00:40:31.000
this particular way by not letting them just. I

640
00:40:31.000 --> 00:40:35.090
mean, die and for us to just normally mourn

641
00:40:35.090 --> 00:40:39.250
their death, but actually turning or sort of turning

642
00:40:39.250 --> 00:40:43.649
them into chatbots or robots. Uh, I, I mean,

643
00:40:43.729 --> 00:40:47.409
that, that's, that might be problematic, right? Because it

644
00:40:47.409 --> 00:40:52.770
might disrupt our normal process of mourning and being

645
00:40:52.770 --> 00:40:55.929
able to move on and to accept that the

646
00:40:55.929 --> 00:40:58.919
person is not there anymore and will not be,

647
00:40:59.129 --> 00:41:02.719
right. Mm. Of course, it can complicate our grieving

648
00:41:02.719 --> 00:41:05.479
process, but of course can also help our grieving

649
00:41:05.479 --> 00:41:10.530
process. For example, if um if your grandmother dies

650
00:41:10.530 --> 00:41:13.120
in a country very far away and you did

651
00:41:13.120 --> 00:41:15.919
not have the chance to say goodbye, then this

652
00:41:15.919 --> 00:41:19.120
kind of of attire maybe just for two weeks

653
00:41:19.120 --> 00:41:21.879
or a day or for one message could be

654
00:41:21.879 --> 00:41:25.979
a great opportunity to say goodbye. Also, um, the

655
00:41:25.979 --> 00:41:31.260
breathing process is, yeah, highly subjective psychological, um, Um,

656
00:41:31.310 --> 00:41:34.080
difficult, so it's a highly complex to understand it,

657
00:41:34.110 --> 00:41:36.689
but it's, um, it also depends on the individual

658
00:41:36.689 --> 00:41:41.229
and um sometimes it's easier to um to have

659
00:41:41.229 --> 00:41:44.350
some conversations with the chatbot or to open up

660
00:41:44.350 --> 00:41:48.189
or to just repeat again, again, um, the same

661
00:41:48.189 --> 00:41:51.550
things then um then to, to do this with

662
00:41:51.550 --> 00:41:52.590
the human being.

663
00:41:54.379 --> 00:41:57.590
Yeah. So uh would you like to tell people

664
00:41:57.590 --> 00:42:00.739
just before we go where they can find you

665
00:42:00.739 --> 00:42:02.669
and your work on the internet?

666
00:42:03.949 --> 00:42:08.590
Yes, so I have a personal homepage, Anna, um,

667
00:42:08.840 --> 00:42:13.719
yeah, Annaucio.com, and yeah, you can find on this

668
00:42:13.719 --> 00:42:16.080
homepage some material, but you can also find me

669
00:42:16.080 --> 00:42:19.760
on social media like Twitter, Instagram, LinkedIn, and my

670
00:42:19.760 --> 00:42:21.590
university homepage. So, yeah.

671
00:42:22.560 --> 00:42:25.290
So, thank you so much for doing this, doctor

672
00:42:25.290 --> 00:42:28.260
Puzzio. It was a fascinating conversation.

673
00:42:28.770 --> 00:42:30.729
Thank you, thank you. It was a pleasure to

674
00:42:30.729 --> 00:42:32.250
talk to you. Thank you.

675
00:42:33.500 --> 00:42:36.020
Hi guys, thank you for watching this interview until

676
00:42:36.020 --> 00:42:38.169
the end. If you liked it, please share it,

677
00:42:38.340 --> 00:42:41.129
leave a like and hit the subscription button. The

678
00:42:41.129 --> 00:42:43.330
show is brought to you by Nights Learning and

679
00:42:43.330 --> 00:42:47.409
Development done differently, check their website at Nights.com and

680
00:42:47.409 --> 00:42:51.129
also please consider supporting the show on Patreon or

681
00:42:51.129 --> 00:42:53.610
PayPal. I would also like to give a huge

682
00:42:53.610 --> 00:42:56.719
thank you to my main patrons and PayPal supporters

683
00:42:56.719 --> 00:43:00.939
Perergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyches Olaf,

684
00:43:01.050 --> 00:43:04.300
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

685
00:43:04.429 --> 00:43:07.719
Erika Lenny, John Connors, Philip Fors Connolly. Then the

686
00:43:07.719 --> 00:43:11.520
Mari Robert Windegaruyasi Zup Mark Nes called in Holbrookfield

687
00:43:11.520 --> 00:43:16.280
governor Michael Stormir, Samuel Andre Francis Forti Agnseroro and

688
00:43:16.280 --> 00:43:21.040
Hal Herzognun Macha Joan Labray and Samuel Corriere, Heinz,

689
00:43:21.120 --> 00:43:24.760
Mark Smith, Jore, Tom Hummel, Sardus France David Sloan

690
00:43:24.760 --> 00:43:29.550
Wilson, asilla dearauurumen Roach Diego London Correa. Yannick Punter

691
00:43:29.550 --> 00:43:34.679
Darusmani Charlotte blinikol Barbara Adamhn Pavlostaevsky nale back medicine,

692
00:43:34.750 --> 00:43:39.340
Gary Galman Sam of Zallirianeioltonin John Barboza, Julian Price,

693
00:43:39.629 --> 00:43:44.070
Edward Hall Edin Bronner, Douglas Fre Franca Bartolotti Gabrielon

694
00:43:44.070 --> 00:43:48.550
Scorteseus Slelitsky, Scott Zachary Fish Tim Duffyani Smith John

695
00:43:48.550 --> 00:43:53.479
Wieman. Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai

696
00:43:53.479 --> 00:43:58.659
Giorgio Theophanous, Chris Williamson, Peter Wozin, David Williams, Diocosta,

697
00:43:58.739 --> 00:44:02.979
Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez, Coralli

698
00:44:02.979 --> 00:44:07.340
Chevalier, bungalow atheists, Larry D. Lee Junior, old Erringbo.

699
00:44:08.100 --> 00:44:12.139
Sterry Michael Bailey, then Sperber, Robert Grassy, Zigoren, Jeff

700
00:44:12.139 --> 00:44:16.699
McMann, Jake Zu, Barnabas radix, Mark Campbell, Thomas Dovner,

701
00:44:16.820 --> 00:44:21.219
Luke Neeson, Chris Storry, Kimberly Johnson, Benjamin Gilbert, Jessica

702
00:44:21.219 --> 00:44:26.860
Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George Eoriatis,

703
00:44:26.939 --> 00:44:32.729
Valentin Steinman, Perrolis, Kate van Goller, Alexander Aubert, Liam

704
00:44:33.000 --> 00:44:38.580
Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner, Ursula

705
00:44:38.580 --> 00:44:43.340
Gudinov, Gregory Hastings, David Pinsoff, Sean Nelson, Mike Levin,

706
00:44:43.709 --> 00:44:46.919
and Jos Net. A special thanks to my producers.

707
00:44:47.030 --> 00:44:49.899
These are Webb, Jim, Frank Lucas Steffinik, Tom Venneden,

708
00:44:50.000 --> 00:44:54.520
Bernardin Curtis Dixon, Benedic Muller, Thomas Trumbull, Catherine and

709
00:44:54.520 --> 00:44:57.750
Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick

710
00:44:57.750 --> 00:45:01.280
Golden, and to my executive producers Matthew Levender, Sergio

711
00:45:01.280 --> 00:45:04.510
Quadrian, Bogdan Kanivets, and Rosie. Thank you for all.

