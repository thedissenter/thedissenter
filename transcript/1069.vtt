WEBVTT

1
00:00:00.219 --> 00:00:03.039
Hello, everyone. Welcome to a new episode of the

2
00:00:03.039 --> 00:00:05.809
Center. I'm your host, as always, Ricardo Lopez, and

3
00:00:05.809 --> 00:00:08.369
today I'm joined by Doctor Todd May. He's professor

4
00:00:08.369 --> 00:00:12.449
of philosophy at Warren Wilson College, and we're focusing

5
00:00:12.449 --> 00:00:15.890
on his book Should We Go Extinct? A Philosophical

6
00:00:15.890 --> 00:00:20.489
dilemma for our unbearable time. So, Todd, welcome to

7
00:00:20.489 --> 00:00:22.340
the show. It's a big pleasure to everyone.

8
00:00:22.610 --> 00:00:24.450
Uh, THANKS, Ricardo. It's an honor to be here.

9
00:00:25.879 --> 00:00:29.079
So should we go extinct? Could you explain a

10
00:00:29.079 --> 00:00:32.279
little bit better what is the precise question you

11
00:00:32.279 --> 00:00:34.099
try to answer in your book?

12
00:00:34.439 --> 00:00:36.400
Sure. uh, uh, LET me put it this way,

13
00:00:36.520 --> 00:00:39.290
Ricardo. It's not the question I'm trying to answer,

14
00:00:39.400 --> 00:00:42.000
it's the question I'm trying to address and deal

15
00:00:42.000 --> 00:00:45.159
with, right? So the, what I wanna do is

16
00:00:45.159 --> 00:00:48.200
I want to raise the question, start a conversation.

17
00:00:49.159 --> 00:00:52.319
Around the question itself, but also around the second

18
00:00:52.319 --> 00:00:53.799
question, the one that appears at the end of

19
00:00:53.799 --> 00:00:57.000
the book, which is what can we do to

20
00:00:57.000 --> 00:01:01.560
justify ourselves so that morally it would be better

21
00:01:01.560 --> 00:01:04.279
for us not to go extinct. So what the

22
00:01:04.279 --> 00:01:06.389
book is trying to do is grapple with that,

23
00:01:06.400 --> 00:01:08.559
that question. Ask, all right, what is it that

24
00:01:08.559 --> 00:01:12.000
we bring to the planet that other species can't

25
00:01:12.000 --> 00:01:13.400
bring or they at least can't bring it to

26
00:01:13.400 --> 00:01:15.959
the same extent. Uh, WHAT is it that we're

27
00:01:15.959 --> 00:01:19.680
doing to the planet, that's, that's harmful, that it

28
00:01:19.680 --> 00:01:21.680
would be better if we weren't here to do

29
00:01:21.680 --> 00:01:27.489
it. And then finally, given that balance or imbalance,

30
00:01:28.000 --> 00:01:30.550
how can we act in such ways as to

31
00:01:30.550 --> 00:01:33.120
justify our continuing to be here morally?

32
00:01:34.330 --> 00:01:37.800
Right. And in general terms, how would you frame

33
00:01:37.800 --> 00:01:41.080
the debate here between people who think that we

34
00:01:41.080 --> 00:01:44.760
should continue to exist and the ones who think

35
00:01:44.760 --> 00:01:47.559
that the world would be better off without this?

36
00:01:47.800 --> 00:01:51.919
Yeah, the, um, the idea, I think, is that

37
00:01:52.279 --> 00:01:56.680
there are those who, who think for two different

38
00:01:56.680 --> 00:01:58.559
sets of reasons, Ricardo and let me get into

39
00:01:58.559 --> 00:02:01.610
each of them. That we ought to go extinct.

40
00:02:01.730 --> 00:02:04.650
So one set, and this is probably the dominant

41
00:02:04.650 --> 00:02:06.690
set of people who think we should go extinct,

42
00:02:06.849 --> 00:02:10.250
think that human life is just unhappy, uh, that

43
00:02:10.250 --> 00:02:13.610
it's, it's bad for humans to be here, that

44
00:02:13.610 --> 00:02:17.759
our lives aren't good enough, worthwhile enough, meaningful enough

45
00:02:18.289 --> 00:02:22.490
for us to continue to propagate ourselves, right? The

46
00:02:22.490 --> 00:02:25.889
other side of that, um, are people who think

47
00:02:25.889 --> 00:02:27.729
because of the damage we do to the planet.

48
00:02:28.300 --> 00:02:32.149
That we ought not to continue to exist. It's

49
00:02:32.149 --> 00:02:35.110
that second side that I'm dealing with. I argue

50
00:02:35.110 --> 00:02:37.100
in the book a bit that it, that I,

51
00:02:37.229 --> 00:02:39.509
I'm not on board with the idea that for

52
00:02:39.509 --> 00:02:41.190
most of us, our human lives are not worth

53
00:02:41.190 --> 00:02:44.589
living. Uh, SO there's that's that side of it

54
00:02:44.589 --> 00:02:48.539
and the the reasons that The, the reasons that

55
00:02:48.539 --> 00:02:52.679
the second group gives are reasons that I canvas

56
00:02:52.679 --> 00:02:56.070
and add to in the book. The, the, uh,

57
00:02:56.080 --> 00:02:59.399
the cruelty of factory farming, uh, the contribution to

58
00:02:59.399 --> 00:03:04.229
the climate crisis, deforestation, scientific testing, things like that.

59
00:03:04.600 --> 00:03:06.919
Right. On the other side of it, there are

60
00:03:06.919 --> 00:03:09.320
those who think it would just be a terrible

61
00:03:09.320 --> 00:03:14.000
tragedy, uh, and one that, that can't be made

62
00:03:14.000 --> 00:03:17.809
up in in any way. If we were to

63
00:03:17.809 --> 00:03:21.600
go extinct. And I talk about some of those

64
00:03:21.600 --> 00:03:24.199
reasons in the book as well, uh, often using

65
00:03:24.199 --> 00:03:29.360
authors that aren't thinking about extinction, but whose arguments

66
00:03:29.360 --> 00:03:33.899
would be relevant to the issue of extinction. Mhm.

67
00:03:34.580 --> 00:03:37.580
Uh, HOW about the ones, the people who just

68
00:03:37.580 --> 00:03:40.789
don't care either way, whether we go extinct or

69
00:03:40.789 --> 00:03:43.860
not, for example, do the nihilists also play a

70
00:03:43.860 --> 00:03:45.529
role in this debate or not?

71
00:03:45.949 --> 00:03:50.020
Uh, um, I, it's a good question. Nobody's ever

72
00:03:50.020 --> 00:03:54.860
asked me that one. I, I, I suppose I

73
00:03:54.860 --> 00:03:57.000
could, I could go meddle with this and say,

74
00:03:57.110 --> 00:04:00.899
I don't care what the nihilists think. But, but,

75
00:04:01.029 --> 00:04:03.669
uh, I, I think I, I don't see a

76
00:04:03.669 --> 00:04:06.389
role for them in the sense that if you

77
00:04:06.389 --> 00:04:08.550
don't care one way or the other about human

78
00:04:08.550 --> 00:04:10.589
existence, the, the, the question isn't gonna grip, get

79
00:04:10.589 --> 00:04:13.820
a grip on you. Mhm.

80
00:04:14.320 --> 00:04:18.410
And when talking about extinction, what kinds of extinction

81
00:04:18.559 --> 00:04:21.600
are we considering here? I mean, how would we

82
00:04:21.600 --> 00:04:22.540
go extinct?

83
00:04:22.640 --> 00:04:25.239
Good. Yeah, this, uh, I, I talk about this

84
00:04:25.239 --> 00:04:27.720
a little bit in the book. So there are

85
00:04:27.720 --> 00:04:30.239
various ways in which you can imagine humans going

86
00:04:30.239 --> 00:04:34.130
extinct. One would be that the climate crisis comes

87
00:04:34.130 --> 00:04:38.119
and takes us all, right? Uh, NOW, having, I,

88
00:04:38.200 --> 00:04:40.720
I live in Asheville, North Carolina, which was devastated

89
00:04:40.720 --> 00:04:44.880
by a hurricane, uh. As you probably know, even

90
00:04:44.880 --> 00:04:47.640
as we speak, there are fires raging out of

91
00:04:47.640 --> 00:04:51.880
control in Los Angeles. So the idea that the

92
00:04:51.880 --> 00:04:54.519
climate crisis might be such that humans go extinct

93
00:04:54.519 --> 00:04:58.079
or nearly extinct, uh, is one that I think

94
00:04:58.079 --> 00:05:01.079
is on the table and that worries people. I

95
00:05:01.079 --> 00:05:04.049
don't think that the climate crisis itself would make

96
00:05:04.049 --> 00:05:06.480
people go extinct, but although it could radically reduce

97
00:05:06.480 --> 00:05:11.250
the population. Uh, THEN there is the specter of

98
00:05:11.250 --> 00:05:14.890
nuclear war, right? That's one that could actually prompt

99
00:05:14.890 --> 00:05:19.000
extinction one way or the other. Uh. Neither of

100
00:05:19.000 --> 00:05:23.579
those. ARE ones that I'm thinking about when I

101
00:05:23.579 --> 00:05:26.579
think about the issue of extinction. The, the image

102
00:05:26.579 --> 00:05:28.980
that I have, and I'll explain why. The image

103
00:05:28.980 --> 00:05:32.660
that I have is kind of what what some

104
00:05:32.660 --> 00:05:35.260
people call a children of men scenario, right? Based

105
00:05:35.260 --> 00:05:39.100
on the book and the film, uh, that considers

106
00:05:39.100 --> 00:05:41.140
what it would be like if all of a

107
00:05:41.140 --> 00:05:44.059
sudden we we, we became infertile. Uh, WE couldn't

108
00:05:44.059 --> 00:05:48.480
reproduce. Uh, THAT, that scenario to try to think

109
00:05:48.480 --> 00:05:51.970
this issue through. WOULD be a more likely one.

110
00:05:52.130 --> 00:05:55.489
The reason being that the climate crisis and nuclear

111
00:05:55.489 --> 00:05:59.279
war are going to devastate not simply us, but

112
00:05:59.489 --> 00:06:03.890
the fellow creatures with whom I'm about whom I'm

113
00:06:03.890 --> 00:06:07.760
thinking, right, in terms of human cruelty. But a

114
00:06:07.760 --> 00:06:10.160
children or men scenario doesn't do that, right? What

115
00:06:10.160 --> 00:06:13.049
it does is it it allows humans to go

116
00:06:13.049 --> 00:06:18.329
extinct uh without other animals necessarily going extinct. Of

117
00:06:18.329 --> 00:06:22.760
course, our extinction would have effects on other animals,

118
00:06:22.839 --> 00:06:24.839
but it won't necessarily make them go extinct.

119
00:06:25.579 --> 00:06:30.260
Mhm. But isn't it inevitable that somewhere in the

120
00:06:30.260 --> 00:06:35.760
future humanity will go extinct sooner or later, or

121
00:06:35.760 --> 00:06:37.109
one way or the other?

122
00:06:37.339 --> 00:06:40.769
Oh yeah, we're we're definitely gonna go extinct. Look,

123
00:06:41.019 --> 00:06:44.609
if nothing else, Ricardo, but the sun's gonna expand

124
00:06:44.609 --> 00:06:48.140
and then some however many years that it it

125
00:06:48.140 --> 00:06:50.609
does that, uh, it's gonna be too hot for

126
00:06:50.700 --> 00:06:53.619
for human life and probably most life on the

127
00:06:53.619 --> 00:06:56.679
planet. So yeah, we will go extinct. The question

128
00:06:56.679 --> 00:07:00.420
for me is, should we do that sooner rather

129
00:07:00.420 --> 00:07:03.339
than later? Is the harm that we're bringing to

130
00:07:03.339 --> 00:07:07.899
the planet, such that we should, that it would

131
00:07:07.899 --> 00:07:11.260
be morally incumbent upon us to go extinct. That's

132
00:07:11.260 --> 00:07:14.019
the question that I'm raising, right? Now, let me

133
00:07:14.019 --> 00:07:16.450
say a a little bit about that as well.

134
00:07:16.859 --> 00:07:19.399
It's a question I'm raising. Do I think that

135
00:07:19.399 --> 00:07:21.779
we're all gonna get together and think, hey, we're

136
00:07:21.779 --> 00:07:23.899
really not doing good stuff for the planet, we

137
00:07:23.899 --> 00:07:27.049
should go extinct. No. Right, it, it, it that's

138
00:07:27.049 --> 00:07:31.390
not. What's likely to happen. So resolves itself not

139
00:07:31.390 --> 00:07:33.549
into a question of what we are likely to

140
00:07:33.549 --> 00:07:37.660
do, but what we are morally obliged to do.

141
00:07:38.250 --> 00:07:42.160
Uh, AND if we're causing more harm collectively, right,

142
00:07:42.910 --> 00:07:46.910
then in fact justifies our existence, then what we

143
00:07:46.910 --> 00:07:50.630
ought to do is go extinct. But now let

144
00:07:50.630 --> 00:07:53.029
me take this one more step, record if that's

145
00:07:53.029 --> 00:07:59.850
OK. The The idea is this. If we were

146
00:07:59.850 --> 00:08:02.730
to look upon ourselves and think, it might not

147
00:08:02.730 --> 00:08:06.220
even be justified for us to be here. The

148
00:08:06.220 --> 00:08:10.089
next question would be not necessarily, well, should maybe

149
00:08:10.089 --> 00:08:12.920
we should go extinct then, but what can we

150
00:08:12.920 --> 00:08:16.690
do in order to justify our being here? In

151
00:08:16.690 --> 00:08:19.529
other words, we, we were faced with what I

152
00:08:19.529 --> 00:08:21.820
call at one point, not in the book, but

153
00:08:21.820 --> 00:08:25.049
elsewhere, the, the prospect of a moral hanging, right,

154
00:08:25.369 --> 00:08:28.200
right, you know, Samuel Johnson said that uh when

155
00:08:28.200 --> 00:08:30.410
a person's facing a hanging in two weeks, it

156
00:08:30.410 --> 00:08:34.429
focuses their mind admirably. But if we were to

157
00:08:35.390 --> 00:08:38.179
think to ourselves, maybe we shouldn't even be justified,

158
00:08:38.349 --> 00:08:41.729
maybe we're not even justified in being here. Then

159
00:08:41.869 --> 00:08:44.590
that should focus our mind on what we should

160
00:08:44.590 --> 00:08:49.099
do to be justified, to continue ourselves.

161
00:08:50.530 --> 00:08:53.799
Right. And looking at the current state of the

162
00:08:53.799 --> 00:08:56.440
world, what would you say are the aspects that

163
00:08:56.440 --> 00:09:00.580
are the most worth considering when evaluating such a,

164
00:09:00.640 --> 00:09:03.359
a question as to whether we should go extinct

165
00:09:03.359 --> 00:09:04.000
or not?

166
00:09:04.239 --> 00:09:09.260
Good. On, on both sides, Ricardo? Yes. OK. So,

167
00:09:09.549 --> 00:09:13.229
on the side for continuing. There are things that

168
00:09:13.229 --> 00:09:17.340
we bring to the planet that other, uh, other

169
00:09:17.890 --> 00:09:20.630
species, other animals cannot bring, or at least they

170
00:09:20.630 --> 00:09:22.830
can't bring it to the same extent, right? Uh,

171
00:09:22.869 --> 00:09:25.989
AND I talked about several things. One is the

172
00:09:25.989 --> 00:09:29.349
levels of happiness that humans can generate, right? And

173
00:09:29.349 --> 00:09:33.190
a happier planet is better than a less happy

174
00:09:33.190 --> 00:09:35.780
planet. So there are dimensions of happiness that we

175
00:09:35.780 --> 00:09:38.539
have. We, we have projects that we can extend

176
00:09:38.539 --> 00:09:42.369
into the future, recognizing certain relationships that will extend

177
00:09:42.369 --> 00:09:44.849
into the future. All of these can bring us

178
00:09:44.849 --> 00:09:49.539
kinds of happiness, dimensions of happiness that non-human animals

179
00:09:49.539 --> 00:09:52.059
aren't capable of nearly to the extent that we

180
00:09:52.059 --> 00:09:56.070
are. That's one thing, right? Another thing. Is that

181
00:09:56.070 --> 00:09:58.780
we bring to the planet the creation of art,

182
00:09:58.950 --> 00:10:01.669
right, and science, and not only the creation of

183
00:10:01.669 --> 00:10:04.669
art and science, but the appreciation of art and

184
00:10:04.669 --> 00:10:07.229
science, and other things like sports. And I think

185
00:10:07.229 --> 00:10:10.270
most of us think that if these things art,

186
00:10:10.400 --> 00:10:13.400
science, sports and whatnot, were to disappear from the

187
00:10:13.400 --> 00:10:16.690
planet. That that would be a loss for the

188
00:10:16.690 --> 00:10:18.969
planet, that the planet would in some ways, ways

189
00:10:18.969 --> 00:10:24.250
be impoverished if we, we, if, if the planet

190
00:10:24.250 --> 00:10:28.489
would be impoverished if that were no longer aspects

191
00:10:28.489 --> 00:10:31.130
of the planet, right? So there are those two

192
00:10:31.130 --> 00:10:34.130
things, right. A third element is one that's brought

193
00:10:34.130 --> 00:10:37.090
up by Samuel Scheffler, right, uh, in his book

194
00:10:37.090 --> 00:10:40.419
Deathin its Afterlives. Uh, HE says, look, one of

195
00:10:40.419 --> 00:10:47.559
the things that Humans That give humans meaningfulness in

196
00:10:47.559 --> 00:10:50.830
their lives, is the sense that whatever they're doing,

197
00:10:51.039 --> 00:10:54.640
there will be other human beings that will continue

198
00:10:54.640 --> 00:10:57.640
it, or they will appreciate it, or, or that

199
00:10:57.640 --> 00:11:00.989
will be able, but somehow to take it up,

200
00:11:01.780 --> 00:11:04.599
right? And if we were to go extinct, That

201
00:11:04.599 --> 00:11:06.960
wouldn't be there. Uh, SO a lot for Scheffler,

202
00:11:07.119 --> 00:11:09.840
a lot of what makes our lives meaningful is

203
00:11:09.840 --> 00:11:12.309
the thought that not just our kids will continue,

204
00:11:12.520 --> 00:11:14.669
but there there will be others that will continue

205
00:11:14.880 --> 00:11:18.479
to take up everything from our research to the

206
00:11:18.479 --> 00:11:21.960
guy who owns this a grocery store down the

207
00:11:21.960 --> 00:11:25.000
block, and other people will continue to shop there.

208
00:11:25.849 --> 00:11:28.609
So all of those things are on, we could

209
00:11:28.609 --> 00:11:32.380
say the plus side. On, on the minus side,

210
00:11:32.760 --> 00:11:34.750
there is, and the, the thing that I talk

211
00:11:34.750 --> 00:11:37.780
about the most in the book, factory farming. Uh,

212
00:11:37.989 --> 00:11:41.750
FACTORY farming creates endless cruelty for billions of animals,

213
00:11:41.869 --> 00:11:44.349
right? Uh, I, I don't have the, the, the

214
00:11:44.349 --> 00:11:46.510
book has the numbers. I don't have the numbers

215
00:11:46.510 --> 00:11:48.869
at my fingertips, but I'm, I'm gonna give myself

216
00:11:48.869 --> 00:11:51.119
a pass because I'm, I'm a philosopher, and you

217
00:11:51.119 --> 00:11:52.630
know how we are with facts. We're not so

218
00:11:52.630 --> 00:11:56.299
great with facts. So, uh, uh, but the book,

219
00:11:56.830 --> 00:12:00.640
uh, the book details, so for instance, A US

220
00:12:00.640 --> 00:12:04.239
citizen who lives, say, 70 some years, uh, is

221
00:12:04.239 --> 00:12:09.520
going to wind up eating hundreds, but most, most

222
00:12:09.520 --> 00:12:12.479
Americans are gonna wind up eating, you know, hundreds

223
00:12:12.479 --> 00:12:16.010
of animals that we're gonna spend their lives, torture

224
00:12:16.010 --> 00:12:21.909
chickens, cows, pig pigs, uh, and that's going to

225
00:12:21.909 --> 00:12:25.520
create all kinds of suffering, right? Simply so that

226
00:12:25.520 --> 00:12:28.640
we can have food that's cheap and tastes good.

227
00:12:29.409 --> 00:12:33.520
Uh, WE also engaged in deforestation in, in good

228
00:12:33.520 --> 00:12:36.960
part in order, right, to raise these animals, and

229
00:12:36.960 --> 00:12:41.719
that Destroys the lives of any number of our

230
00:12:41.719 --> 00:12:46.140
fellow creatures. Uh, WE, the climate crisis, which I've

231
00:12:46.140 --> 00:12:50.309
mentioned before, destroys the lives of fellow creatures. A

232
00:12:50.309 --> 00:12:54.950
lot of scientific testing is done in in ways

233
00:12:54.950 --> 00:12:58.859
that A create a lot of suffering, and B,

234
00:12:58.890 --> 00:13:01.960
just don't even need to be done. So, all

235
00:13:02.010 --> 00:13:06.059
of these elements are elements in which we contribute

236
00:13:06.489 --> 00:13:09.169
to the suffering of our fellow creatures, and I'll

237
00:13:09.169 --> 00:13:11.489
mention one other thing, uh, and this is more

238
00:13:11.489 --> 00:13:14.369
controversial, but it's sort of, I guess, personal for

239
00:13:14.369 --> 00:13:19.250
me, that we, we destroy ecosystems that have their

240
00:13:19.250 --> 00:13:22.890
own network set networks of lives and, and their

241
00:13:22.890 --> 00:13:26.030
own beauty. So I live in western North Carolina

242
00:13:26.030 --> 00:13:28.559
in the mouths. And to me, I wake up

243
00:13:28.559 --> 00:13:30.789
in the morning, I, I can see the mountains.

244
00:13:31.140 --> 00:13:33.640
If we were to sort of pave them over

245
00:13:33.640 --> 00:13:36.760
and and destroy them, right? That seems to me

246
00:13:36.760 --> 00:13:39.280
to be a loss, not simply in the creation

247
00:13:39.280 --> 00:13:42.359
of suffering for the animals that live there, but

248
00:13:42.359 --> 00:13:46.000
for the ecosystem itself, which to me has a

249
00:13:46.000 --> 00:13:49.599
sort of a a beauty and integrity that's worth

250
00:13:49.599 --> 00:13:54.859
preserving. And we're In our expansion, we're undermining a

251
00:13:54.859 --> 00:13:55.489
lot of that.

252
00:13:57.799 --> 00:14:01.969
So, uh, but does it matter if our extinction

253
00:14:01.969 --> 00:14:05.049
occurs voluntarily or involuntarily?

254
00:14:08.729 --> 00:14:14.469
It, it goes back, Ricardo, to Actually how we

255
00:14:14.469 --> 00:14:18.500
go extinct. So, if we were If we were

256
00:14:18.500 --> 00:14:21.859
to go extinct voluntarily, what that would mean is

257
00:14:21.859 --> 00:14:26.140
that we would be taking the decision. That we

258
00:14:26.140 --> 00:14:32.080
shouldn't continue the species, that that. What we're doing

259
00:14:32.309 --> 00:14:37.169
is creating enough harm that we should no longer

260
00:14:37.409 --> 00:14:39.929
propagate ourselves. That would be, that would leave our

261
00:14:39.929 --> 00:14:43.130
own agency, right? But in any way that we

262
00:14:43.130 --> 00:14:47.729
would go extinct involuntarily undermines our agency. So it

263
00:14:47.729 --> 00:14:51.640
undermines an aspect of our own moral character, right?

264
00:14:51.849 --> 00:14:54.859
Now, Regard having said that, we can, we can

265
00:14:54.859 --> 00:14:59.929
also divide into different types of different ways of

266
00:14:59.929 --> 00:15:02.010
going extinct. So if we go extinct in the

267
00:15:02.010 --> 00:15:06.010
children and men scenario, right, that's involuntarily and will

268
00:15:06.010 --> 00:15:11.400
cause suffering to us, uh, outside or beyond our

269
00:15:11.400 --> 00:15:13.760
ability to be able to do anything about it.

270
00:15:14.409 --> 00:15:17.010
If we were to go extinct, so for instance,

271
00:15:17.049 --> 00:15:20.919
through a nuclear holocaust, right, that would wind up

272
00:15:21.140 --> 00:15:25.539
not simply. Uh, PREVENTING us from propagating further species,

273
00:15:25.669 --> 00:15:27.750
but also ending the lives of people who are

274
00:15:27.750 --> 00:15:29.989
currently here. And one of the things in the

275
00:15:29.989 --> 00:15:31.429
book I try to do is say there's a

276
00:15:31.429 --> 00:15:35.830
real difference between going extinct in the sense of

277
00:15:35.830 --> 00:15:40.159
not, not creating further generations, because the not creating

278
00:15:40.159 --> 00:15:44.109
further generations doesn't stop anybody in particular from being

279
00:15:44.109 --> 00:15:47.909
here, right? Because there's nobody, like, there's nobody's waiting

280
00:15:47.909 --> 00:15:50.359
in the wings to get born. So if we

281
00:15:50.359 --> 00:15:53.479
go extinct in that sense by not propagating, right,

282
00:15:53.640 --> 00:15:56.989
then that doesn't bring harm to anybody in particular

283
00:15:56.989 --> 00:16:00.719
who would otherwise be here. But through something like

284
00:16:00.719 --> 00:16:04.229
uh uh a nuclear holocaust, that, what that does

285
00:16:04.229 --> 00:16:06.000
is end the lives of people who are here,

286
00:16:06.010 --> 00:16:07.200
and that's a whole different issue.

287
00:16:09.140 --> 00:16:12.979
Mhm. But, uh, among the, the arguments that you

288
00:16:12.979 --> 00:16:16.580
presented there in my previous question when I asked

289
00:16:16.580 --> 00:16:20.260
you about the, the arguments that people put forth

290
00:16:20.260 --> 00:16:23.739
when, when it comes to defending our continuing to

291
00:16:23.739 --> 00:16:27.460
exist, uh our existence, uh, what would you say

292
00:16:27.460 --> 00:16:30.979
are the best ones or the most compelling ones?

293
00:16:34.169 --> 00:16:37.539
I'll give you my personal view, OK, uh, um,

294
00:16:37.549 --> 00:16:40.789
because there are philosophers right away who are gonna

295
00:16:40.789 --> 00:16:45.729
disagree. I think the argument about the loss of

296
00:16:45.729 --> 00:16:49.849
art and the loss of uh of science and

297
00:16:49.849 --> 00:16:52.369
the loss of the ability to appreciate those, that

298
00:16:52.369 --> 00:16:55.609
to me is the most compelling one. Right. Now,

299
00:16:55.729 --> 00:16:57.469
no utilitarian is gonna agree with me on that.

300
00:16:57.690 --> 00:17:00.640
The utility utilitarians gonna say to me, it's happiness.

301
00:17:00.650 --> 00:17:04.800
We lose happiness, right? And that's the problem. Uh,

302
00:17:04.810 --> 00:17:08.968
AND I, I can see why they would say

303
00:17:08.968 --> 00:17:12.269
that from their perspective. Uh, BUT there seems to

304
00:17:12.269 --> 00:17:16.808
me, uh, in losing science, uh, and in losing

305
00:17:16.808 --> 00:17:19.888
art and sports and these various practices, it seems

306
00:17:19.888 --> 00:17:23.249
to me that we're not losing some quantity of

307
00:17:23.249 --> 00:17:27.368
happiness that other animals might possess, but to a

308
00:17:27.368 --> 00:17:33.099
lesser degree. We're losing whole dimensions. Of life, and

309
00:17:33.099 --> 00:17:36.209
that losing of dimensions of life seems to me

310
00:17:37.300 --> 00:17:43.339
more deeply tragic than just less happiness. Mhm.

311
00:17:44.290 --> 00:17:46.229
But, uh, I mean, when it comes to the

312
00:17:46.229 --> 00:17:49.709
values that humanity brings into the world, there is

313
00:17:49.709 --> 00:17:53.030
one particular kind of argument here that perhaps for

314
00:17:53.030 --> 00:17:55.829
me it's a little bit harder to understand. That

315
00:17:55.829 --> 00:18:00.989
is when people argue that the universe itself would

316
00:18:00.989 --> 00:18:05.469
lose value if humanity went extinct. I mean, could,

317
00:18:05.670 --> 00:18:08.949
could you explain what that means exactly and do

318
00:18:08.949 --> 00:18:10.910
you agree with that kind of argument?

319
00:18:11.199 --> 00:18:13.949
Yeah, uh, I, I talk about this somewhat in

320
00:18:13.949 --> 00:18:17.709
the book. Uh, IF you said the universe would

321
00:18:17.709 --> 00:18:21.670
lose value, it makes it sound as though it's

322
00:18:21.670 --> 00:18:24.349
going to be some loss that the universe is

323
00:18:24.349 --> 00:18:27.750
going to regret, right? But the universe doesn't regret

324
00:18:27.750 --> 00:18:30.020
because that's not the kinds of things universes do,

325
00:18:30.229 --> 00:18:35.030
right? So, so the question then becomes pressing in

326
00:18:35.030 --> 00:18:37.270
the way that you've asked it. Uh, AND the

327
00:18:37.270 --> 00:18:40.339
way I put it is this. If we were

328
00:18:40.339 --> 00:18:43.979
no longer here. That wouldn't be a loss for

329
00:18:43.979 --> 00:18:46.979
any any of the other animals that are here,

330
00:18:47.099 --> 00:18:51.819
right? Nobody would experience that loss. So the question

331
00:18:51.819 --> 00:18:55.229
is, in what sense is it a loss? And

332
00:18:55.400 --> 00:18:57.439
the only place from where it can be a

333
00:18:57.439 --> 00:19:01.280
loss is from our perspective, from our view of

334
00:19:01.280 --> 00:19:05.959
what, what creates richness, what creates richness on the

335
00:19:05.959 --> 00:19:09.839
planets, right, in the world. So, for us to

336
00:19:09.839 --> 00:19:14.839
look and say a planet without science and art

337
00:19:14.839 --> 00:19:19.430
and sports, etc. IS impoverished, is to take a

338
00:19:19.439 --> 00:19:23.719
a human perspective. And say, from the perspective that

339
00:19:23.719 --> 00:19:28.079
we occupy, that seems an impoverishment. Now, if we

340
00:19:28.079 --> 00:19:30.439
weren't here, there would be nobody to feel the

341
00:19:30.439 --> 00:19:34.219
impoverishment, but we are here. And as we are

342
00:19:34.219 --> 00:19:39.260
here, and the only uh As we are here

343
00:19:39.469 --> 00:19:45.630
and. Let me start that sentence again. OK. Uh,

344
00:19:46.030 --> 00:19:48.910
AS we are the only ones who can judge

345
00:19:48.910 --> 00:19:52.469
this from the perspective that we take, there isn't

346
00:19:52.469 --> 00:19:55.660
some other place called the universe that can make

347
00:19:55.660 --> 00:19:58.109
judgment on this, right? We can make judgment on

348
00:19:58.109 --> 00:20:00.670
this. And making judgment on this, we use our

349
00:20:00.670 --> 00:20:04.189
values. And from our values, it would be a

350
00:20:04.189 --> 00:20:07.189
loss for there not to be science and arts,

351
00:20:07.270 --> 00:20:10.689
right, sports, etc. Mhm.

352
00:20:11.010 --> 00:20:14.680
But, but I mean, objectively speaking, it wouldn't be

353
00:20:14.680 --> 00:20:17.280
a loss for the universe itself,

354
00:20:17.489 --> 00:20:22.630
right? Yeah, here's the word objective, Ricardo, and you

355
00:20:22.630 --> 00:20:25.310
know what happens to philosophers when the word objective

356
00:20:25.310 --> 00:20:31.229
comes around, right? Um. It would It would be

357
00:20:31.229 --> 00:20:35.319
objectively a loss in the sense that if we

358
00:20:35.319 --> 00:20:38.989
think of objectivities as what are the, where the

359
00:20:39.000 --> 00:20:43.010
the best reasons go, then it seems to me

360
00:20:43.010 --> 00:20:46.969
the best reasons go toward it would be a

361
00:20:46.969 --> 00:20:50.890
loss not to have these kinds of practices. But

362
00:20:51.270 --> 00:20:54.949
The best reasons are reasons that we give ourselves,

363
00:20:55.140 --> 00:20:57.959
right? But there's no other place for reasons to

364
00:20:57.959 --> 00:21:00.310
come up from the place that we give ourselves.

365
00:21:00.439 --> 00:21:03.880
So if we think of objectivity as outside of

366
00:21:03.880 --> 00:21:06.000
the humans, right? The point of view of the

367
00:21:06.000 --> 00:21:11.670
universe, right? I would say that there, that objectively,

368
00:21:11.959 --> 00:21:15.839
there is no particular loss. But on the other

369
00:21:15.839 --> 00:21:17.829
hand, it's not clear to me there is anything

370
00:21:17.829 --> 00:21:22.359
like objective, objectivity, right? If there's, if objectivity comes

371
00:21:22.359 --> 00:21:25.099
from the best reasons one can offer for a

372
00:21:25.099 --> 00:21:28.319
position, then, in fact, it would be a loss

373
00:21:28.319 --> 00:21:30.430
because those best reasons are reasons that we give

374
00:21:30.430 --> 00:21:31.319
ourselves and one another.

375
00:21:33.670 --> 00:21:36.589
Right. So there is also another kind of question

376
00:21:36.589 --> 00:21:39.510
important to address here that has not so much

377
00:21:39.510 --> 00:21:43.589
to do with the good that humanity supposedly brings

378
00:21:43.589 --> 00:21:47.310
to the world, but as to whether life itself

379
00:21:47.310 --> 00:21:50.270
is even worth living or if we take a

380
00:21:50.270 --> 00:21:53.949
step further, worth starting. So what do you think

381
00:21:53.949 --> 00:21:55.819
about antenatalism?

382
00:21:56.150 --> 00:22:02.839
Yes, uh, the If I take antenatalism, not as

383
00:22:02.839 --> 00:22:05.079
the view that humans should go extinct because of

384
00:22:05.079 --> 00:22:07.829
what we're doing to other creatures or to to

385
00:22:07.829 --> 00:22:13.709
ecosystems, but antenatalism as the view that it's bad

386
00:22:13.709 --> 00:22:15.920
to bring people into the world because their lives

387
00:22:15.920 --> 00:22:19.109
are not going to be worthwhile, right? Uh, AND

388
00:22:19.359 --> 00:22:24.089
the The I think the probably most well-known proponent

389
00:22:24.089 --> 00:22:27.010
of that was David Benatar, right? And with his

390
00:22:27.010 --> 00:22:30.780
book, Better Never to Have Been Born. Uh, AND

391
00:22:31.060 --> 00:22:35.180
his one of his arguments is that we tend

392
00:22:35.339 --> 00:22:40.219
to We tend to have a misplaced view of

393
00:22:40.219 --> 00:22:42.930
how good our existence is. We're Pollyannaish about it,

394
00:22:43.060 --> 00:22:47.780
right? Uh, WE, we justify our, the worthwhileness of

395
00:22:47.780 --> 00:22:50.540
our life to ourselves because it would be too

396
00:22:50.540 --> 00:22:53.660
painful not to. And if you add up With

397
00:22:53.660 --> 00:22:58.050
the pain and suffering. That humans uh experience and

398
00:22:58.050 --> 00:23:00.770
the goodness that they have, the ledger is gonna

399
00:23:00.770 --> 00:23:04.050
come out the wrong way. Uh, I'm not terribly

400
00:23:04.050 --> 00:23:06.609
comfortable with that argument, and I'm not terribly comfortable

401
00:23:06.609 --> 00:23:10.530
for two reasons, right? One is, If we were

402
00:23:10.530 --> 00:23:14.290
to judge. Or to the extent that we judge,

403
00:23:14.699 --> 00:23:17.819
that the pain and suffering is worth it, right?

404
00:23:18.020 --> 00:23:20.300
That seems to me a point to take seriously.

405
00:23:20.369 --> 00:23:22.540
I'm not sure that that's an illusion, right? That's

406
00:23:22.540 --> 00:23:26.400
an evaluation. And I, I would like to take

407
00:23:26.400 --> 00:23:29.260
that evaluation for what it is. Yes, I've lived

408
00:23:29.260 --> 00:23:31.459
this life. It's involved, a lot of pain and

409
00:23:31.459 --> 00:23:34.300
suffering, but what I have gone through is such

410
00:23:34.300 --> 00:23:36.780
that it makes it worth it. Uh, I, I

411
00:23:36.780 --> 00:23:40.859
don't see that as Pollyannaish necessarily, maybe for some

412
00:23:40.859 --> 00:23:43.420
people, but it, um, but I don't, but I

413
00:23:43.420 --> 00:23:46.420
think that it, it might well be a considered

414
00:23:46.420 --> 00:23:49.020
judgment and assessment that people have. That's on the

415
00:23:49.020 --> 00:23:52.589
one side, right? But on the other side. That

416
00:23:52.589 --> 00:23:54.699
discussion really has a lot to do with the

417
00:23:54.699 --> 00:23:57.819
happiness of human lives, right? And I've written elsewhere

418
00:23:57.819 --> 00:23:59.939
that there are ways for lives to be meaningful.

419
00:24:00.760 --> 00:24:04.310
Without necessarily being happy, but that there are various

420
00:24:04.560 --> 00:24:09.069
ways in which our lives can uh can unfold,

421
00:24:09.520 --> 00:24:14.400
uh, in accordance with, say, themes of adventurousness or

422
00:24:14.400 --> 00:24:19.239
intensity, or spontaneity, or curiosity, and that that can

423
00:24:19.239 --> 00:24:21.640
give a sense of meaningfulness to a life that

424
00:24:21.640 --> 00:24:24.359
isn't on the scale of am I happy or

425
00:24:24.359 --> 00:24:27.650
am I not happy, but is this a worthwhile

426
00:24:27.650 --> 00:24:30.979
life for me to be living? Uh, AND if

427
00:24:30.979 --> 00:24:34.380
you, if you pull those together, that, that I

428
00:24:34.380 --> 00:24:37.660
think we're better sources of evaluation of our lives

429
00:24:37.660 --> 00:24:40.280
than people like Benatar seem to think, and that

430
00:24:40.280 --> 00:24:43.140
there are other dimensions that can lend with wildness

431
00:24:43.140 --> 00:24:46.180
to a human life that don't resolve into things

432
00:24:46.180 --> 00:24:51.579
like happiness or objective list theories, etc. ETC. THEN

433
00:24:51.579 --> 00:24:55.969
I think it becomes harder to maintain an antenatalist

434
00:24:55.969 --> 00:24:57.920
position. Mhm.

435
00:24:58.439 --> 00:25:01.040
And when it comes to the harms that we

436
00:25:01.040 --> 00:25:04.160
bring to the world that you mentioned earlier, what

437
00:25:04.160 --> 00:25:07.660
would you say are for you in your perspective,

438
00:25:07.719 --> 00:25:10.109
the most compelling argument?

439
00:25:12.189 --> 00:25:16.520
I focus a lot on factory farming, uh, and

440
00:25:16.729 --> 00:25:20.180
I, I, I, I think that's one of the,

441
00:25:20.609 --> 00:25:26.300
one of the central arguments in part because It

442
00:25:26.300 --> 00:25:31.369
creates so much suffering. And in part because there

443
00:25:31.369 --> 00:25:34.260
are ways not to have to do it. Now,

444
00:25:34.270 --> 00:25:38.119
I want to be careful here, right? Because I

445
00:25:38.119 --> 00:25:40.640
don't want people thinking, well, Todd says if we

446
00:25:40.640 --> 00:25:44.400
just in factory farming tomorrow. Everything's going to be

447
00:25:44.400 --> 00:25:48.310
great. It won't be great. There's a reason that

448
00:25:48.479 --> 00:25:51.430
factory farming continues to exist, which is that it

449
00:25:51.890 --> 00:25:54.319
produces cheap meat. And there are a lot of

450
00:25:54.319 --> 00:25:59.040
people who have who have difficulty affording To, to

451
00:25:59.040 --> 00:26:03.380
be able to buy meat that's humanely raised. So,

452
00:26:03.680 --> 00:26:06.599
what happens is if you cut factory farming up

453
00:26:06.599 --> 00:26:10.599
tomorrow, uh, without any sort of support for people

454
00:26:10.599 --> 00:26:13.319
who are marginalized or impoverished, what you're going to

455
00:26:13.319 --> 00:26:16.959
get is hunger. Uh, AND so what really needs

456
00:26:16.959 --> 00:26:19.959
to be done, it seems to me, are the,

457
00:26:19.969 --> 00:26:23.189
uh, and a gradual elimination of factory farming on

458
00:26:23.189 --> 00:26:26.810
the one hand. With policies that support people who

459
00:26:26.810 --> 00:26:29.569
are marginalized and impoverished so that they can have

460
00:26:29.569 --> 00:26:32.959
an adequate diet. Now, having said that, let me

461
00:26:32.959 --> 00:26:35.849
just say that regardless of what you did with

462
00:26:35.849 --> 00:26:39.010
factory farming, that we ought to be supporting people

463
00:26:39.010 --> 00:26:43.010
so that everyone has access to an adequate and

464
00:26:43.010 --> 00:26:47.290
healthful diet. But if we are If we're going

465
00:26:47.290 --> 00:26:49.770
to intervene on factory farming, it seems to me

466
00:26:49.770 --> 00:26:53.410
that that's a necessary policy element. It's got to

467
00:26:53.410 --> 00:26:57.239
go alongside of it, uh, in order to make

468
00:26:57.239 --> 00:27:00.410
sure that the elimination of factory farming isn't something

469
00:27:00.410 --> 00:27:01.729
that just causes mass hunger.

470
00:27:02.869 --> 00:27:06.689
Mhm. But uh earlier you talked a lot about

471
00:27:06.689 --> 00:27:10.089
how we harm other animals, how we harm the

472
00:27:10.089 --> 00:27:13.969
climate and the planet more generally, but we, we

473
00:27:13.969 --> 00:27:17.760
should also consider here how we harm other fellow

474
00:27:17.760 --> 00:27:18.500
humans,

475
00:27:18.660 --> 00:27:22.319
right? Yes, uh, uh. We could get into a

476
00:27:22.319 --> 00:27:27.979
whole discussion of social media here, couldn't we? Um,

477
00:27:28.229 --> 00:27:31.300
I, I nodded to that in the book. Uh,

478
00:27:31.469 --> 00:27:34.349
WHEN I was talking, uh, with the editor, uh,

479
00:27:34.359 --> 00:27:39.260
at Crown Press, we're going over the book, uh,

480
00:27:39.469 --> 00:27:41.910
he, he raised this question, and I said, I,

481
00:27:41.989 --> 00:27:45.140
I don't want to, I, I want to focus

482
00:27:45.140 --> 00:27:46.430
too much on this question for a couple of

483
00:27:46.430 --> 00:27:50.760
reasons. One, it's its own issue. Uh, AND second,

484
00:27:51.020 --> 00:27:54.569
but if I were to argue that because of

485
00:27:54.569 --> 00:27:57.219
the harm that we do to our fellow human

486
00:27:57.219 --> 00:28:01.579
beings, we should go extinct. It's a very short

487
00:28:01.579 --> 00:28:05.910
step. To the position that in fact human lives

488
00:28:05.910 --> 00:28:09.380
are not worth living. Right. And, and I, I

489
00:28:09.380 --> 00:28:13.579
don't believe that. So, I don't want to argue

490
00:28:13.579 --> 00:28:18.780
that humans, that, that human and human cruelty and

491
00:28:18.780 --> 00:28:23.219
suffering is such that we should, uh, that it

492
00:28:23.219 --> 00:28:25.579
forms an argument for going extinct, right? I mean,

493
00:28:25.859 --> 00:28:28.040
there are plenty of arguments, good arguments about why

494
00:28:28.040 --> 00:28:30.829
we shouldn't do it. But I don't think it

495
00:28:30.829 --> 00:28:34.689
itself stands as an argument for going extinct. Mhm.

496
00:28:35.050 --> 00:28:38.369
And what do you make of ideas like long-termism?

497
00:28:39.619 --> 00:28:46.119
Um, NOT keen on it. Of the Well, first

498
00:28:46.119 --> 00:28:49.969
off, right, long-termism is a development of effective altruism,

499
00:28:50.410 --> 00:28:53.229
right, uh, which I'm also not keen on. Uh,

500
00:28:53.410 --> 00:28:56.209
THERE are a number of difficulties with effective altruism.

501
00:28:56.250 --> 00:28:58.890
Other people have discussed it better than I. Let

502
00:28:58.890 --> 00:29:02.250
me just mention briefly, uh, Larry, the philosopher Larry

503
00:29:02.250 --> 00:29:05.060
Tempkin's book, I think it's Being good in a

504
00:29:05.060 --> 00:29:09.209
World of Need. Uh, Temkin was himself, uh, a

505
00:29:09.209 --> 00:29:12.290
proponent of effective altruism, and he began to rethink

506
00:29:12.290 --> 00:29:15.800
it. Uh, AND as he rethought it, he realized

507
00:29:15.800 --> 00:29:18.890
there's all kinds of problems with it. Uh, JUST

508
00:29:18.890 --> 00:29:23.589
to mention a couple, right. Effective altruism involves giving

509
00:29:23.589 --> 00:29:27.910
to organizations that themselves will will give often to

510
00:29:27.910 --> 00:29:31.829
third world countries. Uh, THAT involves questions of how

511
00:29:31.829 --> 00:29:35.859
efficient the organization is also the question of how,

512
00:29:36.150 --> 00:29:38.150
how the giving goes, because in a lot of

513
00:29:38.150 --> 00:29:40.619
these countries that you, you would be contributing to,

514
00:29:40.790 --> 00:29:44.109
you have to contribute through the, we said the

515
00:29:44.109 --> 00:29:47.510
good graces, I suppose, of the government, which means

516
00:29:47.510 --> 00:29:50.469
it's gonna skim off some. Uh, AND it may

517
00:29:50.469 --> 00:29:53.270
not be supporting its population in the first place.

518
00:29:53.670 --> 00:29:56.520
Right. So, two things are happening. One is you

519
00:29:56.520 --> 00:29:59.780
may be supporting a regime that itself is problematic,

520
00:30:00.119 --> 00:30:03.310
and second, right, you may be supporting a regime

521
00:30:03.680 --> 00:30:06.280
that then feels as though it doesn't need to

522
00:30:06.280 --> 00:30:09.680
take responsibility for its own population, because someone else

523
00:30:09.680 --> 00:30:13.959
is doing. So, effective altruism has any number of

524
00:30:13.959 --> 00:30:18.750
problems. All that long term, long-termism does. Is it

525
00:30:18.750 --> 00:30:23.420
complicates that, right? Because what it says is that

526
00:30:23.430 --> 00:30:26.310
that the people who exist, who are going to

527
00:30:26.310 --> 00:30:30.890
exist in the future, are so numerous. That they

528
00:30:30.890 --> 00:30:34.540
swap the number of people who currently exist. And

529
00:30:34.540 --> 00:30:37.800
therefore, right, we should have much more concern for

530
00:30:37.800 --> 00:30:40.739
the people that we have, that are going to

531
00:30:40.739 --> 00:30:43.660
appear in the future and less concern for the

532
00:30:43.660 --> 00:30:50.150
people that exists now, right? And That's got several

533
00:30:50.150 --> 00:30:53.880
problems. One is the problem, right, that it's just

534
00:30:53.880 --> 00:30:56.109
human centered, right? It's not looking at our fellow

535
00:30:56.109 --> 00:30:59.270
creatures. Uh, SO it's got all of the issues

536
00:30:59.270 --> 00:31:01.750
that I bring up in the book, right. But

537
00:31:01.750 --> 00:31:05.430
second, right, uh, there are philosophers who have shown

538
00:31:05.430 --> 00:31:09.500
that that this can lead to some very counterintuitive

539
00:31:09.500 --> 00:31:14.109
results. So, for instance, if you've got a, a

540
00:31:14.109 --> 00:31:19.349
possibly disastrous Uh, event in the future, say an

541
00:31:19.349 --> 00:31:21.750
asteroid, uh, that could hit the planet, but it

542
00:31:21.750 --> 00:31:25.989
has a very, very small chance of happening. And

543
00:31:25.989 --> 00:31:30.069
yet, that small chance of happening, right, multiplied by

544
00:31:30.069 --> 00:31:33.069
the, the damage it would cause would mean you

545
00:31:33.069 --> 00:31:35.300
ought to put a lot of resources to that,

546
00:31:35.510 --> 00:31:41.670
rather than, right, putting resources toward say, ending impoverishment

547
00:31:41.670 --> 00:31:46.800
now or creating better. Uh, CREATING better policies, better

548
00:31:46.800 --> 00:31:50.719
foreign policy, things like that. So, what long-termism does,

549
00:31:50.729 --> 00:31:52.239
it seems to me, is it takes all of

550
00:31:52.239 --> 00:31:56.239
the problems that effective altruism has, and then just

551
00:31:56.239 --> 00:31:59.560
adds a few of its own. Mhm.

552
00:32:00.010 --> 00:32:03.650
So one last question then, what is your own

553
00:32:03.650 --> 00:32:06.079
conclusion? I mean, do you think we should go

554
00:32:06.079 --> 00:32:07.449
extinct or not?

555
00:32:08.199 --> 00:32:14.140
I don't know. Here, here's the thing, Ricardo. Um,

556
00:32:15.420 --> 00:32:20.119
My I wrote the book to put the question

557
00:32:20.119 --> 00:32:24.459
out, right, for it to further a conversation, right?

558
00:32:24.760 --> 00:32:29.479
Uh, IT, when I see it discussed, it seems

559
00:32:29.479 --> 00:32:31.239
to me that it doesn't get discussed in a

560
00:32:31.239 --> 00:32:34.160
nuanced way. People take one side or they take

561
00:32:34.160 --> 00:32:36.839
the other side, and they cling to it without

562
00:32:36.839 --> 00:32:40.869
ever really looking at nuances and trying to consider,

563
00:32:41.160 --> 00:32:45.140
but seriously, how we should think about this. And

564
00:32:45.140 --> 00:32:47.260
I see the book as a contribution to that.

565
00:32:48.189 --> 00:32:52.170
However, right, it seems to be an initial contribution,

566
00:32:52.550 --> 00:32:54.859
and I think there are others who can add

567
00:32:54.859 --> 00:32:58.589
to this, and, and frankly, Ricardo, others who are

568
00:32:58.589 --> 00:33:00.630
just brighter than I am, right, who are going

569
00:33:00.630 --> 00:33:02.270
to be able to think about this in ways

570
00:33:02.270 --> 00:33:05.069
that that are deeper than I can, right? And

571
00:33:05.069 --> 00:33:10.060
add to that conversation and bring in considerations that

572
00:33:10.310 --> 00:33:13.089
I would have brought in had I thought of

573
00:33:13.089 --> 00:33:16.319
them. Uh, AND so, I'm, what I'm hoping is

574
00:33:16.319 --> 00:33:20.780
that, is that this doesn't start a conversation, conversations

575
00:33:20.780 --> 00:33:23.910
are already happening, right, but that it adds something

576
00:33:23.910 --> 00:33:27.300
to the conversation and opens up ways of thinking

577
00:33:27.790 --> 00:33:32.310
that will allow us to, uh, uh, let me

578
00:33:32.310 --> 00:33:33.910
put this way, that will allow us to think

579
00:33:33.910 --> 00:33:36.229
about this more deeply and allow people who are,

580
00:33:36.459 --> 00:33:38.989
who are deeper thinkers than I am to be

581
00:33:38.989 --> 00:33:40.750
able to engage in it. That's on the one

582
00:33:40.750 --> 00:33:44.400
side. But on the other side, right, the point

583
00:33:44.400 --> 00:33:46.359
of the book was not simply to answer the

584
00:33:46.359 --> 00:33:49.479
question, which ultimately I give consideration so and don't

585
00:33:49.479 --> 00:33:52.680
give us a straight yes or no, but ultimately

586
00:33:52.680 --> 00:33:55.670
to motivate us to think about what we do,

587
00:33:55.839 --> 00:33:58.770
to think about how justified we are here, and

588
00:33:58.770 --> 00:34:00.560
to think about what we can do to make

589
00:34:00.560 --> 00:34:05.310
ourselves either more justified or merely more nearly justified.

590
00:34:05.599 --> 00:34:08.840
And so, The point of the book isn't simply

591
00:34:08.840 --> 00:34:11.918
to say, OK, here's the two sides and uh

592
00:34:11.918 --> 00:34:14.830
good luck and Godspeed, right? Right. The point of

593
00:34:14.830 --> 00:34:17.679
the book is to raise that, to put it

594
00:34:17.679 --> 00:34:21.840
on the table, further that conversation, but also to

595
00:34:21.840 --> 00:34:25.800
point out that this, the mere fact that this

596
00:34:25.800 --> 00:34:28.399
is a live issue, means we really should be

597
00:34:28.399 --> 00:34:31.000
thinking about what we're on about and what we

598
00:34:31.000 --> 00:34:33.918
can do to make our existence more justifiable.

599
00:34:35.580 --> 00:34:37.860
Great. So the book is again, Should we go

600
00:34:37.860 --> 00:34:42.159
extinct, a philosophical dilemma for our unbearable times. I'm

601
00:34:42.159 --> 00:34:44.159
leaving a link to it in the description of

602
00:34:44.159 --> 00:34:47.350
the interview. Uh, AND Todd, just before we go,

603
00:34:47.478 --> 00:34:49.478
would you like to tell people where they can

604
00:34:49.478 --> 00:34:51.909
find you and your work on the internet?

605
00:34:52.759 --> 00:34:56.539
Ah, so I do have a website. Uh, SO

606
00:34:58.319 --> 00:35:01.638
as a baby boomer, Ricardo, right? I, uh, it's,

607
00:35:01.878 --> 00:35:03.599
it's all I can do to get on the

608
00:35:03.599 --> 00:35:11.020
computer. But uh the, the website is ToddmeyPhilosopher.com, right?

609
00:35:11.260 --> 00:35:13.340
Uh, AND I can be reached through there. I

610
00:35:13.340 --> 00:35:16.419
also teach at Warren Wilson College and uh I

611
00:35:16.419 --> 00:35:20.379
have, I have email there. Uh, SO, yeah, there

612
00:35:20.379 --> 00:35:24.879
are, there are ways to reach me, uh, and,

613
00:35:24.889 --> 00:35:30.260
uh, to express your approval, disapproval questions or uh

614
00:35:30.260 --> 00:35:32.379
people who can help me think more deeply about

615
00:35:32.379 --> 00:35:33.899
this than I have already thought.

616
00:35:34.959 --> 00:35:37.439
Great. So thank you so much for the great

617
00:35:37.439 --> 00:35:39.770
talk. It's been a great pleasure to have you

618
00:35:39.770 --> 00:35:40.479
on the show.

619
00:35:40.770 --> 00:35:42.610
Thank you, Ricardo. It's been a pleasure and an

620
00:35:42.610 --> 00:35:43.439
honor to be here.

621
00:35:44.830 --> 00:35:47.350
Hi guys, thank you for watching this interview until

622
00:35:47.350 --> 00:35:49.489
the end. If you liked it, please share it,

623
00:35:49.669 --> 00:35:52.459
leave a like and hit the subscription button. The

624
00:35:52.459 --> 00:35:54.659
show is brought to you by Nights Learning and

625
00:35:54.659 --> 00:35:58.739
Development done differently, check their website at Nights.com and

626
00:35:58.739 --> 00:36:02.459
also please consider supporting the show on Patreon or

627
00:36:02.459 --> 00:36:04.939
PayPal. I would also like to give a huge

628
00:36:04.939 --> 00:36:08.370
thank you to my main patrons and PayPal supporters

629
00:36:08.370 --> 00:36:12.300
Pergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyche Olaf,

630
00:36:12.379 --> 00:36:15.629
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

631
00:36:15.760 --> 00:36:19.050
Erika Lenny, John Connors, Philip Fors Connolly. Then the

632
00:36:19.050 --> 00:36:23.429
Matri Robert Windegaruyasi Zu Mark Nes called Holbrookfield governor

633
00:36:23.840 --> 00:36:27.850
Michael Stormir Samuel Andrea, Francis Forti Agnunseroro and Hal

634
00:36:27.850 --> 00:36:32.409
Herzognun Macha Joan Lays and the Samuel Corriere, Heinz,

635
00:36:32.449 --> 00:36:36.090
Mark Smith, Jore, Tom Hummel, Sardus Fran David Sloan

636
00:36:36.090 --> 00:36:40.399
Wilson, Asila dearraujoro and Roach Diego Londono Correa. Yannick

637
00:36:40.399 --> 00:36:46.399
Punteran Rosmani Charlotte blinikol Barbara Adamhn Pavlostaevskynaleb medicine, Gary

638
00:36:46.399 --> 00:36:51.360
Galman Samov Zaledrianei Poltonin John Barboza, Julian Price, Edward

639
00:36:51.360 --> 00:36:55.879
Hall Edin Bronner, Douglas Fry, Franca Bartolotti Gabrielon Scorte

640
00:36:55.879 --> 00:36:59.879
or Slelisky, Scott Zachary Fish Tim Duffyani Smith John

641
00:36:59.879 --> 00:37:04.820
Wieman. Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai

642
00:37:04.820 --> 00:37:09.320
Giorgio Theophanous, Chris Williamson, Peter Vozin, David Williams, the

643
00:37:09.320 --> 00:37:13.870
Augusta, Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez,

644
00:37:13.899 --> 00:37:18.120
Coralli Chevalier, bungalow atheists, Larry D. Lee Junior, Old

645
00:37:18.120 --> 00:37:23.030
Heringbo. Sterry Michael Bailey, then Sperber, Robert Grassy Zigoren,

646
00:37:23.189 --> 00:37:27.629
Jeff McMahon, Jake Zu, Barnabas radix, Mark Campbell, Thomas

647
00:37:27.629 --> 00:37:31.959
Dovner, Luke Neeson, Chris Stor, Kimberly Johnson, Benjamin Galbert,

648
00:37:32.110 --> 00:37:37.459
Jessica Nowicki, Linda Brendon, Nicholas Carlsson, Ismael Bensleyman. George

649
00:37:37.459 --> 00:37:42.689
Eoriatis, Valentin Steinman, Perkrolis, Kate van Goller, Alexander Aubert,

650
00:37:43.510 --> 00:37:49.350
Liam Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner,

651
00:37:49.469 --> 00:37:54.229
Ursula Gudinov, Gregory Hastings, David Pinsoff Sean Nelson, Mike

652
00:37:54.229 --> 00:37:57.889
Levine, and Jos Net. A special thanks to my

653
00:37:57.889 --> 00:38:00.729
producers. These are Webb, Jim, Frank Lucas Steffinik, Tom

654
00:38:00.729 --> 00:38:05.610
Venneden, Bernard Curtis Dixon, Benedict Muller, Thomas Trumbull, Catherine

655
00:38:05.610 --> 00:38:08.889
and Patrick Tobin, John Carlo Montenegroal Ni Cortiz and

656
00:38:08.889 --> 00:38:12.250
Nick Golden, and to my executive producers Matthew Levender,

657
00:38:12.340 --> 00:38:15.520
Sergio Quadrian, Bogdan Kanivets, and Rosie. Thank you for

658
00:38:15.520 --> 00:38:15.840
all.

