WEBVTT

1
00:00:00.259 --> 00:00:03.140
Hello, everyone. Welcome to a new episode of the

2
00:00:03.140 --> 00:00:05.969
Center. I'm your host, as always, Ricardo Lopes and

3
00:00:05.969 --> 00:00:09.010
today I'm joined by Doctor Jonathan Ichikawa. She, he

4
00:00:09.010 --> 00:00:11.590
is a professor of philosophy and department head at

5
00:00:11.590 --> 00:00:14.970
the University of British Columbia. And today we're talking

6
00:00:14.970 --> 00:00:20.129
about his book, Epistemic Courage. So, Doctor Ichikawa, welcome

7
00:00:20.129 --> 00:00:21.950
to the show. It's a pleasure to everyone.

8
00:00:22.290 --> 00:00:24.010
Uh, THANKS so much. Happy to be here. Looking

9
00:00:24.010 --> 00:00:24.540
forward to talking.

10
00:00:25.770 --> 00:00:29.290
So, epistemic courage, this is, I think, a topic

11
00:00:29.290 --> 00:00:34.169
at the intersection between ethics and epistemology. So, let's

12
00:00:34.169 --> 00:00:37.409
perhaps introduce the topic a little bit. First of

13
00:00:37.409 --> 00:00:40.240
all, what is the ethics of belief?

14
00:00:41.009 --> 00:00:43.830
Good, thanks. That's a good place to start. So

15
00:00:43.830 --> 00:00:47.560
in general, I think um the ethics of anything

16
00:00:47.689 --> 00:00:52.759
is the Um, study of normative questions about that

17
00:00:52.759 --> 00:00:56.720
thing. So if you're working in the ethics of

18
00:00:56.720 --> 00:00:59.599
war, you're interested in when is war justified, what's

19
00:00:59.599 --> 00:01:02.119
the moral importance of various kinds of actions that

20
00:01:02.119 --> 00:01:05.309
you might, um, that, that occur in war. Um,

21
00:01:05.480 --> 00:01:08.080
IF you are looking at the ethics of sex,

22
00:01:08.120 --> 00:01:10.480
you're thinking about when is sex OK, when is

23
00:01:10.480 --> 00:01:13.690
it sexual assaults, um, and what are the interesting

24
00:01:13.690 --> 00:01:16.949
normative things to, to say about, um, Um, those

25
00:01:16.949 --> 00:01:19.790
distinctions. So the ethics of belief has to do

26
00:01:19.790 --> 00:01:24.779
with um normative questions around belief, uh, paradigmatically, um,

27
00:01:24.910 --> 00:01:26.910
when should you believe something, when should you not

28
00:01:26.910 --> 00:01:33.959
believe something. Um, IS, um, having unreasonable beliefs harmful?

29
00:01:34.089 --> 00:01:36.690
What motivation, what, what, uh, um, why is it

30
00:01:36.690 --> 00:01:39.790
important to do a good job believing, um, substantive

31
00:01:39.790 --> 00:01:42.930
questions about, um, under what circumstances does the evidence

32
00:01:42.930 --> 00:01:44.410
tell you to believe or not to, and what

33
00:01:44.410 --> 00:01:48.169
are the theoretical constraints about that. Um, ONE of

34
00:01:48.169 --> 00:01:51.290
the, um, big open questions in the ethics of

35
00:01:51.290 --> 00:01:56.239
belief. Concerns whether um epistemology, which is, you know,

36
00:01:56.279 --> 00:01:59.550
the branch of philosophy that that um particularly focuses

37
00:01:59.550 --> 00:02:02.400
on normative questions about belief, is using its own

38
00:02:02.400 --> 00:02:06.639
like distinctive um form of normativity. So when we

39
00:02:06.639 --> 00:02:08.399
say, you know, you shouldn't believe that because there's

40
00:02:08.399 --> 00:02:11.479
not enough evidence, is that a moral norm or

41
00:02:11.479 --> 00:02:14.279
a different kind of norm or a distinctively epistemic

42
00:02:14.279 --> 00:02:17.080
norm, those are, um, among the, the central questions

43
00:02:17.080 --> 00:02:18.330
in the ethics of belief.

44
00:02:19.169 --> 00:02:22.419
And I mean, in a more general way, in

45
00:02:22.419 --> 00:02:26.619
what ways do ethics and epistemology relate to one

46
00:02:26.619 --> 00:02:27.169
another?

47
00:02:27.580 --> 00:02:29.059
Yeah, this is a great question. This is a

48
00:02:29.059 --> 00:02:32.539
really big question, um, and I, I, I don't

49
00:02:32.539 --> 00:02:35.949
think, um, the answer is at all settled or

50
00:02:35.949 --> 00:02:39.190
even that there's a very strong consensus in the

51
00:02:39.190 --> 00:02:41.220
literature, but I have my own take, which I'll

52
00:02:41.220 --> 00:02:45.380
tell you, um. I think that ethics and epistemology

53
00:02:45.710 --> 00:02:49.380
are importantly different from one another, um, but together,

54
00:02:49.630 --> 00:02:55.110
um, they're quite fundamental normative domains toward, um, human

55
00:02:55.110 --> 00:02:57.270
experience and indeed, you know, um, any kinds of

56
00:02:57.270 --> 00:03:01.750
actors. Um, I think that, uh, any agent who

57
00:03:01.750 --> 00:03:07.309
has interests, um, um, goes around the world facing

58
00:03:07.630 --> 00:03:11.830
complicated decisions about what to do. And uh the

59
00:03:12.169 --> 00:03:15.460
main um strategy that at least agents like us

60
00:03:15.460 --> 00:03:18.389
have to in um doing a good job answering

61
00:03:18.389 --> 00:03:23.710
those questions, um uh involves kind of um simplifying

62
00:03:23.710 --> 00:03:26.029
the complicated question of how to behave in the

63
00:03:26.029 --> 00:03:29.699
world into um uh uh kind of a function

64
00:03:29.699 --> 00:03:32.679
of two, still a complicated but slightly simpler sets

65
00:03:32.679 --> 00:03:34.990
of questions, um, and I think of them as

66
00:03:34.990 --> 00:03:38.100
ethical questions about what sorts of goals are appropriate,

67
00:03:38.389 --> 00:03:42.070
um. And epistemic questions about how to represent what's

68
00:03:42.070 --> 00:03:43.389
going on in the world. If you want to

69
00:03:43.389 --> 00:03:45.509
make good decisions, you basically need to do two

70
00:03:45.509 --> 00:03:47.190
things. You need to, um, you need to have

71
00:03:47.190 --> 00:03:49.350
the right aims and ethics is kind of about

72
00:03:49.350 --> 00:03:52.179
setting the right aims, and he needs to, um,

73
00:03:52.190 --> 00:03:54.669
have good beliefs. You need to do a good

74
00:03:54.669 --> 00:03:57.389
job recognizing what situation you're in so that you

75
00:03:57.389 --> 00:04:00.550
can make strategic decisions to pursue those aims, and

76
00:04:00.550 --> 00:04:03.479
that's what epistemology is about. Some people do think

77
00:04:03.770 --> 00:04:06.759
that um it's much more muddled together than that,

78
00:04:07.009 --> 00:04:09.279
um, to an extreme, some people think that uh

79
00:04:09.529 --> 00:04:12.449
um there that, you know, epistemic norms just are

80
00:04:12.449 --> 00:04:16.010
ethical norms, um, but I, I do think that

81
00:04:16.010 --> 00:04:21.119
they're important theoretical reasons to understand them as, um,

82
00:04:21.130 --> 00:04:23.730
doing importantly different things. That's not to say that

83
00:04:23.730 --> 00:04:27.174
there aren't, um, many morally significant. Get questions with

84
00:04:27.174 --> 00:04:30.015
epistemology and indeed my book I think is um

85
00:04:30.015 --> 00:04:33.744
is really foregrounding, uh, many of them. Um, SO

86
00:04:33.744 --> 00:04:36.744
it's, it's morally important to do a good job

87
00:04:36.744 --> 00:04:39.105
forming beliefs, um, but I do think that there

88
00:04:39.105 --> 00:04:42.825
is a, um, distinctive epistemic kind of normativity, and

89
00:04:42.825 --> 00:04:46.065
it's possible to study kind of, um, uh, the

90
00:04:46.065 --> 00:04:48.945
question of what to believe in a way that's

91
00:04:48.945 --> 00:04:51.760
separate from the question of what to value. Mhm.

92
00:04:52.709 --> 00:04:56.730
So, when talking about courage, of course, we're talking

93
00:04:56.730 --> 00:05:01.269
about virtue and in ethics, there are different kinds

94
00:05:01.269 --> 00:05:04.630
of ethical theories. In this case, we're talking about

95
00:05:04.630 --> 00:05:09.869
virtue ethics specifically. So what is virtue epistemology?

96
00:05:10.890 --> 00:05:15.329
Good question. One can use that label for um

97
00:05:15.329 --> 00:05:19.809
uh more or less distinctive ideas. Um, uh, IF

98
00:05:19.809 --> 00:05:21.559
when use it in a kind of stronger way,

99
00:05:21.850 --> 00:05:24.489
um, kind of parallel to the way that people

100
00:05:24.489 --> 00:05:26.519
often talk about like virtue ethics when you're doing

101
00:05:26.519 --> 00:05:28.959
like an intro ethics course, where you know, there's,

102
00:05:29.410 --> 00:05:31.609
there's the geontological theories and there's the virtue ethics

103
00:05:31.609 --> 00:05:33.290
theories and are like doing quite different kinds of

104
00:05:33.290 --> 00:05:36.190
things. One can do the same thing in epistemology.

105
00:05:36.450 --> 00:05:41.049
Um, um, I'm not making a claim that strong.

106
00:05:41.390 --> 00:05:45.510
When I'm, when I'm exploring, um, the kind of

107
00:05:45.510 --> 00:05:48.230
virtual epistemology that I'm exploring in my book, that's

108
00:05:48.230 --> 00:05:52.149
not because I think like, um, there are no

109
00:05:52.149 --> 00:05:56.269
general norms at the the ontological level, um, that

110
00:05:56.269 --> 00:05:59.739
govern when belief is permissible. Um, I'm not taking

111
00:05:59.739 --> 00:06:03.820
a stance on questions about like epistemic consequentialism. Um,

112
00:06:04.070 --> 00:06:08.179
WHAT I'm doing is I'm emphasizing that there are,

113
00:06:08.429 --> 00:06:16.119
um, important. TRAITS that can be manifested by thinkers,

114
00:06:16.369 --> 00:06:20.290
um, and that it's useful, um, at least some

115
00:06:20.290 --> 00:06:22.290
of the time to be doing some of our

116
00:06:22.290 --> 00:06:26.329
epistemological theorizing, um, at that level. Um, AND I'm,

117
00:06:26.369 --> 00:06:28.089
I'm quite pluralist about this, so I'm not making

118
00:06:28.089 --> 00:06:30.864
any kind of Priority claim. It's not like, um,

119
00:06:30.875 --> 00:06:32.934
uh, you know, the only way you can explain

120
00:06:32.934 --> 00:06:34.584
a good belief is by like what a good

121
00:06:34.584 --> 00:06:36.024
believer would believe. I'm not saying something like that.

122
00:06:36.065 --> 00:06:39.475
Some people, some, some virtu, some virtue epistemologists, um,

123
00:06:39.505 --> 00:06:41.704
do say things like that. Um, WHAT I'm saying

124
00:06:41.704 --> 00:06:44.464
is, is, is more modest in this sense, um.

125
00:06:44.820 --> 00:06:48.420
I'm saying there's certain kinds of um um habits,

126
00:06:48.760 --> 00:06:54.079
character traits, um, belief dispositions um that we um

127
00:06:54.079 --> 00:06:58.079
uh manifest when we're going around the world, and

128
00:06:58.079 --> 00:07:01.640
we can gain some insight into focusing on those

129
00:07:01.640 --> 00:07:06.489
traits of thinkers. An epistemic courage is um Is

130
00:07:06.489 --> 00:07:09.250
the main virtue that I'm um that I'm exploring

131
00:07:09.250 --> 00:07:11.970
in my book and that's really um this trait

132
00:07:11.970 --> 00:07:15.209
of kind of uh not being too skeptical, not

133
00:07:15.209 --> 00:07:18.250
being too reckless or too gullible, um having this

134
00:07:18.250 --> 00:07:23.399
kind of good balance um of um of being

135
00:07:23.399 --> 00:07:27.609
um open to acceptance the right amount and I

136
00:07:27.609 --> 00:07:32.239
think that um. That quite often, um, the way

137
00:07:32.239 --> 00:07:36.420
epistemologists but also, uh, you know, the, um, the

138
00:07:36.420 --> 00:07:39.820
culture at large thinks about epistemology, um, tends to

139
00:07:39.820 --> 00:07:43.619
have, uh, the wrong balance there and, um, employing

140
00:07:43.619 --> 00:07:46.579
uh this, this virtue of courage which I think

141
00:07:46.579 --> 00:07:48.660
in like Aristotelian senses it's kind of a mean

142
00:07:48.660 --> 00:07:50.779
between two vices, um, I think that can be

143
00:07:50.779 --> 00:07:52.540
a helpful corrective to some of the mistakes that

144
00:07:52.540 --> 00:07:54.579
I'm seeing in the culture and the literature.

145
00:07:55.290 --> 00:07:58.220
Mhm. So one of the main questions that you

146
00:07:58.220 --> 00:08:01.390
address in the book is, what is bad belief?

147
00:08:01.600 --> 00:08:05.519
Uh, SO what is actually bad belief and is

148
00:08:05.519 --> 00:08:10.119
this an epistemology question, an ethics question, or

149
00:08:10.119 --> 00:08:20.559
both? Good. Um, I think that, um. Fundamentally, There's

150
00:08:20.559 --> 00:08:23.880
a level of epistemic evaluation, uh sorry, a level

151
00:08:23.880 --> 00:08:26.760
of doxtastic evaluation. So, you know, our beliefs, good

152
00:08:26.760 --> 00:08:30.200
or good beliefs or bad beliefs, um, that is,

153
00:08:30.480 --> 00:08:35.308
um, essentially epistemological, uh, not, not, uh, not ethical.

154
00:08:35.558 --> 00:08:38.030
So, um, and that's mostly what I'm focusing on

155
00:08:38.030 --> 00:08:40.400
in, in my book. So when I talk about

156
00:08:40.400 --> 00:08:43.957
bad beliefs. Um, I, I don't mean, you know,

157
00:08:44.119 --> 00:08:46.109
beliefs that it's morally wrong to have. I mean

158
00:08:46.398 --> 00:08:52.258
beliefs that are, um, inconsistent with epistemic norms. And,

159
00:08:52.318 --> 00:08:54.958
um, uh, and in a way, a focus on

160
00:08:54.958 --> 00:08:58.999
bad beliefs is very characteristic of, um, lots of

161
00:08:58.999 --> 00:09:03.278
mainstream epistemology as well as um mainstream kind of

162
00:09:03.278 --> 00:09:06.828
um uh informal thought about belief in epistemology. So,

163
00:09:06.958 --> 00:09:10.200
I mean just think about. Um, uh, THE way

164
00:09:10.200 --> 00:09:16.190
that, um, the way people Emphasize um the importance

165
00:09:16.190 --> 00:09:19.169
of having enough evidence before you believe. So, um,

166
00:09:19.960 --> 00:09:24.909
if you, if, if you um introduce epistemology, um,

167
00:09:24.919 --> 00:09:28.719
in epistemology course, um, or, you know, um, talking

168
00:09:28.719 --> 00:09:31.200
to a fellow citizen about, you know, what they're

169
00:09:31.200 --> 00:09:33.080
noticing going on in the world, um, what a

170
00:09:33.080 --> 00:09:34.520
lot of people notice is, oh, there's so many

171
00:09:34.520 --> 00:09:36.280
people who just kind of believe whatever they're told

172
00:09:36.280 --> 00:09:38.320
or believe whatever they want to believe even though

173
00:09:38.320 --> 00:09:42.270
there's not good evidence for them. Um, AND, um,

174
00:09:42.280 --> 00:09:45.500
uh, you know, part of epistemology is about trying

175
00:09:45.500 --> 00:09:47.419
to correct that. And so you get, um, you

176
00:09:47.419 --> 00:09:52.059
know, these negative epistemic norms that say make sure

177
00:09:52.059 --> 00:09:54.020
that you get enough evidence before you believe, make

178
00:09:54.020 --> 00:09:56.380
sure you don't have unjustified beliefs, uh, make sure

179
00:09:56.380 --> 00:10:00.950
you're only trusting reliable sources, um. And that's all

180
00:10:00.950 --> 00:10:03.700
well and good. I don't object to that, um,

181
00:10:03.710 --> 00:10:08.020
but I, I do object in my book to

182
00:10:08.349 --> 00:10:11.320
what I see as the undue emphasis on those

183
00:10:11.320 --> 00:10:14.750
questions at the expense of um sort of the

184
00:10:14.750 --> 00:10:18.409
converse set of questions. ABOUT making the opposite mistake.

185
00:10:18.700 --> 00:10:20.229
So the mistake everyone talks about is the mistake

186
00:10:20.229 --> 00:10:22.179
of believing things they shouldn't of, of having bad

187
00:10:22.179 --> 00:10:25.309
beliefs, um, and you know, that's usually based on

188
00:10:25.309 --> 00:10:27.400
evidential terms so the people often emphasize the moral

189
00:10:27.400 --> 00:10:29.210
harms of that which I, I think that are,

190
00:10:29.260 --> 00:10:32.929
are, um, it's absolutely correct to do. Um, I

191
00:10:32.929 --> 00:10:34.650
think people need to pay more attention than they

192
00:10:34.650 --> 00:10:37.289
often do, um, to the other possible mistake you

193
00:10:37.289 --> 00:10:39.729
can make. So epistemology is about doing a good

194
00:10:39.729 --> 00:10:41.770
job deciding what to believe. One mistake you can

195
00:10:41.770 --> 00:10:43.650
make is believing something you shouldn't, believing something that

196
00:10:43.650 --> 00:10:45.090
is not enough evidence for him, having a bad

197
00:10:45.090 --> 00:10:48.650
belief, um. Everyone talks about that one. The other

198
00:10:48.650 --> 00:10:50.570
mistake that you can make, uh, which people need

199
00:10:50.570 --> 00:10:52.450
to talk more about than they do, is the

200
00:10:52.450 --> 00:10:54.830
mistake of being too skeptical, the mistake of not

201
00:10:54.830 --> 00:10:57.010
believing something that you should, the failure to have

202
00:10:57.010 --> 00:10:58.929
good beliefs, or if you like, you can describe

203
00:10:58.929 --> 00:11:01.729
it as like bad suspension of judgment. Um, I

204
00:11:01.729 --> 00:11:05.155
think that those really um deserve equal standing. They're

205
00:11:05.155 --> 00:11:09.265
both very important, um, components of the epistemological project.

206
00:11:09.594 --> 00:11:12.434
Um, AND I think too much of us focus

207
00:11:12.434 --> 00:11:14.794
too much of the time on that, uh, negative

208
00:11:14.794 --> 00:11:17.554
side, uh, on epistemology telling you what not to

209
00:11:17.554 --> 00:11:19.955
believe. We need to recognize the cases that when

210
00:11:19.955 --> 00:11:22.075
epistemology is telling you when you should believe and

211
00:11:22.075 --> 00:11:23.234
what it would be a mistake not to.

212
00:11:24.210 --> 00:11:27.929
And do you think that either of those mistakes

213
00:11:27.929 --> 00:11:30.940
is a bigger problem than the other? There is

214
00:11:30.940 --> 00:11:34.320
people believing too easily things they shouldn't is a

215
00:11:34.320 --> 00:11:37.659
bigger problem than than them failing to believe things

216
00:11:37.659 --> 00:11:40.849
they should or vice versa, or, I mean, how

217
00:11:40.849 --> 00:11:42.219
do you approach this question?

218
00:11:42.679 --> 00:11:44.950
Yeah, it's a complicated question. um, AND in a

219
00:11:44.950 --> 00:11:49.349
way, it's not totally obvious to me, um, what

220
00:11:49.349 --> 00:11:52.070
kind of metric is appropriate for even kind of

221
00:11:52.070 --> 00:11:53.950
measuring like the size of the problem, those kinds

222
00:11:53.950 --> 00:11:56.090
of things. So I'm, I'm not totally sure what

223
00:11:56.090 --> 00:11:59.250
to say. My instinct really is to say Um,

224
00:11:59.989 --> 00:12:03.789
that they're equally important. Um, I, I definitely don't

225
00:12:03.789 --> 00:12:08.390
want, um, to, to be saying, um, that it's

226
00:12:08.390 --> 00:12:12.030
wrong to be focusing on negative epistemic norms. It's,

227
00:12:12.039 --> 00:12:13.909
it's wrong to worry about bad belief. I don't

228
00:12:13.909 --> 00:12:17.030
think that at all. Um, MY book is really,

229
00:12:17.239 --> 00:12:19.190
um, trying to set that aside, not because I

230
00:12:19.190 --> 00:12:20.989
don't think it's important because I think there's plenty

231
00:12:20.989 --> 00:12:24.840
of attention paid to those questions already. Um, SO

232
00:12:24.909 --> 00:12:27.739
my stance is, uh, there's been kind of an,

233
00:12:27.830 --> 00:12:33.270
um, uh, um, nearly exclusive emphasis on the negative

234
00:12:33.270 --> 00:12:35.739
side of epistemology, and I think the positive side

235
00:12:35.739 --> 00:12:38.830
of epistemology is very important, um, you know, I

236
00:12:38.830 --> 00:12:42.750
guess I think probably. Approximately equally important if I,

237
00:12:42.830 --> 00:12:44.739
if I know how to measure that, um, but

238
00:12:44.739 --> 00:12:46.700
because there's been such a focus on the negative,

239
00:12:46.989 --> 00:12:50.630
um, um, in the literature elsewhere, um, I'm trying

240
00:12:50.630 --> 00:12:52.109
to correct that by focusing much more on the

241
00:12:52.109 --> 00:12:53.510
positive in this book. Mhm.

242
00:12:54.489 --> 00:12:57.130
So in your book, you point to the existence

243
00:12:57.130 --> 00:13:01.570
of a negative bias about belief, both in epistemology

244
00:13:01.570 --> 00:13:06.609
and also in non-academic discussions of intelligence, rationality, and

245
00:13:06.609 --> 00:13:10.030
so on. Could you explain that the bias, the,

246
00:13:10.059 --> 00:13:11.369
the negative bias?

247
00:13:12.020 --> 00:13:14.460
Sure, yeah. So I mean, what I mean by

248
00:13:14.460 --> 00:13:16.340
the existence of the, of, of the bias is

249
00:13:16.340 --> 00:13:17.659
kind of just what I said, even though they're

250
00:13:17.659 --> 00:13:20.219
both really important, there's so much more attention that's

251
00:13:20.219 --> 00:13:22.460
put on the negative side. Um, I can say

252
00:13:22.460 --> 00:13:25.380
a little bit um to kind of illustrate why

253
00:13:25.380 --> 00:13:29.460
I think that's so. Um, um, FOR one thing,

254
00:13:30.909 --> 00:13:35.799
So much of our discourse um is wrapped up

255
00:13:35.799 --> 00:13:42.539
around um Question around kind of um uh a

256
00:13:42.539 --> 00:13:49.150
veneration of um careful consideration, where careful is really

257
00:13:49.380 --> 00:13:55.539
um uh stereotypically, conceptually, tightly coded to um being

258
00:13:55.539 --> 00:13:58.679
slow to believe, collecting more evidence before you kind

259
00:13:58.679 --> 00:14:00.880
of reach your conclusion. A lot of the metaphors

260
00:14:00.880 --> 00:14:02.400
that we use to talk about these things, I

261
00:14:02.400 --> 00:14:05.200
think in code, um, um, what I see as

262
00:14:05.200 --> 00:14:09.049
a bias. So, uh, people talk about the importance

263
00:14:09.049 --> 00:14:15.099
of not jumping to conclusions. Um, um, THE implicit,

264
00:14:15.450 --> 00:14:18.750
um, assumption behind the metaphor is, you know, when

265
00:14:18.750 --> 00:14:21.090
you jump, you're leaving somewhere safe to something that

266
00:14:21.090 --> 00:14:24.210
might be dangerous. Um, SO that's kind of assuming

267
00:14:24.210 --> 00:14:26.849
this idea that suspension of judgment is always the

268
00:14:26.849 --> 00:14:30.049
safe thing and believing is the potentially dangerous thing

269
00:14:30.049 --> 00:14:32.169
that you better make sure you've, um, you've done

270
00:14:32.169 --> 00:14:35.789
your due diligence before doing. Um, OUR stereotypes of

271
00:14:35.789 --> 00:14:42.270
rationality, I think really, um, strongly encode, um, slowness

272
00:14:42.270 --> 00:14:46.789
to act, um, um, we, we worry a lot

273
00:14:46.789 --> 00:14:50.820
about kind of, um, making sure you don't get

274
00:14:50.820 --> 00:14:55.190
duped. There's this like, um, there's this, um, deep

275
00:14:55.190 --> 00:14:59.150
stereotypical association between being skeptical and like being rational.

276
00:14:59.325 --> 00:15:01.955
Being intelligent. And sometimes you see this like uh

277
00:15:01.955 --> 00:15:04.344
laid out explicitly, you know, you see people saying,

278
00:15:04.515 --> 00:15:08.385
oh, you know, it's never wrong to um to

279
00:15:08.635 --> 00:15:12.505
be um skeptical, even if it turns out that,

280
00:15:12.594 --> 00:15:16.135
um, you know, the evidential supports, uh, the evidential

281
00:15:16.135 --> 00:15:18.835
source that you're looking at is accurate, um, you

282
00:15:18.835 --> 00:15:21.715
know, scrutinizing it more closely can, uh, never be

283
00:15:21.715 --> 00:15:24.275
a mistake. And I think that's, um, I think

284
00:15:24.275 --> 00:15:27.429
that's. Wrong and in some sense when you look

285
00:15:27.429 --> 00:15:29.619
at it with like the right level of abstraction,

286
00:15:29.789 --> 00:15:32.419
I think that's clearly wrong. I think that there

287
00:15:32.419 --> 00:15:38.549
are substantive and often difficult questions about um when

288
00:15:38.549 --> 00:15:41.150
you've done enough investigation, when you've done enough evidence

289
00:15:41.150 --> 00:15:44.979
collection. Um, SOMETIMES it's hard to know, uh, not

290
00:15:44.979 --> 00:15:47.630
just, um, you know, what your current evidence supports,

291
00:15:47.710 --> 00:15:50.909
but whether you have gathered enough evidence and thought

292
00:15:50.909 --> 00:15:53.489
about it enough, um, to make it reasonable to

293
00:15:53.489 --> 00:15:55.919
an inquiry. And actually the fact that that can

294
00:15:55.919 --> 00:16:01.840
be hard, um, just basically um entails that um

295
00:16:01.840 --> 00:16:04.429
it's not always right to keep being more skeptical,

296
00:16:04.640 --> 00:16:06.880
um because it were if it were then anytime

297
00:16:06.880 --> 00:16:09.080
you, you, anytime you wondered whether to keep going,

298
00:16:09.159 --> 00:16:11.280
you should just keep investigating. um, BUT sometimes you

299
00:16:11.280 --> 00:16:13.719
shouldn't keep investigating. Sometimes you know enough, the question

300
00:16:13.719 --> 00:16:17.630
is settled. And um that you're at best uh

301
00:16:17.630 --> 00:16:20.750
wasting time by investigating further and depending on the

302
00:16:20.750 --> 00:16:24.140
circumstances, um, you know, you might be missing opportunities

303
00:16:24.140 --> 00:16:27.119
to take important actions. You might be disrespecting people

304
00:16:27.119 --> 00:16:30.109
if what you're doing is, um, you know, continuing

305
00:16:30.109 --> 00:16:32.190
to investigate whether their testimony is true and in

306
00:16:32.190 --> 00:16:34.429
fact you have plenty of reason to conclude that

307
00:16:34.429 --> 00:16:39.630
it is, um. This bias I think runs really

308
00:16:39.630 --> 00:16:45.750
deep. Um, um, I I do a little bit

309
00:16:45.750 --> 00:16:52.119
of uh sort of fanciful alternate history, philosophy, imagination

310
00:16:52.119 --> 00:16:57.880
in my book, um, um, um, you know, lots

311
00:16:57.880 --> 00:17:02.020
of, probably, probably the most commonly, um. Uh, Red

312
00:17:02.020 --> 00:17:05.780
work of epistemology and epistemology courses is um the

313
00:17:05.780 --> 00:17:08.608
opening of Descar's meditations, which opens with these very

314
00:17:08.608 --> 00:17:13.420
skeptical, um, kind of natural ruminations. Um, THE meditator

315
00:17:13.420 --> 00:17:15.579
says, oh, you know, I've been, uh, I've been

316
00:17:15.579 --> 00:17:19.459
noticing that, um, I've been wrong about stuff previously

317
00:17:19.459 --> 00:17:21.973
in my life. So here's a, here's a really

318
00:17:22.185 --> 00:17:25.015
um sensible project. I should sit down and make

319
00:17:25.015 --> 00:17:27.704
sure that that doesn't happen anymore. And the way

320
00:17:27.704 --> 00:17:29.665
that I'll do that is I will like get

321
00:17:29.665 --> 00:17:31.584
rid of all my beliefs and examine each one

322
00:17:31.584 --> 00:17:33.055
can only only put the good ones back in.

323
00:17:33.464 --> 00:17:34.984
And you know, that's a bit extreme, but like

324
00:17:34.984 --> 00:17:37.265
the, the kind of uh motivation behind that is

325
00:17:37.265 --> 00:17:40.589
something that intuitively makes sense. Um. In my book,

326
00:17:40.760 --> 00:17:45.339
I imagine um Bizarro Descartes, who, um, so regular,

327
00:17:45.530 --> 00:17:48.609
regular Descartes is motivated by negative epistemology. He wants

328
00:17:48.609 --> 00:17:49.930
to make sure he never makes the mistake of

329
00:17:49.930 --> 00:17:53.209
bad beliefs. Um, THIS hypothetical Bizarro Descartes that I've

330
00:17:53.209 --> 00:17:57.219
invented, um, is, uh. Fixated on the opposite side

331
00:17:57.219 --> 00:18:00.660
of epistemology. He's motivated by the observation that sometimes

332
00:18:00.660 --> 00:18:02.650
things have happened and he didn't, he didn't, uh,

333
00:18:02.699 --> 00:18:05.890
uh, notice that they happened. Um, HE'S really motivated

334
00:18:06.060 --> 00:18:09.619
by making sure that, um, that he believes all

335
00:18:09.619 --> 00:18:13.380
the true things. And so his strategy is to

336
00:18:13.560 --> 00:18:16.459
um believe like absolutely everything and then only reject

337
00:18:16.459 --> 00:18:20.150
those beliefs that he finds evidence against. And um

338
00:18:20.150 --> 00:18:23.750
not only is that, you know, not, um, you

339
00:18:23.750 --> 00:18:28.099
know, what um the historical character Dickard did, um,

340
00:18:28.109 --> 00:18:29.709
it kind of feels like a joke. It it

341
00:18:29.709 --> 00:18:32.619
feels it feels like a completely silly idea and

342
00:18:32.829 --> 00:18:35.010
um. And the reason that I kind of put

343
00:18:35.010 --> 00:18:37.369
these two characters next to each other is, uh,

344
00:18:37.489 --> 00:18:39.130
you know, I, I do think Bizarro Dard is

345
00:18:39.130 --> 00:18:41.719
being quite silly. Um, I think that he is,

346
00:18:41.969 --> 00:18:45.849
um, unduly fixated on one half of epistemology and

347
00:18:45.849 --> 00:18:47.689
he's pursuing a strategy that's gonna make all kinds

348
00:18:47.689 --> 00:18:49.530
of mistakes on the other half. Um, BUT I

349
00:18:49.530 --> 00:18:51.489
actually think exactly the same thing about the like

350
00:18:51.489 --> 00:18:55.849
regulated art of, of the regular meditations, um, focusing

351
00:18:55.849 --> 00:18:59.209
just on one side, um. IS a mistake. Um,

352
00:18:59.920 --> 00:19:02.359
THE fact that like that mistake kind of like

353
00:19:02.359 --> 00:19:04.349
feels sort of like common sense on one side,

354
00:19:04.520 --> 00:19:06.630
but feels like a joke on the other, um,

355
00:19:07.079 --> 00:19:10.310
I, I think also illustrates the degree to which

356
00:19:10.310 --> 00:19:13.680
um we're sort of working in this, um, negatively

357
00:19:13.680 --> 00:19:15.859
biased framing. Now I call it a bias, like,

358
00:19:15.920 --> 00:19:17.920
you know, plenty of people think I'm wrong about

359
00:19:17.920 --> 00:19:22.069
that, that actually there's good reason um why. SHOULD

360
00:19:22.069 --> 00:19:24.849
be more focused on the negative. Um, uh, um,

361
00:19:24.859 --> 00:19:28.699
I, I, I have arguments against that, um, but,

362
00:19:28.709 --> 00:19:30.910
uh, but if you, if you, uh, if you

363
00:19:30.910 --> 00:19:33.430
think as I do, that these are two equally

364
00:19:33.430 --> 00:19:36.790
important sides of the project, um, then, uh, I,

365
00:19:36.869 --> 00:19:40.229
I think it's hard to avoid the conclusion that

366
00:19:40.229 --> 00:19:42.540
there has been a bias toward the negative, um,

367
00:19:42.670 --> 00:19:44.630
in the history of philosophy and in like folk

368
00:19:44.630 --> 00:19:45.630
ideas about rationality.

369
00:19:46.569 --> 00:19:49.719
And in the book you refer to negative epistemology

370
00:19:49.719 --> 00:19:52.959
as an ideology. Could you explain that?

371
00:19:53.569 --> 00:19:57.520
Yeah, ideology is uh It's one of those words,

372
00:19:57.599 --> 00:20:01.819
right? It, it, it, um, it. It means a

373
00:20:01.819 --> 00:20:03.819
lot of things and not always the same in

374
00:20:03.819 --> 00:20:07.579
different um in different bodies of literature, but, um,

375
00:20:07.939 --> 00:20:13.020
but I'm, I'm taking the label um from kind

376
00:20:13.020 --> 00:20:16.819
of it's, it's tradition and critical theory. Um, SO

377
00:20:16.819 --> 00:20:21.489
ideologies are, um, Like, I mean quite generally an

378
00:20:21.489 --> 00:20:24.290
ideology is like a, it's a framework of like

379
00:20:24.290 --> 00:20:27.410
meaning or value in a culture. Um, IDEOLOGIES are

380
00:20:27.410 --> 00:20:30.109
those, those bits of the culture that kind of

381
00:20:30.109 --> 00:20:31.890
shape your understanding of what's going on in the

382
00:20:31.890 --> 00:20:34.449
world and your role in the world, um, in

383
00:20:34.449 --> 00:20:42.260
ways that, um, that. Um, CAN Make it difficult

384
00:20:42.260 --> 00:20:44.780
to see that there's a substantive um framing or

385
00:20:44.780 --> 00:20:47.969
shaping being made. So ideologies kind of by nature

386
00:20:48.189 --> 00:20:54.979
um uh hide themselves. They, they take contingent um

387
00:20:54.979 --> 00:20:59.170
choices about um the way a culture is organized

388
00:20:59.380 --> 00:21:02.219
and they sort of uh disguise them under the

389
00:21:02.219 --> 00:21:08.359
heading of objectivity or uh or necessity, um. Um,

390
00:21:08.619 --> 00:21:13.770
THEY tend to perpetuate themselves, um, uh, um, uh,

391
00:21:13.819 --> 00:21:19.939
through, um, through memes, through habits, through language, stereotypes,

392
00:21:20.180 --> 00:21:23.380
um, basically ideas about what is normal. Um, SOME

393
00:21:23.380 --> 00:21:26.969
people use, some people reserve the word ideology, um,

394
00:21:26.979 --> 00:21:30.790
for what, um, Raymond Goyce calls its pejorative sense,

395
00:21:30.859 --> 00:21:32.900
so it only counts as an ideology if it's

396
00:21:32.900 --> 00:21:38.459
like doing bad stuff, uh, perpetuating oppression, um. Um,

397
00:21:39.589 --> 00:21:42.709
AND I, I, um, I argue in my book

398
00:21:42.920 --> 00:21:47.359
that this negative bias is ideological, um, um, in

399
00:21:47.359 --> 00:21:50.079
both senses. So, um, so it has those features,

400
00:21:50.239 --> 00:21:53.949
um, and does in some ways, um, perpetuate oppression,

401
00:21:54.170 --> 00:21:57.560
um, by kind of reinforcing certain conservative status quo

402
00:21:57.560 --> 00:21:59.560
biases, and I could talk more about why, why

403
00:21:59.560 --> 00:22:02.609
I think that, um. But uh what makes it

404
00:22:02.609 --> 00:22:08.099
ideological, um, is, um is that it has this

405
00:22:08.099 --> 00:22:15.260
characteristic feature of um making, uh, this, um, what

406
00:22:15.260 --> 00:22:17.420
I was just suggesting is a bias, this emphasis

407
00:22:17.420 --> 00:22:19.790
on the negative side of epistemology, and making that

408
00:22:19.790 --> 00:22:22.420
feel kind of obvious, making that feel natural, kind

409
00:22:22.420 --> 00:22:24.709
of not like a choice at all. It's. Transmitted

410
00:22:24.709 --> 00:22:27.430
through these stereotypes of rationality. So I mean, I

411
00:22:27.430 --> 00:22:30.660
mean, the, the epistemological canon is to some degree

412
00:22:30.660 --> 00:22:34.140
like part of the, the um cultural transmission. um,

413
00:22:34.150 --> 00:22:36.699
BUT so much of this happens just in, uh,

414
00:22:36.709 --> 00:22:40.390
more general memes and stereotypes, the metaphors, you know,

415
00:22:40.550 --> 00:22:44.390
the, um, um, um, going out on a limb

416
00:22:44.390 --> 00:22:48.520
as sort of, uh, describing belief, um. Um, THE,

417
00:22:48.540 --> 00:22:52.329
the stereotype of a rational actor as an inactive,

418
00:22:52.680 --> 00:22:56.859
slow, um, deliberate, collecting more evidence kind of, kind

419
00:22:56.859 --> 00:23:00.930
of one. Another feature of ideologies they often um

420
00:23:00.930 --> 00:23:04.010
work in like subtle ways to perpetuate themselves and

421
00:23:04.010 --> 00:23:06.810
in some sense to even kind of um make

422
00:23:06.810 --> 00:23:10.050
some of their commitments true. Ideologies are often um

423
00:23:10.170 --> 00:23:15.170
uh deceptive. They, they, um they cause people um

424
00:23:15.170 --> 00:23:17.810
who um are part of the cultures that they

425
00:23:17.810 --> 00:23:20.650
characterize uh to have false ideas about those cultures.

426
00:23:20.849 --> 00:23:23.859
So, you know, a um. Uh, IT'S part of

427
00:23:23.859 --> 00:23:28.579
a, uh, patriarchal ideology, um, um, a whole suite

428
00:23:28.579 --> 00:23:33.079
of like assumptions about gender essentialism and, you know,

429
00:23:33.219 --> 00:23:35.619
what, um, men are like by nature and what

430
00:23:35.619 --> 00:23:37.949
women are like by nature, and the gender binary

431
00:23:37.949 --> 00:23:42.300
itself, um, and, um, uh, I think that that's

432
00:23:42.300 --> 00:23:45.150
kind of, um, you know, those assumptions are, are

433
00:23:45.150 --> 00:23:47.250
false. There is much more gender diversity than that,

434
00:23:47.420 --> 00:23:52.079
um, but there in some uh Uh, complicated and

435
00:23:52.079 --> 00:23:55.920
subtle way, um, less false than they, um, would

436
00:23:55.920 --> 00:23:58.439
be without the ideology, cause the ideology tends to

437
00:23:58.439 --> 00:24:01.479
enforce itself on people and, um, now that we

438
00:24:01.479 --> 00:24:03.800
have these stereotypes about, you know, what men are

439
00:24:03.800 --> 00:24:08.000
supposed to behave like, um, uh, men are incentivized

440
00:24:08.000 --> 00:24:11.550
for very, you know, um, normal, um, rational reasons

441
00:24:11.829 --> 00:24:15.564
to. THE stereotypes, um, which then makes it true

442
00:24:15.564 --> 00:24:17.875
that men like tend to be, you know, tend,

443
00:24:18.125 --> 00:24:22.125
um, are, um, more ambitious and more aggressive or,

444
00:24:22.165 --> 00:24:24.005
or, um, whatever the example you're looking at. Uh,

445
00:24:24.165 --> 00:24:29.645
WOMEN are motivated, um, um, by ideology, um, to

446
00:24:29.645 --> 00:24:33.364
develop, um, uh, their, uh, nurturing sides and to,

447
00:24:33.405 --> 00:24:37.310
to be mothers, um. And there it's a statistical

448
00:24:37.310 --> 00:24:40.189
fact that um that men tend to behave in

449
00:24:40.189 --> 00:24:41.790
these ways and women tend to behave in those

450
00:24:41.790 --> 00:24:43.589
ways. I don't think it's true that's like by

451
00:24:43.589 --> 00:24:45.949
nature. I think that ideology is kind of causing

452
00:24:45.949 --> 00:24:49.349
people to conform um to um to those patterns.

453
00:24:49.430 --> 00:24:53.180
So ideologies have this tendency also to um to

454
00:24:53.420 --> 00:24:56.989
um socially construct their own um their own, the

455
00:24:56.989 --> 00:25:00.589
truth of their own contents. Um, I think that

456
00:25:00.869 --> 00:25:03.109
the negative bias in epistemology has all of these

457
00:25:03.109 --> 00:25:07.900
features. So, um, uh, I've been talking already about

458
00:25:07.900 --> 00:25:09.979
how it tends to be hidden, how it tends

459
00:25:09.979 --> 00:25:14.540
to kind of disguise itself as objectivity. Um, I

460
00:25:14.540 --> 00:25:16.219
think there are very interesting ways in which it

461
00:25:16.219 --> 00:25:20.050
also tends to, uh, tends to make itself true.

462
00:25:20.219 --> 00:25:23.459
And so, um, what that would, um, what that

463
00:25:23.459 --> 00:25:27.219
would mean in this instance is cases. WHERE, um,

464
00:25:27.449 --> 00:25:30.609
where I think the right thing to do is

465
00:25:30.609 --> 00:25:36.010
to believe, um, the um negative epistemic bias um

466
00:25:36.010 --> 00:25:38.089
tends to push us away from that. Um, AND

467
00:25:38.089 --> 00:25:39.849
in some instances it can do so in a

468
00:25:39.849 --> 00:25:44.479
way that actually um causes it, um, to be,

469
00:25:44.689 --> 00:25:48.540
um, incorrect to believe. Um, SO this is subtle

470
00:25:48.540 --> 00:25:51.050
because of, um, some of the distinctions between, you

471
00:25:51.050 --> 00:25:53.130
know, belief and knowledge and acceptance, but the, but

472
00:25:53.130 --> 00:25:56.770
the general kind of idea is, um, Um, the

473
00:25:56.770 --> 00:26:00.439
more, um, uh, you have this negative epistemic, um,

474
00:26:00.449 --> 00:26:03.609
bias in your culture, um, the more natural it's

475
00:26:03.609 --> 00:26:07.489
going to be, um, to feel doubt about your

476
00:26:07.489 --> 00:26:12.010
commitments, um, and, um, uh, and the feeling of

477
00:26:12.010 --> 00:26:17.609
doubt is something that can, um, uh, um, uh.

478
00:26:18.020 --> 00:26:21.140
Have not merely psychological effects, you know, making you

479
00:26:21.140 --> 00:26:23.579
tend to, um, uh, you know, want to stop

480
00:26:23.579 --> 00:26:27.260
believing, um, but epistemic effects. So for example, uh,

481
00:26:27.310 --> 00:26:29.579
actually, here's, here's a simple example. If you doubt

482
00:26:29.579 --> 00:26:33.369
enough to um make you to, to cause you,

483
00:26:33.609 --> 00:26:37.339
um, to suspend judgment, um, well, that means you

484
00:26:37.339 --> 00:26:40.140
don't, uh, know, you don't know the thing that

485
00:26:40.140 --> 00:26:42.339
you, um, you maybe started out believing or that

486
00:26:42.339 --> 00:26:44.500
you were considering believing, even if you had enough

487
00:26:44.500 --> 00:26:46.900
evidence where if you've been confident enough, you would

488
00:26:46.900 --> 00:26:50.829
have known. Um, WELL, I, I also think, uh,

489
00:26:50.910 --> 00:26:52.910
you know, this is one of my commitments from,

490
00:26:52.949 --> 00:26:54.390
from past work, not really a theme in this

491
00:26:54.390 --> 00:26:56.910
book, but something that I do think, um, is

492
00:26:56.910 --> 00:27:00.989
that, um, knowledge is deeply tied to the norms

493
00:27:00.989 --> 00:27:07.239
of belief. Um, SO, um, uh, some, uh, some

494
00:27:07.239 --> 00:27:10.920
epistemologists have codified this in a fairly simple way,

495
00:27:11.150 --> 00:27:13.689
um, saying that like knowledge is the norm. You

496
00:27:13.689 --> 00:27:16.199
should only believe things if you know them. Um,

497
00:27:16.489 --> 00:27:18.430
I can't sign up to that simple formulation. Because

498
00:27:18.430 --> 00:27:20.060
that would imply that in these cases that I'm

499
00:27:20.060 --> 00:27:24.109
I was just talking about, um, if you um

500
00:27:24.109 --> 00:27:27.219
if you doubt, even though you shouldn't have, um,

501
00:27:27.469 --> 00:27:31.030
that causes you to stop knowing, um, and now

502
00:27:31.030 --> 00:27:33.229
since you don't know, um, the implication would be

503
00:27:33.229 --> 00:27:35.420
that you shouldn't believe. So if you have a

504
00:27:35.420 --> 00:27:40.979
view like that, um, then, um, then the negative

505
00:27:40.979 --> 00:27:46.660
bias itself is going to create genuine norms that

506
00:27:46.829 --> 00:27:48.180
uh tell you not to believe. Now again, that's

507
00:27:48.180 --> 00:27:51.140
not my view of, of epistemic normativity and for

508
00:27:51.140 --> 00:27:53.579
like for for exactly this reason, um, but because

509
00:27:53.579 --> 00:27:57.270
there's such a like um natural idea, um, which,

510
00:27:57.300 --> 00:27:58.859
you know, I was convinced by in past work

511
00:27:58.859 --> 00:28:00.420
and oh well if you don't know it, you

512
00:28:00.420 --> 00:28:02.219
shouldn't believe it, that like that feels like common

513
00:28:02.219 --> 00:28:06.589
sense, um. Um, THAT'S the negative bias kind of,

514
00:28:06.819 --> 00:28:09.030
um, perpetuating itself. It's the reason why you don't

515
00:28:09.030 --> 00:28:10.709
know it is like because of the negative bias

516
00:28:10.709 --> 00:28:13.579
itself. Um, THAT causes more patterns of disbelief. It

517
00:28:13.579 --> 00:28:16.229
reinforces the tendency to behave in this way. We

518
00:28:16.229 --> 00:28:19.500
tend to kind of, um, we tend to, um,

519
00:28:19.510 --> 00:28:22.989
celebrate one another's rationality, um, for being careful in

520
00:28:22.989 --> 00:28:24.989
these ways, uh, quote unquote careful in these ways

521
00:28:24.989 --> 00:28:27.619
that are actually I think making, um, significant epistemic

522
00:28:27.619 --> 00:28:30.709
errors. Um, I feel like that was a long-winded

523
00:28:30.709 --> 00:28:32.550
answer, um, but did I answer?

524
00:28:33.719 --> 00:28:37.359
Yes, uh, and in regards to justification of belief

525
00:28:37.359 --> 00:28:41.359
in epistemology, there is a very strong inclination to

526
00:28:41.359 --> 00:28:44.760
demand the justification of belief, uh, the idea that

527
00:28:44.760 --> 00:28:48.119
belief is something that stands in need of justification,

528
00:28:48.229 --> 00:28:51.319
that we need to justify our belief. So, is

529
00:28:51.319 --> 00:28:54.989
there a problem with that idea and what kind

530
00:28:54.989 --> 00:28:58.760
of message do we get from talking about beliefs

531
00:28:58.760 --> 00:29:01.599
in terms of whether they are justified?

532
00:29:02.189 --> 00:29:04.119
Yeah, great. This is one of the, this is

533
00:29:04.119 --> 00:29:08.839
one of the connections um that convinced me there's

534
00:29:08.839 --> 00:29:13.479
something ideological going on here, um, because Uh, for

535
00:29:13.479 --> 00:29:19.739
someone, you know, trained in Western epistemology, um, You

536
00:29:19.739 --> 00:29:23.459
know, talking about beliefs being justified is like breathing

537
00:29:23.459 --> 00:29:25.819
air. I mean, it's the most natural thing in

538
00:29:25.819 --> 00:29:29.729
the world, um, and I had sort of forgotten,

539
00:29:30.060 --> 00:29:33.250
um, until I started teaching epistemology to introductory students

540
00:29:33.579 --> 00:29:37.420
that that's not like colloquial language. Um, I need

541
00:29:37.420 --> 00:29:40.020
to when I teach inter epistemology, I need to

542
00:29:40.020 --> 00:29:41.900
explain to my students what we mean by justification.

543
00:29:42.219 --> 00:29:43.939
My students come into the class knowing what knowledge

544
00:29:43.939 --> 00:29:46.140
is and they, uh, um, you know, vaguely, you

545
00:29:46.140 --> 00:29:47.459
know, they know how to use the word, they

546
00:29:47.459 --> 00:29:49.839
know how to use the word belief, um. Um,

547
00:29:49.959 --> 00:29:52.329
JUSTIFICATION needs to be explained. Justification really is a

548
00:29:52.329 --> 00:29:57.319
philosopher's term of art, um, and, um, And I

549
00:29:57.319 --> 00:30:00.959
noticed two things, um, in the course of researching

550
00:30:00.959 --> 00:30:04.089
for this book that surprised me. Um, ONE is

551
00:30:04.089 --> 00:30:07.319
just how recent the ubiquity of justification talk is

552
00:30:07.319 --> 00:30:10.439
in the history of epistemology. Um, IT, it basically

553
00:30:10.439 --> 00:30:12.800
comes in the second half of the 20th century.

554
00:30:12.839 --> 00:30:14.969
It basically, it basically comes out of, um, out

555
00:30:14.969 --> 00:30:17.770
of Geer's paper. It's just about you believe knowledge

556
00:30:17.770 --> 00:30:20.689
I mean. YOU know, there are examples of the

557
00:30:20.689 --> 00:30:23.250
use of that word in, um, in the history

558
00:30:23.250 --> 00:30:26.079
of philosophy going back older than that. Um, BUT,

559
00:30:26.209 --> 00:30:30.560
um, but it's really like post Gettier that epistemologists

560
00:30:30.849 --> 00:30:34.630
started saying things like, um, you know, all beliefs

561
00:30:34.630 --> 00:30:37.650
need to be justified and justification is a necessary

562
00:30:37.650 --> 00:30:40.449
condition for knowledge and things like that. And so

563
00:30:40.449 --> 00:30:41.640
that was one thing that I noticed. Oh, that's

564
00:30:41.640 --> 00:30:43.969
a, that's a recent choice. That means it's contingent.

565
00:30:44.050 --> 00:30:45.609
We could have, we could have settled on different

566
00:30:45.609 --> 00:30:51.959
terminology. Um, INTERESTINGLY enough, like Gtier himself, um, uh,

567
00:30:51.969 --> 00:30:53.880
you know, it's only, it's a two-page paper, um,

568
00:30:54.130 --> 00:30:56.040
everyone talks about the counterexamples in the second page.

569
00:30:56.170 --> 00:30:57.969
I think the first page is more interesting than,

570
00:30:58.050 --> 00:31:01.050
than, um, people realize. Um, THAT'S what he lays

571
00:31:01.050 --> 00:31:03.209
out, um, as he puts it, like 3 different

572
00:31:03.209 --> 00:31:05.729
definitions of knowledge that like all share a common

573
00:31:05.729 --> 00:31:07.770
structure. Only one of those three uses the word

574
00:31:07.770 --> 00:31:09.760
justification. Although it also turns up in his title,

575
00:31:09.800 --> 00:31:12.199
which is I think why why everyone settled um

576
00:31:12.199 --> 00:31:14.979
settled on that way of talking. Um BUT uh

577
00:31:14.979 --> 00:31:17.030
yeah, so the contingency of these the word justification,

578
00:31:17.239 --> 00:31:20.069
and then the normative significance. So I, I also

579
00:31:20.069 --> 00:31:24.479
um have have an interest in like um the

580
00:31:24.479 --> 00:31:29.349
normative um character of language just generally. So, um,

581
00:31:29.359 --> 00:31:33.569
in particular, the presuppositions and stereotypes that attached to

582
00:31:33.569 --> 00:31:35.719
words even when we like don't think of them

583
00:31:35.719 --> 00:31:39.770
as officially part of their meaning and And justification

584
00:31:40.140 --> 00:31:45.099
is a very normatively loaded word. Um, JUSTIFICATION is

585
00:31:45.099 --> 00:31:49.699
a word that outside of epistemology, um, we only

586
00:31:49.699 --> 00:31:54.619
use for things that are like, um, uh, normatively

587
00:31:54.619 --> 00:32:02.300
suspect. So if someone is um um like honking

588
00:32:02.300 --> 00:32:07.410
their horn on the street, um um it's natural

589
00:32:07.660 --> 00:32:10.300
to wonder or to ask or to talk about

590
00:32:10.300 --> 00:32:12.780
whether they have any justification for doing that annoying

591
00:32:12.780 --> 00:32:17.290
thing that they're doing, um. If someone is going

592
00:32:17.290 --> 00:32:20.569
for a walk in the snow, um, um, you

593
00:32:20.569 --> 00:32:23.839
know, on public property, um, no one will ask,

594
00:32:24.119 --> 00:32:27.290
um, what justification do they have for going for

595
00:32:27.290 --> 00:32:29.160
a walk in the snow? Or if they do,

596
00:32:29.209 --> 00:32:30.810
they're like communicating that they have a sort of

597
00:32:30.810 --> 00:32:34.079
unusual, um, set of normative assumptions such that like

598
00:32:34.079 --> 00:32:35.829
they really shouldn't. DOING that. You know, maybe they,

599
00:32:35.949 --> 00:32:37.750
maybe if, if, if it's their boss and like

600
00:32:37.750 --> 00:32:39.369
they, they should be at work. Someone might say,

601
00:32:39.380 --> 00:32:41.030
hey, now, what reason do you have for like

602
00:32:41.030 --> 00:32:42.300
going for a walk in the snow right now.

603
00:32:42.469 --> 00:32:43.949
Um, BUT if, you know, if it's the weekend

604
00:32:43.949 --> 00:32:45.790
and someone's just, you know, going for a casual

605
00:32:45.790 --> 00:32:47.670
stroll, which they have every right to do, no

606
00:32:47.670 --> 00:32:49.829
one's gonna talk about the about the justification of

607
00:32:49.829 --> 00:32:55.689
that action. Um, And this, this is something that,

608
00:32:55.800 --> 00:33:00.810
you know, um, uh, philosophers and other subdisciplines know

609
00:33:00.810 --> 00:33:04.439
well. Um, I've, I've more recently in my career

610
00:33:04.439 --> 00:33:07.089
I been reading a lot more ethics, um, legal

611
00:33:07.089 --> 00:33:12.515
theory, and uh And the idea that, um, uh,

612
00:33:12.625 --> 00:33:16.005
you know, standing in need of justification is for

613
00:33:16.005 --> 00:33:18.755
like things that are presumptively wrong is totally common

614
00:33:18.925 --> 00:33:23.685
in those fields. Um, IN epistemology, um, uh, we

615
00:33:23.685 --> 00:33:27.630
kind of either ignore that or, or, or maybe

616
00:33:27.630 --> 00:33:30.189
more likely um haven't noticed. We, we try to

617
00:33:30.189 --> 00:33:34.790
say, um, um, you know, we mean by, by

618
00:33:34.790 --> 00:33:37.910
epistemic justification we just mean, you know, is there

619
00:33:37.910 --> 00:33:40.349
good evidence for the belief without taking any stance

620
00:33:40.349 --> 00:33:42.670
about like whether um beliefs are presumptively good or

621
00:33:42.670 --> 00:33:45.800
bad. But I think the the stereotypes, the power

622
00:33:45.800 --> 00:33:48.479
that's attached to words runs really deep. And I

623
00:33:48.479 --> 00:33:51.599
think the fact that we go around saying things

624
00:33:51.599 --> 00:33:55.430
like, um, uh, you know, all beliefs stand in

625
00:33:55.430 --> 00:34:01.119
need of justification. Um, IS, um, in a straightforward

626
00:34:01.119 --> 00:34:04.150
way reinforcing this negative bias that I'm worried about.

627
00:34:04.479 --> 00:34:09.550
So the negative bias, um, um, is reinforced because

628
00:34:09.840 --> 00:34:11.929
when you talk about something standing in need of

629
00:34:11.929 --> 00:34:15.239
justification, uh, you're in effect communicating, uh, and in

630
00:34:15.239 --> 00:34:16.918
my view is you're doing this through a, a

631
00:34:16.918 --> 00:34:20.649
semantic presupposition. Um, YOU'RE communicating that the thing is

632
00:34:20.649 --> 00:34:24.800
presumptively bad. Um, YOU only talk about, um, uh,

633
00:34:24.850 --> 00:34:28.370
whether something is or is not justified, um, when

634
00:34:28.370 --> 00:34:29.889
it's the kind of thing that stands in need

635
00:34:29.889 --> 00:34:32.610
of justification. Um, AND, and it's important that it's

636
00:34:32.610 --> 00:34:34.889
a presupposition because that means, you know, someone might

637
00:34:34.889 --> 00:34:37.290
someone might, uh, some, some might say, oh, but

638
00:34:37.290 --> 00:34:40.010
look, uh, epistemologists are, uh, plenty of epistemologists think

639
00:34:40.010 --> 00:34:41.728
that justification is easy to come by, you know,

640
00:34:41.770 --> 00:34:44.193
if you're a reliableist. Then you think like lots

641
00:34:44.193 --> 00:34:46.792
of our beliefs are justified, um, and that's true

642
00:34:46.792 --> 00:34:48.443
and so that's less skeptical than, you know, a

643
00:34:48.443 --> 00:34:50.482
skeptic who says, you know, almost none of your

644
00:34:50.482 --> 00:34:52.183
beliefs, almost none of your beliefs are justified that

645
00:34:52.193 --> 00:34:55.462
that stance is, um, you know, less negatively biased.

646
00:34:55.752 --> 00:34:58.012
But my point is that using the word justification

647
00:34:58.012 --> 00:35:00.072
at all in this very general sense, you know,

648
00:35:00.153 --> 00:35:02.982
talking about talking about whether all beliefs are justified,

649
00:35:03.353 --> 00:35:06.522
itself communicates the idea that all beliefs are suspect.

650
00:35:06.792 --> 00:35:09.676
Only things that are. um, ARE things that it

651
00:35:09.676 --> 00:35:11.615
even makes sense to talk about what they're justified.

652
00:35:11.895 --> 00:35:13.696
If you say that it's justified, that's kind of

653
00:35:13.696 --> 00:35:15.446
like saying, oh, this is the kind of thing

654
00:35:15.446 --> 00:35:17.885
and that like, um, it makes sense to be

655
00:35:17.885 --> 00:35:20.095
suspicious about but don't worry, I've checked it out.

656
00:35:20.285 --> 00:35:23.406
Um, THIS one's OK. Um, AND that, that, um,

657
00:35:23.416 --> 00:35:25.256
you know, that's not being skeptical about that particular

658
00:35:25.256 --> 00:35:28.736
belief, but it's still reinforcing this general suspicion about

659
00:35:28.736 --> 00:35:30.736
belief in a way that I think is perpetuating

660
00:35:30.736 --> 00:35:32.176
the negative bias in epistemology.

661
00:35:33.550 --> 00:35:36.969
So another very common idea is that when moral

662
00:35:36.969 --> 00:35:40.889
and practical stakes are high, there should be pragmatic,

663
00:35:41.169 --> 00:35:45.399
pragmatic encroachment and moral encroachment. What are these two

664
00:35:45.399 --> 00:35:48.889
types of encroachment and is there anything wrong with

665
00:35:48.889 --> 00:35:49.199
them?

666
00:35:50.560 --> 00:35:52.959
Good good. Uh, THIS is a big and complicated

667
00:35:52.959 --> 00:35:56.199
question, uh, part both because the subject matter is

668
00:35:56.199 --> 00:35:58.560
complicated and because my, my attitude toward them is

669
00:35:58.560 --> 00:36:01.060
a little bit subtle, um, but I'll start by

670
00:36:01.060 --> 00:36:03.830
just explaining what, what these ideas are. Um, SO,

671
00:36:04.000 --> 00:36:06.600
uh, this gets back to the distinction that I

672
00:36:06.600 --> 00:36:08.469
was talking about at the beginning of the interview,

673
00:36:08.790 --> 00:36:14.110
um, between, um, like moral norms versus epistemic norms.

674
00:36:14.479 --> 00:36:20.149
So a pretty standard way to um Um, separate

675
00:36:20.149 --> 00:36:22.909
the two, is, is to keep it quite separate,

676
00:36:22.989 --> 00:36:24.429
and you know, these, these are like the purest

677
00:36:24.429 --> 00:36:27.270
traditions in epistemology, um, are ones that keep these

678
00:36:27.270 --> 00:36:30.830
quite separate. So, you know, um, roughly speaking, the

679
00:36:30.830 --> 00:36:35.110
idea would be, um, um, the epistemic norms are

680
00:36:35.110 --> 00:36:37.790
just based on what evidence you have, um, whether

681
00:36:37.790 --> 00:36:40.985
you should believe, um. That he depends on your

682
00:36:40.985 --> 00:36:42.955
on only on the evidence that you have that

683
00:36:42.955 --> 00:36:45.435
bears toward P. It doesn't appear, it doesn't depend

684
00:36:45.435 --> 00:36:50.175
at all on, um, you know, moral considerations, um,

685
00:36:50.314 --> 00:36:54.794
like, um, whether it, um, whether believing P or

686
00:36:54.794 --> 00:36:58.104
disbelieving P would have good or bad effects, um,

687
00:36:58.274 --> 00:37:03.354
or, um, um, you know, respect considerations like whether

688
00:37:03.354 --> 00:37:06.114
it would be um disrespectful towards someone to believe

689
00:37:06.114 --> 00:37:07.995
P or to disbelieve P. That's the kind of

690
00:37:07.995 --> 00:37:13.000
standard separation. Um, um, uh, MORAL encroachment or pragmatic

691
00:37:13.000 --> 00:37:18.159
encroachment, um, are labels for, um, the idea that

692
00:37:18.159 --> 00:37:21.520
the moral or the pragmatic, uh, domains encroach on

693
00:37:21.520 --> 00:37:25.040
epistemology in the sense that the epistemic norms are

694
00:37:25.040 --> 00:37:29.350
sensitive to moral considerations or pragmatic considerations. So for

695
00:37:29.350 --> 00:37:35.340
example, um, um, some pragmatic encroachment theorists have said

696
00:37:35.669 --> 00:37:38.909
that, um, whether you should believe something doesn't just

697
00:37:38.909 --> 00:37:41.110
depend on the evidence, it also depends on how

698
00:37:41.110 --> 00:37:44.639
important it is, um, and in particular, um, they

699
00:37:44.639 --> 00:37:46.540
tend to say it depends on like the cost

700
00:37:46.540 --> 00:37:53.250
of um going wrong. So if, um, Uh, um,

701
00:37:53.500 --> 00:37:57.659
if, um, you're trying to decide uh whether to

702
00:37:57.659 --> 00:38:01.969
believe that this bridge is safe, um, that's, um,

703
00:38:02.780 --> 00:38:05.219
you know, because you're considering, you know, walking over

704
00:38:05.219 --> 00:38:09.320
it with your, with your infants, um. Um, THE

705
00:38:09.320 --> 00:38:13.510
stakes are high and according to pragmatic encroachment theorists,

706
00:38:13.760 --> 00:38:17.040
um, that might sort of raise the evidential bar

707
00:38:17.040 --> 00:38:20.439
that's necessary for reasonable belief. Um, SO maybe if

708
00:38:20.439 --> 00:38:22.199
it were, if it were less important, if your

709
00:38:22.199 --> 00:38:25.635
infants weren't there, um, and, uh, Uh, you know,

710
00:38:25.754 --> 00:38:28.514
the, the fall was a less dangerous fall even

711
00:38:28.514 --> 00:38:30.034
if the evidence about the bridge's safety were the

712
00:38:30.034 --> 00:38:31.834
same, then maybe it would be good to believe

713
00:38:31.834 --> 00:38:34.225
that it's safe. Um, BUT because of the high

714
00:38:34.225 --> 00:38:38.554
stakes, according to this tradition, um, um, uh, you

715
00:38:38.554 --> 00:38:40.834
should be more skeptical now because there's a higher

716
00:38:40.834 --> 00:38:44.149
cost of going wrong, um. Um, AND people said

717
00:38:44.149 --> 00:38:47.000
similar things about, you know, um, um, moral encouragement,

718
00:38:47.229 --> 00:38:50.550
affecting belief in in similar ways. Um, SO is

719
00:38:50.550 --> 00:38:55.000
there anything wrong with that? My, um, my view

720
00:38:55.000 --> 00:39:00.439
in the book is, um, that it, it's, it's

721
00:39:00.439 --> 00:39:03.439
officially neutral on whether there is moral and pragmatic

722
00:39:03.439 --> 00:39:07.350
encroachment, um, but, um, I mean, I I I'll

723
00:39:07.350 --> 00:39:09.919
I'll tell you in order to simplify the complexity

724
00:39:09.919 --> 00:39:12.540
of my explanation here. I'm actually pretty suspicious. Of,

725
00:39:12.590 --> 00:39:16.310
of the whole thing. I'm, I'm inclined toward thinking

726
00:39:16.719 --> 00:39:20.120
um that um the the purest idea is, is

727
00:39:20.120 --> 00:39:22.560
more correct and and encroachment is, is wrong of

728
00:39:22.560 --> 00:39:24.199
all kinds. But that's not something I try to

729
00:39:24.199 --> 00:39:26.639
argue for in the book. Um, I, I, what

730
00:39:26.639 --> 00:39:29.000
I, what I argue for in the book is

731
00:39:29.000 --> 00:39:33.310
that, um, is I guess two things. One is,

732
00:39:33.560 --> 00:39:37.570
um, the standard discourse around moral and pragmatic encroachment

733
00:39:37.919 --> 00:39:43.689
also exemplifies this negative bias. Um, um, uh, AND,

734
00:39:43.699 --> 00:39:46.699
um, and to, uh, if you want to be

735
00:39:46.699 --> 00:39:48.459
an encroachment theorist, you should have a more symmetrical

736
00:39:48.459 --> 00:39:50.850
view. So let me explain what I mean. Um,

737
00:39:51.090 --> 00:39:54.060
uh, GO back to that bridge case, um, the,

738
00:39:54.100 --> 00:39:56.459
the normal way to be a pragmatic encroachment theorist

739
00:39:56.459 --> 00:40:01.590
is to say, um, oh, when the, um, Um,

740
00:40:01.679 --> 00:40:03.679
when the stakes are higher, when the cost of

741
00:40:03.679 --> 00:40:06.439
a mistake, um, is more significant, then you should

742
00:40:06.439 --> 00:40:09.550
be more skeptical. Well, that only makes sense, um,

743
00:40:09.560 --> 00:40:12.760
on the assumption, which I think everyone's making, but

744
00:40:12.760 --> 00:40:15.919
few are even bothering to state that, um, the

745
00:40:15.919 --> 00:40:18.239
only relevant kind of mistake you could be worried

746
00:40:18.239 --> 00:40:21.419
about is the mistake of a bad belief. So,

747
00:40:21.629 --> 00:40:23.790
um, and, and you know, that example that I

748
00:40:23.790 --> 00:40:26.590
gave was sort of designed to elicit that set

749
00:40:26.590 --> 00:40:30.199
of assumptions. You're, you're standing there with your kid,

750
00:40:30.379 --> 00:40:32.790
considering whether to cross the bridge, considering whether the

751
00:40:32.790 --> 00:40:37.199
bridge is safe. And um in many normal circumstances,

752
00:40:37.449 --> 00:40:40.250
um, uh, you know, if you're deciding should I

753
00:40:40.250 --> 00:40:44.469
believe, should I not believe, um, if you, um,

754
00:40:44.530 --> 00:40:48.479
if you believe when you were wrong, um, then,

755
00:40:48.739 --> 00:40:50.610
uh, the bridge might break on you and you'll

756
00:40:50.610 --> 00:40:53.530
get seriously injured, you and your kid, um, and

757
00:40:53.530 --> 00:40:55.270
that's You know, that's the bad thing that people

758
00:40:55.270 --> 00:40:58.300
worry about. Um, AND if you make the converse

759
00:40:58.300 --> 00:40:59.939
mistake, which many people don't even register as a

760
00:40:59.939 --> 00:41:02.979
possible mistake, the mistake of suspending judgment when actually

761
00:41:02.979 --> 00:41:06.250
there wasn't good enough evidence to believe, um, then,

762
00:41:06.260 --> 00:41:07.739
well, you know, you didn't get to cross the

763
00:41:07.739 --> 00:41:09.580
bridge. Maybe you have to walk around or maybe

764
00:41:09.580 --> 00:41:11.219
you just go a different way, and it's like

765
00:41:11.219 --> 00:41:12.860
not that big a deal. And there's plenty of

766
00:41:12.860 --> 00:41:16.689
cases that are like that. Um, BUT assuming that

767
00:41:16.689 --> 00:41:19.610
all cases are like that is in effect, um,

768
00:41:19.689 --> 00:41:25.969
to assume, um, that like, um, the, um, the

769
00:41:25.969 --> 00:41:28.209
thing that will happen if you don't form a

770
00:41:28.209 --> 00:41:31.689
belief, which is typically, you know, the thing that

771
00:41:31.689 --> 00:41:35.000
we kind of interpret as not acting, letting things,

772
00:41:35.209 --> 00:41:37.969
uh, proceed as they are, um, if we assume

773
00:41:37.969 --> 00:41:40.649
that that's always gonna be fine, then that negative

774
00:41:40.649 --> 00:41:44.330
bias makes sense. Um, SO this is the sense

775
00:41:44.330 --> 00:41:47.169
in which I think, um, this negative bias in

776
00:41:47.169 --> 00:41:51.169
epistemology tends to perpetuate a certain kind of status

777
00:41:51.169 --> 00:41:54.689
quo bias, a certain kind of conservatism, um, um,

778
00:41:54.699 --> 00:41:58.385
uh, if, um, the only thing you're worried. WORRIED

779
00:41:58.385 --> 00:42:01.544
about is the worry that when you intervene, you're

780
00:42:01.544 --> 00:42:03.844
gonna like mess things up um based on a

781
00:42:03.844 --> 00:42:07.074
bad belief that you had, um, then that motivates

782
00:42:07.264 --> 00:42:09.985
um being more skeptical. But if, as is often

783
00:42:09.985 --> 00:42:12.854
the case, you should be worried about what happens

784
00:42:12.854 --> 00:42:16.850
if you don't do something. Um, IF, um, the

785
00:42:16.850 --> 00:42:21.169
status quo is not a great situation, and, um,

786
00:42:21.209 --> 00:42:22.969
failing to form the belief that you should have

787
00:42:22.969 --> 00:42:25.800
wasn't merely a missed opportunity to do something good,

788
00:42:26.040 --> 00:42:29.280
um, but, um, something that is actually quite costly,

789
00:42:29.600 --> 00:42:33.850
um, um, then, um, the sort of incentive structure

790
00:42:33.850 --> 00:42:35.689
is reversed, and I argue in the book that

791
00:42:35.689 --> 00:42:37.889
if you want to be a pragmatic encroachment theorist

792
00:42:37.889 --> 00:42:41.580
or moral encroachment theorist, then in those cases, Um,

793
00:42:41.820 --> 00:42:44.350
um, you should, um, if you want to be

794
00:42:44.350 --> 00:42:46.729
an encroachment theorist, you should do it symmetrically. So

795
00:42:46.729 --> 00:42:48.610
in some cases, like the ones people focus on,

796
00:42:48.840 --> 00:42:51.780
um, uh, the higher stakes, um, or the moral

797
00:42:51.780 --> 00:42:55.580
cost should push you toward, um, um, skepticism. But

798
00:42:55.580 --> 00:42:59.379
in other cases where the, the, um, the bad

799
00:42:59.379 --> 00:43:01.179
thing that might happen is what happens if you

800
00:43:01.179 --> 00:43:03.379
don't. Believe, um, then maybe if you want to

801
00:43:03.379 --> 00:43:05.939
be an encroachment theorist, you should think that the

802
00:43:05.939 --> 00:43:08.060
moral or practical pressures should push you away from

803
00:43:08.060 --> 00:43:11.010
skepticism towards belief. So there should be cases where,

804
00:43:11.219 --> 00:43:16.250
um, these, uh, pragmatic or moral considerations, um, um,

805
00:43:16.459 --> 00:43:20.179
lower the evidential bar, um, for permissible belief or

806
00:43:20.179 --> 00:43:23.340
even for required belief. Um, SO like, you know,

807
00:43:23.379 --> 00:43:24.540
what are the kinds of cases that might be

808
00:43:24.540 --> 00:43:28.479
like that? Well, um, Um, I think one set

809
00:43:28.479 --> 00:43:33.159
of examples, um, uh, uh, come up pretty naturally

810
00:43:33.159 --> 00:43:38.000
from some of the, um, like, um, agnotology, motivated

811
00:43:38.000 --> 00:43:43.120
ignorance, literature, um, um, manufactured skepticism about kind of,

812
00:43:43.370 --> 00:43:46.600
um, big, complicated, harmful stuff that we need to,

813
00:43:46.719 --> 00:43:49.685
uh, that we need to, um, react. In dramatic

814
00:43:49.685 --> 00:43:52.364
ways to, and climate change is like the um

815
00:43:52.364 --> 00:43:54.284
is a famous example and a and a really

816
00:43:54.284 --> 00:43:59.834
great example um so corporate interests are, um, uh,

817
00:43:59.844 --> 00:44:03.294
you know, the, the um fossil fuel industry um

818
00:44:03.485 --> 00:44:06.554
has an interest in, uh, uh, the status quo

819
00:44:06.764 --> 00:44:12.645
even though it's, um, seriously, um, um, contributing to

820
00:44:12.645 --> 00:44:15.725
suffering uh in the world through climate change, um.

821
00:44:16.360 --> 00:44:20.479
And, um, uh, you know, shifting our energy infrastructure,

822
00:44:20.729 --> 00:44:24.649
um, away from, um, these, you know, carbon-emitting fuel

823
00:44:24.649 --> 00:44:29.669
sources, um. It is complicated and difficult and requires

824
00:44:29.909 --> 00:44:36.350
um tremendous um political motivation and energy um and

825
00:44:36.350 --> 00:44:40.830
a really effective way to stop that from happening

826
00:44:40.830 --> 00:44:43.939
if you're the fossil fuel industry is to make

827
00:44:43.939 --> 00:44:46.510
people feel not quite sure about it, to cause

828
00:44:46.510 --> 00:44:49.669
doubt. Um, AND, uh, you know, the fact that

829
00:44:49.669 --> 00:44:51.270
there's all this negative bias around that I'm talking

830
00:44:51.270 --> 00:44:54.060
about, um, makes that easier in lots of ways.

831
00:44:54.280 --> 00:44:58.939
Um. Um, BUT given the practical situation that we're

832
00:44:58.939 --> 00:45:04.050
in, um, uh, the cost of the status quo

833
00:45:04.050 --> 00:45:07.280
is tremendous, um, the cost of like not doing

834
00:45:07.280 --> 00:45:09.159
anything, and that's what will happen if you continue

835
00:45:09.159 --> 00:45:12.810
to suspend judgment. Um, SO maybe this is the

836
00:45:12.810 --> 00:45:16.159
kind of consideration where the kind of situation, um,

837
00:45:16.169 --> 00:45:18.929
where if you believe in pragmatic encroachment, moral or

838
00:45:18.929 --> 00:45:23.479
moral encroachment, um, uh, that encroachment should actually make

839
00:45:23.479 --> 00:45:26.919
you less skeptical, not more, um, because it's, uh,

840
00:45:27.040 --> 00:45:29.300
because like the, the, the more harmful. The whole

841
00:45:29.300 --> 00:45:32.500
thing is um letting us stay on the course

842
00:45:32.500 --> 00:45:35.179
that we're on right now. Um, OTHER quite different

843
00:45:35.179 --> 00:45:36.580
kinds of cases that come up in the literature,

844
00:45:36.699 --> 00:45:39.379
um, you know, I mentioned one reason people have

845
00:45:39.379 --> 00:45:40.780
been interested in moral encroachment has to do with

846
00:45:40.780 --> 00:45:45.939
like questions about respect. Um, SO, um, if you

847
00:45:45.939 --> 00:45:51.100
would, um, um, this comes up especially in like

848
00:45:51.100 --> 00:45:52.929
some of the literature on like beliefs based on

849
00:45:52.929 --> 00:45:58.370
statistical generalizations. So if someone believes, um, Uh, if

850
00:45:58.370 --> 00:46:01.639
someone knows that, you know, most of the, um,

851
00:46:01.790 --> 00:46:05.209
women in this office are part of the administrative

852
00:46:05.209 --> 00:46:10.689
staff, um, as opposed to, um, um, lawyers, um.

853
00:46:11.270 --> 00:46:14.899
Um, AND you see a woman, uh, maybe on

854
00:46:14.899 --> 00:46:18.830
statistical grounds, uh, uh, that might like tempt you

855
00:46:18.830 --> 00:46:20.669
to believe that this is probably a staff member,

856
00:46:20.750 --> 00:46:22.989
not a lawyer, um, but some people said, oh

857
00:46:22.989 --> 00:46:26.340
no, that's, that would be like disrespectful, even though,

858
00:46:26.600 --> 00:46:28.629
um, the evidence supports it. Both sides of that

859
00:46:28.629 --> 00:46:29.949
are controversial. I'm not sure what I think about

860
00:46:29.949 --> 00:46:31.590
it, but I, but I'm just mention this as

861
00:46:31.590 --> 00:46:37.790
a, as an illustration. Um, um, um, uh. If

862
00:46:37.790 --> 00:46:40.350
that's right, then I think there are also gonna

863
00:46:40.350 --> 00:46:43.580
be cases where you disrespect someone by suspending judgment.

864
00:46:43.699 --> 00:46:45.419
And actually we do know this, like in one

865
00:46:45.419 --> 00:46:47.179
domain, it's not super connected to this literature for

866
00:46:47.179 --> 00:46:49.260
whatever reason, um, but one place where this comes

867
00:46:49.260 --> 00:46:51.939
up, um, um, in the epistemic literature all the

868
00:46:51.939 --> 00:46:55.899
time is, um, in testimonial injustice, right? So, uh,

869
00:46:55.939 --> 00:47:00.340
testimonial injustice, um, um, from Miranda Fricker's book of

870
00:47:00.340 --> 00:47:04.989
Epistemic injustice, um, happens when, uh, Um, you give

871
00:47:04.989 --> 00:47:08.429
someone undue doubts and, uh, so someone, you know,

872
00:47:08.550 --> 00:47:11.310
tells you something and you give them the credibility

873
00:47:11.310 --> 00:47:12.629
to it. You don't believe them as much as

874
00:47:12.629 --> 00:47:15.229
like they deserve to be believed. Um, WELL, that's,

875
00:47:15.429 --> 00:47:19.260
that's being, you know, um, too skeptical. And, uh,

876
00:47:19.270 --> 00:47:21.270
Fricker points out, uh, which is, you know, common

877
00:47:21.270 --> 00:47:24.219
sense, um, that that's not merely an epistemic mistake.

878
00:47:24.429 --> 00:47:27.429
Um, THERE'S something disrespectful about that. There's a kind

879
00:47:27.429 --> 00:47:29.870
of moral injustice happening here, um, and I think

880
00:47:29.870 --> 00:47:32.860
that's right. Um, WELL, if you're a moral encroachment

881
00:47:32.860 --> 00:47:35.699
theorist who thinks that, you know, um, um, the

882
00:47:35.699 --> 00:47:40.419
moral status of your dtastic choices, um, should like

883
00:47:40.419 --> 00:47:44.979
push you toward the thing that, um, um, doesn't

884
00:47:44.979 --> 00:47:48.939
do the moral injury, then testimonial injustice should also

885
00:47:48.939 --> 00:47:51.629
push you for like uh very similar kinds of

886
00:47:51.629 --> 00:47:55.649
uh respect engaging reasons towards belief. Now, yeah, I'm,

887
00:47:55.830 --> 00:47:58.510
I'm kind of suspicious about this whole, this whole

888
00:47:58.510 --> 00:48:00.790
thing. I, I think really if I, uh, if

889
00:48:00.790 --> 00:48:01.909
you want to know what's deep in my heart,

890
00:48:01.989 --> 00:48:04.949
I think probably both moral encroachment and pragmatic encroachment

891
00:48:04.949 --> 00:48:07.239
are, are not correct. Um, BUT in the book,

892
00:48:07.270 --> 00:48:08.830
what I argue is that if you want to

893
00:48:08.830 --> 00:48:11.510
believe in these things, um, don't just believe on

894
00:48:11.510 --> 00:48:14.000
the skeptical side, um, sometimes they push the other

895
00:48:14.000 --> 00:48:15.750
direction, um, as well.

896
00:48:17.530 --> 00:48:20.000
So, now let me ask you just before we

897
00:48:20.000 --> 00:48:24.919
get into epistemic courage, what is epistemic faith and

898
00:48:24.919 --> 00:48:28.790
what would be some illustrative examples where epistemic faith

899
00:48:28.790 --> 00:48:31.260
is proper and necessary?

900
00:48:31.800 --> 00:48:35.790
OK. Um. So, yeah, faith is another one of

901
00:48:35.790 --> 00:48:37.149
these words. People use it to mean a lot

902
00:48:37.149 --> 00:48:41.590
of different things. Um, um, I do think Um,

903
00:48:43.250 --> 00:48:46.510
It's a mistake to think of faith as being

904
00:48:46.889 --> 00:48:49.649
sort of extra evidential by definition. You know, some

905
00:48:49.649 --> 00:48:51.969
people talk about, oh yeah, faith is, uh, faith

906
00:48:51.969 --> 00:48:54.649
is when you believe something that um that there's

907
00:48:54.649 --> 00:48:56.939
not evidence for, um, and you know I I

908
00:48:56.939 --> 00:48:58.649
do think some faith is like that, some misplaced

909
00:48:58.649 --> 00:49:02.209
faith is like that, um, but, um, but I

910
00:49:02.209 --> 00:49:06.250
don't think that's the, that's the general criterion and

911
00:49:06.250 --> 00:49:12.250
here's why, um. I mentioned, I mentioned before that

912
00:49:12.530 --> 00:49:18.489
um. One of the traits I'm really interested in

913
00:49:18.820 --> 00:49:22.530
is this trait of, uh, uh, you know, um,

914
00:49:22.540 --> 00:49:24.179
being confident at the right time at the right

915
00:49:24.179 --> 00:49:26.250
times and doubting at the right times, and that,

916
00:49:26.290 --> 00:49:32.429
and that's hard, um, um. There are many things

917
00:49:32.790 --> 00:49:36.070
um that can make us uh that can tempt

918
00:49:36.070 --> 00:49:39.830
us to doubt even when um that doubt would

919
00:49:39.830 --> 00:49:43.830
be a mistake. Um, um, THIS is what happens

920
00:49:44.149 --> 00:49:47.899
when you do have enough evidence, um, but something

921
00:49:47.899 --> 00:49:50.580
makes you feel not quite sure. There's a mismatch

922
00:49:50.580 --> 00:49:54.300
between like the psychological certainty and the epistemic status.

923
00:49:54.469 --> 00:49:56.489
And this can happen in all kinds of domains.

924
00:49:56.590 --> 00:49:59.520
I, I, uh, in my chapter. On faith, um,

925
00:49:59.600 --> 00:50:01.719
I, I really try to pull from a really

926
00:50:01.719 --> 00:50:05.040
wide variety of, uh, deeply different kinds of examples.

927
00:50:05.350 --> 00:50:08.409
So, uh, you know, one kind of example, um,

928
00:50:08.639 --> 00:50:11.959
um, just has to do with, um, uh, the

929
00:50:11.959 --> 00:50:16.040
dialectic with radical skeptics. So, you know, as people

930
00:50:16.040 --> 00:50:19.479
who have studied epistemology know, um, like refuting the

931
00:50:19.479 --> 00:50:22.310
skeptic on their own terms without begging central questions,

932
00:50:22.560 --> 00:50:26.239
um, is extremely difficult, and many, uh, epistemologists, myself

933
00:50:26.239 --> 00:50:28.520
included, think that um there's a sense in which

934
00:50:28.520 --> 00:50:32.100
that's not possible. Um, BUT that doesn't make us

935
00:50:32.100 --> 00:50:36.350
skeptics, um, that makes us say, uh, yeah, sometimes

936
00:50:36.350 --> 00:50:39.389
the only correct thing is the question begging thing,

937
00:50:39.600 --> 00:50:43.199
um, um, and, uh, you know, that's a, that's

938
00:50:43.199 --> 00:50:48.100
a Sort of, um, epistemically disquieting in fact, um,

939
00:50:48.179 --> 00:50:49.770
but I think it is a normative epistemic fact.

940
00:50:49.810 --> 00:50:52.330
I, I, I, I do think that it's correct

941
00:50:52.330 --> 00:50:56.199
to rely on some like, um, question that argue,

942
00:50:56.370 --> 00:50:58.360
for example, like I'm a, I'm a, I'm a

943
00:50:58.360 --> 00:51:01.699
neoorian about perception. I, I think that like GE

944
00:51:01.699 --> 00:51:03.250
Moore's proof the external world is a pretty good

945
00:51:03.250 --> 00:51:05.530
proof, um, you know, you see hands out hands,

946
00:51:05.570 --> 00:51:09.860
therefore there are um external, um, objects blatantly question

947
00:51:09.860 --> 00:51:12.780
begging against certain kinds of skeptics, um, or idealists,

948
00:51:12.850 --> 00:51:17.409
which was, um. Which was more target, um. Um,

949
00:51:17.489 --> 00:51:20.570
BUT still I think epistemically appropriate. So when you

950
00:51:20.570 --> 00:51:23.689
focus on the like, um, the question beggings of

951
00:51:23.689 --> 00:51:26.969
it, um, that feels sort of disappointing, like we,

952
00:51:27.010 --> 00:51:30.679
we like to have arguments that um would compel

953
00:51:30.679 --> 00:51:34.350
our interlocutors, um. Um, I think, you know, this,

954
00:51:34.459 --> 00:51:37.219
this is not uncontroversial, uh, in this instance, but

955
00:51:37.219 --> 00:51:41.419
I think that, um, uh, the, the correct response

956
00:51:41.419 --> 00:51:45.100
to that felt uncertainty, um, is to get over

957
00:51:45.100 --> 00:51:47.350
it. Uh, I think that, uh, yeah, you know,

958
00:51:47.540 --> 00:51:49.699
it, it would be nice if I had an

959
00:51:49.699 --> 00:51:52.459
argument that, um, would convince someone who didn't have

960
00:51:52.459 --> 00:51:54.219
some of my starting points, but the fact that

961
00:51:54.219 --> 00:51:56.219
I don't isn't a reason for me to doubt

962
00:51:56.219 --> 00:51:58.489
myself. That's, that's what I think about external perception,

963
00:51:58.780 --> 00:52:01.300
um, and, you know, that's generally what kind of

964
00:52:01.300 --> 00:52:04.530
new Marines think about that. Um. Um, LET me

965
00:52:04.530 --> 00:52:07.010
give a couple more pretty different kinds of examples.

966
00:52:07.050 --> 00:52:14.189
Um, um. Fundamental, um, reasoning, logic, a um a

967
00:52:14.189 --> 00:52:19.429
priori investigation. Um, HERE it's maybe more obvious that

968
00:52:19.429 --> 00:52:23.500
like the justification of some of our foundational logical,

969
00:52:23.709 --> 00:52:26.989
uh, reasoning, um, would have to be question begging

970
00:52:26.989 --> 00:52:29.159
against someone who doubted it because you can't reason

971
00:52:29.159 --> 00:52:32.570
about anything without reasoning, so. If you're justifying reasoning,

972
00:52:32.739 --> 00:52:34.020
um, you're gonna have a reason to do it.

973
00:52:34.139 --> 00:52:36.219
So like, you know, you can, uh why is

974
00:52:36.219 --> 00:52:40.560
inferring according to modus opponents, um, OK. Well, I,

975
00:52:40.659 --> 00:52:42.699
I can give, like, uh, I can give a

976
00:52:42.699 --> 00:52:44.429
soundness proof of that reasoning. I can, I can

977
00:52:44.429 --> 00:52:47.629
argue for why, um, inferring according to this method

978
00:52:47.629 --> 00:52:49.179
will never take you from something true to something

979
00:52:49.179 --> 00:52:52.500
false. Um, BUT, um, that explanation that I gave.

980
00:52:52.580 --> 00:52:55.169
It's gonna use conditional reasoning. It's gonna have moduponents

981
00:52:55.169 --> 00:52:58.360
in it. Um, uh, WHEN, when you notice that,

982
00:52:58.649 --> 00:53:00.860
um, that can also make you feel bad. And

983
00:53:00.860 --> 00:53:03.370
a lot of the like, um, uh, epistemology of

984
00:53:03.370 --> 00:53:05.929
the a priori, um, is about, OK, so like

985
00:53:05.929 --> 00:53:07.889
what, what do we do instead, um, or do

986
00:53:07.889 --> 00:53:09.600
we, you know, learn to live with that badness.

987
00:53:09.810 --> 00:53:11.719
Again, kind of like my, you know, Mauren stance

988
00:53:11.719 --> 00:53:15.879
about perception. I, I, um. Um, I learned to

989
00:53:15.879 --> 00:53:18.110
live with that kind of, um, feeling of disappointment.

990
00:53:18.179 --> 00:53:19.620
I don't think it actually is a genuine kind

991
00:53:19.620 --> 00:53:23.139
of epistemic badness. I think for, um, general structural

992
00:53:23.139 --> 00:53:28.340
reasons, um, um, uh, the constraints that you should

993
00:53:28.340 --> 00:53:29.860
never be, you should never be allowed to rely

994
00:53:29.860 --> 00:53:31.820
on something that, um, a certain kind of luck

995
00:53:31.820 --> 00:53:34.770
you wouldn't wouldn't accept, um, it is just a,

996
00:53:34.860 --> 00:53:37.540
it's just a false, um, principle. It's obvious, it

997
00:53:37.540 --> 00:53:40.540
leads immediately to very radical skepticism if you take

998
00:53:40.540 --> 00:53:44.610
it in generality, um. But I also think something

999
00:53:44.610 --> 00:53:46.399
very similar to this happens in extremely different kinds

1000
00:53:46.399 --> 00:53:48.409
of cases. So those were like kind of, uh,

1001
00:53:48.489 --> 00:53:50.979
you know, traditional epistemology kinds of cases, foundational epistemology

1002
00:53:50.979 --> 00:53:53.330
kind of stuff. Um, I think this happens in

1003
00:53:53.330 --> 00:53:58.000
like more socially interesting cases too. Um, SO, um,

1004
00:53:58.290 --> 00:54:03.250
think about gaslighting. Um, SO when, so gaslighting works

1005
00:54:03.250 --> 00:54:07.929
by, um, sort of, um, pressuring someone into doubting

1006
00:54:08.159 --> 00:54:11.570
something that they have excellent reason to believe, um.

1007
00:54:12.169 --> 00:54:15.090
Uh, YOU know, uh, you know, in the, well,

1008
00:54:15.370 --> 00:54:16.290
you know, why don't, why don't we take the

1009
00:54:16.290 --> 00:54:22.510
actual, um, Um, um, um, eponymous, um, example, um,

1010
00:54:22.520 --> 00:54:25.030
from the, from the play in the movie Gaslight,

1011
00:54:25.310 --> 00:54:27.969
um, the protagonist there, you know, sees the lights

1012
00:54:27.969 --> 00:54:32.139
dimming and her abusive husband, um, uh, basically tells

1013
00:54:32.139 --> 00:54:34.919
her, no, don't trust your eyes, uh, you're wrong

1014
00:54:34.919 --> 00:54:39.879
about that, um, and, um, uh, encourages her to

1015
00:54:39.879 --> 00:54:43.790
doubt herself for his own selfish reasons. Um, OK,

1016
00:54:43.949 --> 00:54:47.159
so, um, uh, uh, and gas lighters, you know,

1017
00:54:47.510 --> 00:54:54.750
uh, have many, um, Skills, um, they use, um,

1018
00:54:54.909 --> 00:55:00.429
um, psychological, um, Knowledge to, you know, poke people

1019
00:55:00.429 --> 00:55:03.629
in ways that do tend to um successfully cause

1020
00:55:03.629 --> 00:55:06.649
this kind of doubt. That said, uh, you know,

1021
00:55:06.810 --> 00:55:09.570
humans aren't robots. There's not like a button that

1022
00:55:09.570 --> 00:55:11.810
you can push that just kind of, um, is

1023
00:55:11.810 --> 00:55:14.120
the doubt button that's guaranteed to cause someone to

1024
00:55:14.290 --> 00:55:17.439
to doubt and give up. Um, WHEN you're, when,

1025
00:55:17.449 --> 00:55:24.879
when someone is, um, um, if someone is exposed

1026
00:55:24.879 --> 00:55:30.209
to a gaslighting attempt, um, they, uh, they have

1027
00:55:30.209 --> 00:55:33.290
some, some of their psychology is relevant for what's

1028
00:55:33.290 --> 00:55:37.770
gonna happen, uh, as a result. Um, AND sometimes,

1029
00:55:37.979 --> 00:55:42.340
um, someone tries to gaslight you and you resist

1030
00:55:42.340 --> 00:55:44.899
it. You, you can kind of feel the, you

1031
00:55:44.899 --> 00:55:46.699
can feel the impulse cause they're, they're they're skilled

1032
00:55:46.699 --> 00:55:48.969
at pushing buttons that really do kind of make

1033
00:55:48.969 --> 00:55:51.780
you question yourself, um, but here's a possible thing

1034
00:55:51.780 --> 00:55:54.639
that could happen. Um, YOU think about it, you,

1035
00:55:54.719 --> 00:55:57.600
you see, oh, should I really, do I really

1036
00:55:57.600 --> 00:55:59.810
remember that clearly? Should I trust myself about this?

1037
00:56:00.080 --> 00:56:02.679
Um, A possible thing that you can do, and

1038
00:56:02.679 --> 00:56:04.439
sometimes a good thing that you can do is

1039
00:56:04.439 --> 00:56:07.159
to answer yes, you can resist the gaslighting. That

1040
00:56:07.159 --> 00:56:09.850
I think, um, is also a clear instance of

1041
00:56:09.850 --> 00:56:13.560
epistemic faith. Um, ALL of these cases are cases

1042
00:56:13.560 --> 00:56:17.639
where you have some sort of like natural felt

1043
00:56:17.639 --> 00:56:22.540
dissatisfaction with your um epistemic investigation, um, what, um,

1044
00:56:22.560 --> 00:56:26.889
Jennifer Nagle calls epistemic anxiety. Um, AND sometimes when

1045
00:56:26.889 --> 00:56:29.199
you have epistemic anxiety, that's a good clue that

1046
00:56:29.199 --> 00:56:31.709
you should, you should keep investigating. But sometimes when

1047
00:56:31.709 --> 00:56:34.750
you have epistemic anxiety, um, uh, that's just cause

1048
00:56:34.750 --> 00:56:36.510
you're being anxious even though you, you had plenty

1049
00:56:36.510 --> 00:56:39.229
of evidence. Um, AND when you rely on your

1050
00:56:39.229 --> 00:56:42.909
own judgments, um, even though, um, you have this

1051
00:56:42.909 --> 00:56:46.429
kind of feeling, um, maybe manufactured or maybe just

1052
00:56:46.429 --> 00:56:48.989
kind of because of the general structure of the

1053
00:56:48.989 --> 00:56:52.830
dialectic of, uh, dissatisfaction of uncertainty, um, even though

1054
00:56:52.830 --> 00:56:55.149
you have that feeling, you can say, no, I

1055
00:56:55.149 --> 00:56:57.909
have enough evidence, I'm gonna trust it and you

1056
00:56:57.909 --> 00:57:01.300
can be right about that. When that happens, um,

1057
00:57:01.540 --> 00:57:06.899
you are, um, exhibiting proper epistemic faith. Epistemic faith

1058
00:57:06.899 --> 00:57:10.060
is when you believe despite this kind of, um,

1059
00:57:10.070 --> 00:57:13.060
temptation toward a feeling of uncertainty, and, you know,

1060
00:57:13.320 --> 00:57:14.780
sometimes that's bad that you shouldn't, you shouldn't put

1061
00:57:14.780 --> 00:57:17.379
epistemic faith, um, in things that you don't have

1062
00:57:17.379 --> 00:57:20.500
enough evidence for. Um, BUT sometimes, uh, sometimes you

1063
00:57:20.500 --> 00:57:22.979
do have evidence and you should continue to have

1064
00:57:22.979 --> 00:57:25.139
faith anyway. Um, GIVE me just one more example.

1065
00:57:25.179 --> 00:57:26.840
This one's like from the cover of my book,

1066
00:57:27.060 --> 00:57:30.149
um, this is the Grand Canyon Skywalk. Um, SO,

1067
00:57:30.429 --> 00:57:32.790
um, uh, it's a, it's a bridge over the

1068
00:57:32.790 --> 00:57:36.020
Grand Canyon that has a transparent bottom. And, uh,

1069
00:57:36.030 --> 00:57:38.530
when people, you know, walk out over it, um,

1070
00:57:38.629 --> 00:57:41.580
they have plenty of evidence that it's perfectly safe.

1071
00:57:41.889 --> 00:57:44.629
Um, uh, uh, YOU know, it's been manufactured very

1072
00:57:44.629 --> 00:57:45.989
carefully and people go over it all the time,

1073
00:57:46.030 --> 00:57:48.830
and, you know, it's never fallen while someone's on

1074
00:57:48.830 --> 00:57:50.820
it. It's just not a thing that happens. Um,

1075
00:57:51.070 --> 00:57:53.800
BUT there are very normal psychological mechanisms. When you

1076
00:57:53.800 --> 00:57:55.120
walk out and you look down and you see

1077
00:57:55.120 --> 00:57:56.949
the Grand Canyon and like stretched up below you,

1078
00:57:57.199 --> 00:58:01.909
um, that tends to make you, um, uh, feel

1079
00:58:02.239 --> 00:58:05.600
afraid. um, AND, you know, there's, there's, you know,

1080
00:58:05.840 --> 00:58:07.879
there, there's fear of falling, but that can get

1081
00:58:07.879 --> 00:58:09.620
into your belief too, right? And sometimes that can

1082
00:58:09.620 --> 00:58:12.139
motivate you to, to doubt whether it's safe. That,

1083
00:58:12.149 --> 00:58:14.560
that might make you say, oh, maybe I should

1084
00:58:14.560 --> 00:58:18.320
like, um, um, watch 10 more people go. Before

1085
00:58:18.320 --> 00:58:19.929
I'm confident about it, that I think would be

1086
00:58:19.929 --> 00:58:23.280
incorrect. You have plenty of evidence already, um, um,

1087
00:58:23.570 --> 00:58:29.409
trusting, um, your, um, assessment of the situation, um,

1088
00:58:29.530 --> 00:58:31.330
which was correct in the first place and based

1089
00:58:31.330 --> 00:58:36.120
on good evidence, um, uh, despite the existence of,

1090
00:58:36.290 --> 00:58:40.280
you know, reasonable pressure towards doubt, um, is, uh,

1091
00:58:40.290 --> 00:58:43.209
when epistemic faith is good. Sorry, that was a

1092
00:58:43.209 --> 00:58:44.370
long answer to, but yeah.

1093
00:58:45.709 --> 00:58:49.479
So let's talk then about epistemic courage, the central

1094
00:58:49.479 --> 00:58:52.550
topic of the book. What is epistemic courage then

1095
00:58:52.550 --> 00:58:55.239
and how does it relate to moral

1096
00:58:55.239 --> 00:59:01.060
courage? OK. So actually having just explained, um, epistemic

1097
00:59:01.060 --> 00:59:04.669
faith, um, it's relatively simple to kind of state

1098
00:59:04.669 --> 00:59:07.679
what epistemic courage is in my view. Um, EPISTEMIC

1099
00:59:07.679 --> 00:59:13.429
courage, epistemic courage is the tendency to um have

1100
00:59:13.429 --> 00:59:18.199
epistemic faith in the right circumstances. Um, SO I

1101
00:59:18.199 --> 00:59:20.510
mentioned at the beginning, I think of it on

1102
00:59:20.510 --> 00:59:24.270
the kind of Aristotelian virtue model, um, there's a

1103
00:59:24.270 --> 00:59:29.929
mean between two vices. So, um, um, there's one

1104
00:59:29.929 --> 00:59:33.689
epistemic vice of epistemic recklessness where you're not paying

1105
00:59:33.689 --> 00:59:35.530
attention to to the evidence and you're just kind

1106
00:59:35.530 --> 00:59:39.399
of believing things, uh, without regard for whether, um,

1107
00:59:39.530 --> 00:59:43.500
whether it's a bad belief, um. And, um, and

1108
00:59:43.500 --> 00:59:45.540
you know, that's the thing that like mainstreamistemology is

1109
00:59:45.540 --> 00:59:48.419
mostly focused on. Don't be epistemically reckless. Um, BUT

1110
00:59:48.419 --> 00:59:51.209
there's an opposite vice that hasn't gotten enough attention,

1111
00:59:51.419 --> 00:59:54.100
and that's what I call epistemic cowardice, um, and

1112
00:59:54.100 --> 00:59:59.699
that's when you, um, you don't have epistemic faith

1113
00:59:59.699 --> 01:00:02.824
in the cases when you should. Um, ANYTIME. Um,

1114
01:00:02.905 --> 01:00:05.225
YOUR belief is challenged, or anytime you notice the

1115
01:00:05.225 --> 01:00:08.504
respect in which, uh, you know, you, um, could

1116
01:00:08.504 --> 01:00:12.074
have investigated more, you feel like, oh, I shouldn't

1117
01:00:12.074 --> 01:00:14.254
make up my mind. I should continue suspending judgment.

1118
01:00:14.425 --> 01:00:16.735
I just don't feel sure about that. Um, THAT'S,

1119
01:00:16.824 --> 01:00:18.864
uh, that's the mistake of kind of bad suspending

1120
01:00:18.864 --> 01:00:20.905
judgment, and that's the mistake that like my book

1121
01:00:20.905 --> 01:00:22.735
is trying to, it's trying to be uh corrective

1122
01:00:22.735 --> 01:00:27.219
about. Um, SO epistemic courage is, um, is the

1123
01:00:27.219 --> 01:00:31.219
virtuous mean between the two. SO someone has epistemic

1124
01:00:31.219 --> 01:00:34.580
courage to the extent to which, um, they don't

1125
01:00:34.580 --> 01:00:37.139
make either one of those mistakes. Uh, AND this

1126
01:00:37.139 --> 01:00:41.219
is very parallel to, um, you know, at least

1127
01:00:41.219 --> 01:00:45.020
like an Aristotelian understanding of moral courage. So moral

1128
01:00:45.020 --> 01:00:51.929
courage is, um, uh, is, um, you know, Feeling

1129
01:00:51.929 --> 01:00:56.489
and being guided by um the correct uh amount

1130
01:00:56.489 --> 01:00:59.820
of fear. Um, YOU don't want to be reckless

1131
01:00:59.820 --> 01:01:01.810
and, you know, be doing stupid dangerous things all

1132
01:01:01.810 --> 01:01:05.320
the time. Uh, THAT'S not courage, that's recklessness. Um,

1133
01:01:05.489 --> 01:01:08.409
YOU, you also don't want to be, um, cowardly

1134
01:01:08.689 --> 01:01:11.199
and never take any chances. Or never do anything

1135
01:01:11.199 --> 01:01:13.280
that you like, uh, you know, feel might be

1136
01:01:13.280 --> 01:01:15.080
a little bit dangerous. Uh, YOU want to have

1137
01:01:15.080 --> 01:01:18.030
a well calibrated sense of what you're capable of,

1138
01:01:18.280 --> 01:01:22.120
and you want to, um, subject yourself to the,

1139
01:01:22.360 --> 01:01:25.159
um, the correct to the amount of danger that's

1140
01:01:25.159 --> 01:01:27.080
kind of justified by the practical situations that you're

1141
01:01:27.080 --> 01:01:31.610
in. Um, THAT'S what moral encourages. Epistemic courage is

1142
01:01:31.610 --> 01:01:33.929
uh just the analog to that in the Dotastic

1143
01:01:33.929 --> 01:01:39.729
realm. Um, SO you aren't too skeptical, you aren't

1144
01:01:39.729 --> 01:01:43.360
too gullible, um, you have a good accurate sense

1145
01:01:43.360 --> 01:01:47.020
of your own, um, dotastic capacities, um, and the

1146
01:01:47.020 --> 01:01:50.659
evidential situation such that, um, you are good at

1147
01:01:50.659 --> 01:01:53.090
recognizing those points where you know enough to form

1148
01:01:53.090 --> 01:01:55.250
a belief. Mhm.

1149
01:01:55.989 --> 01:01:59.469
So, an illustrative example of what you explored in

1150
01:01:59.469 --> 01:02:04.439
the book in regards to negative epistemic bias, skepticism,

1151
01:02:04.469 --> 01:02:08.510
and epistemic faith is the case of testimony and

1152
01:02:08.510 --> 01:02:12.070
rape culture. Could you tell us about it and

1153
01:02:12.070 --> 01:02:15.959
how we can achieve epistemic courage in this particular

1154
01:02:15.959 --> 01:02:16.310
case?

1155
01:02:17.229 --> 01:02:20.100
Good, thanks. Yeah, that's an extended case study that

1156
01:02:20.110 --> 01:02:21.860
that I that I used late in the book.

1157
01:02:21.949 --> 01:02:25.110
And um in some ways, it's a, it's a

1158
01:02:25.110 --> 01:02:27.429
really good example. In some ways it's, it's, it's

1159
01:02:27.429 --> 01:02:30.169
a more complicated in some ways. I guess I

1160
01:02:30.169 --> 01:02:31.389
don't want to say it's a bad example, but

1161
01:02:31.389 --> 01:02:34.989
it's it's a complicated example, um, um, because the

1162
01:02:34.989 --> 01:02:37.899
things that I want to say about it, um,

1163
01:02:38.629 --> 01:02:42.149
well, are complicated, um, uh, part of the reason,

1164
01:02:42.159 --> 01:02:46.229
um, that I use that example is as a

1165
01:02:46.229 --> 01:02:51.100
historical matter, um, thinking about testimony and rape culture,

1166
01:02:51.310 --> 01:02:54.110
um, is kind of what got me, um, to

1167
01:02:54.110 --> 01:02:55.429
this project in the first place in a way

1168
01:02:55.429 --> 01:02:57.500
it was a generalization of it. So maybe I'll,

1169
01:02:57.590 --> 01:02:59.979
um, I'll trace you through some of my own

1170
01:02:59.979 --> 01:03:02.790
um um history of reasoning through these kinds of

1171
01:03:02.790 --> 01:03:07.639
things. So I, I started um a number of

1172
01:03:07.639 --> 01:03:11.510
years ago, a project on testimony and skepticism and

1173
01:03:11.510 --> 01:03:15.469
rape culture, um, um, just based on this kind

1174
01:03:15.469 --> 01:03:20.949
of, um, relatively simple, um, um, and good but

1175
01:03:20.949 --> 01:03:27.870
incomplete observation that, um. That quite often um people

1176
01:03:27.870 --> 01:03:32.939
respond with undue skepticism to sexual assault complaints. And,

1177
01:03:33.020 --> 01:03:37.179
uh, uh, you know, this was, um, this was

1178
01:03:37.179 --> 01:03:40.820
made super famous um um during the Me Too

1179
01:03:40.820 --> 01:03:44.459
movement. Um, THE profession of academic philosophy got a

1180
01:03:44.459 --> 01:03:45.860
little bit of a head start a few years

1181
01:03:45.860 --> 01:03:48.419
before that, we had some high profile cases of,

1182
01:03:48.459 --> 01:03:52.020
um, uh, you know, um, students or former students

1183
01:03:52.020 --> 01:03:55.060
discussing misconduct by, by their professors and um some

1184
01:03:55.060 --> 01:03:56.800
of that discourse was like playing out in the

1185
01:03:56.800 --> 01:04:00.570
philosophy blogs in ways that, um, seems to me.

1186
01:04:01.014 --> 01:04:04.935
Both kind of um morally harmful in obvious ways,

1187
01:04:05.094 --> 01:04:07.695
but epistemically interesting. People were, you know, this, this

1188
01:04:07.695 --> 01:04:09.685
rhetoric of, oh, you know, people are innocent until

1189
01:04:09.685 --> 01:04:11.614
proven guilty or it's a he said she said

1190
01:04:11.614 --> 01:04:14.655
situation and you know, we shouldn't jump to conclusions,

1191
01:04:14.935 --> 01:04:17.064
um, you know, I, I, I saw that that

1192
01:04:17.064 --> 01:04:20.604
was um harmful. I saw that it was like

1193
01:04:20.604 --> 01:04:23.135
epistemically wrong, but absolutely wrong, epistemically wrong in a

1194
01:04:23.135 --> 01:04:25.054
kind of interesting way, which made me want to

1195
01:04:25.054 --> 01:04:28.175
like um uh dig into that in, in more

1196
01:04:28.175 --> 01:04:31.270
detail. And so my early work on that topic,

1197
01:04:31.510 --> 01:04:36.669
um, was really, um, about, um, you know, moving

1198
01:04:36.669 --> 01:04:40.310
away from the negative mistake there. Basically, I noticed,

1199
01:04:40.340 --> 01:04:44.270
so people are making complaints that are credible, that,

1200
01:04:44.350 --> 01:04:47.939
you know, amount to really strong evidence, um. But,

1201
01:04:47.949 --> 01:04:51.590
um, too many people are, uh, being skeptical about

1202
01:04:51.590 --> 01:04:55.189
them, um, motivated in part by deference to the

1203
01:04:55.189 --> 01:04:57.949
status quo, deference to existing power, um, you know,

1204
01:04:58.110 --> 01:04:59.550
not wanting to rock the boat, things like that.

1205
01:04:59.629 --> 01:05:02.590
And those, those themes are definitely, um, uh, still

1206
01:05:02.590 --> 01:05:04.750
major themes in, in like the, the book that

1207
01:05:04.750 --> 01:05:08.520
I've written here. Um, THE big thing that changed,

1208
01:05:08.750 --> 01:05:13.399
um, and complexified the story for me, um. Between

1209
01:05:13.399 --> 01:05:15.790
some of my earlier work on that topic and

1210
01:05:15.800 --> 01:05:17.729
and the chapter that I've written in this book,

1211
01:05:18.060 --> 01:05:25.699
um, is, um, an appreciation for, um, especially the,

1212
01:05:25.739 --> 01:05:31.520
the Intersectional complexity of um questions about um sexual

1213
01:05:31.520 --> 01:05:36.560
assault and testimony and credibility. Um, SO, um uh

1214
01:05:36.560 --> 01:05:40.929
one big piece of the problem, um, was that

1215
01:05:40.929 --> 01:05:43.649
kind of undue skepticism, um, and I was focusing

1216
01:05:43.649 --> 01:05:46.685
on these cases where you have You know, these,

1217
01:05:46.774 --> 01:05:49.854
these were, these are kind of um the cases

1218
01:05:49.854 --> 01:05:52.334
that are um most prevalent in the public imagination,

1219
01:05:52.614 --> 01:05:54.804
uh, where you have kind of a, a high

1220
01:05:54.804 --> 01:05:59.125
status individual, typically a man um who is running

1221
01:05:59.125 --> 01:06:02.094
um a subordinate or a student or someone in

1222
01:06:02.094 --> 01:06:07.004
a, in a position of of relative, um, powerlessness,

1223
01:06:07.334 --> 01:06:09.965
um, and, and I think in those cases the

1224
01:06:09.965 --> 01:06:11.385
dynamic does play out and basically the way that

1225
01:06:11.385 --> 01:06:14.000
it was focused on, um. But as I was

1226
01:06:14.000 --> 01:06:19.939
reading more about um rape culture and testimony um

1227
01:06:19.939 --> 01:06:22.929
and credibility, I was reminded that there's a really

1228
01:06:22.929 --> 01:06:26.149
important sort of other side of this, um, uh,

1229
01:06:26.159 --> 01:06:30.399
which is that, um, in some cases, um, that

1230
01:06:30.399 --> 01:06:37.080
people are extremely credulous about, um, misconduct allegations, and

1231
01:06:37.080 --> 01:06:41.245
those are cases where um the uh Um, the,

1232
01:06:41.284 --> 01:06:45.245
the power and the privilege, um, um, are, are

1233
01:06:45.245 --> 01:06:47.054
flipped from the kinds of cases, the examples that

1234
01:06:47.054 --> 01:06:49.485
I talked about earlier. Um, SO, uh, you know,

1235
01:06:49.534 --> 01:06:52.524
the, the most obvious instance of this, um, is

1236
01:06:52.524 --> 01:06:59.364
the, um, historical um prevalence of, um, uh, of,

1237
01:06:59.604 --> 01:07:04.485
um, you know, credulous and violent responses to white

1238
01:07:04.485 --> 01:07:08.520
women accusing black men of sexual misconduct. um. Uh,

1239
01:07:08.719 --> 01:07:10.959
ESPECIALLY like, you know, in the American 20th century,

1240
01:07:11.280 --> 01:07:13.840
uh, early 20th century, um, but, um, you know,

1241
01:07:14.000 --> 01:07:17.550
much more broadly as well. Um. So this made

1242
01:07:17.550 --> 01:07:21.219
me realize and this kind of um uh helps

1243
01:07:21.550 --> 01:07:25.989
um the chapter that was developing um fit into

1244
01:07:25.989 --> 01:07:28.389
some of the dialect about courage as a sort

1245
01:07:28.389 --> 01:07:32.830
of wisdom requiring Aristotelian mean. Um, THIS made it

1246
01:07:32.830 --> 01:07:37.989
really vivid that uh the solution to the undue

1247
01:07:37.989 --> 01:07:42.524
skepticism that I. NOTICING about um sexual misconduct allegations

1248
01:07:42.955 --> 01:07:45.675
can't be anything remotely as simple as like turning

1249
01:07:45.675 --> 01:07:48.675
up the credibility dial. Um, WE won't make things

1250
01:07:48.675 --> 01:07:53.554
better if we just kind of um uh reflexively

1251
01:07:53.554 --> 01:07:56.955
increase the degree to which we um uh um

1252
01:07:56.955 --> 01:08:03.149
believe all um sexual misconduct allegations, um. That that

1253
01:08:03.149 --> 01:08:04.750
might make some of the cases that I started

1254
01:08:04.750 --> 01:08:06.830
by thinking about um better um but it would

1255
01:08:06.830 --> 01:08:09.949
make these other kinds of cases um worse, um,

1256
01:08:10.030 --> 01:08:13.010
with sexual assault testimony just like anything else, um,

1257
01:08:13.149 --> 01:08:15.389
uh, you need, you need to pay attention to

1258
01:08:15.389 --> 01:08:18.149
both sides. You need to, uh, not have bad

1259
01:08:18.149 --> 01:08:19.890
beliefs and you need to not have bad suspect

1260
01:08:19.890 --> 01:08:23.430
or judgment, um, and you know, both things are

1261
01:08:23.430 --> 01:08:26.350
happening, um, uh, both things are happening like quite

1262
01:08:26.350 --> 01:08:32.310
a bit. So, um, reflecting on, um, the intersectional

1263
01:08:32.310 --> 01:08:35.830
complexity of these questions about, um, what degree of

1264
01:08:35.830 --> 01:08:39.709
credibility should be offered, um, really drove home to

1265
01:08:39.709 --> 01:08:46.118
me how um How subtle um the um the

1266
01:08:46.118 --> 01:08:48.828
virtue of epistemic courage has to be. Like really,

1267
01:08:49.389 --> 01:08:52.957
if the virtue is, um, um, being credible at

1268
01:08:52.957 --> 01:08:55.278
the right time, being credible when you should, and

1269
01:08:55.278 --> 01:08:59.828
doubting um when you should doubt, um, that requires,

1270
01:09:00.038 --> 01:09:03.599
um, a lot of um social understanding that requires

1271
01:09:03.599 --> 01:09:06.957
sensitivity to these intersectional considerations and that requires, um,

1272
01:09:07.078 --> 01:09:09.188
you know, attention to the particulars of the situation

1273
01:09:09.398 --> 01:09:12.198
that one is in. Uh, THERE'S no like, you

1274
01:09:12.198 --> 01:09:14.999
know, one simple trick to, to doing a good

1275
01:09:14.999 --> 01:09:18.108
job here. Um, um, AND, and, you know, in

1276
01:09:18.108 --> 01:09:20.559
this, in this particular example, like I was mentioning,

1277
01:09:20.599 --> 01:09:22.758
I think mistakes on both sides are quite common.

1278
01:09:22.868 --> 01:09:25.358
There, you know, there's cowardice in some circumstances, there's

1279
01:09:25.358 --> 01:09:27.999
recklessness in other circumstances, and both of those are

1280
01:09:27.999 --> 01:09:32.358
really serious moral harms. Um, SO courage is, um,

1281
01:09:32.519 --> 01:09:35.568
courage is difficult. Courage is, uh, requires a lot

1282
01:09:35.568 --> 01:09:37.278
of knowledge, it requires a lot of wisd. It

1283
01:09:37.278 --> 01:09:38.948
requires a lot of attention to these kinds of

1284
01:09:38.948 --> 01:09:40.627
complexities. I think it requires a lot of, uh,

1285
01:09:40.798 --> 01:09:44.758
um, you know, understanding of the ideological forces that

1286
01:09:44.758 --> 01:09:47.127
are at play, the patriarchal ones, the white supremacist

1287
01:09:47.127 --> 01:09:51.158
ones, um, and, um, uh, that, that's kind of

1288
01:09:51.158 --> 01:09:54.548
what it takes to do a good job recognizing

1289
01:09:54.548 --> 01:09:55.999
when there is and there is not enough evidence

1290
01:09:55.999 --> 01:09:56.639
to form belief.

1291
01:09:58.169 --> 01:10:00.339
So I would like to talk a little bit

1292
01:10:00.339 --> 01:10:03.899
about the context now before we get into the

1293
01:10:03.899 --> 01:10:07.060
last topic of our conversation today. So that was

1294
01:10:07.060 --> 01:10:11.180
about the idea of epistemic contextualism or the idea

1295
01:10:11.180 --> 01:10:14.419
that the language we use to talk about knowledge

1296
01:10:14.419 --> 01:10:19.649
is context sensitive and how it contributes to truth

1297
01:10:19.649 --> 01:10:20.220
conditions.

1298
01:10:20.950 --> 01:10:24.359
Good, yeah. So, uh, this is something that I

1299
01:10:24.359 --> 01:10:27.229
would, I focused on a lot earlier in my

1300
01:10:27.229 --> 01:10:28.870
career, and my, my previous book before this one

1301
01:10:29.069 --> 01:10:32.189
was really all about epistemic contextualism. Um, uh, SO

1302
01:10:32.189 --> 01:10:35.950
epistemic contextualism is, um, it's a linguistic view. It's

1303
01:10:35.950 --> 01:10:38.310
the idea that the language we use to talk

1304
01:10:38.310 --> 01:10:41.709
about epistemic matters and, and I, like many contextualists

1305
01:10:41.709 --> 01:10:46.229
focus on the word nose, um, um, is context

1306
01:10:46.229 --> 01:10:50.689
sensitive. So language context sensitive, that means that in

1307
01:10:50.689 --> 01:10:56.040
different conversational contexts, um, we use it to um

1308
01:10:56.330 --> 01:11:01.410
um contribute importantly different things to um the meaning

1309
01:11:01.410 --> 01:11:04.970
of what is said. So, um, uh, to take,

1310
01:11:05.040 --> 01:11:07.620
uh, you know, the simplest kinds of examples of

1311
01:11:07.620 --> 01:11:11.979
context sensitive language that everybody understands are indexicals, words

1312
01:11:11.979 --> 01:11:15.250
like I or you or here or now. Um,

1313
01:11:15.419 --> 01:11:19.419
THESE are all context-sensitive words, um, which means that,

1314
01:11:19.620 --> 01:11:22.899
um, you know, uh, uh, if two different people

1315
01:11:22.899 --> 01:11:25.419
say the, the same sentence, you know, if I,

1316
01:11:25.500 --> 01:11:30.299
if I say, um, um. I wrote a book

1317
01:11:30.299 --> 01:11:33.859
called Epistemic Courage. Um, WHAT I, what I say,

1318
01:11:33.899 --> 01:11:36.290
if I say that sentence is something true, namely

1319
01:11:36.290 --> 01:11:39.220
that that I, Jonathan, wrote a book called Epistemic

1320
01:11:39.220 --> 01:11:42.370
Courage. Um, IF you uttered that very same sentence,

1321
01:11:42.580 --> 01:11:45.379
you'd be saying something quite different, um, and unless

1322
01:11:45.379 --> 01:11:46.859
you have a different book that I don't know

1323
01:11:46.859 --> 01:11:50.459
about, it would be false. Um, um, SO the

1324
01:11:50.459 --> 01:11:53.060
word I is a context sensitive word. It contributes

1325
01:11:53.060 --> 01:11:54.939
to the truth conditions of sentences in different ways

1326
01:11:54.939 --> 01:11:58.879
in different conversational contexts. Epistemic contextualists, and I'm one

1327
01:11:58.879 --> 01:12:01.870
of them, um, think that the word nose is

1328
01:12:02.120 --> 01:12:05.470
in one interesting sense, similar to the word I,

1329
01:12:05.640 --> 01:12:09.850
um, in this respect. Uh, SO, um, I think

1330
01:12:09.850 --> 01:12:14.850
that, um, in different conversational contexts, um, the word

1331
01:12:14.850 --> 01:12:19.689
knows expresses an epistemic relation that requires like different

1332
01:12:19.689 --> 01:12:24.319
kinds of standards for, um, for evidence, basically. So,

1333
01:12:24.600 --> 01:12:28.754
um, there are very skeptical conversational. Contexts where we

1334
01:12:28.754 --> 01:12:31.194
use the word nose in a way that's quite

1335
01:12:31.194 --> 01:12:33.875
strong such that it's difficult to count as uh

1336
01:12:33.875 --> 01:12:38.214
as as knowing um and uh in other contexts,

1337
01:12:38.475 --> 01:12:40.234
um, we use it in a much more lax

1338
01:12:40.234 --> 01:12:43.194
way. This is uh a bit analogous, maybe another,

1339
01:12:43.475 --> 01:12:45.475
another analogy that would be useful to to put

1340
01:12:45.475 --> 01:12:47.754
on the table, um, is the way that um

1341
01:12:48.299 --> 01:12:52.850
Gradeable adjectives work. So, um, you know, tall is

1342
01:12:52.850 --> 01:12:56.149
a gradable adjective. Um, HOW tall do you have

1343
01:12:56.149 --> 01:12:58.700
to be to count as tall, um, is also

1344
01:12:58.700 --> 01:13:00.660
a matter that is sort of settled from context

1345
01:13:00.660 --> 01:13:03.109
to context. Um, SO if you're talking, you know,

1346
01:13:03.540 --> 01:13:07.339
famously, if you're talking about basketball players, someone who's

1347
01:13:07.339 --> 01:13:09.859
6' 3 might not count as tall, but if

1348
01:13:09.859 --> 01:13:13.140
you're talking about philosophers, someone who's 6'3 is a

1349
01:13:13.140 --> 01:13:16.919
tall person. Um. Uh, OK, so, so I think

1350
01:13:16.919 --> 01:13:21.640
contextualism, um, is, um, is also true about the

1351
01:13:21.640 --> 01:13:26.439
word nose, um, which means, um, that, uh, you

1352
01:13:26.439 --> 01:13:30.600
know, um, sentences like, um, oh I don't know,

1353
01:13:30.640 --> 01:13:34.870
um. Um, uh, WHAT'S a, what's a good example?

1354
01:13:34.950 --> 01:13:40.319
Um, I, um, I know that my, um, dog

1355
01:13:40.319 --> 01:13:48.270
is downtown right now. Um, um. Whether that sentence

1356
01:13:48.569 --> 01:13:53.419
is true um might depend on how skeptical a

1357
01:13:53.419 --> 01:13:57.250
conversation um we're having. Um, SO, you know, I

1358
01:13:57.250 --> 01:13:59.729
do have quite strong evidence that she's downtown, um,

1359
01:13:59.970 --> 01:14:05.009
um, um, but, um, you know, something surprising might

1360
01:14:05.009 --> 01:14:07.770
have happened, um, you know, I haven't, I, I

1361
01:14:07.770 --> 01:14:09.200
haven't looked at my phone in the last hour,

1362
01:14:09.419 --> 01:14:12.330
um, so, so, you know, um, maybe, uh, maybe

1363
01:14:12.330 --> 01:14:17.810
someone took her away, um, um. Um, SO in

1364
01:14:17.810 --> 01:14:22.200
some, in some conversational contexts that might be um

1365
01:14:22.330 --> 01:14:25.729
a serious enough, uh, you know, counter possibility for

1366
01:14:25.729 --> 01:14:28.319
me to not count as knowing, and others, um,

1367
01:14:28.330 --> 01:14:33.169
it doesn't. That by itself, um, uh, you know,

1368
01:14:33.410 --> 01:14:35.330
everything I've just been saying in response to, to

1369
01:14:35.330 --> 01:14:39.640
your question, um, um, has been, um, a bit

1370
01:14:39.640 --> 01:14:42.089
like orthogonal to the main themes of, of my

1371
01:14:42.089 --> 01:14:46.209
book, um, but it's, it's important background for the

1372
01:14:46.209 --> 01:14:50.129
last chapter of my book, which, um, which points

1373
01:14:50.129 --> 01:14:53.000
to ways in which if you're an epistemic contextualist,

1374
01:14:53.290 --> 01:14:56.689
um, that negative bias can can have harmful effects

1375
01:14:56.689 --> 01:15:00.149
at the linguistic level. So if you think as

1376
01:15:00.149 --> 01:15:02.350
I do, that there can be more or less

1377
01:15:02.350 --> 01:15:07.060
skeptical conversational contexts, um, uh, and that can influence

1378
01:15:07.060 --> 01:15:12.149
whether sentences ascribing knowledge, um, are actually true or

1379
01:15:12.149 --> 01:15:16.990
not. Then, um, there's a sort of, um, additional

1380
01:15:16.990 --> 01:15:18.500
way addition to the ones I was talking about

1381
01:15:18.500 --> 01:15:20.770
way back at the beginning, in which the like

1382
01:15:21.020 --> 01:15:25.819
the ideological negative bias and epistemology can like, um,

1383
01:15:25.859 --> 01:15:28.779
can make itself be true. Um, IT can cause

1384
01:15:28.779 --> 01:15:33.850
people to speak in more skeptical contexts, um, um,

1385
01:15:34.020 --> 01:15:41.040
and accurately deny that someone knows something, um. Even

1386
01:15:41.049 --> 01:15:44.709
though they could have, and maybe they should have

1387
01:15:45.100 --> 01:15:48.740
spoken in a less skeptical context um where um

1388
01:15:48.740 --> 01:15:51.660
they would have counted as knowing. And the reason

1389
01:15:51.669 --> 01:15:54.140
that can be, that can make a practical difference

1390
01:15:54.750 --> 01:15:58.770
is that your choice of an episthetic standard um

1391
01:15:59.310 --> 01:16:02.189
suggests in, in some way and uh you know,

1392
01:16:02.229 --> 01:16:04.490
I'm neutral about this about just the mechanics in

1393
01:16:04.490 --> 01:16:07.549
the book. My my previous book, um, contextualizing Knowledge

1394
01:16:07.549 --> 01:16:10.470
has has a particular proposal, but, um, but I

1395
01:16:10.470 --> 01:16:13.910
think everyone should agree that choosing a more skeptical

1396
01:16:13.910 --> 01:16:19.479
conversational standard, um, somehow communicates the idea. That like

1397
01:16:19.479 --> 01:16:22.180
that threshold for knowledge is the one that matters.

1398
01:16:22.399 --> 01:16:27.509
Um, AND, um, and if it, if that's false,

1399
01:16:27.669 --> 01:16:30.759
if actually your practical situation was one where a

1400
01:16:30.759 --> 01:16:34.649
lower standard would have been appropriate. Um, THEN you

1401
01:16:34.649 --> 01:16:36.740
can, you can say, hey, you don't know that.

1402
01:16:37.259 --> 01:16:40.990
And you can express the truth because contextualism um

1403
01:16:40.990 --> 01:16:44.069
has this kind of flexibility. We accommodate the um

1404
01:16:44.069 --> 01:16:49.060
the linguistic um uh conversational context, um, in order

1405
01:16:49.060 --> 01:16:50.970
to make the sentence come out as being true.

1406
01:16:51.220 --> 01:16:54.990
um, BUT you're sending this false message that, you

1407
01:16:54.990 --> 01:16:57.459
know, you shouldn't do this thing, uh, you should,

1408
01:16:57.520 --> 01:17:00.899
um, uh, maybe even you shouldn't believe, um, um,

1409
01:17:00.910 --> 01:17:03.750
because you're sending the message that, um, the threshold

1410
01:17:03.750 --> 01:17:06.419
for appropriate belief is higher than it actually is.

1411
01:17:07.049 --> 01:17:09.410
Um, AND one interesting thing about that is it

1412
01:17:09.410 --> 01:17:14.000
means that, um, uh, it's another way in which

1413
01:17:14.250 --> 01:17:16.089
the bias is stealthy, it's another way in which

1414
01:17:16.089 --> 01:17:19.330
the problem can hide, because you can't solve this

1415
01:17:19.330 --> 01:17:23.250
problem by um focusing on the truth of what

1416
01:17:23.250 --> 01:17:26.700
people say. If you complain about someone making a

1417
01:17:26.700 --> 01:17:31.100
too skeptical utterance, um, if, uh, contextual, if epidemic

1418
01:17:31.100 --> 01:17:33.410
contextualism is right, um, if you complain about someone

1419
01:17:33.700 --> 01:17:35.220
choosing a bar for knowledge that's too high and

1420
01:17:35.220 --> 01:17:37.620
saying, oh, you don't know that, um, they have

1421
01:17:37.620 --> 01:17:41.819
this really rhetorically powerful retort available, namely, well, no,

1422
01:17:41.899 --> 01:17:44.915
what I said is true. Um, AND, and according

1423
01:17:44.915 --> 01:17:47.694
to like contextualism, they might well be right about

1424
01:17:47.694 --> 01:17:50.854
that. Um, SO to explain what went wrong there,

1425
01:17:51.024 --> 01:17:53.424
you have to, um, you have to kind of

1426
01:17:53.424 --> 01:17:54.544
get under the hood of the language a bit.

1427
01:17:54.584 --> 01:17:56.825
You have to understand contextualism and you have to

1428
01:17:56.825 --> 01:17:59.674
understand the ways that these pragmatic, um, ideas get

1429
01:17:59.865 --> 01:18:04.229
conveyed, um. And since these things are, uh, you

1430
01:18:04.229 --> 01:18:06.589
know, not obvious, not widely understood. I mean, you

1431
01:18:06.589 --> 01:18:08.990
know, they're, it's controversial even though they're true, but

1432
01:18:08.990 --> 01:18:11.180
I think they're true. Um, CERTAINLY since it's controversial,

1433
01:18:11.189 --> 01:18:13.629
that means, you know, not everyone, um, knows that

1434
01:18:13.629 --> 01:18:17.759
they're true. um. Um, uh, THAT, that, that's another

1435
01:18:17.759 --> 01:18:23.069
respect in which, um, this, uh, negative bias can

1436
01:18:23.069 --> 01:18:27.709
operate to perpetuate itself kind of, um, in this,

1437
01:18:27.839 --> 01:18:29.839
in this hidden way, and that's another respect in

1438
01:18:29.839 --> 01:18:31.799
which in which it's ideological.

1439
01:18:32.850 --> 01:18:37.399
OK, so on that precise topic, what is contextual

1440
01:18:37.399 --> 01:18:40.569
injustice and how does it relate to the ways

1441
01:18:40.569 --> 01:18:42.839
the negative bias can play out?

1442
01:18:43.169 --> 01:18:45.970
All right, good. Um, SO yeah, contextual injustice is

1443
01:18:45.970 --> 01:18:49.450
my label for um doing that bad kind of

1444
01:18:49.450 --> 01:18:52.259
thing I was just describing. So contextual injustice is

1445
01:18:52.259 --> 01:18:58.859
what happens when someone picks a conversational standard, when

1446
01:18:58.859 --> 01:19:02.129
someone sets the the parameters in the context, um,

1447
01:19:02.299 --> 01:19:04.939
in a way that's, you know, that successfully influences

1448
01:19:04.939 --> 01:19:08.970
the truth conditions of, of their utterances, um, but,

1449
01:19:09.180 --> 01:19:13.140
um, but is unjust, um. And why might it

1450
01:19:13.140 --> 01:19:18.100
be unjust? Well, um, um, one way, uh, you

1451
01:19:18.100 --> 01:19:20.500
know, the most obvious kinds of ways for my

1452
01:19:20.500 --> 01:19:24.339
purposes in the book, um, are by communi communicating

1453
01:19:24.339 --> 01:19:28.580
the idea that, um, uh, there's, you know, not

1454
01:19:28.580 --> 01:19:31.220
enough evidence for, for some particular purpose by setting

1455
01:19:31.220 --> 01:19:34.129
a high threshold for knowledge. Um, SO maybe, uh,

1456
01:19:34.180 --> 01:19:36.779
maybe thinking more about testimonial and justice cases would

1457
01:19:36.779 --> 01:19:41.169
be, um, would be eliminating here. Um, IF, um,

1458
01:19:41.870 --> 01:19:45.830
If um someone tells me, oh actually we can

1459
01:19:45.830 --> 01:19:47.069
go to a, we can go to, we can

1460
01:19:47.069 --> 01:19:50.810
go to a sexual assault allegation if um If

1461
01:19:50.810 --> 01:19:57.240
someone tells me, um. That um there, uh, someone

1462
01:19:57.240 --> 01:20:00.160
comes to me as department head and says that

1463
01:20:00.160 --> 01:20:02.750
their a student comes and says that their instructor,

1464
01:20:03.080 --> 01:20:09.529
um, uh, sexually harassed them. Um, IF, um, and

1465
01:20:09.529 --> 01:20:12.509
they, and they give me, um, you know, um,

1466
01:20:12.529 --> 01:20:15.500
uh, a fair amount of evidence, uh, corroborating that,

1467
01:20:15.770 --> 01:20:18.129
um, but, you know, given the nature of these

1468
01:20:18.129 --> 01:20:19.310
things, you know, they didn't show me a video

1469
01:20:19.310 --> 01:20:22.080
of it happening, um, it's the kind of situation

1470
01:20:22.330 --> 01:20:25.830
where, um, uh, you know, it's certainly possible and

1471
01:20:25.830 --> 01:20:28.470
natural to, um, to have doubt and have questions.

1472
01:20:28.709 --> 01:20:30.959
Um, BUT let's imagine a version of the case

1473
01:20:31.209 --> 01:20:34.115
where, um, I have enough evidence where It would

1474
01:20:34.115 --> 01:20:38.814
be reasonable for me to believe, um, and, um,

1475
01:20:38.845 --> 01:20:43.995
and I could, um, uh, very reasonably speak um

1476
01:20:43.995 --> 01:20:47.745
in conversational contexts where I would count as knowing

1477
01:20:47.875 --> 01:20:51.910
that that um sexual harassment um has occurred. If

1478
01:20:51.910 --> 01:20:57.020
instead I pick a higher standard for knowledge, um,

1479
01:20:57.149 --> 01:20:59.870
if I say, uh, uh, you know, if I

1480
01:20:59.870 --> 01:21:01.970
speak in a context where like, you know, testimony

1481
01:21:01.970 --> 01:21:04.990
in general can't provide knowledge, um, I, you know,

1482
01:21:05.029 --> 01:21:07.229
I think contexts like that are possible. Um, YOU

1483
01:21:07.229 --> 01:21:09.509
know, I'd say, oh well, you know, I, I,

1484
01:21:09.629 --> 01:21:12.069
I know that you've told me that, um, but

1485
01:21:12.069 --> 01:21:14.270
since I wasn't there, I don't have first personal

1486
01:21:14.270 --> 01:21:17.270
knowledge, uh, you know, there's possible scenarios where you

1487
01:21:17.270 --> 01:21:18.950
could have been lying, so I don't really know

1488
01:21:18.950 --> 01:21:22.500
whether that's true. Um. According to contextualism, I, I

1489
01:21:22.500 --> 01:21:26.009
could say all those things and express truths. Um,

1490
01:21:26.299 --> 01:21:28.819
BUT I think under the circumstances, you know, depending

1491
01:21:28.819 --> 01:21:30.609
on exactly how this, this case is spelled out,

1492
01:21:30.899 --> 01:21:34.930
um, there's possible versions of this story, um, where

1493
01:21:35.180 --> 01:21:37.379
it would be morally wrong for me to use

1494
01:21:37.379 --> 01:21:41.129
a standard like that. Um, IT would communicate, uh,

1495
01:21:41.140 --> 01:21:45.970
distrust of the student, it would, um, tend to,

1496
01:21:46.180 --> 01:21:50.990
um, it would tend to suggest. Um, IN action,

1497
01:21:51.200 --> 01:21:52.600
um, so I'm not gonna do anything about it

1498
01:21:52.600 --> 01:21:54.720
because I don't know whether it's true. Um, AND

1499
01:21:54.720 --> 01:21:57.279
I think that that is, that would be an

1500
01:21:57.279 --> 01:22:00.509
abdication of my responsibilities. Um, I think that, um,

1501
01:22:00.680 --> 01:22:02.629
I owe it to the student in this scenario,

1502
01:22:02.879 --> 01:22:06.560
um, to, uh, take the testimony seriously, and part

1503
01:22:06.560 --> 01:22:07.600
of the way that I would do that is

1504
01:22:07.600 --> 01:22:10.330
by choosing an appropriate contextual standard so that we

1505
01:22:10.330 --> 01:22:12.350
can talk about it in a way that uh

1506
01:22:12.350 --> 01:22:19.359
communicates, um, that, um, that it's credible. Um, SO

1507
01:22:19.359 --> 01:22:23.520
epidemic uh contextual injustice is unjustly choosing the wrong

1508
01:22:23.520 --> 01:22:26.669
contextual parameter. Um, YOU can do that with, uh,

1509
01:22:26.680 --> 01:22:28.669
the bar for knowledge if you're contexious about knowledge.

1510
01:22:28.759 --> 01:22:30.439
It's a more general notion too, so I have

1511
01:22:30.439 --> 01:22:33.479
a, I, I have a, um, excuse me. I

1512
01:22:33.479 --> 01:22:38.770
have a paper um um that discusses contextual justice,

1513
01:22:39.069 --> 01:22:41.819
um, uh, with a couple of central examples. One

1514
01:22:41.819 --> 01:22:43.759
is the knowledge example that I focused on in

1515
01:22:43.759 --> 01:22:45.939
the book, um, but I also talk in that

1516
01:22:45.939 --> 01:22:50.859
paper about gender terms, um, so, um, uh, I,

1517
01:22:51.180 --> 01:22:54.299
I, I think there may be a, uh, a

1518
01:22:54.299 --> 01:22:57.299
strong case to be made that, um, gender terms.

1519
01:22:57.529 --> 01:23:01.060
Might also be context sensitive, um, in particular, there

1520
01:23:01.060 --> 01:23:05.649
might be trans-inclusive, um, ways of speaking and trans-exclusive

1521
01:23:05.649 --> 01:23:09.399
ways of speaking, um, and I think for, uh,

1522
01:23:09.410 --> 01:23:14.000
you know, general, uh, moral intersectional reasons that, um,

1523
01:23:14.009 --> 01:23:17.770
in, uh, um, most contexts, uh, maybe, maybe all

1524
01:23:17.770 --> 01:23:22.560
contexts, um, the, uh, trans inclusive. Ways are the

1525
01:23:22.560 --> 01:23:24.879
better ways to speak and that it's unjust to

1526
01:23:24.879 --> 01:23:27.870
uh speak in these other ways, um, but, um,

1527
01:23:28.200 --> 01:23:31.200
but I don't think it's impossible to use these

1528
01:23:31.200 --> 01:23:35.879
words to express um trans exclusionary, um, you know,

1529
01:23:36.009 --> 01:23:40.240
gender ideas. Um, I think, um, often that would

1530
01:23:40.240 --> 01:23:42.359
be an example of an injustice too, um, and

1531
01:23:42.359 --> 01:23:44.189
once you have the general framework, I think that,

1532
01:23:44.200 --> 01:23:48.939
um, um. Contextual injustice can happen in quite a

1533
01:23:48.939 --> 01:23:51.060
wide variety of areas. Only some of them have

1534
01:23:51.060 --> 01:23:52.819
to do with like the central themes about the

1535
01:23:52.819 --> 01:23:54.700
negative bias and epistemology here, which is why I

1536
01:23:54.700 --> 01:23:56.569
focus on the bar for knowledge in this book.

1537
01:23:58.060 --> 01:24:02.209
One last question, uh, how should we approach phenomena

1538
01:24:02.209 --> 01:24:06.979
like misinformation and conspiracy theories then, uh, and aren't

1539
01:24:06.979 --> 01:24:11.069
those cases where a negative bias should prevail?

1540
01:24:11.660 --> 01:24:15.850
Good. Uh, THIS is a very common thought. Um,

1541
01:24:16.089 --> 01:24:19.399
I, uh, to some degree, I want to resist

1542
01:24:19.399 --> 01:24:21.410
it. I mean, I, so, like I said at

1543
01:24:21.410 --> 01:24:25.729
the beginning, um, I do think, um, the negative

1544
01:24:25.729 --> 01:24:28.479
side is important, um, the positive side is too.

1545
01:24:28.689 --> 01:24:30.410
Uh, I focus on the positive side because the

1546
01:24:30.410 --> 01:24:35.750
negative side gets plenty of attention. Um. Um, And

1547
01:24:35.750 --> 01:24:40.750
I certainly think that there are examples of mistakes

1548
01:24:40.750 --> 01:24:43.379
that people make with respect to misinformation and conspiracy

1549
01:24:43.379 --> 01:24:46.189
theories, um, where focusing on the negative side of

1550
01:24:46.189 --> 01:24:48.620
the story is, is, uh, is the correct thing

1551
01:24:48.620 --> 01:24:52.029
to explain. Um, BUT I actually don't think that's

1552
01:24:52.029 --> 01:24:53.830
the norm. I don't think that's the typical case.

1553
01:24:54.629 --> 01:24:58.649
Um, uh, EVEN though, um, I recognize that it's

1554
01:24:58.649 --> 01:25:00.810
very natural to think that. So, so this, this

1555
01:25:00.810 --> 01:25:03.129
I think is, is one of the, uh, less

1556
01:25:03.129 --> 01:25:06.160
obvious or more surprising things that I think. Um,

1557
01:25:09.100 --> 01:25:12.060
In the public imagination, the problem with say conspiracy

1558
01:25:12.060 --> 01:25:16.600
theories, um, actually both conspiracy theories and misinformation, is

1559
01:25:16.600 --> 01:25:23.100
that people believe them too easily. Um, uh. When

1560
01:25:23.100 --> 01:25:25.459
you talk about like conspiracy theorists, like if you

1561
01:25:25.459 --> 01:25:27.100
look up definitions of like what it is to

1562
01:25:27.100 --> 01:25:30.740
be a conspiracy theorist, um, uh, you'll typically see,

1563
01:25:30.819 --> 01:25:32.580
oh, you know, it's someone who like believes in

1564
01:25:32.580 --> 01:25:36.100
these certain kinds of um of uh conspiracies despite

1565
01:25:36.100 --> 01:25:39.100
um they're not being um strong evidence for them.

1566
01:25:39.930 --> 01:25:45.779
Um, But in fact, when I look around at

1567
01:25:45.779 --> 01:25:50.259
the problems, the conspiracy theories and misinformation are raising,

1568
01:25:50.629 --> 01:25:53.100
um, I think that quite a lot of the

1569
01:25:53.100 --> 01:25:57.939
time the problems that they cause are um uh

1570
01:25:57.939 --> 01:26:03.810
problems of undue skepticism, not undue belief, um. It's

1571
01:26:03.810 --> 01:26:08.270
possible to read, um, you know, that, um, you

1572
01:26:08.270 --> 01:26:12.540
know that. COVID vaccines are doing a 5G thing

1573
01:26:12.540 --> 01:26:16.890
that's like, um, part of a nebulous global conspiracy.

1574
01:26:16.979 --> 01:26:18.580
It's possible to believe that and I'm sure some

1575
01:26:18.580 --> 01:26:20.540
people do believe that, and since there's not good

1576
01:26:20.540 --> 01:26:23.020
evidence for it, that that belief would be a

1577
01:26:23.020 --> 01:26:26.330
mistake of the kind that negativeismology is about, um,

1578
01:26:26.700 --> 01:26:29.660
but, um, by far the more common thing that

1579
01:26:29.660 --> 01:26:32.939
happens, I think it's just like every um vaccine

1580
01:26:32.939 --> 01:26:35.459
skeptic I've talked to that was worried. About these

1581
01:26:35.459 --> 01:26:39.339
kinds of things. Um, uh, THEY, they're thinking about

1582
01:26:39.339 --> 01:26:44.990
these, um, conspiracy theories in, um, a doubt kind

1583
01:26:44.990 --> 01:26:47.140
of framing. They say, well, I'm not really sure

1584
01:26:47.140 --> 01:26:51.569
what to think, um, but I've heard, um, uh,

1585
01:26:51.580 --> 01:26:56.129
you know, these bad theories, these bad hypotheses, and,

1586
01:26:56.140 --> 01:26:58.939
um, I've also heard, you know, the government telling

1587
01:26:58.939 --> 01:27:04.109
me that it's fine. Um, BUT like I'm just

1588
01:27:04.109 --> 01:27:07.270
not that comfortable putting something in my body without

1589
01:27:07.270 --> 01:27:11.069
being pretty sure that um it's not gonna, uh,

1590
01:27:11.109 --> 01:27:13.549
you know, turn me into a 5G zombie, um.

1591
01:27:14.479 --> 01:27:16.390
And if that's what people are saying, and I

1592
01:27:16.390 --> 01:27:19.189
think on the whole that is like the more

1593
01:27:19.189 --> 01:27:21.589
common thing that's out there, um, if that's what

1594
01:27:21.589 --> 01:27:24.200
people are saying, then uh that's a problem of

1595
01:27:24.200 --> 01:27:26.029
bad suspension of judgment, not a problem of bad

1596
01:27:26.029 --> 01:27:28.589
belief, because they're not believing they're they're like, uh,

1597
01:27:28.669 --> 01:27:30.189
you know, they're taking it more seriously than I

1598
01:27:30.189 --> 01:27:31.830
want them to, uh, but it's not they just

1599
01:27:31.830 --> 01:27:35.350
like accept that it's true, um, um, it's that

1600
01:27:35.350 --> 01:27:37.990
they're accepting it as a life hypothesis, but accepting

1601
01:27:37.990 --> 01:27:40.350
more life hypotheses that just is kind of keeping

1602
01:27:40.350 --> 01:27:44.310
the question open, um, and so. This is part

1603
01:27:44.310 --> 01:27:48.020
of why it's so important I think to uh

1604
01:27:48.020 --> 01:27:51.419
notice um that there's two sides to this and

1605
01:27:51.600 --> 01:27:54.549
and to uh two sides to epistemology and uh

1606
01:27:54.549 --> 01:27:56.990
recognize that it's important to focus on the positive

1607
01:27:56.990 --> 01:27:58.830
side, because if you're only focused on the negative,

1608
01:27:58.839 --> 01:28:01.169
the way that like so many people are. And

1609
01:28:01.169 --> 01:28:05.169
if you, to my mind, falsely believe that um

1610
01:28:05.169 --> 01:28:08.009
the problem with conspiracy theories is that people believe

1611
01:28:08.009 --> 01:28:10.709
them too easily. Um, THEN you're gonna reach for

1612
01:28:10.709 --> 01:28:12.950
a solution that pushes toward more skepticism. You're gonna

1613
01:28:12.950 --> 01:28:15.790
say, OK, hey everybody, um, make sure you don't

1614
01:28:15.790 --> 01:28:17.669
get duped. Make sure you like check your sources.

1615
01:28:17.790 --> 01:28:20.069
Make sure you like get more evidence, uh, before

1616
01:28:20.069 --> 01:28:22.629
you form beliefs. Um, AND for the kind of

1617
01:28:22.629 --> 01:28:25.509
person that I'm talking about who has heard these

1618
01:28:25.509 --> 01:28:28.620
theories and isn't sure what to think, um, and,

1619
01:28:28.629 --> 01:28:30.140
uh, just has kind of doubt about the whole

1620
01:28:30.140 --> 01:28:31.799
thing, and so wants to like, you know, quote

1621
01:28:31.799 --> 01:28:33.620
unquote, play it safe by not getting the vaccine,

1622
01:28:33.870 --> 01:28:36.020
for that kind of person, the hey, don't be

1623
01:28:36.020 --> 01:28:38.830
too gullible advice is exactly the opposite advice what

1624
01:28:38.830 --> 01:28:41.450
they need. Um, uh, CAUSE their problem in the

1625
01:28:41.450 --> 01:28:48.120
first place was, um, they, um, haven't, um, recognized

1626
01:28:48.370 --> 01:28:52.729
that there's compelling evidence, um, for, um, the kind

1627
01:28:52.729 --> 01:28:55.370
of, you know, standard story about the, uh, in

1628
01:28:55.370 --> 01:28:58.450
this case, the vaccines, um, being more skeptical would

1629
01:28:58.450 --> 01:29:03.740
make things worse, not better. Um. Um, Misinformation, this,

1630
01:29:03.859 --> 01:29:07.500
this may be even more conspicuous, uh, and again

1631
01:29:07.500 --> 01:29:11.089
I think, I think people just often don't, um,

1632
01:29:11.100 --> 01:29:16.680
quite, um, think clearly enough about, um, uh, or,

1633
01:29:16.720 --> 01:29:18.919
or just think enough about uh the mechanics of

1634
01:29:18.919 --> 01:29:21.270
how this works, you know, there, there's this, uh,

1635
01:29:21.279 --> 01:29:25.680
simple idea, oh, someone, uh, you know, saw a

1636
01:29:25.680 --> 01:29:30.520
story from a fake news source, and then they

1637
01:29:30.759 --> 01:29:33.120
believed it and that's the problem. And yeah, sometimes

1638
01:29:33.120 --> 01:29:36.319
that's the problem. Um, BUT a lot of the

1639
01:29:36.319 --> 01:29:40.560
misinformation, um, is there, I mean, and, and this

1640
01:29:40.560 --> 01:29:43.839
is really, you know, um, there's good sociological research

1641
01:29:43.839 --> 01:29:46.640
about, about, about this. It's intended not to be

1642
01:29:46.640 --> 01:29:49.560
believed, but to just kind of, uh, throw up

1643
01:29:49.560 --> 01:29:51.160
so much, so much chaff that like nobody knows

1644
01:29:51.160 --> 01:29:57.709
what to believe. Um, AND, um, and so, you

1645
01:29:57.709 --> 01:30:01.560
know, being flooded with with misinformation. Isn't just bad

1646
01:30:01.560 --> 01:30:04.729
because people will believe it. It's bad because um

1647
01:30:04.729 --> 01:30:07.169
it'll make people feel like they don't have any

1648
01:30:07.169 --> 01:30:09.330
way to know um what to believe and to

1649
01:30:09.330 --> 01:30:13.430
prevent them to believe from believing the truth. Of

1650
01:30:13.430 --> 01:30:15.930
course it all works very selectively, right? Um, SO,

1651
01:30:16.060 --> 01:30:21.540
so, um, uh, this, this, these strategic kind of

1652
01:30:21.930 --> 01:30:25.660
throw up all this, all this bullshit, um, um,

1653
01:30:26.189 --> 01:30:29.790
it's especially effective when, uh, you know, the thing

1654
01:30:29.790 --> 01:30:34.200
that's, um, That is being or belief is being

1655
01:30:34.200 --> 01:30:36.189
prevented is something that it would be like unpleasant

1656
01:30:36.189 --> 01:30:40.870
or inconvenient to believe anyway, um, um, so, uh,

1657
01:30:40.879 --> 01:30:44.089
you know, we, we use our skeptical instincts, um,

1658
01:30:44.290 --> 01:30:47.910
selectively and politically, um, but it all is part

1659
01:30:47.910 --> 01:30:50.759
of this kind of, um, ideological framework and that's

1660
01:30:50.759 --> 01:30:53.240
why I think, you know, this, uh, this conservative

1661
01:30:53.240 --> 01:30:55.689
bias, the status quo bias, and this negative epistemology

1662
01:30:55.689 --> 01:30:59.830
bias, they, they, they, they work tightly together to

1663
01:31:00.120 --> 01:31:06.240
um prevent people from feeling like they should um

1664
01:31:06.479 --> 01:31:10.479
um believe something surprising and do something that would,

1665
01:31:10.600 --> 01:31:13.520
you know, amount to radical reform. Mhm.

1666
01:31:14.060 --> 01:31:17.479
Great. So the book is again epistemic courage. I'm

1667
01:31:17.479 --> 01:31:19.399
leaving a link to it in the description of

1668
01:31:19.399 --> 01:31:22.430
the interview. And just before we go, Doctor Ichika,

1669
01:31:22.720 --> 01:31:25.359
would you like to tell people apart from the

1670
01:31:25.359 --> 01:31:27.959
book where they can find your work on the

1671
01:31:27.959 --> 01:31:28.870
internet? Oh,

1672
01:31:28.919 --> 01:31:32.839
sure, thanks. Yeah, um, I, I, uh, I have

1673
01:31:32.839 --> 01:31:36.080
a, I have a website jchikawa.net, um, where I

1674
01:31:36.080 --> 01:31:37.520
have, you know, a bit of an overview of

1675
01:31:37.520 --> 01:31:40.120
some of what I'm up to. Um, I used

1676
01:31:40.120 --> 01:31:43.240
to be active on, uh, what used to be

1677
01:31:43.240 --> 01:31:45.660
Twitter. Um, I do still have an account there,

1678
01:31:45.680 --> 01:31:48.870
although I may delete it soon. Um, uh, I,

1679
01:31:48.959 --> 01:31:50.529
I, I, I'd like to kind of get back

1680
01:31:50.529 --> 01:31:53.759
into more active public engagement, social media, and I

1681
01:31:53.759 --> 01:31:56.040
hope to do that, um, soon and maybe on

1682
01:31:56.040 --> 01:31:58.160
a different platform. Uh, THE past couple of years

1683
01:31:58.160 --> 01:32:01.044
I've been I'm pretty busy as a department head,

1684
01:32:01.125 --> 01:32:03.504
so, um, I haven't found the time to like,

1685
01:32:03.754 --> 01:32:06.205
um, be doing that all that much, but my

1686
01:32:06.205 --> 01:32:10.915
website's still there, um, and I, I, um, post,

1687
01:32:10.964 --> 01:32:13.604
you know, um, preprinted papers and new things up

1688
01:32:13.604 --> 01:32:15.765
there, descriptions of my new research. My new, my

1689
01:32:15.765 --> 01:32:20.089
new big research project is, um, um. Is one

1690
01:32:20.089 --> 01:32:23.609
focused on um feminist sexual ethics and the role

1691
01:32:23.609 --> 01:32:27.209
of consent, um, and so I'm, I'm, I've, I've

1692
01:32:27.209 --> 01:32:29.080
written some papers on that, you can find those

1693
01:32:29.080 --> 01:32:31.129
on the website and I'm hoping I'll be writing

1694
01:32:31.129 --> 01:32:33.839
a book about that um um before too long.

1695
01:32:34.089 --> 01:32:37.850
Um, SO I'm happy to hear from people about

1696
01:32:37.850 --> 01:32:39.890
um any of these ideas. I love talking about

1697
01:32:39.890 --> 01:32:42.040
this stuff, and I, I appreciate the time.

1698
01:32:43.000 --> 01:32:44.950
Great. So thank you so much for taking the

1699
01:32:44.950 --> 01:32:46.350
time to come on the show. It's been a

1700
01:32:46.350 --> 01:32:47.779
real pleasure to talk with you.

1701
01:32:48.149 --> 01:32:49.589
Uh, LIKEWISE, it has been great. Thank you very

1702
01:32:49.589 --> 01:32:49.899
much.

1703
01:32:51.229 --> 01:32:53.750
Hi guys, thank you for watching this interview until

1704
01:32:53.750 --> 01:32:55.890
the end. If you liked it, please share it,

1705
01:32:56.069 --> 01:32:58.859
leave a like and hit the subscription button. The

1706
01:32:58.859 --> 01:33:01.060
show is brought to you by Nights Learning and

1707
01:33:01.060 --> 01:33:05.140
Development done differently, check their website at Nights.com and

1708
01:33:05.140 --> 01:33:08.859
also please consider supporting the show on Patreon or

1709
01:33:08.859 --> 01:33:11.339
PayPal. I would also like to give a huge

1710
01:33:11.339 --> 01:33:14.770
thank you to my main patrons and PayPal supporters

1711
01:33:14.770 --> 01:33:18.700
Pergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyche Olaf,

1712
01:33:18.779 --> 01:33:22.020
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

1713
01:33:22.160 --> 01:33:25.450
Erika Lenny, John Connors, Philip Fors Connolly. Then the

1714
01:33:25.450 --> 01:33:29.250
Mari Robert Windegaruyasi Zu Mark Nes calling in Holbrookfield

1715
01:33:29.250 --> 01:33:34.040
governor Michael Stormir Samuel Andrea, Francis Forti Agnseroro and

1716
01:33:34.040 --> 01:33:38.370
Hal Herzognun Macha Joan Lays and the Samuel Corriere,

1717
01:33:38.529 --> 01:33:42.200
Heinz, Mark Smith, Jore, Tom Hummel, Sardus Fran David

1718
01:33:42.200 --> 01:33:46.060
Sloan Wilson, Asila dearraujoro and Roach Diego Londono Correa.

1719
01:33:46.439 --> 01:33:52.410
Yannick Punteran Rosmani Charlotte blinikol Barbara Adamhn Pavlostaevskynalebaa medicine,

1720
01:33:52.479 --> 01:33:57.069
Gary Galman Samov Zaledrianei Poltonin John Barboza, Julian Price,

1721
01:33:57.359 --> 01:34:01.799
Edward Hall Edin Bronner, Douglas Fry, Franca Bartolotti Gabrielon

1722
01:34:01.799 --> 01:34:06.279
Scorteus Slelitsky, Scott Zachary Fish Tim Duffyani Smith John

1723
01:34:06.279 --> 01:34:11.220
Wieman. Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai

1724
01:34:11.220 --> 01:34:15.720
Giorgio Theophanous, Chris Williamson, Peter Vozin, David Williams, the

1725
01:34:15.720 --> 01:34:20.270
Augusta, Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez,

1726
01:34:20.299 --> 01:34:24.520
Coralli Chevalier, bungalow atheists, Larry D. Lee Junior, Old

1727
01:34:24.520 --> 01:34:29.430
Heringbo. Sterry Michael Bailey, then Sperber, Robert Grassy Zigoren,

1728
01:34:29.589 --> 01:34:34.029
Jeff McMahon, Jake Zu, Barnabas radix, Mark Campbell, Thomas

1729
01:34:34.029 --> 01:34:38.359
Dovner, Luke Neeson, Chris Stor, Kimberly Johnson, Benjamin Galbert,

1730
01:34:38.509 --> 01:34:43.859
Jessica Nowicki, Linda Brendon, Nicholas Carlsson, Ismael Bensleyman. George

1731
01:34:43.859 --> 01:34:49.089
Eoriatis, Valentin Steinman, Perrolis, Kate van Goller, Alexander Aubert,

1732
01:34:49.910 --> 01:34:55.750
Liam Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner,

1733
01:34:55.870 --> 01:35:00.629
Ursula Gudinov, Gregory Hastings, David Pinsoff Sean Nelson, Mike

1734
01:35:00.629 --> 01:35:04.290
Levine, and Jos Net. A special thanks to my

1735
01:35:04.290 --> 01:35:07.129
producers. These are Webb, Jim, Frank Lucas Steffinik, Tom

1736
01:35:07.129 --> 01:35:12.009
Venneden, Bernard Curtis Dixon, Benedict Muller, Thomas Trumbull, Catherine

1737
01:35:12.009 --> 01:35:15.290
and Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and

1738
01:35:15.290 --> 01:35:18.649
Nick Golden, and to my executive producers Matthew Lavender,

1739
01:35:18.740 --> 01:35:21.919
Sergio Quadrian, Bogdan Kanivets, and Rosie. Thank you for

1740
01:35:21.919 --> 01:35:22.240
all.

