WEBVTT

1
00:00:00.009 --> 00:00:02.730
Hello, everybody. Welcome to a new episode of the

2
00:00:02.740 --> 00:00:05.610
Center. I'm your host as always Ricardo Lob. And

3
00:00:05.619 --> 00:00:08.909
today I'm joined by Dr Joshua May. He's Professor

4
00:00:08.920 --> 00:00:11.949
of Philosophy and Psychology at the University of Alabama

5
00:00:11.960 --> 00:00:15.140
at Birmingham. And today we're talking about his latest

6
00:00:15.149 --> 00:00:20.190
book, Neuroethics Agency in the Age of Brain Science.

7
00:00:20.200 --> 00:00:22.209
So Dr May welcome to the show. It's a

8
00:00:22.219 --> 00:00:27.290
pleasure to everyone. Thanks for having me. So, in

9
00:00:27.299 --> 00:00:30.549
the book, you go through actually several different uh

10
00:00:30.559 --> 00:00:34.669
topics that I've explored on the show with other

11
00:00:34.680 --> 00:00:37.430
people as well. But before we get into the

12
00:00:37.439 --> 00:00:42.400
specific topics just to introduce what neuroethics is to

13
00:00:42.409 --> 00:00:44.090
the audience. So, could you tell us a little

14
00:00:44.099 --> 00:00:46.990
bit about this? I'm not sure if we should

15
00:00:47.000 --> 00:00:50.439
call it a specific sub discipline or something like

16
00:00:50.450 --> 00:00:53.549
that. But what is neuroethics? And what kinds of

17
00:00:53.560 --> 00:00:55.270
questions does it deal with?

18
00:00:56.459 --> 00:00:59.970
Sure. Yeah, neuroethics is a fairly new discipline. It's

19
00:00:59.979 --> 00:01:02.520
really only about, let's see, 2025 years old is

20
00:01:02.529 --> 00:01:04.360
around the turn of the century that it got

21
00:01:04.370 --> 00:01:07.319
its own label and, and got pretty clearly defined.

22
00:01:07.680 --> 00:01:09.849
But the idea is, it's sort of evolved out

23
00:01:09.860 --> 00:01:13.260
of bioethics. It's kind of like bioethics for the

24
00:01:13.269 --> 00:01:17.069
brain. But it's not just that it's also about

25
00:01:17.080 --> 00:01:20.949
how neuroscience can tell us about some classic old

26
00:01:20.959 --> 00:01:23.400
questions and ethics. So it's usually thought of as

27
00:01:23.500 --> 00:01:26.120
well. We can think of neuroethics is partly just

28
00:01:26.160 --> 00:01:29.239
how do we do neuroscience ethically. That's kind of

29
00:01:29.250 --> 00:01:31.900
like bioethics for the brain. But then there's also

30
00:01:31.910 --> 00:01:34.180
the idea that what can neuroscience tell us about

31
00:01:34.190 --> 00:01:37.860
ethics. So there's this, this nice phrase, the neuroscience

32
00:01:37.870 --> 00:01:40.010
of ethics and the ethics of neuroscience. And and

33
00:01:40.019 --> 00:01:41.510
it includes both of those which, which is really

34
00:01:41.519 --> 00:01:44.650
great. It means it's a very wide ranging field

35
00:01:44.980 --> 00:01:47.449
and it's one that is, is both trying to

36
00:01:47.459 --> 00:01:50.870
sort of ethically police neuroscience, but also to, to

37
00:01:50.879 --> 00:01:52.819
learn from it. And that's one of the things

38
00:01:52.830 --> 00:01:54.339
that's so exciting for me is that we get

39
00:01:54.349 --> 00:01:57.139
to ask both of those kinds of questions in

40
00:01:57.150 --> 00:02:00.349
neuroethics. So we can ask questions like when is

41
00:02:00.360 --> 00:02:04.400
it ethical to manipulate people's brains? There's a lot

42
00:02:04.410 --> 00:02:07.940
of new technologies coming out. Uh The are promising

43
00:02:07.949 --> 00:02:11.559
to intervene on people's brains for, for medical purposes,

44
00:02:11.570 --> 00:02:15.039
but also really just for the average consumer. Uh

45
00:02:15.050 --> 00:02:17.399
Elon Musk has this, this company neuralink and there,

46
00:02:17.410 --> 00:02:19.089
there are many others that are coming out with,

47
00:02:19.100 --> 00:02:22.470
with a lot of um brain computer interfaces. Um

48
00:02:22.479 --> 00:02:24.250
But then there's also questions like, well, what is

49
00:02:24.259 --> 00:02:26.800
neuroscience telling us about how moral judgment works? Um

50
00:02:26.809 --> 00:02:30.839
OR about about addiction or mental illness? Uh Things

51
00:02:30.850 --> 00:02:33.410
like, like neurodiversity and, and we're, we're learning sort

52
00:02:33.419 --> 00:02:35.169
of back and forth going from thinking about the

53
00:02:35.179 --> 00:02:37.770
ethics and thinking about what the neuroscience tells us

54
00:02:37.940 --> 00:02:40.750
about ethics. So I try to tackle at least

55
00:02:40.759 --> 00:02:42.490
some of those questions in the book. I couldn't

56
00:02:42.500 --> 00:02:44.490
cover them all. I mean, it's, it's already, I

57
00:02:44.500 --> 00:02:47.000
think overly ambitious trying to cover a wide range

58
00:02:47.009 --> 00:02:49.375
of topics. Um, A lot that I left out.

59
00:02:49.514 --> 00:02:51.735
Um BUT, but I, I go from talking about

60
00:02:51.744 --> 00:02:57.074
free will to addiction to brain technologies, brain interventions,

61
00:02:57.085 --> 00:02:59.664
uh and the use of neuroscience in the courtroom

62
00:02:59.785 --> 00:03:02.964
as well to, to potentially um you know, find

63
00:03:02.975 --> 00:03:06.175
people guilty or innocent based on their brain activity.

64
00:03:06.675 --> 00:03:09.854
Uh So something that you mentioned there, I, I

65
00:03:09.865 --> 00:03:15.324
think that is probably very contentious. So for example,

66
00:03:15.335 --> 00:03:20.899
when it comes to uh saying that neuroscience can

67
00:03:20.910 --> 00:03:25.929
inform ethics, uh I mean, the ways that science

68
00:03:25.940 --> 00:03:30.800
can inform a sort of a normative field, let's

69
00:03:30.809 --> 00:03:34.309
say if that science can tell us what we

70
00:03:34.339 --> 00:03:37.399
should do, what we ought to do, that is

71
00:03:37.410 --> 00:03:40.520
the kinds of questions that ethics explores. I mean,

72
00:03:40.529 --> 00:03:43.134
I would imagine that, but uh that would be

73
00:03:43.145 --> 00:03:48.934
among moral philosophers and ethicists, a very contentious question,

74
00:03:48.945 --> 00:03:51.335
right? Because that it is not at all a

75
00:03:51.345 --> 00:03:55.134
straight, it doesn't have a at all a straightforward

76
00:03:55.145 --> 00:03:57.735
answer to it. Right. Right.

77
00:03:57.865 --> 00:04:00.755
Yeah. And there's this, this old is art gap

78
00:04:00.764 --> 00:04:03.294
that, you know, David Hume famously introduced us to.

79
00:04:03.304 --> 00:04:05.065
He said, and you can't just go from, here's

80
00:04:05.074 --> 00:04:07.820
how things are, uh, to how, here's how things

81
00:04:07.830 --> 00:04:11.179
ought to be. And that question I think is

82
00:04:11.190 --> 00:04:14.869
still not exactly resolved, but most people who are

83
00:04:14.880 --> 00:04:18.160
working in neuroethics just concede that that is a

84
00:04:18.170 --> 00:04:20.570
gap and that we need to bridge it with

85
00:04:20.579 --> 00:04:22.570
various principles. So we can't just go from, you

86
00:04:22.579 --> 00:04:24.559
know, here's how the brain works to, you know,

87
00:04:24.570 --> 00:04:28.649
here's how free will and more responsibility and moral

88
00:04:28.660 --> 00:04:31.359
judgment, you know, ought to work. But we can

89
00:04:31.369 --> 00:04:33.390
bridge that with various principles and we could say,

90
00:04:33.399 --> 00:04:35.529
ok, well, if you know, this is how things

91
00:04:35.540 --> 00:04:37.829
are working in the brain, um, maybe it can

92
00:04:37.839 --> 00:04:41.019
tell us something about ethics. So I think that

93
00:04:41.029 --> 00:04:42.709
you have to have some of those assumptions in

94
00:04:42.720 --> 00:04:45.589
there and it's why we do need, um, ethics

95
00:04:45.600 --> 00:04:48.119
involved when we're trying to draw those conclusions. We

96
00:04:48.130 --> 00:04:50.700
can't just go from the science to the ethics

97
00:04:50.709 --> 00:04:52.980
without some bridging kinds of principles.

98
00:04:53.309 --> 00:04:56.250
Mhm. Yes. And I guess that by exploring some

99
00:04:56.260 --> 00:04:58.929
of the topics we're going to talk about here

100
00:04:58.940 --> 00:05:03.049
today, people will also get the notion of how

101
00:05:03.059 --> 00:05:07.769
the two way street between science or neuroscience specifically

102
00:05:07.779 --> 00:05:11.350
in this case and ethics work. So, uh, let's

103
00:05:11.359 --> 00:05:14.750
talk a little bit about free will here. I

104
00:05:14.760 --> 00:05:19.559
know that from a moral perspective, free will connects

105
00:05:19.570 --> 00:05:25.940
with questions regarding moral responsibility, agency, stuff like that.

106
00:05:25.950 --> 00:05:28.700
But I mean, in what ways do you think

107
00:05:28.709 --> 00:05:34.149
that neuroscience as a scientific discipline can be informative

108
00:05:34.160 --> 00:05:37.700
here. I mean, how would people tackle neuros uh

109
00:05:37.709 --> 00:05:42.940
uh free will from a neuroscientific perspective? Because I

110
00:05:42.950 --> 00:05:46.739
mean, I, I posed this question actually to um

111
00:05:46.829 --> 00:05:49.720
last year to Kevin Mitchell who is a, a

112
00:05:49.730 --> 00:05:54.390
neuroscientist himself. Uh I asked him, uh so why

113
00:05:54.399 --> 00:05:58.359
did you get into a question that originally or

114
00:05:58.369 --> 00:06:03.000
traditionally has been tackled by philosophers and not scientists?

115
00:06:03.010 --> 00:06:05.579
And since you are a philosopher, I guess that

116
00:06:05.589 --> 00:06:07.489
I want to ask you the same thing. But

117
00:06:07.500 --> 00:06:11.450
from the other perspective, how can science help you,

118
00:06:11.459 --> 00:06:12.049
let's say,

119
00:06:12.940 --> 00:06:15.200
yeah, talk about uh controversial. That's one of the

120
00:06:15.209 --> 00:06:17.660
the oldest most controversial questions in philosophy, right? Free

121
00:06:17.670 --> 00:06:19.730
will. But, but I think it is, you're right,

122
00:06:19.739 --> 00:06:20.739
it's a good way of thinking about it. There's

123
00:06:20.750 --> 00:06:23.399
a two way street and for a long time,

124
00:06:23.410 --> 00:06:26.119
philosophers have talked about free will without really thinking

125
00:06:26.130 --> 00:06:29.410
much about the science. And they haven't really been

126
00:06:29.420 --> 00:06:31.470
able to, to know much about what's going on

127
00:06:31.480 --> 00:06:33.399
in the brain because, you know, neuroscience is, is

128
00:06:33.410 --> 00:06:36.329
not that old of a discipline uh either. And

129
00:06:36.339 --> 00:06:38.570
so you see some philosophers who aren't paying much

130
00:06:38.579 --> 00:06:40.339
attention to the, to the science, but also some

131
00:06:40.350 --> 00:06:42.660
scientists who maybe aren't paying much attention to the

132
00:06:42.670 --> 00:06:44.609
philosophy. So I think the best work in this

133
00:06:44.619 --> 00:06:47.980
area brings the two together and as philosophers, we

134
00:06:47.989 --> 00:06:50.130
should be humble and, and pay attention to what's

135
00:06:50.140 --> 00:06:52.529
going on in science. But the same goes in

136
00:06:52.540 --> 00:06:55.269
reverse So I think for a long time, you

137
00:06:55.279 --> 00:06:58.630
know, philosophers have been worried about free will by

138
00:06:58.640 --> 00:07:01.540
looking at physics. They say, well, maybe uh physics

139
00:07:01.549 --> 00:07:04.079
shows that the, the world is determined. And so

140
00:07:04.089 --> 00:07:06.619
that means that all of our actions are determined

141
00:07:06.950 --> 00:07:09.309
and that seems like it might threaten free will

142
00:07:09.320 --> 00:07:10.660
if, you know, I have no real choice in

143
00:07:10.670 --> 00:07:12.630
the matter of what I'm going to do. Maybe

144
00:07:12.640 --> 00:07:15.429
it's all predetermined by the laws of physics and

145
00:07:15.440 --> 00:07:18.869
in the past. But the, the more modern threat

146
00:07:18.880 --> 00:07:21.709
that's coming from neuroscience is a little bit different.

147
00:07:21.920 --> 00:07:24.329
The idea is that we're not just learning that

148
00:07:24.339 --> 00:07:27.589
our actions are determined by physical forces, but that

149
00:07:27.600 --> 00:07:31.869
they might be largely determined by unconscious forces. And

150
00:07:31.880 --> 00:07:33.739
that's something that it does seem like we're getting

151
00:07:33.750 --> 00:07:37.109
a bit of consensus out of neuroscience and cognitive

152
00:07:37.119 --> 00:07:39.940
science more generally that a lot of our decisions

153
00:07:39.950 --> 00:07:43.640
are driven by unconscious forces. Uh Freud was at

154
00:07:43.649 --> 00:07:46.200
least roughly right about that. Um There are a

155
00:07:46.209 --> 00:07:49.950
lot of unconscious factors now, they may not actually

156
00:07:49.959 --> 00:07:52.790
deal with, you know, um repression and, and a

157
00:07:52.799 --> 00:07:55.329
lot of uh problems with, with our parents and,

158
00:07:55.529 --> 00:07:58.630
and the upbringing, but that's part of it and

159
00:07:58.640 --> 00:08:00.790
part of it is just, you know, incidental factors

160
00:08:00.799 --> 00:08:02.970
that, that influence us in the present moment that

161
00:08:02.980 --> 00:08:06.600
we don't understand uh things like biases. Um OR

162
00:08:06.609 --> 00:08:09.899
if you're just being particularly irritable, um there's something

163
00:08:10.109 --> 00:08:11.899
a foul smell in the room or something that

164
00:08:11.910 --> 00:08:14.040
could be influencing your decisions and you might not

165
00:08:14.049 --> 00:08:17.140
realize that or think that it is. So that

166
00:08:17.149 --> 00:08:19.980
is certainly a, it's a consensus now that a

167
00:08:19.989 --> 00:08:23.269
lot of our decisions are influenced by unconscious forces

168
00:08:23.279 --> 00:08:26.570
in the brain. Now, for many philosophers and scientists

169
00:08:26.579 --> 00:08:29.829
that seems like it threatens free will because we

170
00:08:29.839 --> 00:08:33.609
tend to think that our free choices come from

171
00:08:33.630 --> 00:08:35.669
our conscious mind. It has to be that I

172
00:08:35.679 --> 00:08:38.669
consciously choose, you know, to say certain words to

173
00:08:38.679 --> 00:08:41.448
people or, or make certain decisions for that to

174
00:08:41.457 --> 00:08:44.968
be free. And that does pose a major challenge.

175
00:08:44.979 --> 00:08:46.348
But we have to do that both that work

176
00:08:46.359 --> 00:08:48.229
from philosophy and science. You have to look at

177
00:08:48.239 --> 00:08:49.718
science say, what is it really showing, you know,

178
00:08:49.729 --> 00:08:52.609
how much of our decisions are influenced by unconscious

179
00:08:52.619 --> 00:08:55.460
forces. But then also look at the philosophy and

180
00:08:55.469 --> 00:08:57.859
say, well, does that really show that free will

181
00:08:57.869 --> 00:09:00.099
doesn't exist? What do we mean by free will?

182
00:09:00.219 --> 00:09:02.109
And I think that actually the science can help

183
00:09:02.119 --> 00:09:03.780
us understand that as well. We can do studies

184
00:09:03.789 --> 00:09:05.349
to try to figure out how do people understand

185
00:09:05.359 --> 00:09:07.619
free will, you know, what does it mean? And

186
00:09:07.630 --> 00:09:09.000
my own view is that I think once we

187
00:09:09.010 --> 00:09:11.809
get all that settled and on the table that

188
00:09:11.820 --> 00:09:14.650
we don't necessarily have a threat to free will,

189
00:09:14.659 --> 00:09:16.619
we might need to revise our conception of free

190
00:09:16.630 --> 00:09:18.340
will. Maybe it's a little different than we thought

191
00:09:18.349 --> 00:09:20.239
it was, but it doesn't necessarily mean that we

192
00:09:20.250 --> 00:09:21.190
have to reject it.

193
00:09:22.289 --> 00:09:25.380
And I guess that there's also two different sets

194
00:09:25.390 --> 00:09:30.299
of interesting questions here. One is whether free will

195
00:09:30.309 --> 00:09:33.940
exists is a real thing, whatever it might be

196
00:09:33.950 --> 00:09:37.780
or not. And the other is whether free will

197
00:09:37.789 --> 00:09:42.900
exists or not, people have reasons for attributing more

198
00:09:42.909 --> 00:09:46.070
or less free will to other people in different

199
00:09:46.080 --> 00:09:49.280
circumstances. Like for example, when they want to blame

200
00:09:49.289 --> 00:09:53.750
them or praise them, they usually attribute more freedom

201
00:09:53.760 --> 00:09:57.369
to their actions. But uh if they think that,

202
00:09:57.380 --> 00:09:59.724
I mean, they are, for example, the victims of

203
00:09:59.734 --> 00:10:04.015
their circumstances or something like that, something bad happens

204
00:10:04.025 --> 00:10:06.294
to them that they had no control over, perhaps

205
00:10:06.304 --> 00:10:10.854
they are more, are less willing to attribute freedom

206
00:10:10.864 --> 00:10:14.065
to their actions. Right? So I, I mean, those

207
00:10:14.075 --> 00:10:18.955
are two different sets of, I would say interesting

208
00:10:18.965 --> 00:10:21.015
and important questions here, right?

209
00:10:21.630 --> 00:10:24.270
Yeah, that's a good distinction to draw partly because

210
00:10:24.280 --> 00:10:26.989
philosophers for a long time only thought about either

211
00:10:27.000 --> 00:10:29.280
free will exists or it doesn't. And they haven't

212
00:10:29.289 --> 00:10:30.690
thought about it coming in degrees. And I think

213
00:10:30.700 --> 00:10:33.549
that's a great development in the philosophy side. And

214
00:10:33.559 --> 00:10:37.000
many philosophers today would say, well, no freedom. And

215
00:10:37.010 --> 00:10:41.409
so responsibility and blame can come in degrees. And

216
00:10:41.419 --> 00:10:43.320
I think that's right. And so we could even

217
00:10:43.330 --> 00:10:46.090
see the science as suggesting that maybe we have

218
00:10:46.099 --> 00:10:50.070
less freedom than we thought, but we don't completely

219
00:10:50.080 --> 00:10:52.820
lack it. Now. I think that that's, that's right

220
00:10:52.830 --> 00:10:54.510
that we're, we're learning that there's a lot that

221
00:10:54.520 --> 00:10:57.239
influences our, our decisions that we weren't quite fully

222
00:10:57.250 --> 00:10:59.979
aware of and that might actually mitigate blame. I

223
00:11:00.020 --> 00:11:01.200
said, well, it's kind of like when you find

224
00:11:01.210 --> 00:11:03.770
out that somebody, um, you know, stepped on your

225
00:11:03.780 --> 00:11:07.070
toe accidentally, um, or, you know, you think that

226
00:11:07.080 --> 00:11:09.849
somebody's been really not a very good friend lately

227
00:11:09.859 --> 00:11:12.190
but you find out that, well, it's because they're

228
00:11:12.200 --> 00:11:14.130
going through a divorce. Right. Or they've got some

229
00:11:14.140 --> 00:11:16.440
ailing parents or they just haven't been sleeping because

230
00:11:16.450 --> 00:11:18.369
they've got a newborn. And so you go, oh,

231
00:11:18.380 --> 00:11:20.489
ok. And you said, well, I guess I'll blame

232
00:11:20.500 --> 00:11:22.739
them a little bit less, but you don't treat

233
00:11:22.750 --> 00:11:24.799
them like there's some machine that has no free

234
00:11:24.809 --> 00:11:26.299
will. You just say, well, you know, I'm gonna

235
00:11:26.309 --> 00:11:28.994
blame them less or hold them less accountable. And

236
00:11:29.005 --> 00:11:30.914
I think that happens all the time through ordinary

237
00:11:30.924 --> 00:11:33.025
life. So maybe it's not such a surprise if

238
00:11:33.034 --> 00:11:35.375
we find out that the science is suggesting that,

239
00:11:35.385 --> 00:11:36.914
that we, we need to do some more of

240
00:11:36.924 --> 00:11:39.044
that. We need to say, ok, people have a

241
00:11:39.054 --> 00:11:41.625
lot of different forces that are influencing them sometimes.

242
00:11:41.635 --> 00:11:43.724
That means they're a little bit less responsible, but

243
00:11:43.734 --> 00:11:46.554
we're still responsible in general and we're still the

244
00:11:46.565 --> 00:11:48.955
kinds of creatures who can be held accountable for

245
00:11:48.965 --> 00:11:52.294
our actions and make free choices, even if it's

246
00:11:52.304 --> 00:11:54.445
true that we have less freedom than, than we

247
00:11:54.455 --> 00:11:54.895
thought.

248
00:11:55.929 --> 00:11:58.570
So, at a certain point there, when I asked

249
00:11:58.580 --> 00:12:01.429
you, first about free will. You mentioned the fact

250
00:12:01.440 --> 00:12:05.369
that we now know through science, neuroscience, psychology and

251
00:12:05.380 --> 00:12:10.239
so on, uh that we have many different unconscious

252
00:12:10.250 --> 00:12:14.369
mechanisms operating in our minds that of course influence,

253
00:12:14.380 --> 00:12:18.210
at least to some extent our behavior, our decision

254
00:12:18.219 --> 00:12:22.280
making and so on. Do you think that consciousness

255
00:12:22.289 --> 00:12:27.070
is strictly necessary for free will to exist? Or

256
00:12:27.080 --> 00:12:29.900
do you think that perhaps free will would still

257
00:12:29.909 --> 00:12:34.640
be possible even in the absence of consciousness?

258
00:12:35.309 --> 00:12:37.359
Hm. Yeah, that's a great question because my own

259
00:12:37.369 --> 00:12:41.359
view is that consciousness isn't always necessary for freedom,

260
00:12:41.840 --> 00:12:43.549
but it would be a little bit different if

261
00:12:43.559 --> 00:12:46.049
you had no conscious choice in the matter at

262
00:12:46.059 --> 00:12:48.119
all. And, and that would be, you know, this

263
00:12:48.130 --> 00:12:52.010
idea of, of Pien very, very mouthful of a,

264
00:12:52.020 --> 00:12:53.929
of a label there. But the idea that our

265
00:12:53.940 --> 00:12:57.070
conscious choices are pi phenomena or, or they just

266
00:12:57.080 --> 00:12:58.340
come along for the ride, they're just kind of

267
00:12:58.349 --> 00:13:01.409
byproduct's if that really was the case that, that

268
00:13:01.419 --> 00:13:04.799
our conscious decisions never really played a role in,

269
00:13:04.809 --> 00:13:07.989
in our actions that I think could be potentially

270
00:13:08.000 --> 00:13:10.530
a threat to free will. But I think what

271
00:13:10.539 --> 00:13:13.409
the science is actually showing us is that those

272
00:13:13.419 --> 00:13:16.179
conscious choices just play a bit less of a

273
00:13:16.190 --> 00:13:18.760
role in our minds. And so to me, that

274
00:13:18.770 --> 00:13:21.219
doesn't necessarily mean we, we lack free will. Uh

275
00:13:21.229 --> 00:13:24.280
PARTLY because I think that we are these complex

276
00:13:24.289 --> 00:13:28.020
agents, we have both conscious and unconscious forces. And

277
00:13:28.030 --> 00:13:30.659
I think that we shouldn't just identify ourselves with

278
00:13:30.669 --> 00:13:33.119
one or the other. We tend to want to

279
00:13:33.130 --> 00:13:35.530
identify ourselves with the conscious part. So, you know,

280
00:13:35.539 --> 00:13:38.159
I'm the one who consciously decides and all this

281
00:13:38.169 --> 00:13:40.760
unconscious, you know, biases and things that I'm not

282
00:13:40.770 --> 00:13:43.460
aware of. Not part of me. Uh I think

283
00:13:43.469 --> 00:13:45.590
that's a mistake. I, I think it's true that

284
00:13:45.599 --> 00:13:48.429
we tend to identify ourselves more with the conscious

285
00:13:48.440 --> 00:13:51.070
parts, but this is all part of me. You

286
00:13:51.080 --> 00:13:53.119
know, it's not part of anybody else. Uh You

287
00:13:53.130 --> 00:13:56.039
know, the, the unconscious biases and thoughts I have

288
00:13:56.049 --> 00:13:58.390
are, are, are not part of you. They're part

289
00:13:58.400 --> 00:14:00.739
of me. And the analogy I like to draw

290
00:14:00.750 --> 00:14:05.320
is with a corporation or really a large agency

291
00:14:05.479 --> 00:14:08.700
where you have places like Apple has a CEO

292
00:14:08.710 --> 00:14:11.179
and we often tend to identify the company with

293
00:14:11.190 --> 00:14:13.609
the CEO. We said, well, you know, Apple kind

294
00:14:13.619 --> 00:14:17.179
of is primarily driven by Tim Cook the CEO,

295
00:14:17.590 --> 00:14:19.659
but of course, Apple is a much bigger entity

296
00:14:19.669 --> 00:14:21.650
than that. It's not just the CEO, it's not

297
00:14:21.659 --> 00:14:24.090
just the part that's playing that sort of executive

298
00:14:24.099 --> 00:14:27.880
function role, as we often say in neuroscience, it's

299
00:14:27.890 --> 00:14:29.969
part of it and it's important part, but it's

300
00:14:29.979 --> 00:14:31.099
not the only part, there are a lot of

301
00:14:31.109 --> 00:14:33.809
other features of Apple, like the, you know, level

302
00:14:33.820 --> 00:14:36.309
level managers, all the workers and they all actually

303
00:14:36.320 --> 00:14:38.559
direct the company and move it in different ways.

304
00:14:38.669 --> 00:14:41.130
Now, Tim Cook might not be aware of a

305
00:14:41.140 --> 00:14:43.830
lot of that activity. Um But it's part of

306
00:14:43.840 --> 00:14:45.539
what Apple does, it's part of its agency. It's

307
00:14:45.549 --> 00:14:47.099
part of how it acts and makes choices in

308
00:14:47.109 --> 00:14:50.659
the world. And if you have no CEO whatsoever,

309
00:14:50.669 --> 00:14:53.659
you might have a completely disorganized organization and feel

310
00:14:53.669 --> 00:14:56.659
like there's not really any single entity here. So

311
00:14:56.669 --> 00:14:58.640
maybe a CEO can help to kind of unify

312
00:14:58.650 --> 00:15:01.799
things and provide some sort of direction, but it's

313
00:15:01.809 --> 00:15:05.130
not the whole agency. So I think similarly, you

314
00:15:05.140 --> 00:15:07.820
know, our conscious minds probably have to play some

315
00:15:07.830 --> 00:15:12.539
role in unifying our, our, our otherwise uh disorganized

316
00:15:12.549 --> 00:15:15.109
mental states, all the kinds of automatic thoughts and

317
00:15:15.119 --> 00:15:17.940
desires that pop up all the time. But all

318
00:15:17.950 --> 00:15:20.500
those things are part of who we are and

319
00:15:20.510 --> 00:15:22.979
we, we can't distance ourselves too much from them.

320
00:15:23.250 --> 00:15:25.270
And we, we need to maybe revise our conception

321
00:15:25.280 --> 00:15:27.849
of our, of ourselves and our freedom to say

322
00:15:27.859 --> 00:15:30.140
a lot of that is part of our ourselves

323
00:15:30.150 --> 00:15:33.030
and our choices. Um EVEN if uh the conscious

324
00:15:33.039 --> 00:15:35.820
parts must be involved as well.

325
00:15:37.280 --> 00:15:41.169
Uh And I guess that also related to consciousness,

326
00:15:41.179 --> 00:15:45.659
there's the fact that even if uh lots or

327
00:15:45.669 --> 00:15:48.679
even most of what happens of or what goes

328
00:15:48.690 --> 00:15:52.590
around in our minds is uh unconscious or plays

329
00:15:52.599 --> 00:15:56.210
out at an unconscious level. I mean, at the

330
00:15:56.219 --> 00:16:00.250
very least, uh I would say that we have

331
00:16:00.260 --> 00:16:05.239
veto power over what comes out of those mechanisms

332
00:16:05.250 --> 00:16:08.679
in terms of how they influence our behavior. Right.

333
00:16:08.690 --> 00:16:12.070
I mean, a, at the very least before we

334
00:16:12.080 --> 00:16:16.409
do something we can decide not to do it

335
00:16:16.419 --> 00:16:20.450
or go through some other course of action.

336
00:16:21.169 --> 00:16:23.450
Mm. Right. That might be enough for, for free

337
00:16:23.460 --> 00:16:26.309
will. Yeah. The, the neuroscientist who is responsible for

338
00:16:26.320 --> 00:16:29.039
a lot of this early work. Benjamin Labette actually

339
00:16:29.049 --> 00:16:30.979
did think that we had free will just because

340
00:16:30.989 --> 00:16:32.669
of that. He thought we had, well, he called

341
00:16:32.679 --> 00:16:35.049
it free won't, uh, that, you know, we can

342
00:16:35.059 --> 00:16:37.580
decide not to do things. You know, I'm not

343
00:16:37.590 --> 00:16:39.619
so sure that that's enough though for, for free

344
00:16:39.630 --> 00:16:42.789
will. I think that we, we really don't always

345
00:16:42.799 --> 00:16:45.400
have that possibility to, to veto the actions. I

346
00:16:45.409 --> 00:16:48.390
think that the science does suggest that we often

347
00:16:48.440 --> 00:16:52.090
are kind of on autopilot a lot. There's actually,

348
00:16:52.099 --> 00:16:54.090
uh, one philosopher who's been really pointing this out

349
00:16:54.099 --> 00:16:56.530
a lot lately is, uh, Shandra Sri Pata. He's

350
00:16:56.539 --> 00:16:59.239
at Michigan and he said we need to really

351
00:16:59.250 --> 00:17:02.200
think about the default state of the human mind

352
00:17:02.260 --> 00:17:05.319
as basically more automatic. It's kind of like it's,

353
00:17:05.329 --> 00:17:06.880
it's like a car where if, you know, you

354
00:17:06.890 --> 00:17:09.660
let off the, the brake, it just tends to

355
00:17:09.670 --> 00:17:13.550
move forward. Um, EXCEPT apparently on Teslas is different,

356
00:17:13.560 --> 00:17:15.130
but, you know, on, on most cars you let

357
00:17:15.140 --> 00:17:16.358
off the brake, it just sort of goes out

358
00:17:16.368 --> 00:17:17.479
of it. And I think that that's kind of

359
00:17:17.489 --> 00:17:19.140
how the mind works as well. And cer certainly,

360
00:17:19.150 --> 00:17:21.689
I think Sru Pada does as well. That there

361
00:17:21.699 --> 00:17:23.319
are all these different kinds of thoughts that just

362
00:17:23.329 --> 00:17:26.969
bubble up automatically, these different desires and impulses that,

363
00:17:26.979 --> 00:17:29.979
that come on automatically. And we sometimes have the

364
00:17:29.989 --> 00:17:32.420
ability to, to veto and control. But if a

365
00:17:32.430 --> 00:17:34.839
lot of it's unconscious and automatic, then we might

366
00:17:34.849 --> 00:17:36.650
not be aware of it. And so it's hard

367
00:17:36.660 --> 00:17:39.349
to veto something that you're not aware of. So

368
00:17:39.359 --> 00:17:41.150
I think if we said, uh we have free

369
00:17:41.160 --> 00:17:43.739
will as long as we can veto every possible

370
00:17:43.750 --> 00:17:46.949
automatic urge or thought that comes up, then I

371
00:17:46.959 --> 00:17:48.969
think the science would, would suggest that we, we

372
00:17:48.979 --> 00:17:52.020
don't necessarily have that. Um, BUT I do think

373
00:17:52.030 --> 00:17:55.459
that we have some role for consciously influencing our

374
00:17:55.469 --> 00:17:58.839
automatic thoughts and having that kind of cognitive control

375
00:17:58.890 --> 00:18:03.479
sometimes to maybe direct those urges and thoughts or

376
00:18:03.489 --> 00:18:05.400
to, to tamp them down and not let them

377
00:18:05.410 --> 00:18:08.260
influence behavior. But a lot of times we're just

378
00:18:08.270 --> 00:18:11.000
kind of on autopilots and, you know, I may

379
00:18:11.010 --> 00:18:13.560
just automatically have some thoughts and, and let them

380
00:18:13.569 --> 00:18:16.400
fly at a party. Um, AND some of them

381
00:18:16.410 --> 00:18:19.000
might be, you know, a little bit insensitive, but

382
00:18:19.010 --> 00:18:22.079
I can't say, well, I didn't consciously choose to,

383
00:18:22.089 --> 00:18:24.959
to veto that insensitive remark. Um, OR I didn't

384
00:18:24.969 --> 00:18:27.880
consciously choose to generate that insensitive remark. It just,

385
00:18:27.890 --> 00:18:30.599
it just happened, it was automatic. Um I think

386
00:18:30.609 --> 00:18:34.125
rightly, a lot of still hold each other accountable

387
00:18:34.135 --> 00:18:35.594
for that. We said, well, you know, you, but

388
00:18:35.604 --> 00:18:38.045
it was, you wasn't anybody else, uh who, who

389
00:18:38.055 --> 00:18:40.944
said that insensitive remark and, you know, it's part

390
00:18:40.954 --> 00:18:43.314
of, it represents part of who you are, uh

391
00:18:43.324 --> 00:18:46.954
represents part of the kinds of automatic values and,

392
00:18:46.964 --> 00:18:49.234
and um desires that you have going on in

393
00:18:49.244 --> 00:18:51.194
your mind. And so I think that we still

394
00:18:51.204 --> 00:18:54.275
have to, to some degree identify with those aspects

395
00:18:54.285 --> 00:18:57.055
of, of uh our agency. It, it'd be kind

396
00:18:57.064 --> 00:18:58.555
of like if, you know, Tim Cook says, you

397
00:18:58.564 --> 00:19:01.439
know, I didn't realize, you know, there's all this

398
00:19:01.449 --> 00:19:03.560
like spying going on with, you know, all of

399
00:19:03.569 --> 00:19:05.650
our devices, you know, listening to you through your

400
00:19:05.660 --> 00:19:07.930
iphones, I didn't realize it's just that the company,

401
00:19:07.939 --> 00:19:10.550
all these managers and people, he still is going

402
00:19:10.560 --> 00:19:12.400
to have to be, he and the company is

403
00:19:12.410 --> 00:19:13.959
going to be held accountable for that. It's part

404
00:19:13.969 --> 00:19:15.869
of what Apple did if they're, if they're listening

405
00:19:15.880 --> 00:19:18.459
to us on our iphones, uh even if the

406
00:19:18.469 --> 00:19:21.199
CEO wasn't fully aware of that, it kind of

407
00:19:21.209 --> 00:19:23.319
happened on his watch, so to speak.

408
00:19:24.300 --> 00:19:26.800
Uh WHEN it comes to this kind of topic,

409
00:19:26.810 --> 00:19:29.469
one of the examples and uh since you talk

410
00:19:29.479 --> 00:19:33.739
about being on auto autopilot mode there or something

411
00:19:33.750 --> 00:19:35.810
like that, uh One of the examples that I

412
00:19:35.819 --> 00:19:38.660
love the most, even though of course, compared to

413
00:19:38.670 --> 00:19:42.079
other kinds of bigger decisions that we have to

414
00:19:42.089 --> 00:19:45.479
make in our lives, it's relatively small. But uh

415
00:19:45.489 --> 00:19:48.930
is the one of driving a car. I mean,

416
00:19:48.939 --> 00:19:51.670
because uh for example, if I go to a

417
00:19:51.680 --> 00:19:55.530
new city, uh, in my car and I don't

418
00:19:55.540 --> 00:19:57.520
know the city, I don't know the streets, the

419
00:19:57.530 --> 00:20:00.689
roads, I mean, I usually need to pay more

420
00:20:00.699 --> 00:20:04.810
attention and so the attention really brings a con

421
00:20:05.069 --> 00:20:07.949
anxiousness to the forefront because I really have to

422
00:20:07.959 --> 00:20:10.890
pay lots of attention to all of what I'm

423
00:20:10.900 --> 00:20:14.310
doing. But if I'm driving on the road I've

424
00:20:14.319 --> 00:20:19.540
already driven on 500 times. I'm probably on autopilot.

425
00:20:19.550 --> 00:20:22.910
Right. Because I really don't need to pay much

426
00:20:22.920 --> 00:20:25.900
attention at all to what I'm doing. And, uh,

427
00:20:25.910 --> 00:20:29.839
I mean, we, we drive relatively well, even without

428
00:20:29.849 --> 00:20:34.000
paying much attention if we are experienced drivers at

429
00:20:34.010 --> 00:20:34.459
least.

430
00:20:34.739 --> 00:20:37.800
Right. Yeah. Yeah. It's such a familiar, you know,

431
00:20:37.810 --> 00:20:40.810
experience that it's almost scary sometimes, you know, you're

432
00:20:40.819 --> 00:20:42.859
driving and then you arrive home and you realize,

433
00:20:42.869 --> 00:20:45.359
like, I can't remember any of what I just

434
00:20:45.369 --> 00:20:48.550
did. Uh, YOUR mind was wandering elsewhere and, you

435
00:20:48.560 --> 00:20:50.349
know, you just did all, but it wasn't, it

436
00:20:50.359 --> 00:20:52.910
wasn't like you weren't making choices. I mean, and

437
00:20:52.920 --> 00:20:55.140
responding even to the environment, you know, they had

438
00:20:55.150 --> 00:20:59.140
their stoplights and their pedestrians and so your brain

439
00:20:59.150 --> 00:21:01.699
is still processing all of that and responding to

440
00:21:01.709 --> 00:21:04.890
it. It's just that your conscious mind wasn't really

441
00:21:04.900 --> 00:21:06.750
attending to it. And so I think it's a

442
00:21:06.760 --> 00:21:09.670
great example of, of choices we still make that

443
00:21:09.680 --> 00:21:12.310
we still make freely even though they're not fully

444
00:21:12.319 --> 00:21:15.270
conscious. And so if somebody, you know, is, is

445
00:21:15.280 --> 00:21:17.540
on autopilot while they're driving and then they make

446
00:21:17.550 --> 00:21:20.310
a mistake. Um, THAT'S still something that they've done,

447
00:21:20.339 --> 00:21:23.430
still something they could be held accountable for. And

448
00:21:23.439 --> 00:21:25.699
I do think that the, the neuroscience is suggesting

449
00:21:25.709 --> 00:21:29.199
that more of our behavior is like this, then

450
00:21:29.209 --> 00:21:32.040
we tend to think, but that, that's brings us

451
00:21:32.050 --> 00:21:33.500
to the, so we have to do the science

452
00:21:33.510 --> 00:21:34.810
there and figure out what is it really telling

453
00:21:34.819 --> 00:21:37.060
us? Um, AND I think it's not telling us

454
00:21:37.069 --> 00:21:40.699
that that everything's on autopilot. So some scientists do

455
00:21:40.709 --> 00:21:43.050
believe that that kind of epiphenomenal is that it's

456
00:21:43.060 --> 00:21:45.810
just never involved that our conscious choices, you know,

457
00:21:45.819 --> 00:21:47.719
seem like they're involved, but they're really just kind

458
00:21:47.729 --> 00:21:50.599
of floating along uh like the steam that comes

459
00:21:50.609 --> 00:21:53.060
out of uh you know, a train engine, it's

460
00:21:53.069 --> 00:21:55.099
not really actually causing the train to move, it's

461
00:21:55.109 --> 00:21:57.369
just a by product. I don't think it's showing

462
00:21:57.380 --> 00:21:59.670
us that. So it's showing us that well, our

463
00:21:59.680 --> 00:22:02.020
conscious decisions don't play quite as much of a

464
00:22:02.030 --> 00:22:03.949
role as we might have thought. But then we

465
00:22:03.959 --> 00:22:05.329
can now get a switch in philosophy mode and

466
00:22:05.339 --> 00:22:07.619
say, well, does that show that we don't have

467
00:22:07.630 --> 00:22:10.229
free will? Uh DEPENDS on what we, we mean

468
00:22:10.239 --> 00:22:12.550
by that, I suppose. And, and that itself is,

469
00:22:12.560 --> 00:22:16.310
is controversial, but the basic idea of free will

470
00:22:16.319 --> 00:22:18.000
and, and there have been studies on this suggesting

471
00:22:18.010 --> 00:22:19.859
that this is a very ordinary conception is just

472
00:22:19.869 --> 00:22:23.989
the ability to make choices among genuine options in

473
00:22:24.000 --> 00:22:26.319
light of reasons, you know, to, to, to make

474
00:22:26.329 --> 00:22:28.530
choices in, respond to the reasons that you, you've

475
00:22:28.540 --> 00:22:30.650
got. And I think that that can happen even

476
00:22:30.660 --> 00:22:33.280
when it's pretty automatic and unconscious. So if you're

477
00:22:33.290 --> 00:22:36.219
on autopilot driving, you're not consciously aware of every

478
00:22:36.229 --> 00:22:40.060
decision that that's being made. But you, your brain

479
00:22:40.150 --> 00:22:42.969
is making choices responding to reasons. You know, if

480
00:22:42.979 --> 00:22:45.760
the light is red, uh then you, you push

481
00:22:45.770 --> 00:22:48.089
on the brake and you stop the car and

482
00:22:48.099 --> 00:22:50.680
that is a choice, responding to reasons among different

483
00:22:50.689 --> 00:22:52.890
kinds of options. You, you could have punched it

484
00:22:52.900 --> 00:22:55.689
and blown through the light right, or not stopped

485
00:22:55.949 --> 00:22:59.640
and that I think can be enough for free

486
00:22:59.650 --> 00:23:03.170
will. Um As long as we've got, you know,

487
00:23:03.180 --> 00:23:06.123
a complex that's unified in the right sorts of

488
00:23:06.133 --> 00:23:07.902
ways that has a certain identity. It's not just

489
00:23:07.912 --> 00:23:12.093
this disorganized kinds of urges and automatic thoughts. Uh

490
00:23:12.103 --> 00:23:14.512
BUT much of our mental lives may be on

491
00:23:14.522 --> 00:23:17.613
autopilot and yet still fit that basic definition of

492
00:23:17.623 --> 00:23:19.483
free will that, that we can make, we can

493
00:23:19.493 --> 00:23:23.042
respond to reasons uh in our environment. And that

494
00:23:23.052 --> 00:23:26.032
is a pretty, it's not completely uncontroversial. Um BUT

495
00:23:26.042 --> 00:23:28.562
it's a very popular view about what free will

496
00:23:28.672 --> 00:23:32.586
means among Phil first, but apparently also the general

497
00:23:32.595 --> 00:23:35.056
public, there have been, you know, psychology studies on

498
00:23:35.066 --> 00:23:37.245
this asking people, you know, what are the factors

499
00:23:37.255 --> 00:23:40.245
that are relevant to free will giving them hypothetical

500
00:23:40.255 --> 00:23:42.225
different kinds of scenarios to see if they think

501
00:23:42.235 --> 00:23:44.656
someone acted freely or not. And so there are

502
00:23:44.666 --> 00:23:46.556
a lot of scientists who have a certain conception

503
00:23:46.566 --> 00:23:48.586
of free will, they think, well, it has to

504
00:23:48.595 --> 00:23:51.666
be conscious decision all the time that can like

505
00:23:51.676 --> 00:23:54.495
defy the laws of nature and defy your own

506
00:23:54.505 --> 00:23:57.709
brain chemistry. Certainly we don't have that power. Um

507
00:23:57.719 --> 00:23:59.939
The neuroscience shows we don't, we don't have that,

508
00:23:59.949 --> 00:24:02.890
uh we are, you know, uh physical creatures with,

509
00:24:02.900 --> 00:24:06.140
with these electrochemical brains. And if we're going to

510
00:24:06.150 --> 00:24:08.290
make choices, it has to work through that and

511
00:24:08.300 --> 00:24:10.959
a lot of it's unconscious, but we don't necessarily

512
00:24:10.969 --> 00:24:13.609
have to define free will that way. Uh A

513
00:24:13.619 --> 00:24:15.569
lot of scientists do and I know he had

514
00:24:15.579 --> 00:24:18.300
a Robert Souls on who's, who's got a recent

515
00:24:18.310 --> 00:24:19.859
book like like this. And he, you know, he

516
00:24:19.869 --> 00:24:21.180
thinks, well, this, this means we don't have free

517
00:24:21.189 --> 00:24:24.410
will. Well, a certain conception of free will, I

518
00:24:24.420 --> 00:24:28.089
suppose. But the analogy I like is with um

519
00:24:28.300 --> 00:24:31.719
you know, witches versus solidity. So when we find

520
00:24:31.729 --> 00:24:34.280
out that there's no magic or what have you,

521
00:24:34.290 --> 00:24:35.369
we say, well, then that means there are no

522
00:24:35.380 --> 00:24:37.599
witches because to, to be a witch is to

523
00:24:37.609 --> 00:24:41.890
have certain magical powers. But then science revealed us

524
00:24:42.079 --> 00:24:45.800
to us also that um solid objects are mostly

525
00:24:45.810 --> 00:24:49.130
empty space. A bowling ball is mostly empty space.

526
00:24:49.540 --> 00:24:51.719
And yet we don't say, ah, so there's no

527
00:24:51.729 --> 00:24:54.520
such thing as solidity, we say, oh, well, solidity

528
00:24:54.530 --> 00:24:55.790
is a little bit different than we thought it

529
00:24:55.800 --> 00:24:58.150
was. And I think that we can and should

530
00:24:58.160 --> 00:25:00.170
say the same thing about free will that we

531
00:25:00.180 --> 00:25:02.239
often make this mistake of saying, well, we have

532
00:25:02.250 --> 00:25:04.719
to always make conscious choices if it's going to

533
00:25:04.729 --> 00:25:07.140
be free. But the more we learn about what

534
00:25:07.150 --> 00:25:10.150
kinds of creatures and agents we are, we can

535
00:25:10.160 --> 00:25:12.949
start saying, ok, well, maybe that's not really what

536
00:25:12.959 --> 00:25:15.489
we are. We are these complex beings that are

537
00:25:15.500 --> 00:25:17.839
full of conscious and unconscious choices that play a

538
00:25:17.849 --> 00:25:19.609
role in our decision making.

539
00:25:20.739 --> 00:25:23.739
So, in sort of a related topic in the

540
00:25:23.750 --> 00:25:28.920
book, you also explore the issue of brain manipulation.

541
00:25:28.930 --> 00:25:32.680
So before we get into specific cases of brain

542
00:25:32.689 --> 00:25:36.829
brain manipulation, and you go through for example, ethical

543
00:25:36.839 --> 00:25:42.250
questions associated with medicine. Uh WHAT is brain manipulation?

544
00:25:42.260 --> 00:25:45.719
What counts as brain manipulation? Exactly?

545
00:25:46.890 --> 00:25:49.900
Yeah. When we think brain manipulation, we think coercion,

546
00:25:49.910 --> 00:25:52.420
you know, forcing someone to, to behave or think

547
00:25:52.430 --> 00:25:55.329
a certain way. And uh I was thinking of

548
00:25:55.339 --> 00:25:58.449
it more broadly as it could be even also

549
00:25:58.459 --> 00:26:03.089
the uses of brain interventions to treat um medical

550
00:26:03.099 --> 00:26:05.229
conditions. So there are all kinds of ways we

551
00:26:05.239 --> 00:26:08.339
can manipulate our brains and they, they are classic

552
00:26:08.349 --> 00:26:12.369
ones like drugs. Um Ssris or other antidepressants are

553
00:26:12.380 --> 00:26:15.800
ways of manipulating our brains. And back in the

554
00:26:15.810 --> 00:26:17.589
nineties, there are a lot of ethicists worried about

555
00:26:17.599 --> 00:26:19.270
that. They thought, oh no. Now we're starting to

556
00:26:19.280 --> 00:26:22.930
change our very personality and that could be kind

557
00:26:22.939 --> 00:26:26.349
of ethically problematic, but we've become a lot more

558
00:26:26.359 --> 00:26:28.869
comfortable with that so. Well. Ok. Sure. I'm taking

559
00:26:28.880 --> 00:26:31.410
a pill that, that alters the serotonin in my

560
00:26:31.420 --> 00:26:33.930
brain. But that's something that I can freely choose

561
00:26:33.939 --> 00:26:36.569
to do and it may alter my personality a

562
00:26:36.579 --> 00:26:38.790
bit. But, but that's something still that I've chosen

563
00:26:38.939 --> 00:26:42.050
to do freely. Now, a lot of ethicists are

564
00:26:42.060 --> 00:26:46.040
getting more concerned about even more direct brain manipulation.

565
00:26:46.380 --> 00:26:49.689
Uh THINGS like brain stimulation, uh It could be

566
00:26:49.699 --> 00:26:54.969
invasive or noninvasive or other kinds of drugs like

567
00:26:54.979 --> 00:26:57.380
psychedelics. So there are a lot of uh there's

568
00:26:57.390 --> 00:26:59.439
a lot of research now about psychedelics suggesting that

569
00:26:59.449 --> 00:27:01.239
we could be used as a, as a form

570
00:27:01.250 --> 00:27:05.020
of uh treatment for medical conditions, uh possibly also

571
00:27:05.030 --> 00:27:08.339
a form of, of enhancement. But a lot of

572
00:27:08.349 --> 00:27:10.520
people are concerned now that that might be an

573
00:27:10.530 --> 00:27:14.189
unethical way of manipulating our brains. It certainly does

574
00:27:14.199 --> 00:27:16.359
change the brain when you, when you have psychedelic

575
00:27:16.369 --> 00:27:19.270
experiences. And there are all these kinds of ethical

576
00:27:19.280 --> 00:27:21.310
questions that, that are raised when we start to

577
00:27:21.319 --> 00:27:25.680
mess with um or manipulate the brain because it's

578
00:27:25.689 --> 00:27:28.439
not really just like having an operation on your

579
00:27:28.449 --> 00:27:31.979
arm or your leg. Uh Now we're talking about

580
00:27:31.989 --> 00:27:33.380
what I like to think of as the seed

581
00:27:33.390 --> 00:27:35.880
of the self. You know, this is where our

582
00:27:35.890 --> 00:27:38.459
free will and our personality and our values is

583
00:27:38.469 --> 00:27:41.500
is roughly housed in the brain and our nervous

584
00:27:41.510 --> 00:27:44.479
system. And so it does raise this question of,

585
00:27:44.489 --> 00:27:48.160
you know, are we uh impairing someone's autonomy by

586
00:27:48.170 --> 00:27:51.040
manipulating their brain or just having a relevant brain

587
00:27:51.050 --> 00:27:54.219
intervention? Um OR can we, can we freely choose

588
00:27:54.229 --> 00:27:57.750
to change our own personality and identity? Uh OR

589
00:27:57.760 --> 00:28:00.890
are we risking too much harm by manipulating the

590
00:28:00.900 --> 00:28:03.520
brain? Especially when we don't really know a lot

591
00:28:03.530 --> 00:28:06.479
about how it actually works just yet. Uh We're

592
00:28:06.489 --> 00:28:08.510
still quite far from that actually, we're, we're still

593
00:28:08.520 --> 00:28:10.170
learning a lot about how the brain works. And

594
00:28:10.180 --> 00:28:13.189
there's, it's a big mystery how all this firing

595
00:28:13.199 --> 00:28:17.260
of neurons gives rise to compliment uh complicated psychological

596
00:28:17.270 --> 00:28:19.920
states, like emotions and decisions and all of that.

597
00:28:20.219 --> 00:28:22.319
And so there might be a real concern about

598
00:28:22.329 --> 00:28:26.280
whether, you know, neuroscience is ready for, you know,

599
00:28:26.290 --> 00:28:29.530
the big stage of, of manipulating people's brains to

600
00:28:29.540 --> 00:28:32.089
try to treat even very serious kinds of medical

601
00:28:32.099 --> 00:28:32.709
conditions.

602
00:28:33.310 --> 00:28:36.650
It's very important that you clarify that because I

603
00:28:36.660 --> 00:28:39.510
mean, I would imagine that most people, when they

604
00:28:39.520 --> 00:28:43.380
hear the term brain manipulation, they would think mostly

605
00:28:43.390 --> 00:28:49.209
of surgery, perhaps deep brain stimulation. Some people also

606
00:28:49.219 --> 00:28:53.839
think about that but not necessarily about uh drugs

607
00:28:53.849 --> 00:28:58.464
in general, like for example, antidepressants or anxiety medication.

608
00:28:58.474 --> 00:29:02.444
But here you are including anything that can have

609
00:29:02.454 --> 00:29:08.354
any sort of uh biochemical electrochemical effect on the

610
00:29:08.364 --> 00:29:10.064
brain as well. Right.

611
00:29:10.175 --> 00:29:12.895
Yes. Yeah. And I think that's important because we

612
00:29:12.905 --> 00:29:17.099
shouldn't be concerned about, you know, brain stimulation just

613
00:29:17.109 --> 00:29:20.160
because it's a bit new and involves even more

614
00:29:20.170 --> 00:29:22.310
direct manipulation of the brain. That doesn't seem to

615
00:29:22.319 --> 00:29:25.420
be a morally relevant kind of distinction. There's still

616
00:29:25.430 --> 00:29:28.920
all the same moral questions arise when we're talking

617
00:29:28.930 --> 00:29:34.209
about things like uh psychedelics or antidepressants, which you

618
00:29:34.219 --> 00:29:37.209
know, many, many more people are taking. You know,

619
00:29:37.219 --> 00:29:38.520
there are a lot of patients who have been

620
00:29:38.530 --> 00:29:41.660
doing things like deep brain stimulation for Parkinson's and

621
00:29:41.670 --> 00:29:45.650
other neurological conditions. But there are many, many more

622
00:29:45.660 --> 00:29:49.430
people who are taking drugs for mental disorders, things

623
00:29:49.439 --> 00:29:52.089
like depression and so forth. So we need to,

624
00:29:52.099 --> 00:29:55.109
I think raise the same kinds of questions there.

625
00:29:55.199 --> 00:29:57.339
But of course, what's new and, and a little

626
00:29:57.349 --> 00:29:59.750
bit more kind of the Wild West for ethics

627
00:29:59.800 --> 00:30:03.619
is direct brain manipulation. And as you say, that

628
00:30:03.630 --> 00:30:05.959
could be done through things like deep brain stimulation

629
00:30:06.675 --> 00:30:09.224
where you have to actually have surgery. They, they

630
00:30:09.234 --> 00:30:12.984
cut open the skull to implant very thin electrodes

631
00:30:12.994 --> 00:30:16.244
deep in the brain to, to treat neurological conditions

632
00:30:16.255 --> 00:30:20.744
like Parkinson's that involve degeneration of the neurons deep

633
00:30:20.755 --> 00:30:23.780
in the brain. And, and that actually has been

634
00:30:24.040 --> 00:30:27.890
quite successful for, for certain patients who have movement

635
00:30:27.900 --> 00:30:31.479
disorders like Parkinson's or essential tremor, but it's being

636
00:30:31.489 --> 00:30:35.109
expanded now to a lot of other conditions, psychiatric

637
00:30:35.119 --> 00:30:41.099
conditions like treatment resistant depression, um anorexia, nervosa, uh

638
00:30:41.109 --> 00:30:43.810
O CD. And so there are concerns I think

639
00:30:43.819 --> 00:30:46.050
about, you know, are, are we using this uh

640
00:30:46.060 --> 00:30:50.079
appropriately for all of these different ranges of conditions?

641
00:30:50.089 --> 00:30:52.109
You know, is it safe? Is it really going

642
00:30:52.119 --> 00:30:54.390
to benefit people. But those are the same kinds

643
00:30:54.400 --> 00:30:57.630
of questions that we can ask about uh antidepressants.

644
00:30:57.640 --> 00:31:01.020
And there have been big debates about whether antidepressants

645
00:31:01.030 --> 00:31:03.839
are actually doing much for people and whether they're

646
00:31:03.849 --> 00:31:06.895
very effective and whether they have risks that we

647
00:31:06.905 --> 00:31:11.175
might not be so aware of. And uh it's

648
00:31:11.185 --> 00:31:14.015
something that has affected me personally because I've taken

649
00:31:14.025 --> 00:31:16.145
drugs before that have been on the market for

650
00:31:16.155 --> 00:31:18.354
decades. And then we find out later that they,

651
00:31:18.415 --> 00:31:21.464
you know, increase the risk of heart disease. Um

652
00:31:21.604 --> 00:31:23.844
I, I get migraines and so I, I've had

653
00:31:23.854 --> 00:31:26.015
a drug that got pulled for that. Ok. Well,

654
00:31:26.025 --> 00:31:28.035
uh surprising because that drug was out for, for

655
00:31:28.045 --> 00:31:29.675
decades and then we find out that it's causing

656
00:31:29.685 --> 00:31:31.790
some harm. So I think we always have to

657
00:31:31.800 --> 00:31:34.199
worry about that with any kind of medical intervention,

658
00:31:34.209 --> 00:31:36.630
but especially with brain interventions, we have to figure

659
00:31:36.640 --> 00:31:40.719
out uh is it really benefiting patients and how

660
00:31:40.729 --> 00:31:43.449
much and then weigh that against the risks and

661
00:31:43.459 --> 00:31:47.640
say, well, is it actually risking more harm than

662
00:31:47.650 --> 00:31:49.000
good? So we always have to do that risk

663
00:31:49.010 --> 00:31:51.770
benefit analysis. And when it comes to manipulating the

664
00:31:51.780 --> 00:31:54.140
brain, we also have to factor in its potential

665
00:31:54.150 --> 00:31:57.859
risks. Uh THINGS like changes to a person's identity

666
00:31:57.869 --> 00:32:00.689
and personality. Um And those are, those are relevant

667
00:32:00.699 --> 00:32:02.119
things to weigh into that calculation.

668
00:32:02.869 --> 00:32:06.699
Uh You know, when earlier I mentioned surgery, I

669
00:32:06.709 --> 00:32:11.000
was also thinking about probably even more invasive things

670
00:32:11.010 --> 00:32:15.150
like lobotomies because you know, I am from Portugal

671
00:32:15.160 --> 00:32:18.709
and Agus Muniz who got the Nobel Prize because

672
00:32:18.719 --> 00:32:22.369
he was one of the pioneers of uh lobotomy

673
00:32:22.380 --> 00:32:25.959
procedures. I mean, we know now that people who

674
00:32:25.969 --> 00:32:29.420
got that kind of intervention back in the fifties,

675
00:32:29.430 --> 00:32:33.130
sixties and so on due to epilepsy, for example,

676
00:32:33.140 --> 00:32:37.520
to cure epilepsy, the epilepsy got cured, but then

677
00:32:37.530 --> 00:32:40.920
they didn't uh end up in a very good

678
00:32:40.930 --> 00:32:46.170
psychological state. They lost some important, I mean, the

679
00:32:46.180 --> 00:32:52.420
decision uh important mechanisms related to decision making, emotion

680
00:32:52.430 --> 00:32:55.750
processing and all of that. So I guess that

681
00:32:55.760 --> 00:32:58.630
would be uh also a very good example of

682
00:32:58.640 --> 00:33:01.569
brain manipulation in the extreme.

683
00:33:02.079 --> 00:33:04.560
Right. Right. Yeah. And we look back at the

684
00:33:04.569 --> 00:33:07.099
days of lobotomies as a kind of dark period

685
00:33:07.109 --> 00:33:09.989
of, of neurology. But it's important as you say,

686
00:33:10.000 --> 00:33:13.020
I mean, this was Nobel Prize winning work and

687
00:33:13.030 --> 00:33:16.500
it did actually, you know, work in some cases.

688
00:33:16.760 --> 00:33:20.060
The problem really was when you get humans who

689
00:33:20.069 --> 00:33:23.099
are, who are fallible, uh taking control of this

690
00:33:23.109 --> 00:33:25.739
kind of discovery and you have these sort of

691
00:33:25.750 --> 00:33:29.939
overzealous um surgeons like um uh Walter Freeman who

692
00:33:29.949 --> 00:33:32.979
who really just took it too far. And it

693
00:33:32.989 --> 00:33:35.040
was the problem there was that kind of risk

694
00:33:35.050 --> 00:33:37.510
benefit analysis. He was, you know, overs selling the

695
00:33:37.520 --> 00:33:41.750
benefits and downplaying the risks and, and actually still

696
00:33:41.760 --> 00:33:44.609
today, of course, we we use things not quite

697
00:33:44.619 --> 00:33:48.430
um you know, frontal lobe lobotomies, but there are

698
00:33:48.439 --> 00:33:52.339
effective treatments that involve removing portions of the brain.

699
00:33:52.719 --> 00:33:55.829
Uh Michael J Fox, who, who has Parkinson's, you

700
00:33:55.839 --> 00:33:58.810
know, one of the most famous people really who,

701
00:33:58.819 --> 00:34:01.699
who is, who's had the disease. He had that

702
00:34:01.709 --> 00:34:05.349
before deep brain stimulation was, was really uh widely

703
00:34:05.359 --> 00:34:07.390
used. So he, he had portions of his brain

704
00:34:07.400 --> 00:34:10.830
removed. I I read his um his memoir and

705
00:34:10.840 --> 00:34:13.679
that's a normal technique that can be helpful. Same

706
00:34:13.688 --> 00:34:15.899
for epilepsy. Uh One of the cases I discussed

707
00:34:15.909 --> 00:34:18.320
in the book as a patient who has epilepsy.

708
00:34:18.330 --> 00:34:20.760
And so he had portions of his temporal lobe

709
00:34:20.770 --> 00:34:24.435
removed actually twice. And then it did have this

710
00:34:24.465 --> 00:34:28.475
problem of causing him to have um some deviant

711
00:34:28.485 --> 00:34:33.364
sexual desires. And he ultimately started downloading child pornography

712
00:34:33.375 --> 00:34:35.685
and got caught by the, the FBI in the

713
00:34:35.695 --> 00:34:39.293
United States. And so there are these questions about,

714
00:34:39.304 --> 00:34:42.895
are we changing who, who people are by either

715
00:34:42.905 --> 00:34:46.774
removing portions of the brain um giving them stimulators

716
00:34:46.784 --> 00:34:50.014
that, that will, you know, continuously be stimulating parts

717
00:34:50.024 --> 00:34:51.947
of the brain. It can only change who they

718
00:34:51.958 --> 00:34:54.779
are but maybe cause them to, to even engage

719
00:34:54.789 --> 00:34:58.039
in, in criminal behavior. So it's just a different

720
00:34:58.049 --> 00:35:00.878
kind of risk benefit analysis that we're doing. And

721
00:35:00.888 --> 00:35:02.958
I think we always have to be very careful,

722
00:35:02.968 --> 00:35:05.999
uh especially when we're talking about an organ that

723
00:35:06.009 --> 00:35:10.138
we don't really understand fundamentally um how it works.

724
00:35:11.250 --> 00:35:13.250
So I, I don't know what you think about

725
00:35:13.260 --> 00:35:16.939
this idea, but I guess that the most important

726
00:35:16.949 --> 00:35:20.070
thing here to consider would be. So, first of

727
00:35:20.080 --> 00:35:24.709
all, have the science uh right, because we need

728
00:35:24.719 --> 00:35:28.379
to know really what works, what doesn't in what

729
00:35:28.389 --> 00:35:32.989
cases and only what works should go on the

730
00:35:33.000 --> 00:35:37.290
market and should be allowed for doctors to perform

731
00:35:37.449 --> 00:35:42.260
uh in patient. Uh AND then uh informed consent

732
00:35:42.270 --> 00:35:45.679
because the patients really do need to know what

733
00:35:45.689 --> 00:35:49.000
are the risks, what are the benefits? Because there's

734
00:35:49.010 --> 00:35:53.169
always risks associated with any kind of intervention. And

735
00:35:53.179 --> 00:35:58.370
also, I mean, if the patient agrees with whatever

736
00:35:58.379 --> 00:36:03.459
kind of uh proper intervention that there, that there's

737
00:36:03.469 --> 00:36:07.399
out there, I mean, I guess we shouldn't consider

738
00:36:07.409 --> 00:36:10.110
that a bad thing, even though in some cases

739
00:36:10.120 --> 00:36:14.179
it might be very invasive or something like that.

740
00:36:14.439 --> 00:36:18.090
Right. Right. And I think informed consent helps alleviate

741
00:36:18.100 --> 00:36:20.620
a lot of these concerns. Um BUT it's not

742
00:36:20.629 --> 00:36:23.300
perfect, you know, think about with, with Freeman and

743
00:36:23.310 --> 00:36:27.000
lobotomies. Um YOU know, if you, if you oversell

744
00:36:27.010 --> 00:36:30.159
the benefits and undersell the, the harms, then we're

745
00:36:30.169 --> 00:36:32.719
going to have, you know, informed consent, but it,

746
00:36:32.729 --> 00:36:35.489
it might not be tracking what the real risk

747
00:36:35.500 --> 00:36:38.080
benefit analysis is. And I think that there is,

748
00:36:38.090 --> 00:36:39.600
there's a danger of that always. And it's not

749
00:36:39.610 --> 00:36:42.770
just with, with neuroscience and neurology, but throughout all

750
00:36:42.780 --> 00:36:45.360
of science that we have to worry about over

751
00:36:45.370 --> 00:36:49.500
selling the benefits and um under appreciating the harms.

752
00:36:49.669 --> 00:36:52.370
And I've been quite influenced by philosopher science here,

753
00:36:52.379 --> 00:36:55.590
uh Jacobs who has tried to you know, raise

754
00:36:55.600 --> 00:36:57.689
the alarm bells here about some of this and

755
00:36:57.699 --> 00:37:01.419
realize that we probably the structures of medicine and

756
00:37:01.429 --> 00:37:05.780
partly through human fallibility and the financial incentives and

757
00:37:05.790 --> 00:37:08.580
big pharma and so forth. We do have certain

758
00:37:08.590 --> 00:37:13.300
structure that will tend towards over selling benefits and

759
00:37:13.360 --> 00:37:17.159
under exploring harms. He's got this great phrase, the

760
00:37:17.169 --> 00:37:19.639
Hollow Hunt for harms and medicine that we don't

761
00:37:19.649 --> 00:37:21.209
actually look for the harms as well as we

762
00:37:21.219 --> 00:37:23.219
should. And so we do have to get, I

763
00:37:23.229 --> 00:37:25.699
think the science right? And yet we're working within

764
00:37:25.709 --> 00:37:29.239
a structure that will tend to bias us towards

765
00:37:29.250 --> 00:37:32.290
um, overs selling benefits and under appreciating harms. It

766
00:37:32.300 --> 00:37:34.899
doesn't mean that, you know, we're, we're in a

767
00:37:34.909 --> 00:37:37.250
terrible state, shouldn't trust, you know, medicine and all

768
00:37:37.260 --> 00:37:38.500
that. But it just means that we have to

769
00:37:38.510 --> 00:37:41.959
be very careful, especially when we're looking at potentially

770
00:37:41.969 --> 00:37:46.760
really invasive and sometimes not reversible treatments that will

771
00:37:46.770 --> 00:37:50.270
change a person's personality and identity have profound effects

772
00:37:50.280 --> 00:37:52.679
on their lives. But, but as you suggest, we

773
00:37:52.689 --> 00:37:54.340
also have to weigh that against, you know, potentially

774
00:37:54.350 --> 00:37:57.669
serious benefits. A lot of these cases involve, you

775
00:37:57.679 --> 00:38:00.689
know, people who have severe movement disorders that can

776
00:38:00.699 --> 00:38:03.959
be so debilitating, they can't live independently. Uh, FOR

777
00:38:03.969 --> 00:38:06.689
others, it involves things like major depression that's completely

778
00:38:06.699 --> 00:38:09.510
resistant to other forms of treatment. And, you know,

779
00:38:09.520 --> 00:38:12.399
we're talking about the potential loss of life, they

780
00:38:12.409 --> 00:38:14.879
might actually take their own lives. Um These are

781
00:38:14.889 --> 00:38:18.070
potentially great benefits if we can, you know, minimize

782
00:38:18.080 --> 00:38:20.790
the number of people who are dying by suicide.

783
00:38:20.899 --> 00:38:22.820
I can have major benefits for people who can

784
00:38:22.830 --> 00:38:26.510
live more independently. And so I certainly understand um

785
00:38:26.520 --> 00:38:30.574
the motivation and for both physicians, but also entrepreneurs

786
00:38:30.584 --> 00:38:32.364
who are trying to develop different kinds of devices

787
00:38:32.375 --> 00:38:37.185
and techniques to treat these very serious neurological conditions.

788
00:38:37.435 --> 00:38:40.574
And I think that just it gets more difficult

789
00:38:40.584 --> 00:38:42.754
as we are getting to the frontiers of this

790
00:38:42.764 --> 00:38:45.895
research. And as we try to expand it to

791
00:38:45.905 --> 00:38:50.665
cases that aren't necessarily so severe debilitating or treatment

792
00:38:50.675 --> 00:38:54.050
resistant, I'll give you one recent example. Um THAT

793
00:38:54.060 --> 00:38:57.570
actually just involves an oral drug. I don't know

794
00:38:57.580 --> 00:38:59.149
if you heard about this, there's a drug for

795
00:38:59.159 --> 00:39:02.020
a LS that got pulled from the market recently

796
00:39:02.100 --> 00:39:02.860
called Reliv.

797
00:39:03.419 --> 00:39:05.949
Where did that happen in the US

798
00:39:05.969 --> 00:39:08.320
in the US? Yes, there's a company that developed

799
00:39:08.330 --> 00:39:11.570
it in the US and it got pulled after

800
00:39:11.580 --> 00:39:13.570
it wasn't really around for maybe a couple of

801
00:39:13.580 --> 00:39:16.129
years, but it was tricky sort of thing is

802
00:39:16.139 --> 00:39:18.290
again, you know, a LS is a very serious

803
00:39:18.300 --> 00:39:24.590
debilitating neurological disorder and there's no real effective treatments

804
00:39:24.600 --> 00:39:27.300
for it. And so the, the FDA, the Food

805
00:39:27.310 --> 00:39:29.500
and Drug administration here, the regulators are trying to

806
00:39:29.510 --> 00:39:32.530
figure out should we allow this, this drug even

807
00:39:32.540 --> 00:39:35.689
though it didn't complete um the full phase three

808
00:39:35.699 --> 00:39:38.040
human trials, but they said there was some early

809
00:39:38.050 --> 00:39:40.659
promising results and they're trying to weigh that against,

810
00:39:40.669 --> 00:39:43.044
but there's no real treatment for this very serious

811
00:39:43.054 --> 00:39:45.675
neurological condition. So they allowed it to go forward.

812
00:39:45.685 --> 00:39:47.074
But they said you do need to complete these

813
00:39:47.084 --> 00:39:50.665
trials. And recently they, they finished out the trials

814
00:39:50.675 --> 00:39:52.864
and they, they found evidence, you know, with much

815
00:39:52.875 --> 00:39:55.834
larger samples, found evidence that there was no difference

816
00:39:55.844 --> 00:39:58.500
between the drug and placebo. And so I think

817
00:39:58.510 --> 00:40:00.540
that the company did the right thing which is

818
00:40:00.550 --> 00:40:02.540
to pull the drug. And they said, ok, well,

819
00:40:02.550 --> 00:40:04.340
we did the science and it looks like it's

820
00:40:04.350 --> 00:40:06.449
not really working and yet it's a, it's a

821
00:40:06.459 --> 00:40:09.530
very expensive drug. Um, IT'S about, I think it

822
00:40:09.540 --> 00:40:11.899
was 100 and $58,000 a year that it would

823
00:40:11.909 --> 00:40:14.590
cost, um, you know, per patient if they, if

824
00:40:14.600 --> 00:40:16.780
they had to continuously take it every day, so

825
00:40:16.790 --> 00:40:19.580
very expensive drug. Uh, WE don't want, you know,

826
00:40:19.590 --> 00:40:21.540
people in the system to be paying for it

827
00:40:21.550 --> 00:40:24.560
if it's not doing anything. Uh, AND it potentially

828
00:40:24.570 --> 00:40:26.429
has, you know, side effects. And I think that

829
00:40:26.439 --> 00:40:28.010
the company did the right thing as they said,

830
00:40:28.020 --> 00:40:29.620
well, let's follow the science, but there are, of

831
00:40:29.629 --> 00:40:32.429
course, so many incentives in, in medicine to go

832
00:40:32.439 --> 00:40:34.979
the other direction. And just about a year ago,

833
00:40:34.989 --> 00:40:37.929
this, this drug, um, Sudafed pe is what they

834
00:40:37.939 --> 00:40:40.510
call it and, and United States, but basically AAA

835
00:40:40.520 --> 00:40:44.510
new form of, uh, like a Sudafed decongestant that

836
00:40:44.520 --> 00:40:47.810
you can't turn into meth, an illegal drug. So

837
00:40:47.820 --> 00:40:49.399
there's great hope that we could have a different

838
00:40:49.409 --> 00:40:51.629
alternative to something you can't turn into an illegal

839
00:40:51.639 --> 00:40:54.030
drug. And they found out after a few decades

840
00:40:54.040 --> 00:40:56.389
that it doesn't do anything, uh, that doesn't work

841
00:40:56.399 --> 00:40:58.969
better than, than, than taking nothing. But that took,

842
00:40:58.979 --> 00:41:00.260
you know, it took, I think it was about

843
00:41:00.270 --> 00:41:02.340
1520 years. It's been on the market and it's

844
00:41:02.350 --> 00:41:04.300
in, it's in everything everywhere, all kinds of cold

845
00:41:04.310 --> 00:41:06.949
medication. So it doesn't always go that way. We

846
00:41:06.959 --> 00:41:08.850
don't always discover early on that something is not

847
00:41:08.860 --> 00:41:11.939
very effective um or that it's, it's quite harmful.

848
00:41:11.949 --> 00:41:13.199
So I think we need to be better about

849
00:41:13.209 --> 00:41:16.060
that and be especially cautious when we're talking about

850
00:41:16.070 --> 00:41:17.699
manipulating the human brain.

851
00:41:18.360 --> 00:41:21.899
Yeah. And unfortunately for you guys in the US

852
00:41:21.909 --> 00:41:26.909
many, many times drugs, uh their price is also

853
00:41:26.919 --> 00:41:31.409
over inflated in comparison to other countries. But, well,

854
00:41:31.419 --> 00:41:36.060
I guess that's a discussion for another day. Um

855
00:41:36.070 --> 00:41:38.219
00 and by the way, for the audience, I

856
00:41:38.229 --> 00:41:40.610
also have an interview on the show with Doctor

857
00:41:40.620 --> 00:41:44.209
J Jacob Stegen. So if people are interested, they

858
00:41:44.219 --> 00:41:46.709
can go and look it up on the channel

859
00:41:46.719 --> 00:41:51.179
or the website. So um moving on, uh let's

860
00:41:51.189 --> 00:41:54.649
talk a little bit about mental disorders because there

861
00:41:54.659 --> 00:41:58.330
are also different aspects to consider here. But first

862
00:41:58.340 --> 00:42:01.320
of all, how do you think we should approach

863
00:42:01.580 --> 00:42:05.649
the question? What is a mental disorder? Because this

864
00:42:05.659 --> 00:42:10.080
is also highly debated and even also among scientists,

865
00:42:10.090 --> 00:42:13.719
psychiatrists, clinical psychologists and so on So as a

866
00:42:13.729 --> 00:42:17.159
philosopher, what would you say about it?

867
00:42:18.060 --> 00:42:20.120
Yeah, I think it, it still is very controversial

868
00:42:20.129 --> 00:42:23.669
and there's no exact consensus, I think among, as

869
00:42:23.679 --> 00:42:27.629
you say, philosopher, psychiatrists. But the standard rough view,

870
00:42:27.639 --> 00:42:30.320
I think that's pretty dominant, at least for the

871
00:42:30.330 --> 00:42:35.780
diagnostic manuals is something like Jerome Wakefield's harmful dysfunction

872
00:42:35.790 --> 00:42:37.840
view. The idea is that while mental disorder has

873
00:42:37.850 --> 00:42:40.639
to involve um both harm to the individual patient

874
00:42:40.649 --> 00:42:42.899
in some form, it's got to disrupt their, their

875
00:42:42.909 --> 00:42:45.850
life, their work and so forth. And also should

876
00:42:45.860 --> 00:42:50.810
involve some form of dysfunction in the mind or

877
00:42:50.820 --> 00:42:53.169
the brain. And one way to see this is,

878
00:42:53.179 --> 00:42:56.840
is with the, the tragic case of having homosexuality

879
00:42:56.850 --> 00:42:59.790
included in these diagnostic manuals for so long in

880
00:42:59.800 --> 00:43:01.889
the United States, it was pulled out only in

881
00:43:01.899 --> 00:43:04.179
the seventies and it was partly on the basis

882
00:43:04.189 --> 00:43:06.600
of this idea. They said, well, sure there can

883
00:43:06.610 --> 00:43:11.129
be harm to people who, who are gay in,

884
00:43:11.139 --> 00:43:13.629
say the United States where at the time uh

885
00:43:13.639 --> 00:43:17.280
was, you know, marginalized and it was, you know,

886
00:43:17.500 --> 00:43:21.709
used as a form of discrimination to, to say

887
00:43:21.719 --> 00:43:24.475
that people had a mental disorder here. But, but

888
00:43:24.485 --> 00:43:27.455
really, it's just because society is intolerant that, that

889
00:43:27.465 --> 00:43:31.155
we're experiencing harm. Some people who, who would say,

890
00:43:31.165 --> 00:43:33.814
well, yes, this is causing me problems. But um

891
00:43:33.824 --> 00:43:36.375
it's mostly through society and it's not a dysfunction

892
00:43:36.645 --> 00:43:40.155
in my mind or brain. So we always have

893
00:43:40.165 --> 00:43:41.745
to navigate that and, and I think it's still

894
00:43:41.754 --> 00:43:45.675
an evolving conversation about which things really do count

895
00:43:45.685 --> 00:43:48.905
as a kind of dysfunction in the mind versus

896
00:43:48.915 --> 00:43:52.030
just a mere difference or variation. And so I

897
00:43:52.040 --> 00:43:55.989
think that's roughly a, a decent conception of, of

898
00:43:56.000 --> 00:43:59.419
mental disorder, even if it's not entirely perfect. But

899
00:43:59.449 --> 00:44:02.270
these days I'm, I'm happy with, with imperfect as

900
00:44:02.280 --> 00:44:03.739
long as it's somewhat helpful.

901
00:44:04.290 --> 00:44:06.669
And I guess that one of the things you

902
00:44:06.679 --> 00:44:10.310
were alluding to there is the issue of stigma,

903
00:44:10.320 --> 00:44:14.189
right? Because some of uh sometimes uh I mean,

904
00:44:14.199 --> 00:44:17.479
in the case of homosexuality, that's very obvious, but

905
00:44:17.489 --> 00:44:20.810
there are other cases where people, because they are

906
00:44:20.820 --> 00:44:25.080
attached a particular label. And even, I mean, nowadays,

907
00:44:25.090 --> 00:44:29.520
even with common conditions, sometimes people just because they

908
00:44:29.530 --> 00:44:34.159
have a label of uh depression or because they

909
00:44:34.169 --> 00:44:37.300
are bipolar or something like that, they are not

910
00:44:37.310 --> 00:44:42.969
treated exactly the same way as normal people are

911
00:44:42.979 --> 00:44:45.750
by other people. And so I, I mean, there's

912
00:44:45.760 --> 00:44:49.090
an issue here that the label, I mean, in

913
00:44:49.100 --> 00:44:52.820
terms of uh the medical system, the legal system

914
00:44:52.830 --> 00:44:56.290
and so on is important and it can also

915
00:44:56.300 --> 00:44:59.239
help the patient to know uh to have a

916
00:44:59.250 --> 00:45:02.649
better understanding of what's going on in their head.

917
00:45:02.659 --> 00:45:05.540
And perhaps that it's not their fault and so

918
00:45:05.550 --> 00:45:09.860
on. But also it might be very stigmatizing to

919
00:45:09.870 --> 00:45:15.300
have certain kinds of labels attached to oneself socially.

920
00:45:15.439 --> 00:45:18.659
Right? Yes, I think it's a real dilemma that

921
00:45:18.669 --> 00:45:20.669
we have here that you're pointing to where on

922
00:45:20.679 --> 00:45:22.659
the one hand, we want to be able to

923
00:45:22.669 --> 00:45:26.290
label mental disorders, to help people get treatment, to

924
00:45:26.300 --> 00:45:29.139
have self understanding. That can be really important for

925
00:45:29.149 --> 00:45:30.929
people to say, ok, this is what I have

926
00:45:30.939 --> 00:45:33.620
and this is helping me understand my condition. And

927
00:45:33.629 --> 00:45:37.209
yet at the same time, it's also potentially stigmatizing

928
00:45:37.219 --> 00:45:40.229
to say that someone has a mental disorder. We

929
00:45:40.239 --> 00:45:43.530
don't really always do that with physical disorders. There

930
00:45:43.540 --> 00:45:45.290
are a lot of times people say, well, I'm

931
00:45:45.300 --> 00:45:48.139
sick, I have the flu. Um OR maybe, you

932
00:45:48.149 --> 00:45:50.739
know, I've just got a temporary physical illness or

933
00:45:50.750 --> 00:45:52.800
even if it's chronic, you might say, well, if

934
00:45:52.810 --> 00:45:55.939
someone has a physical disability and, and that can

935
00:45:55.949 --> 00:45:58.629
be stigmatizing sometimes, but we've come become much more

936
00:45:58.639 --> 00:46:01.229
comfortable. I think with realizing that we need to

937
00:46:01.239 --> 00:46:04.570
be aware of that potential stigma. But it's harder

938
00:46:04.580 --> 00:46:07.070
in the case of mental disorder because people, there

939
00:46:07.080 --> 00:46:08.840
have been studies on this. People feel like if

940
00:46:08.850 --> 00:46:12.370
somebody has a mental disorder, they are maybe um

941
00:46:12.379 --> 00:46:16.520
less rational uh potentially more dangerous, partly because it

942
00:46:16.530 --> 00:46:19.209
is a mental disorder. It's not just a physical

943
00:46:19.219 --> 00:46:22.219
disorder and someone who has a physical disability, like

944
00:46:22.229 --> 00:46:24.600
being in a wheelchair, it's not necessarily affecting their

945
00:46:24.610 --> 00:46:27.810
mind. And so people aren't necessarily making judgments about

946
00:46:27.870 --> 00:46:31.850
the likelihood of them being, you know, dangerous or

947
00:46:31.860 --> 00:46:34.840
untrustworthy. So I think it really is a bind

948
00:46:34.850 --> 00:46:37.810
that we're in. But I think there's, there's hope

949
00:46:37.820 --> 00:46:41.290
that we can recognize that kind of stigma and

950
00:46:41.300 --> 00:46:43.409
realize that we don't necessarily have to have it

951
00:46:43.469 --> 00:46:46.850
while at the same time still categorizing at least

952
00:46:46.860 --> 00:46:50.739
certain conditions as disorders. But some people will say

953
00:46:50.750 --> 00:46:52.770
that we just need to, to pull things out

954
00:46:52.780 --> 00:46:56.570
of the relevant diagnostic manuals. Um Like with, with

955
00:46:56.580 --> 00:47:00.129
homosexuality, the response there was not, well, we'll keep

956
00:47:00.139 --> 00:47:02.580
it in but try not to, to stigmatize it.

957
00:47:02.729 --> 00:47:04.679
Uh We say no, it's just, it, it itself

958
00:47:04.689 --> 00:47:06.889
is not a disorder. And so I think in

959
00:47:06.899 --> 00:47:09.429
a lot of cases, we're trying to navigate that

960
00:47:09.439 --> 00:47:12.189
um when it comes to things like like autism

961
00:47:12.199 --> 00:47:16.540
and A DH D and dyslexia and even deafness.

962
00:47:16.600 --> 00:47:19.429
So there's, there's an ongoing debate about the boundaries

963
00:47:19.439 --> 00:47:21.080
of, of what counts as something that's still a

964
00:47:21.090 --> 00:47:23.719
disorder, but we shouldn't stigmatize it versus something that

965
00:47:23.729 --> 00:47:26.979
just shouldn't be considered a disorder at all. And

966
00:47:26.989 --> 00:47:29.199
it's just part of this idea of, of neurodiversity.

967
00:47:30.080 --> 00:47:34.500
Yeah. And related to neurodiversity. Uh This movement is

968
00:47:34.510 --> 00:47:38.770
also associated with ideas. Uh AGAIN, uh with the

969
00:47:38.780 --> 00:47:43.510
goal of destigmatize some of these things that are

970
00:47:43.520 --> 00:47:47.679
still considered conditions, they might not be anymore one

971
00:47:47.689 --> 00:47:50.840
day, I don't know. Uh uh BUT also to

972
00:47:50.989 --> 00:47:54.540
uh raise awareness for the fact that there perhaps

973
00:47:54.550 --> 00:47:58.840
are people out there who process information in different

974
00:47:58.850 --> 00:48:03.350
ways who think differently. We have different worldviews that

975
00:48:03.360 --> 00:48:09.090
are non normative and there's probably nothing particularly wrong

976
00:48:09.100 --> 00:48:12.149
with that. And I mean, that is the case

977
00:48:12.159 --> 00:48:16.469
for autism and even in more extreme cases. Like,

978
00:48:16.479 --> 00:48:19.800
for example, I, I mean, I, I've read a

979
00:48:19.810 --> 00:48:23.129
lot on these and I've, I've watched some documentaries

980
00:48:23.139 --> 00:48:27.540
from, for example, the Netherlands Belgium where they have

981
00:48:27.550 --> 00:48:32.479
medically assisted suicide. And I mean, if they don't

982
00:48:32.489 --> 00:48:37.550
have any underlying medical condition, even a psychiatric condition

983
00:48:37.780 --> 00:48:42.719
and if someone uh, declares under, uh, uh, I

984
00:48:42.729 --> 00:48:45.449
mean, a particular set of conditions which I can't

985
00:48:45.459 --> 00:48:48.239
really reproduce here because I don't, I don't have

986
00:48:48.250 --> 00:48:50.909
them memorized, but, uh they have to follow a

987
00:48:50.919 --> 00:48:55.399
particular protocol, uh if they express their wish to

988
00:48:55.409 --> 00:48:59.500
no longer live because, I mean, life for them

989
00:48:59.510 --> 00:49:03.439
is no longer meaningful or something like that. I

990
00:49:03.449 --> 00:49:09.040
mean, they are allowed medically assisted suicide. And I

991
00:49:09.050 --> 00:49:12.340
mean, I know that is a very, an extremely

992
00:49:12.350 --> 00:49:15.939
sensitive topic but I also think that on the

993
00:49:15.949 --> 00:49:20.530
other hand, perhaps is, uh, suicide should not be

994
00:49:20.540 --> 00:49:25.600
considered exclusively a medical issue, but also a philosophical

995
00:49:25.610 --> 00:49:28.340
and an existential issue. And I, and I guess

996
00:49:28.350 --> 00:49:32.929
that people should also be allowed to not like

997
00:49:33.060 --> 00:49:35.969
to live, I guess. I mean, that's just my

998
00:49:36.060 --> 00:49:38.229
two cents on, on that. But,

999
00:49:38.669 --> 00:49:41.719
yeah, yeah, it's a very difficult puzzle about trying

1000
00:49:41.729 --> 00:49:44.409
to respect people's autonomy and if they make the

1001
00:49:44.419 --> 00:49:46.739
choice that they want to end their lives, we

1002
00:49:46.750 --> 00:49:49.679
want to respect that. That's certainly the, the sentiment

1003
00:49:49.689 --> 00:49:51.149
I think in the Netherlands which has the most

1004
00:49:51.159 --> 00:49:55.340
permissive euthanasia laws, I think in the world and,

1005
00:49:55.350 --> 00:49:57.100
and yet we're trying to balance that against uh

1006
00:49:57.110 --> 00:49:59.909
unnecessary harm. And especially this issue about how some

1007
00:49:59.919 --> 00:50:02.570
of the harms that can arise and the struggles

1008
00:50:02.580 --> 00:50:05.419
that people have can arise from a society that

1009
00:50:05.429 --> 00:50:10.179
is not accommodating or stigmatizing their condition. Uh One

1010
00:50:10.189 --> 00:50:11.530
of the cases that came out of the Netherlands

1011
00:50:11.540 --> 00:50:13.300
and not that long ago, could have been 10

1012
00:50:13.310 --> 00:50:15.510
years ago now, uh was a, a man in

1013
00:50:15.520 --> 00:50:18.949
his forties who about my age who had struggled

1014
00:50:18.959 --> 00:50:22.110
with alcoholism for decades and it was just, there

1015
00:50:22.120 --> 00:50:23.520
was nothing he could do. He had been in

1016
00:50:23.530 --> 00:50:27.219
and out of uh rehabilitation centers multiple times and

1017
00:50:27.229 --> 00:50:29.620
he just couldn't kick the habit, couldn't get to

1018
00:50:29.629 --> 00:50:31.790
a place where he was happy with his life.

1019
00:50:31.800 --> 00:50:34.620
And so he, he did take advantage of um

1020
00:50:34.889 --> 00:50:38.250
the euthanasia abilities in, in the Netherlands and and

1021
00:50:38.260 --> 00:50:41.129
ended his life. These are very difficult cases partly

1022
00:50:41.139 --> 00:50:43.290
because, yeah, again, we want to respect the person's

1023
00:50:43.300 --> 00:50:47.129
decision. They're an adult um um competent to make

1024
00:50:47.139 --> 00:50:49.649
decisions about their own life. But also you might

1025
00:50:49.659 --> 00:50:53.409
worry that maybe there's not enough accommodation and treatment

1026
00:50:53.419 --> 00:50:56.149
and help for, for those kinds of conditions. So

1027
00:50:56.159 --> 00:50:57.939
yeah, bring it back to, to mental disorder. I

1028
00:50:57.949 --> 00:51:00.100
think it bumps up against all these very difficult

1029
00:51:00.110 --> 00:51:04.300
questions. We want to respect people's individual choices and

1030
00:51:04.310 --> 00:51:07.209
they may construe their own condition as a mere

1031
00:51:07.219 --> 00:51:10.739
difference and, and not a deficit. And we need

1032
00:51:10.750 --> 00:51:12.520
to keep in mind a lot of the insights

1033
00:51:12.530 --> 00:51:14.649
from the neurodiversity movement, which is that there is

1034
00:51:14.659 --> 00:51:17.030
a lot of stigma out there and a and

1035
00:51:17.040 --> 00:51:20.129
a lack of accommodations and they're trying to draw.

1036
00:51:20.139 --> 00:51:23.250
And I think quite rightly uh some movements from

1037
00:51:23.260 --> 00:51:26.360
disability rights activists who, who have tried to show

1038
00:51:26.370 --> 00:51:28.280
that, you know, you can actually live a very

1039
00:51:28.290 --> 00:51:30.969
good life with physical disability as long as there's

1040
00:51:30.979 --> 00:51:34.620
not so much stigma and failure to accommodate. And

1041
00:51:34.629 --> 00:51:37.120
so there's a similar, I think movement for mental

1042
00:51:37.129 --> 00:51:40.530
disorder and, and but there's, there's questions about, you

1043
00:51:40.540 --> 00:51:43.570
know, do we keep it as a disorder in

1044
00:51:43.580 --> 00:51:46.300
the diagnostic manuals but just be more accommodating and

1045
00:51:46.310 --> 00:51:49.469
not stigmatizing or do we remove it as we

1046
00:51:49.479 --> 00:51:51.379
did with the case of homosexuality? And I think

1047
00:51:51.389 --> 00:51:54.780
those conversations are still still ongoing for different kinds

1048
00:51:54.790 --> 00:51:56.449
of neurological types.

1049
00:51:57.290 --> 00:52:01.030
And what about the particular case of addiction? Because

1050
00:52:01.040 --> 00:52:05.949
that's also a highly debated topic in psychiatry and

1051
00:52:05.959 --> 00:52:10.939
also when it comes to particular kinds of proposals

1052
00:52:10.949 --> 00:52:14.010
that people have for particular kinds of addiction, like

1053
00:52:14.020 --> 00:52:17.810
for example, sometimes people talk about or propose video

1054
00:52:17.820 --> 00:52:21.629
game addiction, porn addiction and stuff like that. I

1055
00:52:21.639 --> 00:52:26.580
mean, new kinds of addictions. Uh How do you

1056
00:52:26.590 --> 00:52:32.030
approach that specific kind of disorder if it's a

1057
00:52:32.040 --> 00:52:33.530
disorder at all?

1058
00:52:34.739 --> 00:52:36.760
Yeah, I think addiction is a very interesting case

1059
00:52:36.770 --> 00:52:39.350
because it has the same sort of problem we're

1060
00:52:39.360 --> 00:52:41.850
trying to work on stigma and make sure that

1061
00:52:41.860 --> 00:52:44.550
we're not stigmatizing individuals with addiction, you know, for

1062
00:52:44.560 --> 00:52:46.449
a long time, addiction was just treated as a

1063
00:52:46.459 --> 00:52:49.070
moral failing. And there's been a move to, to

1064
00:52:49.080 --> 00:52:51.179
really medicalize it and say that this is no,

1065
00:52:51.189 --> 00:52:53.340
it's, it's a disorder and not just a disorder,

1066
00:52:53.389 --> 00:52:56.739
but even a disease of the brain. And clearly

1067
00:52:56.750 --> 00:52:58.570
the motivation there is to, to try to move

1068
00:52:58.580 --> 00:53:03.280
away from that moralization towards treatment and, and trying

1069
00:53:03.290 --> 00:53:06.179
to remove all the blame that we've, we've pushed

1070
00:53:06.189 --> 00:53:09.179
on addicts for so many decades and, and for

1071
00:53:09.189 --> 00:53:11.969
for potentially a much longer. So I think that's

1072
00:53:11.979 --> 00:53:14.919
a noble goal. But there's a real question about

1073
00:53:14.929 --> 00:53:17.949
whether we have enough relevant science to suggest that

1074
00:53:17.959 --> 00:53:20.189
it is a brain disease. And this is one

1075
00:53:20.199 --> 00:53:22.469
of my favorite topics I dug into in the

1076
00:53:22.479 --> 00:53:24.840
book, partly because I changed my mind on it,

1077
00:53:24.850 --> 00:53:27.370
which is a sort of miraculous for a philosopher.

1078
00:53:27.379 --> 00:53:29.850
But I went into it thinking, you know, this,

1079
00:53:29.860 --> 00:53:34.500
this seems like the dominant view, most clinicians, healthcare

1080
00:53:34.510 --> 00:53:39.719
professionals, neuroscientists, even potentially most philosophers do now think

1081
00:53:39.729 --> 00:53:43.879
that addiction is a brain disease. And the more

1082
00:53:43.889 --> 00:53:45.389
I dug, the more I thought, I'm not sure

1083
00:53:45.399 --> 00:53:48.860
that that any aspects of that view really hold

1084
00:53:48.870 --> 00:53:50.939
water. So there are three main components, as far

1085
00:53:50.949 --> 00:53:53.270
as I can tell, it's the idea that addiction

1086
00:53:53.280 --> 00:53:56.310
is a disease, but also that it's a disease

1087
00:53:56.320 --> 00:53:58.689
of the brain that that's where the relevant dysfunction

1088
00:53:58.699 --> 00:54:01.570
is. But also that part of that disease is

1089
00:54:01.580 --> 00:54:04.729
that it compromises um self control so that it

1090
00:54:04.739 --> 00:54:10.129
involves irresistible um compulsions that ultimately lead to relapse.

1091
00:54:10.590 --> 00:54:12.860
And there are all kinds of concerns about each

1092
00:54:12.870 --> 00:54:15.939
of those elements. If we just take um the,

1093
00:54:15.979 --> 00:54:19.850
the compulsion parts, there's a lot of real cases

1094
00:54:19.860 --> 00:54:22.780
of, of people with addictions, very serious addictions who

1095
00:54:22.790 --> 00:54:25.840
do exert a lot of control, they do deliberately

1096
00:54:25.850 --> 00:54:29.209
abstain from using for various reasons. Um There study

1097
00:54:29.219 --> 00:54:34.260
suggesting that people will actually take money instead of

1098
00:54:34.270 --> 00:54:37.000
a hit of their drug of choice and, and

1099
00:54:37.010 --> 00:54:38.629
that just doesn't make sense if it really is

1100
00:54:38.639 --> 00:54:41.310
an irresistible impulse, you know, if you have right

1101
00:54:41.320 --> 00:54:42.629
in front of you, the option to take this

1102
00:54:42.639 --> 00:54:44.469
drug that you're addicted to versus just, you know,

1103
00:54:44.479 --> 00:54:47.100
like $20 you would think that the brain disease

1104
00:54:47.110 --> 00:54:48.530
picture would say, well, well, of course, you're just

1105
00:54:48.540 --> 00:54:50.949
going to be compelled uh to, to, to choose

1106
00:54:50.959 --> 00:54:53.459
the drug, but, but many people will choose the

1107
00:54:53.469 --> 00:54:56.030
money. So there's a lot of evidence suggesting people

1108
00:54:56.040 --> 00:54:58.300
can exert control even if it's not perfect and

1109
00:54:58.310 --> 00:55:01.310
it often does fail. You might say, well, ok,

1110
00:55:01.320 --> 00:55:05.379
maybe addiction is not necessarily involving always a compulsion

1111
00:55:05.389 --> 00:55:07.989
that's irresistible, but maybe it's still a brain disease.

1112
00:55:08.000 --> 00:55:09.669
We do have a lot of evidence suggesting that

1113
00:55:09.679 --> 00:55:12.959
there are circuits in the brain that are impaired

1114
00:55:12.969 --> 00:55:16.250
or, or changed at least uh after people have

1115
00:55:16.260 --> 00:55:18.889
been taking drugs for a long time. But even

1116
00:55:18.899 --> 00:55:20.870
that too, it's not so clear that we should

1117
00:55:20.879 --> 00:55:23.949
think of that as due to at least ingestion

1118
00:55:23.959 --> 00:55:26.189
of the drug. The brain disease model really is

1119
00:55:26.199 --> 00:55:28.790
this idea that if, if you take the drug,

1120
00:55:29.350 --> 00:55:32.229
it somehow by taking the drug, it causes changes

1121
00:55:32.239 --> 00:55:34.750
in the brain that then lead to, you know,

1122
00:55:34.760 --> 00:55:38.239
compulsions, uh, to use the drug. And it's just

1123
00:55:38.250 --> 00:55:41.469
not so clear that that's really the primary source

1124
00:55:41.479 --> 00:55:43.860
of addiction. So there are a lot of people

1125
00:55:43.870 --> 00:55:47.139
who take drugs, addictive drug, very powerful addictive drugs

1126
00:55:47.149 --> 00:55:49.899
and don't become addicted. Um I was kind of

1127
00:55:49.909 --> 00:55:51.979
surprised to learn that it's only about 10 to

1128
00:55:51.989 --> 00:55:54.600
20% of people who even take um very serious

1129
00:55:54.610 --> 00:55:57.020
drugs like heroin or meth. Uh THE vast majority

1130
00:55:57.030 --> 00:55:59.489
don't get addicted. It's only about 10 to 20%

1131
00:55:59.500 --> 00:56:02.949
or do get addicted. And it also seems to

1132
00:56:02.959 --> 00:56:05.979
be that there's a lot of other factors distinct

1133
00:56:05.989 --> 00:56:09.179
from ingestion of the drug that contribute to addiction,

1134
00:56:09.580 --> 00:56:16.300
things like trauma, poverty, unemployment, uh co occurring mental

1135
00:56:16.310 --> 00:56:20.060
disorders like depression and anxiety. It's, it's really sad

1136
00:56:20.070 --> 00:56:21.959
actually, many people who end up using and becoming

1137
00:56:21.969 --> 00:56:25.739
addicted, it's partly to cope with things like depression

1138
00:56:25.750 --> 00:56:29.879
and anxiety. And so those are brain factors but

1139
00:56:29.889 --> 00:56:33.810
they're not due to ingestion of the drug. So

1140
00:56:33.820 --> 00:56:36.449
it's, it's not so clear that it's a brain

1141
00:56:36.459 --> 00:56:38.540
disease. I mean, there's something going on in the

1142
00:56:38.550 --> 00:56:42.090
brain, but it might be more a problem of

1143
00:56:42.100 --> 00:56:45.969
trauma isolation, unemployment, depression, anxiety, these are not due

1144
00:56:45.979 --> 00:56:47.669
to ingestion of any drugs. They're, they're sort of

1145
00:56:47.679 --> 00:56:50.489
background factors and they seem to be really important

1146
00:56:50.500 --> 00:56:53.050
factors when it comes to addiction. So then I

1147
00:56:53.060 --> 00:56:55.669
started thinking, well, ok, maybe it's not necessarily a

1148
00:56:55.679 --> 00:56:58.229
brain disease, not due to, to ingestion of the

1149
00:56:58.239 --> 00:57:00.030
drug that causes damage to the circuits in the

1150
00:57:00.040 --> 00:57:03.120
brain. Um, BUT maybe it's still appropriate to call

1151
00:57:03.129 --> 00:57:07.560
it a disease. And, you know, that's also controversial.

1152
00:57:07.570 --> 00:57:08.689
What do you, what do you count as a

1153
00:57:08.699 --> 00:57:11.989
disease versus, you know, maybe a mere disorder. But

1154
00:57:12.000 --> 00:57:15.659
we often think of diseases as involving a concrete

1155
00:57:16.320 --> 00:57:19.379
physiological mechanism that we understand as a kind of

1156
00:57:19.389 --> 00:57:22.010
uh dysfunction. And I don't think that we have

1157
00:57:22.020 --> 00:57:23.919
that we do have this picture of addiction as

1158
00:57:23.929 --> 00:57:28.149
largely involving the social and environmental factors and, and

1159
00:57:28.159 --> 00:57:30.139
other kinds of co occurring mental disorders that don't

1160
00:57:30.149 --> 00:57:34.239
necessarily have this clear underlying pathology. And so we

1161
00:57:34.250 --> 00:57:36.469
get left with this idea that well, maybe doesn't

1162
00:57:36.479 --> 00:57:38.370
make sense to call it the disorder. There are

1163
00:57:38.379 --> 00:57:40.870
a lot of just, just normal brain functions at

1164
00:57:40.879 --> 00:57:44.100
work in the background of, of things like trauma

1165
00:57:44.110 --> 00:57:47.229
and, and poverty and isolation. So it's not so

1166
00:57:47.239 --> 00:57:50.040
clear that we should call it a disease even

1167
00:57:50.050 --> 00:57:53.409
if that might seem helpful for stigma. And it's

1168
00:57:53.419 --> 00:57:55.159
not even so clear that it's helpful for stigma

1169
00:57:55.169 --> 00:57:57.300
uh as we were talking about before calling something

1170
00:57:57.310 --> 00:57:59.530
a disorder disease doesn't necessarily mean that we did

1171
00:57:59.540 --> 00:58:02.889
destigmatize it. Um Actually, it sometimes exacerbates it, we

1172
00:58:02.899 --> 00:58:05.429
think, oh no, this person has a disorder. Uh

1173
00:58:05.439 --> 00:58:08.629
MENTAL disorders it might be more dangerous or, or

1174
00:58:08.639 --> 00:58:11.110
less trustworthy. So it's not so clear that that's

1175
00:58:11.120 --> 00:58:14.760
actually helping us to, to destigmatize addiction in the

1176
00:58:14.770 --> 00:58:15.110
end.

1177
00:58:16.370 --> 00:58:19.169
And I guess that one of the main issues

1178
00:58:19.179 --> 00:58:23.550
here when it comes to whether addiction would be

1179
00:58:23.560 --> 00:58:28.050
a brain disease or not is uh reductionism, right?

1180
00:58:28.060 --> 00:58:32.510
I mean, reductionism in the epistemological sense, I guess

1181
00:58:32.520 --> 00:58:36.590
because I, I mean, there's this very common idea

1182
00:58:36.600 --> 00:58:40.629
nowadays and it's very prevalent in psychiatry as well

1183
00:58:40.639 --> 00:58:45.649
of trying to reduce every condition to something uh

1184
00:58:45.659 --> 00:58:50.000
biological, something happening in the brain. It could be

1185
00:58:50.010 --> 00:58:53.669
genes, it could be hormones, it could be neurotransmitters,

1186
00:58:53.679 --> 00:58:57.889
brain connections, something like that. But uh I mean,

1187
00:58:58.229 --> 00:59:01.370
it, we, I guess we should be wary of

1188
00:59:01.379 --> 00:59:05.780
even if there's some sort of psychological predisposition and

1189
00:59:05.790 --> 00:59:09.840
it's caused by partly by genes, partly by hormones,

1190
00:59:09.850 --> 00:59:14.320
partly by neurotransmitters or something like that. Um We

1191
00:59:14.330 --> 00:59:17.280
should be wary of trying to reduce all the

1192
00:59:17.290 --> 00:59:22.560
explanatory power to those factors, right? Because there are

1193
00:59:22.570 --> 00:59:27.699
other uh external environmental factors playing the role as

1194
00:59:27.709 --> 00:59:28.449
well.

1195
00:59:28.459 --> 00:59:30.530
Right. Right. Yeah. And I think there's a lot

1196
00:59:30.540 --> 00:59:34.320
of confusion about this, uh especially among non philosophers

1197
00:59:34.330 --> 00:59:37.419
because they, they might confuse uh these different issues

1198
00:59:37.429 --> 00:59:39.469
here and think that if you're saying there's not

1199
00:59:39.479 --> 00:59:43.149
a reduction of the psychological to the neurobiological, you

1200
00:59:43.159 --> 00:59:45.129
must be a dualist and you must think that

1201
00:59:45.139 --> 00:59:47.820
these, there's some sort of sort of separate spiritual

1202
00:59:47.830 --> 00:59:50.129
kind of world and, and these just aren't neurobiological

1203
00:59:50.139 --> 00:59:53.489
phenomena. But yeah, that is a different debate. And

1204
00:59:53.500 --> 00:59:55.080
really the the question here is just we could

1205
00:59:55.090 --> 00:59:57.229
assume that, you know, the mind just is a

1206
00:59:57.239 --> 01:00:00.149
physical thing that just is, is involves activity in

1207
01:00:00.159 --> 01:00:02.510
the brain. But there's really a question about whether

1208
01:00:02.520 --> 01:00:04.929
it's just brain circuits that are going to be

1209
01:00:04.939 --> 01:00:08.479
the relevant source of explanation uh rather than more

1210
01:00:08.489 --> 01:00:11.709
environmental factors. And of course, those are connected, I

1211
01:00:11.719 --> 01:00:13.840
mean, think about with addiction, one of the major

1212
01:00:13.850 --> 01:00:16.409
factors that you might think of as like environmental

1213
01:00:16.439 --> 01:00:20.169
is stress. So people who are in stressful environments,

1214
01:00:20.179 --> 01:00:23.179
um there's a famous study of Vietnam veterans who

1215
01:00:23.189 --> 01:00:25.739
were in the war and they used heroin at

1216
01:00:25.750 --> 01:00:27.770
much higher rates, but then stopped when they came

1217
01:00:27.780 --> 01:00:30.760
home. So they became less stressed there at back

1218
01:00:30.770 --> 01:00:34.209
home. And there too, you'd say, well that's environmental.

1219
01:00:34.850 --> 01:00:38.050
Sure. But of course, stress manifests in the brain,

1220
01:00:38.060 --> 01:00:40.310
there's brain activity that's associated with stress. So, so

1221
01:00:40.320 --> 01:00:42.370
of course, it all comes anything mental comes down

1222
01:00:42.379 --> 01:00:44.939
to something in the brain. But that's why I

1223
01:00:44.949 --> 01:00:47.169
like this distinction between, you know, is it something

1224
01:00:47.179 --> 01:00:49.229
in the brain caused by ingestion of the drugs

1225
01:00:49.239 --> 01:00:53.510
or is it caused by more environmental factors? And

1226
01:00:53.520 --> 01:00:56.510
I think that's a major source of confusion that

1227
01:00:56.520 --> 01:00:59.805
the debate is about whether it's a physical phenomenon

1228
01:00:59.815 --> 01:01:01.756
or something to do with the brain. Uh But

1229
01:01:01.766 --> 01:01:04.996
which aspect of the brain and which is doing

1230
01:01:05.006 --> 01:01:07.395
the most explanation for us should we explain it

1231
01:01:07.406 --> 01:01:12.686
in terms of uh neurobiological dysfunction or more environmental,

1232
01:01:12.696 --> 01:01:18.325
social things like unemployment and poverty and stressful conditions.

1233
01:01:18.496 --> 01:01:20.402
So I think that's a really important distinction and

1234
01:01:20.412 --> 01:01:21.781
it's one that, you know, psychiatry, as you say,

1235
01:01:21.791 --> 01:01:25.001
a lot of them think that it's obvious. But,

1236
01:01:25.011 --> 01:01:26.501
but it's an open question. I was just talking

1237
01:01:26.511 --> 01:01:28.382
to a psychiatrist the other day and we were

1238
01:01:28.392 --> 01:01:29.981
on this very issue. And so I asked him,

1239
01:01:29.991 --> 01:01:31.362
I said, so do you think like all the

1240
01:01:31.372 --> 01:01:34.221
disorders that are in these diagnostic manuals, do you

1241
01:01:34.231 --> 01:01:39.362
think they will ultimately have a very straightforward neurobiological

1242
01:01:39.412 --> 01:01:43.142
pathology like Alzheimer's or Parkinson's? And he said, yeah,

1243
01:01:43.152 --> 01:01:46.290
sure, sure. But I thought that's kind of weird

1244
01:01:46.300 --> 01:01:48.929
because it seems like an open question, an open

1245
01:01:48.939 --> 01:01:51.489
scientific question. It could be that it's not uh

1246
01:01:51.500 --> 01:01:55.399
one analogy here is with, um with race for

1247
01:01:55.409 --> 01:01:56.929
a long time. I think people assume that of

1248
01:01:56.939 --> 01:01:59.770
course race has some sort of biological reality. There

1249
01:01:59.780 --> 01:02:02.429
must be something, you know, deep in our genes

1250
01:02:02.439 --> 01:02:05.939
that differentiates the different races. But the consensus view

1251
01:02:05.949 --> 01:02:07.929
now based on a lot of scientific research is,

1252
01:02:07.939 --> 01:02:10.719
is that's not really right, that race is largely

1253
01:02:10.729 --> 01:02:13.219
a social construct. And so, you know, we could

1254
01:02:13.229 --> 01:02:15.270
have said, oh, it would be obvious, you know,

1255
01:02:15.280 --> 01:02:16.510
that it's going to turn out a certain way.

1256
01:02:16.520 --> 01:02:18.850
But I think similarly, it could turn out that

1257
01:02:18.860 --> 01:02:22.340
things like addiction, uh don't really primarily involve a

1258
01:02:22.350 --> 01:02:24.909
kind of underlying pathology in the brain and the

1259
01:02:24.919 --> 01:02:28.060
way that Alzheimer's and Parkinson's does. But I think

1260
01:02:28.070 --> 01:02:30.159
that there are certain cases, it might be, it

1261
01:02:30.169 --> 01:02:32.750
could be that schizophrenia turns out that way. Uh,

1262
01:02:32.760 --> 01:02:35.919
MAYBE not though, same thing with, with autism, maybe

1263
01:02:35.929 --> 01:02:37.560
if we don't, we consider it just a mere

1264
01:02:37.570 --> 01:02:40.560
difference or variation, not, not a disorder. Uh, I,

1265
01:02:40.570 --> 01:02:42.959
I worked in an autism lab for a couple

1266
01:02:42.969 --> 01:02:45.659
of years as part of my, my neuroscience training

1267
01:02:45.810 --> 01:02:49.399
and uh researcher there said, you know, I've been

1268
01:02:49.409 --> 01:02:52.310
looking for a biomarker for, for autism for decades

1269
01:02:52.320 --> 01:02:55.100
and, and they can't find one. AND a sort

1270
01:02:55.110 --> 01:02:59.199
of neurobiological change that they could indicate that somebody

1271
01:02:59.209 --> 01:03:02.409
has, has autism and it's just not there. So

1272
01:03:02.419 --> 01:03:04.350
it could turn out for any of these cases

1273
01:03:04.360 --> 01:03:08.469
that there's just not a clear discrete underlying kind

1274
01:03:08.479 --> 01:03:11.340
of brain difference or pathology. And I think we

1275
01:03:11.350 --> 01:03:13.159
just have to keep an open mind about which

1276
01:03:13.169 --> 01:03:15.840
cases will turn out one way versus the other.

1277
01:03:15.850 --> 01:03:18.129
And that matters a lot for, for fixing and

1278
01:03:18.139 --> 01:03:19.649
helping these. But so for addiction, which is a

1279
01:03:19.659 --> 01:03:22.719
major medical problem right now, especially with the opioid

1280
01:03:22.729 --> 01:03:25.159
crisis, it really matters how we treat it. So

1281
01:03:25.169 --> 01:03:27.010
if we think of it as largely just some

1282
01:03:27.020 --> 01:03:30.310
sort of underlying brain pathology, um that will focus

1283
01:03:30.320 --> 01:03:33.790
our efforts on neurobiological interventions. But if we think

1284
01:03:33.800 --> 01:03:37.939
that addiction is largely a social problem of unemployment,

1285
01:03:37.949 --> 01:03:41.229
poverty and stress and trauma, well then our relevant

1286
01:03:41.239 --> 01:03:44.139
interventions could be very different. Uh, WE might be

1287
01:03:44.149 --> 01:03:46.080
looking at more like, sort of economic changes and

1288
01:03:46.090 --> 01:03:47.090
policy changes.

1289
01:03:48.120 --> 01:03:50.919
Yes. And I guess that the biggest issue here

1290
01:03:50.929 --> 01:03:54.959
is perhaps not so much that many people out

1291
01:03:54.969 --> 01:04:00.219
there, particularly psychiatrists believe that uh everything is reducible

1292
01:04:00.229 --> 01:04:04.659
to biology or neurobiology. But the fact that they

1293
01:04:04.750 --> 01:04:08.840
think that that's for sure, the case, I mean,

1294
01:04:08.850 --> 01:04:13.270
as a scientist using words, like it's obvious it's

1295
01:04:13.280 --> 01:04:17.590
for sure like this, it is certainly this way.

1296
01:04:17.729 --> 01:04:20.889
I mean, I guess that's no longer doing science,

1297
01:04:20.899 --> 01:04:24.659
right? Because this might sound a bit pedantic, but

1298
01:04:24.909 --> 01:04:29.149
uh science is ever evolving. And uh at least

1299
01:04:29.360 --> 01:04:32.979
uh when you first get into an issue, you

1300
01:04:32.989 --> 01:04:37.830
shouldn't go into it with particular kinds of assumptions

1301
01:04:37.840 --> 01:04:40.679
in one way or the other. And in this

1302
01:04:40.689 --> 01:04:46.239
particular case, assuming, assuming that for sure, all medical

1303
01:04:46.250 --> 01:04:50.679
or all mental conditions would be reducible to biological

1304
01:04:50.689 --> 01:04:56.520
factors, uh raises many uh alarms. Right.

1305
01:04:57.250 --> 01:04:59.260
Yes, I mean, I, I, I've become myself more

1306
01:04:59.270 --> 01:05:02.100
realistic about, about science and how it works partly

1307
01:05:02.110 --> 01:05:04.919
by doing it myself with collaborators and, and working

1308
01:05:04.929 --> 01:05:07.790
with scientists. I think that we often have this,

1309
01:05:08.149 --> 01:05:10.810
well, this background assumption that the science has to

1310
01:05:10.820 --> 01:05:14.040
be completely objective and that we, we shouldn't bring

1311
01:05:14.050 --> 01:05:16.909
in any background assumptions. I think really in the

1312
01:05:16.919 --> 01:05:19.479
end, that's just sort of psychologically impossible that we

1313
01:05:19.489 --> 01:05:21.770
are humans, we, you know, science is done by

1314
01:05:21.780 --> 01:05:24.040
humans and we of course, are going to bring

1315
01:05:24.050 --> 01:05:27.679
all of our baggage to the table, our assumptions,

1316
01:05:27.709 --> 01:05:30.540
our biases. And I think we have to just

1317
01:05:30.550 --> 01:05:32.689
sort of live with that and accept it, but

1318
01:05:32.699 --> 01:05:35.489
there are better or worse ways of doing that.

1319
01:05:35.620 --> 01:05:37.129
And I think we do need to, I think

1320
01:05:37.139 --> 01:05:41.350
as the general public especially, uh be more realistic

1321
01:05:41.360 --> 01:05:44.429
about how science works and realize that, you know,

1322
01:05:44.439 --> 01:05:47.989
this does involve certain kinds of assumptions and the

1323
01:05:48.000 --> 01:05:51.729
addiction cases. It's very evident um where a lot

1324
01:05:51.739 --> 01:05:55.649
of this brain disease model comes from, uh the

1325
01:05:55.659 --> 01:05:57.810
the relevant kinds of agencies that do a lot

1326
01:05:57.820 --> 01:05:59.729
of the funding for this work. At least in

1327
01:05:59.739 --> 01:06:02.629
the United States, the National Institute for drug abuse

1328
01:06:02.639 --> 01:06:08.489
has been very influential in, in developing and doing

1329
01:06:08.500 --> 01:06:11.429
research on and supporting the brain disease model of

1330
01:06:11.439 --> 01:06:14.760
addiction. And so it's kind of no surprise that,

1331
01:06:14.770 --> 01:06:17.669
you know, the whole institution structures are all pushing

1332
01:06:17.679 --> 01:06:20.250
a certain framework that that's going to be the

1333
01:06:20.260 --> 01:06:23.620
dominant framework. And I think that it sort of

1334
01:06:23.629 --> 01:06:26.699
makes sense that that's what we're working with now.

1335
01:06:26.969 --> 01:06:29.139
But there are, I think deep concerns about whether

1336
01:06:29.149 --> 01:06:33.659
we're being sufficiently critical about it. Uh So I

1337
01:06:33.669 --> 01:06:35.429
think it's great to always have an open mind

1338
01:06:35.520 --> 01:06:37.489
when it comes to science. But I think it's

1339
01:06:37.500 --> 01:06:40.044
also almost more important as the general public and

1340
01:06:40.054 --> 01:06:43.854
scientists ourselves to just realize that, that we're fallible

1341
01:06:43.864 --> 01:06:45.905
and we can't, we can't do that. I mean,

1342
01:06:45.915 --> 01:06:47.485
a lot of philosophers of science have have come

1343
01:06:47.495 --> 01:06:49.354
to this point. They say we should give up

1344
01:06:49.364 --> 01:06:52.475
the value free ideal in science. It's just not

1345
01:06:52.485 --> 01:06:54.905
possible. And, and I'm totally on board with that.

1346
01:06:54.915 --> 01:06:57.864
I think once we look at human psychology and,

1347
01:06:57.875 --> 01:07:00.834
and do the neuroscience realize this, this is just

1348
01:07:00.844 --> 01:07:03.284
how the human brain works that we can't get

1349
01:07:03.294 --> 01:07:07.370
away from our own uh biases, our own assumptions

1350
01:07:07.459 --> 01:07:10.239
and our own motivations for wanting to seek out

1351
01:07:10.250 --> 01:07:12.649
certain answers rather than others.

1352
01:07:13.110 --> 01:07:15.439
And by the way with that in mind and

1353
01:07:15.449 --> 01:07:20.860
referring again to how certain conditions or were considered

1354
01:07:20.870 --> 01:07:24.610
conditions in the past, like homosexuality and they are

1355
01:07:24.620 --> 01:07:29.209
no longer considered as such nowadays. Uh We, we

1356
01:07:29.219 --> 01:07:31.979
also have to keep in mind that science and

1357
01:07:31.989 --> 01:07:38.620
psychiatry specifically operates within a particular socio context and

1358
01:07:38.629 --> 01:07:44.760
also um responds to the kinds of uh moral

1359
01:07:44.770 --> 01:07:49.899
norms that operate in the societies where they, I

1360
01:07:49.909 --> 01:07:52.620
mean, where the med uh where the medical and

1361
01:07:52.629 --> 01:07:54.899
the scientific knowledge is produced.

1362
01:07:55.500 --> 01:07:58.530
Right. Right. Yeah. And, and there are these forces

1363
01:07:58.540 --> 01:08:01.389
that are there that are not necessarily forces for,

1364
01:08:01.399 --> 01:08:03.129
for truth. There is, there is a lot of

1365
01:08:03.139 --> 01:08:07.179
money involved in medicine and it's not just in,

1366
01:08:07.189 --> 01:08:09.649
in big pharma, people often think about pills as

1367
01:08:09.659 --> 01:08:11.979
being the relevant concern here. But it's for any

1368
01:08:11.989 --> 01:08:14.659
any medical intervention, there's, there's going to be some

1369
01:08:14.669 --> 01:08:17.720
kind of financial factors at play. And I think

1370
01:08:17.729 --> 01:08:19.788
a lot of the public, you know, uh as

1371
01:08:19.798 --> 01:08:21.849
they say, in the South. Uh uh BLESS their

1372
01:08:21.858 --> 01:08:25.698
heart think that, uh you know, well, this is

1373
01:08:25.809 --> 01:08:28.417
what happens in industry and, and when you have

1374
01:08:28.429 --> 01:08:29.979
a lot of like, scientists are funded by the

1375
01:08:29.988 --> 01:08:32.639
government that, of course, there's no biases and everything

1376
01:08:32.648 --> 01:08:35.769
is ok, but it's not just, I think financial

1377
01:08:35.778 --> 01:08:38.288
incentives that, that play a role in how we

1378
01:08:38.298 --> 01:08:41.724
do science, it's all kinds of just ordinary human

1379
01:08:41.773 --> 01:08:44.943
kinds of motivations. Uh We're still working within a

1380
01:08:44.953 --> 01:08:47.363
social structure and we still have these kinds of,

1381
01:08:47.374 --> 01:08:50.523
as you say, socio cultural assumptions, but it's not

1382
01:08:50.533 --> 01:08:53.173
even just our, our moral views, like about what

1383
01:08:53.184 --> 01:08:57.563
is deviant versus what's not. It's also just that

1384
01:08:57.573 --> 01:08:59.563
there are a lot of motivations like, um I,

1385
01:08:59.573 --> 01:09:01.542
I've worked in, in labs and, you know, there's

1386
01:09:01.554 --> 01:09:03.934
a lot of motivation to say, make sure that

1387
01:09:03.943 --> 01:09:07.290
your graduate students or your postdocs get jobs. And

1388
01:09:07.299 --> 01:09:10.209
so that's going to influence maybe how you spin

1389
01:09:10.220 --> 01:09:11.799
some of your research. Like we need to get

1390
01:09:11.810 --> 01:09:13.830
this published in a good journal so that my,

1391
01:09:13.839 --> 01:09:17.750
and that's a perfectly morally laudable motivation. Um But

1392
01:09:17.759 --> 01:09:20.229
it might not be a motivation that's necessarily tied

1393
01:09:20.240 --> 01:09:23.000
to the truth. Um There are conflicts there, right?

1394
01:09:23.009 --> 01:09:26.740
Sometimes, you know, being completely truthful and, and uh

1395
01:09:26.750 --> 01:09:30.399
honest about your, your work and its limitations will

1396
01:09:30.410 --> 01:09:33.089
mean that you'll have costs to whether you can

1397
01:09:33.100 --> 01:09:35.270
help the people who depend on you in your

1398
01:09:35.279 --> 01:09:37.209
lab. And so there are all kinds of human

1399
01:09:37.220 --> 01:09:40.759
motivations like that. Um ALSO just sort of social

1400
01:09:40.770 --> 01:09:44.169
standing and credit, you know, scientists are human beings.

1401
01:09:44.180 --> 01:09:46.290
And so they're influenced by what their peers think

1402
01:09:46.299 --> 01:09:48.140
of them and they, they want to not necessarily

1403
01:09:48.149 --> 01:09:49.299
make a lot of money. Actually, a lot of

1404
01:09:49.310 --> 01:09:51.910
people want to do things like win Nobel prizes

1405
01:09:51.919 --> 01:09:54.799
or get status within that profession. And that sometimes

1406
01:09:54.810 --> 01:09:57.470
is at odds with, with the truth, sometimes the

1407
01:09:57.479 --> 01:10:00.029
truth is not necessarily going to be as interesting.

1408
01:10:00.319 --> 01:10:02.620
And so it's not that scientists are frauds and

1409
01:10:02.629 --> 01:10:05.100
you know, they're trying to cut corners deliberately. But

1410
01:10:05.109 --> 01:10:07.459
again, it comes back to full circle to, to

1411
01:10:07.470 --> 01:10:09.069
how a lot of our choices are influenced by

1412
01:10:09.080 --> 01:10:13.419
unconscious forces and even scientists can be unconsciously influenced

1413
01:10:13.430 --> 01:10:17.970
by a lot of these factors to maybe inadvertently

1414
01:10:18.040 --> 01:10:21.509
cut corners and, and not necessarily doing the best

1415
01:10:21.520 --> 01:10:23.799
science or, or I was watching out for the

1416
01:10:23.810 --> 01:10:26.629
limitations. So I think we have to just take

1417
01:10:26.640 --> 01:10:29.020
that all on board, be open about it and

1418
01:10:29.029 --> 01:10:31.209
say we've got our biases. Let's just try to

1419
01:10:31.220 --> 01:10:33.810
read them out, let's try to understand them, but

1420
01:10:33.819 --> 01:10:36.000
we'll probably never get rid of them entirely.

1421
01:10:36.509 --> 01:10:39.850
So, shifting gears a little bit. Now, I would

1422
01:10:39.859 --> 01:10:43.600
like to ask you about a particular topic within

1423
01:10:43.609 --> 01:10:46.520
the realm of morality that you explore in the

1424
01:10:46.529 --> 01:10:51.240
book. Uh TELL us about moral enhancement. So what

1425
01:10:51.250 --> 01:10:54.140
is that? And what does it have to do

1426
01:10:54.149 --> 01:10:55.660
with neuroscience?

1427
01:10:56.169 --> 01:10:58.750
Hm. Yes, I mean, moral enhancement. Would just mean

1428
01:10:58.970 --> 01:11:03.069
improving yourself morally. And the contrast there is with

1429
01:11:03.080 --> 01:11:06.990
treatment. So there's a, a distinction between treatment versus

1430
01:11:07.000 --> 01:11:09.149
enhancement and much of medicine is just all about

1431
01:11:09.160 --> 01:11:14.560
treating problems, disorders, uh dysfunction disease. But then if

1432
01:11:14.569 --> 01:11:17.629
we go beyond normal, we're in the realm of,

1433
01:11:17.640 --> 01:11:21.129
of enhancement. And that's something that we, we want

1434
01:11:21.140 --> 01:11:23.759
to have in morality because we are not morally

1435
01:11:23.770 --> 01:11:27.279
perfect beings. There's always room for improvement uh individually

1436
01:11:27.290 --> 01:11:29.669
and as a society and, and we see that,

1437
01:11:29.680 --> 01:11:31.600
you know, with things like uh having a better

1438
01:11:31.609 --> 01:11:35.310
understanding about, about homosexuality and about diversity, we see

1439
01:11:35.319 --> 01:11:37.790
a lot of this improvement happening, but there's always

1440
01:11:37.799 --> 01:11:39.939
more to be done. So I think it's, it's

1441
01:11:39.950 --> 01:11:41.819
laudable for us to say how can we be

1442
01:11:41.830 --> 01:11:46.129
better? And we usually do that not through neuroscience

1443
01:11:46.140 --> 01:11:49.839
or manipulating our brains, at least not directly. But

1444
01:11:49.850 --> 01:11:51.950
as we were saying before, of course, everything, every

1445
01:11:51.959 --> 01:11:54.520
mental phenomenon comes down to something in the brain.

1446
01:11:54.529 --> 01:11:56.120
So if you're going to be a morally better

1447
01:11:56.129 --> 01:11:59.540
person, have better beliefs and better motivations and better

1448
01:11:59.549 --> 01:12:02.339
character traits, it's going to involve changes in the

1449
01:12:02.350 --> 01:12:06.839
brain. So when we educate Children about morality or

1450
01:12:06.850 --> 01:12:09.470
when they read books or, or watch movies and

1451
01:12:09.479 --> 01:12:12.310
learn something about morality, then there's a change in

1452
01:12:12.319 --> 01:12:14.939
their brain. Um But that's kind of indirect. That's,

1453
01:12:14.950 --> 01:12:17.379
that's traditional kinds of enhancement. We try to make

1454
01:12:17.390 --> 01:12:20.930
sure that people are being taught good lessons and

1455
01:12:20.939 --> 01:12:22.830
we, we learn in in various modalities just by

1456
01:12:22.839 --> 01:12:27.290
watching other people. But also we could potentially more

1457
01:12:27.299 --> 01:12:29.959
directly manipulate our brains to enhance ourselves. There's been

1458
01:12:29.970 --> 01:12:32.799
a lot of interest lately about things like using

1459
01:12:32.810 --> 01:12:37.770
brain stimulation, potentially psychedelics or, or other drugs to

1460
01:12:37.779 --> 01:12:40.970
directly manipulate our brains in order to enhance ourselves

1461
01:12:40.979 --> 01:12:45.060
morally. And that is a question and ethic partly

1462
01:12:45.069 --> 01:12:47.040
because it, it might be that, you know, we

1463
01:12:47.049 --> 01:12:48.810
shouldn't be doing this, we shouldn't be trying to

1464
01:12:48.819 --> 01:12:51.330
directly manipulate our brains partly because it might not

1465
01:12:51.339 --> 01:12:54.569
be safe or we might be kind of manipulating

1466
01:12:54.580 --> 01:12:57.509
ourselves in a way that seems like we're constraining

1467
01:12:57.520 --> 01:13:01.009
our freedom or uh maybe not being very authentic.

1468
01:13:01.689 --> 01:13:03.500
And so we have some of those same kinds

1469
01:13:03.509 --> 01:13:05.520
of questions about brain interventions that, but now applied

1470
01:13:05.529 --> 01:13:10.029
to uh enhancement that maybe we shouldn't be embarking

1471
01:13:10.040 --> 01:13:13.379
on that kind of dystopian, you know, project of

1472
01:13:13.390 --> 01:13:16.790
trying to perfect ourselves through, you know, taking drugs

1473
01:13:16.799 --> 01:13:19.580
and, and, you know, getting do it yourself, brain

1474
01:13:19.589 --> 01:13:21.270
stimulators and all of that. So there's been a

1475
01:13:21.279 --> 01:13:24.830
lot of concerns among ethicists about that. And for

1476
01:13:24.839 --> 01:13:26.950
me, I, I'm, I'm less concerned, I think that

1477
01:13:26.959 --> 01:13:29.029
it can be done. Well, there are concerns about

1478
01:13:29.040 --> 01:13:32.770
enhancement. We don't want, you know, the government mandating

1479
01:13:33.029 --> 01:13:36.640
that, you know, we lace the water with Oxytocin

1480
01:13:36.649 --> 01:13:39.140
so that everyone becomes a little bit more compassionate

1481
01:13:39.149 --> 01:13:43.020
or something that's certainly going to constrain freedom. And,

1482
01:13:43.029 --> 01:13:45.040
you know, that's, that's very concerning. So I've been

1483
01:13:45.049 --> 01:13:48.419
trying to think myself about what is a possible

1484
01:13:48.430 --> 01:13:52.959
route of directly manipulating our brains to make ourselves

1485
01:13:52.970 --> 01:13:55.899
better people that we could, we could still respect

1486
01:13:55.910 --> 01:13:57.180
and say that this is, this is something that's

1487
01:13:57.189 --> 01:14:00.540
not concerning. And I think it depends on, on

1488
01:14:00.709 --> 01:14:04.700
the intervention so it could be their ways of

1489
01:14:04.709 --> 01:14:06.979
doing this is very unsafe. I mean, you can,

1490
01:14:07.540 --> 01:14:08.970
I don't want to recommend anyone to do this,

1491
01:14:08.979 --> 01:14:10.890
but you can, you can buy various, you know,

1492
01:14:10.899 --> 01:14:13.290
components online and, and get a little do it

1493
01:14:13.299 --> 01:14:15.839
yourself. Brain stimulator, it's probably not gonna be very

1494
01:14:15.850 --> 01:14:17.709
effective, it's not invasive. It's just things that you

1495
01:14:17.720 --> 01:14:19.830
can put on the outside of, of your head

1496
01:14:19.839 --> 01:14:22.270
and it will just give some surface level stimulation,

1497
01:14:22.370 --> 01:14:26.169
things like um transcranial direct current stimulation, probably not

1498
01:14:26.180 --> 01:14:28.950
going to be very effective and also may not

1499
01:14:28.959 --> 01:14:31.069
be very harmful for that reason because it's not

1500
01:14:31.080 --> 01:14:34.140
so invasive. But the thing I think is especially

1501
01:14:34.149 --> 01:14:38.399
interesting is is psychedelics because those can have very

1502
01:14:38.410 --> 01:14:40.470
profound effects on people. And yet there's a big

1503
01:14:40.479 --> 01:14:44.529
concern about, about safety and, and psychedelics might be

1504
01:14:44.540 --> 01:14:46.779
one of those interventions that does have a direct

1505
01:14:46.790 --> 01:14:49.209
effect on morality. A lot of people who do

1506
01:14:49.220 --> 01:14:52.270
have psychedelic experiences report that they have this kind

1507
01:14:52.279 --> 01:14:55.370
of uh ego dissolution where they feel more at

1508
01:14:55.379 --> 01:14:58.200
one with other people and the universe. And so

1509
01:14:58.209 --> 01:15:01.509
they become less self centered, more compassionate, more open

1510
01:15:01.520 --> 01:15:04.370
minded. These are all, you know, great traits as

1511
01:15:04.379 --> 01:15:07.379
far as we can tell from morality perspective, so

1512
01:15:07.390 --> 01:15:08.879
it might be that that could be a source

1513
01:15:08.890 --> 01:15:12.229
of, of moral enhancement. And yet there are maybe

1514
01:15:12.240 --> 01:15:15.410
some reasonable concerns about whether it's safe and whether

1515
01:15:15.419 --> 01:15:17.910
it's something that is, is really a kind of

1516
01:15:17.919 --> 01:15:19.600
future that we want to embark on.

1517
01:15:20.660 --> 01:15:24.419
Uh, I mean, whenever I hear the word enhancement,

1518
01:15:24.569 --> 01:15:28.479
uh I get at least a little bit worried

1519
01:15:28.490 --> 01:15:32.569
because some of that sounds like uh I mean,

1520
01:15:32.580 --> 01:15:35.379
of course, the connection I'm making here is not

1521
01:15:35.390 --> 01:15:40.370
immediately obvious, but it sounds like eugenics, at least

1522
01:15:40.379 --> 01:15:43.410
to a certain extent in the sense that you're

1523
01:15:43.419 --> 01:15:47.169
making a claim that there are morally superior and

1524
01:15:47.180 --> 01:15:52.140
morally inferior people, or at least I would imagine

1525
01:15:52.149 --> 01:15:55.330
that some people would make that claim and then

1526
01:15:55.339 --> 01:16:01.450
moral enhancement would be a way of morally improving

1527
01:16:01.459 --> 01:16:05.770
the people who are morally inferior or that would

1528
01:16:05.779 --> 01:16:12.959
produce, for example, morally inferior, um offspring, some something

1529
01:16:12.970 --> 01:16:16.040
like that. So, I, I mean, does that, does

1530
01:16:16.049 --> 01:16:18.790
this make any sense or not? Is this something

1531
01:16:18.799 --> 01:16:21.970
that probably we should worry about?

1532
01:16:22.790 --> 01:16:25.229
Yeah, I think that is a serious concern. We

1533
01:16:25.240 --> 01:16:28.200
want to make sure that we are not imposing

1534
01:16:28.209 --> 01:16:30.729
a narrow conception of what it means to be

1535
01:16:30.740 --> 01:16:33.589
a good person. So I think to avoid that

1536
01:16:33.600 --> 01:16:35.569
we would have to think of this as, as

1537
01:16:35.580 --> 01:16:38.709
very individualized that we wouldn't want to say that

1538
01:16:38.720 --> 01:16:41.379
the, the state or even the scientific community or

1539
01:16:41.390 --> 01:16:43.750
anyone is, is saying, here's the way to be

1540
01:16:43.759 --> 01:16:45.729
a better person and we should all enhance, you

1541
01:16:45.740 --> 01:16:48.319
know, to this particular goal. But I do think

1542
01:16:48.330 --> 01:16:51.870
that we could allow a more pluralistic approach where

1543
01:16:51.879 --> 01:16:54.970
we say, well, look, everyone kind of already engages

1544
01:16:54.979 --> 01:16:57.979
in various forms of self improvement, right? We read

1545
01:16:57.990 --> 01:17:01.259
books, self help books. Uh We listen to podcasts.

1546
01:17:01.270 --> 01:17:04.950
We, we study the stoics and meditation and, and

1547
01:17:04.959 --> 01:17:06.549
those sorts of things to try to make ourselves

1548
01:17:06.560 --> 01:17:10.750
better. And that's just indirect manipulation of our brains.

1549
01:17:10.759 --> 01:17:12.589
We're trying to, you know, change our beliefs and

1550
01:17:12.600 --> 01:17:15.529
values and character traits. Indirectly, I think we could

1551
01:17:15.540 --> 01:17:18.359
do something similar with, with direct manipulation of our

1552
01:17:18.370 --> 01:17:21.979
brains. So we just allow everyone to explore on

1553
01:17:21.990 --> 01:17:24.750
their own, have their own sort of personal moral

1554
01:17:24.759 --> 01:17:27.620
journey of enhancement and they might use various kinds

1555
01:17:27.629 --> 01:17:30.509
of direct forms of stimulation to, to do that.

1556
01:17:31.640 --> 01:17:34.080
But I hesitate to, to say it to, to

1557
01:17:34.089 --> 01:17:36.029
as if it's a recommendation. But I do think

1558
01:17:36.040 --> 01:17:39.979
that one possible story that this could turn into

1559
01:17:39.990 --> 01:17:43.899
is if some people maybe using um having psychedelic

1560
01:17:43.910 --> 01:17:46.680
experiences, at least, you know, if it's legal in

1561
01:17:46.689 --> 01:17:49.569
their location, uh which is changing rapidly, at least

1562
01:17:49.580 --> 01:17:52.299
across the United States, then they might use that

1563
01:17:52.310 --> 01:17:55.069
as a way of, of exploring their own moral

1564
01:17:55.080 --> 01:17:58.649
values and potentially enhancing themselves. And, and it's really

1565
01:17:58.660 --> 01:18:00.415
not science. I mean, a lot of people do

1566
01:18:00.424 --> 01:18:03.004
this already in the psychedelic community. They, they see

1567
01:18:03.015 --> 01:18:06.634
it as not treatment for a therapy, even though

1568
01:18:06.645 --> 01:18:08.145
that's what a lot of the research is about

1569
01:18:08.154 --> 01:18:12.375
now, but as a way of enhancing themselves morally

1570
01:18:12.384 --> 01:18:15.345
and to, to get to a more enlightened kind

1571
01:18:15.354 --> 01:18:17.865
of position in their life. And so I think

1572
01:18:17.875 --> 01:18:21.345
that is a potential source of moral enhancement, but

1573
01:18:21.354 --> 01:18:23.470
have to be done safely. It would have to

1574
01:18:23.479 --> 01:18:27.290
be done legally and it would have to be

1575
01:18:27.299 --> 01:18:31.930
done with caution, with, with understanding that sometimes engaging

1576
01:18:31.939 --> 01:18:34.979
these experiences could lead to moral regression. It could

1577
01:18:34.990 --> 01:18:37.500
lead someone down to a path where they actually

1578
01:18:37.509 --> 01:18:41.250
have a worse moral character traits. But it does

1579
01:18:41.259 --> 01:18:43.990
look like it's, it's a possibility for many people

1580
01:18:44.000 --> 01:18:46.169
that it could improve it. Uh INSOFAR as it

1581
01:18:46.180 --> 01:18:49.410
makes them less self centered. Uh For example, so

1582
01:18:49.419 --> 01:18:50.939
that that's one possibility, I don't know if there

1583
01:18:50.950 --> 01:18:52.970
are others, you know, a lot of the brain

1584
01:18:52.979 --> 01:18:55.870
stimulation I don't think is gonna have very precise

1585
01:18:55.879 --> 01:19:00.660
and profound effects on morality specifically. Um But there

1586
01:19:00.669 --> 01:19:04.819
could be certain kinds of experiences uh enhanced with,

1587
01:19:04.830 --> 01:19:07.450
with certain kinds of drugs that could potentially lead

1588
01:19:07.459 --> 01:19:10.629
people down a personal journey. Uh So that we're

1589
01:19:10.640 --> 01:19:12.470
not saying there's one view, you know, this is

1590
01:19:12.479 --> 01:19:14.830
the eugenic view of everyone should be moving down

1591
01:19:14.839 --> 01:19:16.319
this path. It could be, there would be lots

1592
01:19:16.330 --> 01:19:18.160
of disagreements. Somebody could say I'm, I'm trying to

1593
01:19:18.169 --> 01:19:22.339
have certain experiences um maybe doing a long meditation

1594
01:19:22.350 --> 01:19:26.100
retreat uh with some uh psilocybin as an aid

1595
01:19:26.390 --> 01:19:29.399
that allows them to, you know, become less self

1596
01:19:29.410 --> 01:19:32.180
centered. Uh So they can be, you know, maybe

1597
01:19:32.189 --> 01:19:34.790
more compassionate towards other humans. Maybe another person might

1598
01:19:34.799 --> 01:19:37.660
be doing it to explore their attitudes towards nonhuman

1599
01:19:37.669 --> 01:19:40.259
animals to increase their compassion there. And so it

1600
01:19:40.270 --> 01:19:42.370
might be very different views about how we're going

1601
01:19:42.379 --> 01:19:45.930
to explore it, but it could be individual and,

1602
01:19:45.939 --> 01:19:48.629
and pluralistic so that we don't have one single

1603
01:19:48.640 --> 01:19:51.180
conception of what the, the right person is, which

1604
01:19:51.189 --> 01:19:53.640
was the real, the real downfall of, of eugenics

1605
01:19:53.649 --> 01:19:54.370
and the problem there.

1606
01:19:54.680 --> 01:19:57.620
Mhm. No, I really appreciate that sort of more

1607
01:19:57.629 --> 01:20:03.149
pluralistic and individualistic approach to it because, I mean,

1608
01:20:03.160 --> 01:20:06.899
I, I guess that even from just an ethics

1609
01:20:06.910 --> 01:20:11.430
standpoint and even more specifically a meta ethics standpoint,

1610
01:20:11.640 --> 01:20:16.390
another thing that could probably be an issue here,

1611
01:20:16.439 --> 01:20:22.930
uh, is, uh, whether moral realism or moral anti

1612
01:20:22.939 --> 01:20:27.149
realism is. Right. Correct. Because, I mean, I was

1613
01:20:27.160 --> 01:20:31.950
just thinking that, for example, uh, what would moral

1614
01:20:31.959 --> 01:20:36.660
enhancement really mean in this particular case? I mean,

1615
01:20:36.669 --> 01:20:42.470
would it be, for example, someone developing psychological traits

1616
01:20:42.479 --> 01:20:46.990
that would allow them to better follow whatever particular

1617
01:20:47.000 --> 01:20:52.669
kinds of moral norms prevail in their specific society?

1618
01:20:52.680 --> 01:20:58.089
Would it be a supposed set of universal moral

1619
01:20:58.100 --> 01:21:02.359
values that people would deem to be the best

1620
01:21:02.370 --> 01:21:05.049
ones out there? Because I, I mean, there's also

1621
01:21:05.060 --> 01:21:08.709
this debate between the moral realists and the anti

1622
01:21:08.720 --> 01:21:11.700
realists, right? And I would imagine that it would

1623
01:21:11.709 --> 01:21:15.850
also, uh, play a role here. Right.

1624
01:21:16.350 --> 01:21:18.729
Yeah. These, these are some really thorny questions. Uh,

1625
01:21:18.740 --> 01:21:22.100
MOSTLY I've tried to conveniently avoid but I think

1626
01:21:22.109 --> 01:21:24.339
you're right that it's, that it seems relevant and

1627
01:21:24.350 --> 01:21:25.729
I'll try to give you one reason for thinking.

1628
01:21:25.740 --> 01:21:28.629
Maybe we don't have to settle those deep questions

1629
01:21:28.640 --> 01:21:30.609
and meta ethics in order to, to pursue the

1630
01:21:30.620 --> 01:21:32.930
project of moral enhancement. So, here's one way to

1631
01:21:32.939 --> 01:21:36.330
think about it, uh, uh, standard kind of simple

1632
01:21:36.620 --> 01:21:41.279
moral realist view would be something like utilitarianism. So

1633
01:21:41.290 --> 01:21:44.819
you might think there's, there's one true morality and

1634
01:21:44.830 --> 01:21:47.680
it comes down to which actions are going to

1635
01:21:47.689 --> 01:21:50.879
produce the best consequences or maybe make the most

1636
01:21:50.890 --> 01:21:54.399
people happy. Standard way of interpreting that as is

1637
01:21:54.410 --> 01:21:56.890
a realist view. There are these, you know, objective

1638
01:21:56.899 --> 01:21:59.169
facts of the matter and ethics and they just

1639
01:21:59.180 --> 01:22:01.140
come down to what's gonna make most people happy.

1640
01:22:01.600 --> 01:22:04.879
You now, even on that view, there's still going

1641
01:22:04.890 --> 01:22:08.649
to be lots of variation and pluralism. So I,

1642
01:22:08.660 --> 01:22:12.160
I myself, I'm not a, a devout utilitarian, but

1643
01:22:12.169 --> 01:22:14.299
I think that many of them who, who are

1644
01:22:14.310 --> 01:22:16.299
utilitarians would say that would say this, they'd say,

1645
01:22:16.310 --> 01:22:19.629
well, ok, she wanna make people happy. But what

1646
01:22:19.640 --> 01:22:23.660
makes people happy in India versus, you know, the

1647
01:22:23.669 --> 01:22:25.970
United States or Portugal is going to be very

1648
01:22:25.979 --> 01:22:28.709
different, just just as a matter of empirical fact,

1649
01:22:28.720 --> 01:22:30.839
you know, there's just different social norms, there's different

1650
01:22:30.850 --> 01:22:34.040
experiences and backgrounds. So certain things that make people

1651
01:22:34.049 --> 01:22:36.220
happy in the United States might not make people

1652
01:22:36.229 --> 01:22:39.040
happy in other kinds of countries or other subcultures.

1653
01:22:39.259 --> 01:22:40.350
And so even there, you're going to have a

1654
01:22:40.359 --> 01:22:42.660
lot of pluralism, even if you think there's still

1655
01:22:42.669 --> 01:22:45.770
one objective fact about ethics, which is that we

1656
01:22:45.779 --> 01:22:48.759
should make the most people happy. So I think

1657
01:22:48.770 --> 01:22:50.979
even there, it could justify a kind of pluralism.

1658
01:22:50.990 --> 01:22:52.089
So you won't say that we're trying to, you

1659
01:22:52.100 --> 01:22:54.879
know, see the one way that everyone ought, ought

1660
01:22:54.890 --> 01:22:57.379
to be, but we're just trying to figure out

1661
01:22:57.390 --> 01:22:59.689
how to make the most people happy. And that

1662
01:22:59.700 --> 01:23:02.129
might involve uh quite a bit of variation across

1663
01:23:02.140 --> 01:23:04.160
cultures and, and subcultures.

1664
01:23:05.189 --> 01:23:07.740
So I would like to ask you now about

1665
01:23:07.750 --> 01:23:13.089
neuro marketing. So, uh could you tell uh particularly

1666
01:23:13.100 --> 01:23:17.580
for the audience what it is uh and whether

1667
01:23:17.589 --> 01:23:20.549
it works or not and also then we can

1668
01:23:20.560 --> 01:23:23.049
discuss whether it's ethical or not.

1669
01:23:23.439 --> 01:23:26.339
Hm. Yeah. So, so neuromarketing would just be using

1670
01:23:26.350 --> 01:23:30.169
the tools of neuroscience to try to read the

1671
01:23:30.180 --> 01:23:32.649
minds of consumers to try to understand what their

1672
01:23:32.660 --> 01:23:36.240
preferences are and really how, how to, to sell

1673
01:23:36.250 --> 01:23:40.290
them products better. And that has been done there.

1674
01:23:40.299 --> 01:23:42.729
There are some groups like one of the largest

1675
01:23:42.740 --> 01:23:45.379
marketing firms in the United States is Nielsen and

1676
01:23:45.390 --> 01:23:47.779
they did have a kind of neuromarketing division for

1677
01:23:47.790 --> 01:23:52.270
a long time, but reportedly they've closed it in

1678
01:23:52.279 --> 01:23:55.009
the past few years, partly because it seemed like

1679
01:23:55.020 --> 01:23:58.600
it wasn't really delivering on the investment. So there

1680
01:23:58.609 --> 01:24:01.069
are, I think open questions about whether we have

1681
01:24:01.080 --> 01:24:04.540
the tools to be able to really understand people's

1682
01:24:04.549 --> 01:24:08.359
preferences by looking at their brain activity. Now, I

1683
01:24:08.370 --> 01:24:10.720
mean that it really is a deep puzzle because

1684
01:24:10.729 --> 01:24:12.129
you can do focus groups and all of that.

1685
01:24:12.140 --> 01:24:14.140
But that doesn't necessarily mean that, you know, people

1686
01:24:14.149 --> 01:24:16.750
are reliable reporting what their preferences are. You know,

1687
01:24:16.759 --> 01:24:18.569
this again goes back to the stuff on free

1688
01:24:18.580 --> 01:24:20.680
will and unconsciousness. So we don't always know exactly

1689
01:24:20.689 --> 01:24:23.000
why we want the things we do. Uh My

1690
01:24:23.009 --> 01:24:24.950
favorite example here is the ipad. I remember when

1691
01:24:24.959 --> 01:24:28.390
the ipad came out, I like many people thought

1692
01:24:28.399 --> 01:24:30.589
this is a ridiculous product. Why do I need

1693
01:24:30.600 --> 01:24:32.990
something slightly bigger than the phone and slightly smaller

1694
01:24:33.000 --> 01:24:34.629
than the laptop? And a lot of people thought

1695
01:24:34.640 --> 01:24:36.799
this is not, you know, uh this is like

1696
01:24:36.810 --> 01:24:39.209
the downfall of apple and yet it completely took

1697
01:24:39.220 --> 01:24:42.060
off and I have one now. Uh AND I

1698
01:24:42.069 --> 01:24:44.520
didn't realize that I actually would like it. There's

1699
01:24:44.529 --> 01:24:46.290
this device that I really needed I didn't realize.

1700
01:24:46.669 --> 01:24:49.200
So there's a real puzzle about whether you can

1701
01:24:49.209 --> 01:24:52.799
get past what people say about their own preferences

1702
01:24:52.810 --> 01:24:54.709
and just look directly at their brain activity to

1703
01:24:54.720 --> 01:24:56.399
figure out, you know, what do they really want.

1704
01:24:57.160 --> 01:24:59.419
And I don't know that we have the ability

1705
01:24:59.430 --> 01:25:03.049
to, to get there. It's a real, as I

1706
01:25:03.060 --> 01:25:05.979
said, a puzzle about how the neural activity in

1707
01:25:05.990 --> 01:25:08.339
the brain gives rise to very particular kinds of

1708
01:25:08.350 --> 01:25:12.620
thoughts and preferences, but it's rapidly advancing. So in

1709
01:25:12.629 --> 01:25:15.379
the book, I'm, I'm very skeptical and say, you

1710
01:25:15.390 --> 01:25:17.790
know, we're a long way from really understanding particularly

1711
01:25:17.870 --> 01:25:21.009
the preferences. Uh But, you know, the book came

1712
01:25:21.020 --> 01:25:24.250
out right around the time that Chat G BT

1713
01:25:24.259 --> 01:25:26.850
got released. So that was kind of like already

1714
01:25:26.859 --> 01:25:28.419
in press and ready to go. And then we

1715
01:25:28.430 --> 01:25:31.540
had this major advancement in artificial intelligence and, and

1716
01:25:31.549 --> 01:25:34.740
that's being applied now to, to brain reading and

1717
01:25:34.750 --> 01:25:37.979
it's had some really remarkable results. So I don't

1718
01:25:37.990 --> 01:25:40.000
know if you saw, but um so there was

1719
01:25:40.009 --> 01:25:44.899
something recently maybe last year, a group who had

1720
01:25:44.910 --> 01:25:47.089
a patient with a LS who can't speak anymore.

1721
01:25:47.240 --> 01:25:50.169
And they used uh machine learning and the same

1722
01:25:50.180 --> 01:25:52.810
kinds of like, you know, neuro network technology to

1723
01:25:52.819 --> 01:25:56.810
decode her brain activity and convert it into speech.

1724
01:25:56.819 --> 01:25:59.430
So she can't, you know, ver verbally articulate speech

1725
01:25:59.439 --> 01:26:02.950
anymore. But just by thinking about saying certain sentences,

1726
01:26:02.959 --> 01:26:06.049
they could decode it using machine learning and then

1727
01:26:06.060 --> 01:26:10.209
produce, produce speech in a computer that's really impressive.

1728
01:26:10.459 --> 01:26:14.299
Um Also just about a month ago, uh Elon

1729
01:26:14.310 --> 01:26:18.680
Musk's Neuralink Company released a video of their first

1730
01:26:18.689 --> 01:26:22.910
human who has their brain implant. And with this

1731
01:26:22.919 --> 01:26:26.910
implant, uh they are able to uh play chess

1732
01:26:27.299 --> 01:26:29.470
just by thinking. So this is a patient who

1733
01:26:29.479 --> 01:26:32.020
also uh you know, can't, can't move, I think

1734
01:26:32.029 --> 01:26:33.350
uh below his neck, I think he may have

1735
01:26:33.359 --> 01:26:36.359
a spinal cord injury so he can't really control

1736
01:26:36.370 --> 01:26:39.350
a lot except with, with his, his mouth. And

1737
01:26:39.359 --> 01:26:43.620
now through having a neuralink implant, he can control

1738
01:26:43.629 --> 01:26:47.759
a chessboard. So things are advancing very rapidly. Um

1739
01:26:47.770 --> 01:26:49.740
Yeah, I think there are some, still some serious

1740
01:26:49.750 --> 01:26:51.500
limitations and one thing is I like to do

1741
01:26:51.509 --> 01:26:54.080
when it comes to neuroscience in the marketplace is

1742
01:26:54.089 --> 01:26:56.259
compare it to what we already have out there.

1743
01:26:56.819 --> 01:27:00.049
And what we already have out there is all

1744
01:27:00.060 --> 01:27:03.200
these companies who have massive amounts of big data

1745
01:27:03.209 --> 01:27:07.700
on our purchases, on our social networks, on who

1746
01:27:07.709 --> 01:27:10.640
we're friends with. Um OUR location data with our

1747
01:27:10.649 --> 01:27:15.439
smartphones. And all of that information is extremely valuable

1748
01:27:15.459 --> 01:27:18.560
for decoding what people's preferences are, what they're more

1749
01:27:18.569 --> 01:27:21.395
likely to buy or not. And, and uh for

1750
01:27:21.404 --> 01:27:24.555
figuring out how to, how to sell things to

1751
01:27:24.564 --> 01:27:28.725
people based just on that non neurobiological data, that's

1752
01:27:28.734 --> 01:27:32.984
just purchasing behavior, location and social networks that is

1753
01:27:32.995 --> 01:27:36.325
has been immensely powerful for companies. I don't know

1754
01:27:36.464 --> 01:27:39.685
that this neuroscience technology is ever going to be

1755
01:27:39.694 --> 01:27:43.495
cost effective enough to supersede that. I think that

1756
01:27:43.504 --> 01:27:45.484
we could live in a world one day where

1757
01:27:45.785 --> 01:27:48.879
as neuralink hopes, you have a lot of people

1758
01:27:48.890 --> 01:27:50.839
who are just ordinary consumers, they don't have a

1759
01:27:50.850 --> 01:27:53.250
spinal cord injury, they don't have a LS, they're

1760
01:27:53.259 --> 01:27:56.399
just ordinary consumers who get a brain implant like

1761
01:27:56.410 --> 01:27:58.580
they would, you know, wear an apple watch that

1762
01:27:58.589 --> 01:28:01.720
uh provides feedback on their heart rate and they

1763
01:28:01.729 --> 01:28:04.580
could use that to, you know, maybe decode some

1764
01:28:04.589 --> 01:28:07.200
of their thoughts to get neural feedback on their

1765
01:28:07.209 --> 01:28:09.970
emotional states. Now, that's a lot of brain data.

1766
01:28:10.169 --> 01:28:11.759
Uh THAT of course, would just be like your

1767
01:28:11.770 --> 01:28:13.299
location on your phone. It would just be like

1768
01:28:13.310 --> 01:28:15.850
your purchases and your social networks and these companies

1769
01:28:15.859 --> 01:28:18.899
would have access to all of that data. So

1770
01:28:18.910 --> 01:28:21.589
there are deep questions about privacy if we move

1771
01:28:21.600 --> 01:28:23.959
into that world. But I think there are also

1772
01:28:23.970 --> 01:28:27.234
real questions about whether people will actually do that.

1773
01:28:27.245 --> 01:28:28.384
I don't know that people are going to get

1774
01:28:28.395 --> 01:28:31.444
brain implants uh just like they would a smartwatch,

1775
01:28:31.814 --> 01:28:34.924
but uh it may not ever be actually as

1776
01:28:34.935 --> 01:28:37.444
effective as all the other kinds of data that

1777
01:28:37.455 --> 01:28:39.845
we allow companies to have already

1778
01:28:40.750 --> 01:28:43.549
a and when it comes to the marketing, uh

1779
01:28:43.560 --> 01:28:47.259
bit of it, uh I mean, it also uh

1780
01:28:47.270 --> 01:28:51.410
I think raises particular kinds of ethical questions because

1781
01:28:52.189 --> 01:28:54.740
I mean, at least to some extent, I would

1782
01:28:54.750 --> 01:29:01.290
imagine that the ethicists can question not just neuromarketing

1783
01:29:01.299 --> 01:29:05.890
specifically, but marketing in general because at least to

1784
01:29:05.899 --> 01:29:10.419
some extent it's plausible to say and accurate to

1785
01:29:10.430 --> 01:29:15.209
say that uh it's a little bit manipulating, right?

1786
01:29:15.220 --> 01:29:18.419
I mean, uh uh I mean, you're not really

1787
01:29:18.430 --> 01:29:24.020
telling the entire truth when you're doing marketing to

1788
01:29:24.029 --> 01:29:27.359
sell product services and all of that. So, I

1789
01:29:27.370 --> 01:29:29.859
mean, it isn't it, at least to some extent

1790
01:29:29.870 --> 01:29:33.299
a little bit deceiving and if so, isn't that

1791
01:29:33.310 --> 01:29:37.779
also something that in the particular context of neuromarketing

1792
01:29:37.790 --> 01:29:39.859
we should, uh, care about.

1793
01:29:40.750 --> 01:29:42.620
Right. Yeah, I think it's the very same general

1794
01:29:42.629 --> 01:29:45.350
problem we have with marketing that it might be

1795
01:29:45.430 --> 01:29:49.000
manipulative in ways that might to, to, again go

1796
01:29:49.009 --> 01:29:51.029
back to, to free will sort of bypass our

1797
01:29:51.040 --> 01:29:53.100
own, our own free choice. But given that I

1798
01:29:53.109 --> 01:29:56.410
have this view that the unconscious forces in our

1799
01:29:56.419 --> 01:29:58.140
minds are still part of who we are, still

1800
01:29:58.149 --> 01:30:01.660
part of our free agency. Uh I think that

1801
01:30:01.669 --> 01:30:04.720
some of those concerns are, are not as serious

1802
01:30:04.729 --> 01:30:07.700
as they might seem at first blush, especially if

1803
01:30:07.709 --> 01:30:12.350
we can exert control over those influences. So it's

1804
01:30:12.359 --> 01:30:14.709
true that we have a lot of influences, especially

1805
01:30:14.720 --> 01:30:17.439
nowadays, you know, we've got this, this attention economy

1806
01:30:17.450 --> 01:30:19.830
where everything on our smartphone and the watches are

1807
01:30:19.839 --> 01:30:22.330
all trying to pull our attention uh towards different

1808
01:30:22.339 --> 01:30:25.189
features of our, our environment. And I think that

1809
01:30:25.200 --> 01:30:28.390
does have an effect on our agency and, and

1810
01:30:28.399 --> 01:30:31.060
our, and our freedom and we need to be

1811
01:30:31.069 --> 01:30:33.100
alert to that and, but there are ways I

1812
01:30:33.109 --> 01:30:36.560
think of exercising our agency in light of, of

1813
01:30:36.569 --> 01:30:38.810
that new environment. So this is part of the,

1814
01:30:38.819 --> 01:30:41.069
the view I'm hoping that overall the book can

1815
01:30:41.080 --> 01:30:43.589
give is, is the complexity of our agency as

1816
01:30:43.600 --> 01:30:46.660
involving these unconscious and conscious forces, but we can

1817
01:30:46.700 --> 01:30:49.580
harness that in certain ways. Um I'll give you

1818
01:30:49.589 --> 01:30:52.640
one example. So there are various kinds of reward

1819
01:30:52.649 --> 01:30:56.410
programs that, that people have for different businesses or

1820
01:30:56.419 --> 01:30:58.549
credit cards. If you have a certain credit card,

1821
01:30:58.569 --> 01:31:01.459
then you'll get certain rewards. And you know, that's

1822
01:31:01.470 --> 01:31:03.950
something that can influence you unconsciously if you have

1823
01:31:04.009 --> 01:31:05.270
a I, that's the whole point of these rewards

1824
01:31:05.279 --> 01:31:07.609
programs, right? So you've got an Amazon credit card.

1825
01:31:07.620 --> 01:31:09.100
The whole point is that, you know, you'll buy

1826
01:31:09.109 --> 01:31:11.459
more products on Amazon because you get Amazon points,

1827
01:31:11.470 --> 01:31:13.669
right? For doing that. So that's a kind of

1828
01:31:13.709 --> 01:31:18.270
automatic and potentially unconscious influence on your choices. But

1829
01:31:18.279 --> 01:31:21.930
we can take some control and change the environment

1830
01:31:21.939 --> 01:31:24.669
we're in, you could not get that credit card

1831
01:31:24.680 --> 01:31:26.669
or you could get a different one that is

1832
01:31:26.680 --> 01:31:28.560
for a company that you maybe would rather be

1833
01:31:28.569 --> 01:31:30.470
spending more money at. Uh So I think you

1834
01:31:30.479 --> 01:31:32.600
can make those kinds of choices about the environment

1835
01:31:32.609 --> 01:31:35.479
you're in. Um YOU can do a digital detox

1836
01:31:35.490 --> 01:31:38.040
and you know, uh delete some of your, your

1837
01:31:38.049 --> 01:31:40.520
the worst apps that you've got. Uh THERE are

1838
01:31:40.529 --> 01:31:42.029
choices that we can make. So I think a

1839
01:31:42.040 --> 01:31:43.990
lot of times we can play the victim and

1840
01:31:44.000 --> 01:31:45.689
say, well, look, there are all these companies who

1841
01:31:45.700 --> 01:31:48.060
are manipulating us. Um But also I think we

1842
01:31:48.069 --> 01:31:50.750
need to take some individual agency and say, but

1843
01:31:50.759 --> 01:31:52.459
we're also kind of letting them and it's up

1844
01:31:52.470 --> 01:31:55.149
to us to make a choice about whether we,

1845
01:31:55.160 --> 01:31:59.009
we let them get into our, our personal lives

1846
01:31:59.020 --> 01:32:01.464
that much in influence. You have a choice. You

1847
01:32:01.475 --> 01:32:03.734
can, you can go into your, you know, your

1848
01:32:03.745 --> 01:32:06.944
Google account and, and have them delete your data,

1849
01:32:06.955 --> 01:32:09.174
you know, after a month of, of storing it

1850
01:32:09.185 --> 01:32:10.834
or they can keep it forever. That that's a

1851
01:32:10.845 --> 01:32:13.944
choice that we have as consumers. And I think

1852
01:32:13.955 --> 01:32:15.294
uh what it calls for us to do is

1853
01:32:15.305 --> 01:32:18.584
is exert more of our own agency and control

1854
01:32:18.595 --> 01:32:22.634
over what's influencing us automatically and unconsciously and that

1855
01:32:22.645 --> 01:32:25.004
at least can alleviate some of the moral concerns

1856
01:32:25.015 --> 01:32:26.305
that people have about this.

1857
01:32:27.149 --> 01:32:29.919
So let me ask you then one final question

1858
01:32:29.930 --> 01:32:33.970
that is also sort of related to neuromarketing and

1859
01:32:33.979 --> 01:32:36.410
some of the other issues you raise there like

1860
01:32:36.419 --> 01:32:40.120
free will agency and all of that. Um I

1861
01:32:40.129 --> 01:32:44.089
mean, there's this issue of neuro hype, I mean,

1862
01:32:44.100 --> 01:32:47.959
people hyping up certain kinds of, for example, new

1863
01:32:47.970 --> 01:32:51.450
technologies, I don't know if you hear, for example,

1864
01:32:51.459 --> 01:32:55.120
Elon Musk saying that through neuralink, somewhere in the

1865
01:32:55.129 --> 01:32:57.660
near future, you will be able to have access

1866
01:32:57.669 --> 01:33:01.600
to the internet via your brain directly or even

1867
01:33:01.609 --> 01:33:05.720
download information into your brain, something like that. Or

1868
01:33:05.729 --> 01:33:09.232
if you hear people claiming that if you make

1869
01:33:09.243 --> 01:33:12.783
your kids, your three year old listen to baby

1870
01:33:12.792 --> 01:33:15.632
Einstein, then he's like you will go up by

1871
01:33:15.643 --> 01:33:18.982
20 points or something like that. I mean, there,

1872
01:33:18.993 --> 01:33:23.792
there's a lot of high surrounding products related to

1873
01:33:23.803 --> 01:33:28.803
uh neuronal applications, neuroscience, the brain and so on.

1874
01:33:28.812 --> 01:33:32.143
So, uh how do you think we could have

1875
01:33:32.342 --> 01:33:36.675
a of more nuanced and balanced approach to neuro

1876
01:33:36.715 --> 01:33:39.666
science and to the ethical side of it.

1877
01:33:40.226 --> 01:33:42.255
Right. Yeah, it's hard because there, there are a

1878
01:33:42.266 --> 01:33:45.065
lot of exciting developments and as I say, we

1879
01:33:45.076 --> 01:33:47.116
can be doubtful. But then in a couple of

1880
01:33:47.125 --> 01:33:49.746
years, you have this, you know, major leaps in

1881
01:33:50.056 --> 01:33:53.525
artificial intelligence systems. So it's hard to make predictions,

1882
01:33:53.536 --> 01:33:55.945
I think for decades, people say, well, you know,

1883
01:33:55.956 --> 01:33:57.715
we're going to have the new A I revolution

1884
01:33:57.726 --> 01:33:59.640
and never came. And then I think in the

1885
01:33:59.649 --> 01:34:01.640
past couple of years, people have realized, OK, maybe,

1886
01:34:01.649 --> 01:34:03.890
maybe it's finally come where we've made a serious

1887
01:34:03.899 --> 01:34:07.790
leap in the capabilities of these technologies. And so

1888
01:34:07.799 --> 01:34:09.990
I, I tend to, to think we should not

1889
01:34:10.000 --> 01:34:11.549
overhype. It's a kind of a big theme of

1890
01:34:11.560 --> 01:34:13.990
the book is that there's a lot of excessive

1891
01:34:14.000 --> 01:34:17.310
uh enthusiasm about some of these neurotechnology and we

1892
01:34:17.319 --> 01:34:20.459
need to really look at their limitations and by

1893
01:34:20.470 --> 01:34:22.259
looking at the limitations, we can also see that

1894
01:34:22.270 --> 01:34:25.500
maybe some of the ethical concerns aren't as powerful.

1895
01:34:25.759 --> 01:34:27.290
But I do think we need to be open

1896
01:34:27.299 --> 01:34:30.330
minded about the, the possibility that we, we could

1897
01:34:30.339 --> 01:34:34.220
develop technology that uh have all of these, these

1898
01:34:34.229 --> 01:34:36.819
promised outcomes. But there are going to be built

1899
01:34:36.830 --> 01:34:38.899
in limitations. A lot of times there's this assumption

1900
01:34:38.930 --> 01:34:43.100
that every bit of technology just gets better and

1901
01:34:43.109 --> 01:34:46.750
cheaper. We've seen that with, with smartphones. Uh IT'S

1902
01:34:46.759 --> 01:34:49.390
just been radical in just my lifetime. You know

1903
01:34:49.399 --> 01:34:51.740
how now we have what effectively would be considered

1904
01:34:51.750 --> 01:34:56.069
a supercomputer in my pocket. Um When you just

1905
01:34:56.080 --> 01:34:57.509
a couple of decades ago, we didn't have the

1906
01:34:57.520 --> 01:35:00.200
internet at all. So it's, it's possible, I suppose,

1907
01:35:00.209 --> 01:35:01.870
but there are built in limitations. So a lot

1908
01:35:01.879 --> 01:35:04.689
of this decoding that's being done like with, with

1909
01:35:04.700 --> 01:35:06.549
neuralink where they have these, I mean, first of

1910
01:35:06.560 --> 01:35:09.799
all, you have to get a implant directly onto,

1911
01:35:09.870 --> 01:35:12.640
you know, the cortex. Uh AND, and these are

1912
01:35:12.649 --> 01:35:15.029
only electrode arrays that go into the cortex of

1913
01:35:15.040 --> 01:35:16.859
the brain, which doesn't reach every part of the

1914
01:35:16.870 --> 01:35:18.439
brain, right? There are a lot of structure in

1915
01:35:18.450 --> 01:35:21.140
the brain involved with, you know, motivation and desire

1916
01:35:21.220 --> 01:35:23.790
that would be much deeper in the brain. Uh

1917
01:35:23.819 --> 01:35:27.759
So you have to have surgery, you know, effectively

1918
01:35:27.770 --> 01:35:30.890
to, to even get this kind of real powerful

1919
01:35:30.899 --> 01:35:33.799
kind of implant that can get really good signal

1920
01:35:33.810 --> 01:35:36.120
out of all the noise of the brain activity.

1921
01:35:36.479 --> 01:35:38.029
So that by itself, I think is, is a

1922
01:35:38.040 --> 01:35:43.299
major limitation for, for widespread uh consumer applications. But,

1923
01:35:43.310 --> 01:35:45.879
you know, maybe that's something people will be comfortable

1924
01:35:45.890 --> 01:35:49.129
with, but there's also there's got to be effectively,

1925
01:35:49.140 --> 01:35:53.220
you know, these incredibly powerful computers and algorithms that,

1926
01:35:53.229 --> 01:35:56.140
that have to be attached to the decoding. So

1927
01:35:56.149 --> 01:35:58.750
whether that's something that, you know, could actually be

1928
01:35:58.759 --> 01:36:01.950
very portable that people will want to, to have

1929
01:36:01.959 --> 01:36:04.589
on constantly is a real open question. And I

1930
01:36:04.600 --> 01:36:07.339
think that there's a lot of fears attached to

1931
01:36:07.350 --> 01:36:10.740
that, but we don't necessarily have to worry if

1932
01:36:10.750 --> 01:36:13.649
it's not ever going to be a reality. And

1933
01:36:13.660 --> 01:36:15.459
there may be people who say that they're fed

1934
01:36:15.470 --> 01:36:18.660
up with al already, how much of our privacy

1935
01:36:18.669 --> 01:36:21.160
has been, you know, given over to companies. And

1936
01:36:21.169 --> 01:36:23.370
they might say this is the line. You know,

1937
01:36:23.379 --> 01:36:25.529
I've, I've got my smart watch, I've got my,

1938
01:36:25.540 --> 01:36:27.529
my phone that tracks my location but you could

1939
01:36:27.540 --> 01:36:30.270
see widespread, a lot of consumers saying I'm not

1940
01:36:30.279 --> 01:36:32.379
going to, to the brain, you know, we're, we're

1941
01:36:32.390 --> 01:36:33.620
not, I'm not going to give that up to

1942
01:36:33.629 --> 01:36:36.209
companies and throw it up in the cloud. And,

1943
01:36:36.220 --> 01:36:38.140
and I think that could be a very wise

1944
01:36:38.149 --> 01:36:40.189
decision on the part of a lot of consumers

1945
01:36:40.279 --> 01:36:42.379
and, and it could be, could be a reality.

1946
01:36:42.390 --> 01:36:44.379
So I think there's a real choice point for,

1947
01:36:44.390 --> 01:36:46.319
for us as humans right now, we could draw

1948
01:36:46.330 --> 01:36:47.939
that line in the sand and say that we

1949
01:36:47.950 --> 01:36:52.069
want to have a certain level of privacy here.

1950
01:36:52.250 --> 01:36:54.560
Uh And that it could actually be quite powerful

1951
01:36:54.569 --> 01:36:57.149
and we may just reject that uh that new

1952
01:36:57.160 --> 01:37:00.970
frontier, at least when it goes beyond treatment for

1953
01:37:00.979 --> 01:37:04.819
serious kinds of neurological conditions. But I think it

1954
01:37:04.830 --> 01:37:08.029
remains to be seen whether we're going to actually,

1955
01:37:08.089 --> 01:37:10.879
you know, uh hear the hype come to reality.

1956
01:37:10.890 --> 01:37:12.759
But in general, I think we need to, to

1957
01:37:12.770 --> 01:37:14.529
be a bit more cautious and say, what are

1958
01:37:14.540 --> 01:37:19.680
the realistic limitations and, and not be too overenthusiastic

1959
01:37:19.689 --> 01:37:23.819
and correspondingly overly alarmist about the, the moral concerns

1960
01:37:23.830 --> 01:37:26.069
here. I think there's a tendency in neuro ethics

1961
01:37:26.080 --> 01:37:28.959
to, to think far into the potential future with

1962
01:37:28.970 --> 01:37:31.129
a lot of science fiction scenarios and to worry

1963
01:37:31.140 --> 01:37:34.569
about that and maybe it's just my own dispositions

1964
01:37:34.580 --> 01:37:36.839
and personality type. But, but I'm, I'm a bit

1965
01:37:36.850 --> 01:37:38.419
allergic to that and I like to think about

1966
01:37:38.430 --> 01:37:41.180
the real questions we've got pressing us right now,

1967
01:37:41.350 --> 01:37:44.290
um, about things like brain interventions, addiction and neuro

1968
01:37:44.310 --> 01:37:47.479
diversity that we have to address now. Um RATHER

1969
01:37:47.490 --> 01:37:50.890
than, you know, very, you know, speculative claims about

1970
01:37:50.899 --> 01:37:53.220
a potential future. We do need to be ahead

1971
01:37:53.229 --> 01:37:55.640
of the science and technology and think about the

1972
01:37:55.649 --> 01:37:57.779
ethical issues. Um But I think there's a lot

1973
01:37:57.790 --> 01:37:59.939
of focus and attention going to things that might

1974
01:37:59.950 --> 01:38:01.899
not ever come to full fruition.

1975
01:38:03.229 --> 01:38:07.620
So the book is again, Neuroethics Agency in the

1976
01:38:07.629 --> 01:38:10.379
Age of Brain Science. I'm leaving a link to

1977
01:38:10.390 --> 01:38:12.709
it in the description of the interview when Dr

1978
01:38:12.979 --> 01:38:16.020
May, would you like to tell people apart from

1979
01:38:16.029 --> 01:38:18.740
the book where they can find you and your

1980
01:38:18.750 --> 01:38:19.939
work on the internet?

1981
01:38:20.680 --> 01:38:23.799
Sure. I mean, maybe with all my, my concerns

1982
01:38:23.810 --> 01:38:26.140
and qualms about uh privacy and all I might

1983
01:38:26.149 --> 01:38:28.819
realize I'm not, I'm not that active on social

1984
01:38:28.830 --> 01:38:32.080
media. Uh BUT I do have a website uh

1985
01:38:32.149 --> 01:38:34.439
Josh D a.com and I have all kinds of

1986
01:38:34.450 --> 01:38:37.120
information about the book among other things up there.

1987
01:38:37.290 --> 01:38:39.270
And I really do hope that the, the book

1988
01:38:39.279 --> 01:38:42.359
could be accessible to people who are outside of

1989
01:38:42.370 --> 01:38:46.399
philosophy, but also outside of academic research. It's, it's

1990
01:38:46.410 --> 01:38:49.160
meant to, to be as maybe overly ambitious that's

1991
01:38:49.169 --> 01:38:52.399
trying to reach, you know, a wide audience, but

1992
01:38:52.410 --> 01:38:56.680
also be a contribution to the academic literature. So

1993
01:38:56.689 --> 01:38:59.232
I push my own view, it's very opinionated. Uh

1994
01:38:59.243 --> 01:39:02.312
BUT it's also meant to, to give a introduction

1995
01:39:02.453 --> 01:39:04.333
to this field, which I think is a really

1996
01:39:04.342 --> 01:39:07.033
important field and it is going to continue to,

1997
01:39:07.042 --> 01:39:09.922
to shape the future of, of ethics in the

1998
01:39:09.933 --> 01:39:12.333
way we think about ourselves. So I've got all

1999
01:39:12.342 --> 01:39:14.353
kinds of information up there. I've got summaries of

2000
01:39:14.362 --> 01:39:16.703
chapters, even the case studies that I, I talk

2001
01:39:16.712 --> 01:39:18.723
about in the book. I've got summaries of those

2002
01:39:18.922 --> 01:39:21.672
and I continue to try to post my research

2003
01:39:21.683 --> 01:39:24.545
on there. I'm still working on things and neuroethics

2004
01:39:24.556 --> 01:39:28.436
and neurodiversity. Um But I'm also doing other projects

2005
01:39:28.445 --> 01:39:30.826
um like uh on how do we make more

2006
01:39:30.835 --> 01:39:33.255
progress on on factory farming? That's, that's one of

2007
01:39:33.266 --> 01:39:35.706
my, my next big projects with actually a guest

2008
01:39:35.715 --> 01:39:37.476
you had on, I think in the past Victor

2009
01:39:37.485 --> 01:39:40.275
Kumar. So he's a collaborator of mine on this

2010
01:39:40.286 --> 01:39:41.875
topic and seeing how he and I are thinking

2011
01:39:41.886 --> 01:39:44.306
a lot about that. How do we understand uh

2012
01:39:44.315 --> 01:39:46.826
the the ethical problem of factory farming and how

2013
01:39:46.835 --> 01:39:48.386
can we make progress on it by looking at

2014
01:39:48.395 --> 01:39:49.306
human psychology?

2015
01:39:50.509 --> 01:39:53.250
Great. So look, I really love the book and

2016
01:39:53.259 --> 01:39:56.419
I hope that people from my audience really run

2017
01:39:56.430 --> 01:39:58.689
and buy it. It's a very interesting read and

2018
01:39:58.700 --> 01:40:02.310
also it connects to other interviews with other people

2019
01:40:02.319 --> 01:40:05.129
I did on the show, which we mentioned throughout

2020
01:40:05.140 --> 01:40:08.450
our conversation. So thank you so much for coming

2021
01:40:08.459 --> 01:40:10.359
on the show. It was really fun to talk

2022
01:40:10.370 --> 01:40:10.770
with you.

2023
01:40:11.310 --> 01:40:13.250
Yeah, thanks so much. I really enjoyed the conversation.

2024
01:40:14.339 --> 01:40:17.069
Hi guys. Thank you for watching this interview. Until

2025
01:40:17.080 --> 01:40:19.240
the end. If you liked it, please share it.

2026
01:40:19.250 --> 01:40:22.049
Leave a like and hit the subscription button. The

2027
01:40:22.060 --> 01:40:24.160
show is brought to you by N Lights learning

2028
01:40:24.169 --> 01:40:27.180
and development. Then differently check the website at N

2029
01:40:27.189 --> 01:40:31.140
lights.com and also please consider supporting the show on

2030
01:40:31.149 --> 01:40:34.189
Patreon or paypal. I would also like to give

2031
01:40:34.200 --> 01:40:36.500
a huge thank you to my main patrons and

2032
01:40:36.509 --> 01:40:40.720
paypal supporters, Perera Larson, Jerry Muller and Frederick Suno

2033
01:40:40.770 --> 01:40:43.839
Bernard Seche O of Alex Adam, Castle Matthew Whitting

2034
01:40:43.879 --> 01:40:47.120
B no Wolf, Tim Hall, Erica J, Connors Philip

2035
01:40:47.129 --> 01:40:50.040
Forrest Connolly. Then the Met Robert Wine in Nai

2036
01:40:50.399 --> 01:40:53.790
Z Mark Nevs calling in Holbrook Field, Governor Mikel

2037
01:40:53.799 --> 01:40:57.629
Stormer Samuel Andre Francis for Agns Ferger Ken Herz

2038
01:40:58.660 --> 01:41:02.060
J and Lain Jung Y and the K Hes

2039
01:41:02.069 --> 01:41:05.669
Mark Smith J. Tom Hummel s friends, David Sloan

2040
01:41:05.759 --> 01:41:11.140
Wilson Yasa dear, Roman Roach Diego, Jan Punter, Romani

2041
01:41:11.359 --> 01:41:14.580
Charlotte Bli Nicole Barba, Adam Hunt Pavlo Stassi, Nale

2042
01:41:15.259 --> 01:41:17.919
me, Gary G Alman, Samo, Zal Ari and Ye

2043
01:41:18.080 --> 01:41:21.750
Polton John Barboza, Julian Price Edward Hall, Eden Broner

2044
01:41:22.819 --> 01:41:30.129
Douglas Fry Franka Gilon Cortez Solis Scott. Zachary. Ftw

2045
01:41:30.310 --> 01:41:34.750
Daniel Friedman, William Buckner, Paul Giorgino, Luke Loki, Georgio

2046
01:41:35.220 --> 01:41:38.910
Theophano Chris Williams and Peter Wo David Williams Di

2047
01:41:38.919 --> 01:41:42.939
A Costa Anton Erickson Charles Murray, Alex Shaw, Marie

2048
01:41:42.950 --> 01:41:47.810
Martinez, Coralie Chevalier, Bangalore Fist, Larry Dey junior, Old

2049
01:41:48.330 --> 01:41:52.290
Einon Starry Michael Bailey then Spur by Robert Grassy

2050
01:41:52.359 --> 01:41:56.850
Zorn, Jeff mcmahon, Jake Zul Barnabas Radis Mark Kemple

2051
01:41:56.859 --> 01:42:01.279
Thomas Dvor Luke Neeson, Chris Tory Kimberley Johnson, Benjamin

2052
01:42:01.379 --> 01:42:05.629
Gilbert Jessica. No, Linda Brendan Nicholas Carlson, Ismael Bensley

2053
01:42:05.810 --> 01:42:10.549
Man George Katis, Valentine Steinman, Perros, Kate Von Goler,

2054
01:42:10.910 --> 01:42:18.129
Alexander Albert Liam Dan Biar Masoud Ali Mohammadi Perpendicular

2055
01:42:18.140 --> 01:42:21.979
J Ner Urla. Good enough Gregory Hastings David Pins

2056
01:42:22.240 --> 01:42:26.004
of Sean Nelson, Mike Levin and Jos Net. A

2057
01:42:26.205 --> 01:42:29.044
special thanks to my producers is our web, Jim

2058
01:42:29.055 --> 01:42:32.274
Frank Luca Stina, Tom Vig and Bernard N Cortes

2059
01:42:32.515 --> 01:42:36.274
Dixon, Benedikt Muller Thomas Trumble, Catherine and Patrick Tobin,

2060
01:42:36.285 --> 01:42:39.674
John Carl Negro, Nick Ortiz and Nick Golden. And

2061
01:42:39.685 --> 01:42:43.234
to my executive producers Matthew Lavender, Si Adrian Bogdan

2062
01:42:43.854 --> 01:42:45.495
Knit and Rosie. Thank you for all

