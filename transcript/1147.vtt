WEBVTT

1
00:00:00.340 --> 00:00:03.099
Hello, everyone. Welcome to a new episode of the

2
00:00:03.099 --> 00:00:06.170
Dissenter. I'm your host, as always, Ricardo Lopez, and

3
00:00:06.170 --> 00:00:09.239
today I'm joined for a second time by Doctor

4
00:00:09.569 --> 00:00:13.289
Charlotte Bliss. And today, we're going to talk about

5
00:00:13.289 --> 00:00:17.690
her latest book, Doctor Bot, Why Doctors Can Fail

6
00:00:17.690 --> 00:00:22.059
Us and how AI Could Save Life. So, Charlotte,

7
00:00:22.170 --> 00:00:24.209
welcome back to the show. It's always a pleasure

8
00:00:24.209 --> 00:00:25.100
to to everyone.

9
00:00:25.620 --> 00:00:27.850
It's great to be here, Ricardo. Delighted to be

10
00:00:27.850 --> 00:00:29.040
here talking about the book.

11
00:00:30.360 --> 00:00:34.869
So in regards to how and why doctors can

12
00:00:34.869 --> 00:00:38.950
fail us, what are medical errors or, or what

13
00:00:38.950 --> 00:00:42.770
is medical error and how many people are affected

14
00:00:42.770 --> 00:00:43.250
by it?

15
00:00:44.259 --> 00:00:48.169
Yeah, medical error is, it's estimated in one study

16
00:00:48.169 --> 00:00:50.680
in the states to be the 3rd leading cause

17
00:00:50.680 --> 00:00:55.490
of death after heart disease and cancer. So it's

18
00:00:55.490 --> 00:00:59.290
responsible for the deaths, certainly in the states where

19
00:00:59.290 --> 00:01:02.240
much of the research into these kinds of problems

20
00:01:02.529 --> 00:01:06.529
has has taken place. It it affects the lives

21
00:01:06.529 --> 00:01:09.209
it kills by a quarter of a million people

22
00:01:09.209 --> 00:01:12.665
annually. Um, SO the way I I opened the

23
00:01:12.665 --> 00:01:16.074
book, I say that every few days, the equivalent

24
00:01:16.074 --> 00:01:23.464
of an Airbus for Airbuses carrying 170 passengers crashes

25
00:01:23.464 --> 00:01:25.625
on the ground, but it doesn't make the 24

26
00:01:25.625 --> 00:01:29.665
hour news cycle. Um, AND but it's happening and

27
00:01:29.665 --> 00:01:31.745
it's happening on American soil. This is the equivalent

28
00:01:31.745 --> 00:01:34.055
of this happening on American soil when it comes

29
00:01:34.055 --> 00:01:39.184
to medical error. So, um, yeah, it's a big

30
00:01:39.184 --> 00:01:39.625
problem.

31
00:01:40.120 --> 00:01:42.860
Mhm. But and what causes it? I mean, what

32
00:01:42.860 --> 00:01:45.849
are the causes behind medical error?

33
00:01:46.459 --> 00:01:51.339
Yeah, so there's a variety of different things. Misdiagnosis

34
00:01:51.339 --> 00:01:55.500
is a big one. A diagnostic error affects. Again,

35
00:01:55.750 --> 00:02:00.150
the estimates here are varied, but it's on sort

36
00:02:00.150 --> 00:02:03.910
of charitable estimates affects about 1 in 20 visits

37
00:02:03.910 --> 00:02:07.790
to the doctor. But some researchers working in this

38
00:02:07.790 --> 00:02:10.419
area say it's probably could be as much as

39
00:02:10.419 --> 00:02:15.070
20% of visits. So in 5 to 20%. Um

40
00:02:15.070 --> 00:02:17.240
AND autopsy reports bear out the sort of the

41
00:02:17.240 --> 00:02:22.539
higher figure. Um. It it also is things like

42
00:02:22.539 --> 00:02:27.539
medication errors, handoff problems, communication breakdowns are a big

43
00:02:27.539 --> 00:02:32.619
challenge in medicine. Uh THAT that's particularly involves uh

44
00:02:32.619 --> 00:02:36.500
younger doctors as well, junior doctors, uh, where they

45
00:02:36.500 --> 00:02:39.380
may not be as familiar with processes. So the

46
00:02:39.380 --> 00:02:42.949
systems in which which doctors work can also affect

47
00:02:42.949 --> 00:02:45.139
and and the pressures that are on them within

48
00:02:45.139 --> 00:02:50.210
those systems can certainly affect uh the likelihood of

49
00:02:50.449 --> 00:02:53.020
of some kinds of communication breakdowns and so on

50
00:02:53.020 --> 00:02:56.729
too, but I don't think that they So it's

51
00:02:56.729 --> 00:03:00.360
not entirely the full story. So we are our

52
00:03:00.360 --> 00:03:03.210
cognition obviously is affected by the environment in which

53
00:03:03.210 --> 00:03:05.330
we work in, but I want to say there's

54
00:03:05.330 --> 00:03:06.970
an upper limit to what we can do with

55
00:03:06.979 --> 00:03:10.250
with the systems as they currently are, and how

56
00:03:10.250 --> 00:03:14.009
that could improve things like diagnostic error rates.

57
00:03:14.690 --> 00:03:17.229
Mhm. Yes, we're going to talk a little bit

58
00:03:17.229 --> 00:03:20.919
more about that, but uh does the medical system

59
00:03:20.919 --> 00:03:24.320
affect doctors themselves and would that be one of

60
00:03:24.320 --> 00:03:28.410
the reasons why doctors make errors?

61
00:03:28.639 --> 00:03:33.929
Absolutely. It's a it's a huge issue. And um

62
00:03:35.050 --> 00:03:37.800
To some extent it's kind of taboo as well

63
00:03:37.800 --> 00:03:42.679
within medicine, so doctors are really burnt out. The

64
00:03:42.679 --> 00:03:46.000
studies show that in, for example, the US and

65
00:03:46.000 --> 00:03:49.509
the UK about 50% of doctors are burnt out

66
00:03:50.199 --> 00:03:52.559
during COVID, that figure was even higher, it was

67
00:03:52.559 --> 00:03:57.000
quite staggering. Um, MENTAL health problems are an issue.

68
00:03:57.169 --> 00:03:59.000
So burn out sort of is related to that.

69
00:03:59.009 --> 00:04:03.520
You've got fatigue, uh, sort of withdrawal from, uh,

70
00:04:03.850 --> 00:04:06.770
uh, doing your job but your your top capacity,

71
00:04:06.809 --> 00:04:10.639
but also a feeling of dissatisfaction with the work.

72
00:04:10.970 --> 00:04:14.490
Um, THIS drives errors. So for example, one study

73
00:04:14.490 --> 00:04:17.899
found Uh doctors are depressed, about 20% of doctors

74
00:04:17.899 --> 00:04:23.410
in the state are report being depressed. Um, AND

75
00:04:23.619 --> 00:04:26.899
that leads to around depression and self can cause

76
00:04:26.899 --> 00:04:30.410
error. So one study finds a sixfold increase in

77
00:04:30.410 --> 00:04:35.730
medication errors. Um. And you know, when it comes

78
00:04:35.730 --> 00:04:41.500
to suicide, about 300 to 400 doctors every year

79
00:04:41.500 --> 00:04:44.820
in the states take their own lives. A very

80
00:04:44.820 --> 00:04:48.850
recent that's equivalent of one medical school graduating class

81
00:04:48.850 --> 00:04:53.459
in a colleague of mine in Karolinska, Emma Brulin

82
00:04:53.459 --> 00:04:57.910
one study. Very recently published in Sweden that found

83
00:04:57.910 --> 00:05:00.869
there was a treble trebling of the rate of

84
00:05:00.869 --> 00:05:04.950
suicide among psychiatrists. So there are mental health professions

85
00:05:04.950 --> 00:05:07.859
that burn out as well compared to the population.

86
00:05:08.029 --> 00:05:12.570
So, um, mental health issues, feelings of burnout fatigue,

87
00:05:13.029 --> 00:05:17.910
uh, working long hours, very, very good reasons why

88
00:05:17.910 --> 00:05:20.470
doctors are exhausted by their job.

89
00:05:21.540 --> 00:05:25.179
In the book at a certain point you talk

90
00:05:25.179 --> 00:05:29.739
about how doctors often resist inputs from outsiders. I

91
00:05:29.739 --> 00:05:32.899
mean, why is that important and in what ways

92
00:05:32.899 --> 00:05:36.730
do you think those inputs could improve medical practice?

93
00:05:37.260 --> 00:05:41.529
Yeah, um. I think they do, and it's interesting

94
00:05:41.529 --> 00:05:47.420
because medicine is a very hierarchical institution and field

95
00:05:47.420 --> 00:05:50.130
in a way. Um, AND in some ways that's

96
00:05:50.130 --> 00:05:53.859
part of the education of a doctor. Um, BECAUSE

97
00:05:53.859 --> 00:05:56.700
it's almost an apprenticeship, you learn from the people

98
00:05:56.700 --> 00:05:59.619
higher up. So there's very and in some respects,

99
00:06:00.140 --> 00:06:03.149
traditional medicine or medicine as we know it, that's

100
00:06:03.149 --> 00:06:05.619
that's important for how you learn. Some of it

101
00:06:05.619 --> 00:06:08.130
is copying, some of it's learning on the job,

102
00:06:08.700 --> 00:06:11.700
the processes and all the rest of it. So

103
00:06:11.700 --> 00:06:14.070
it does tend to be quite a hierarchical field

104
00:06:14.070 --> 00:06:17.850
and that psychology of status is somewhat problematic within

105
00:06:17.850 --> 00:06:21.670
it. But for us as patients, as well as

106
00:06:21.670 --> 00:06:24.739
for people who are lower in the pecking order.

107
00:06:24.950 --> 00:06:27.630
But I think that's part of the reason why

108
00:06:27.630 --> 00:06:30.549
domain experts in other fields tend to be a

109
00:06:30.549 --> 00:06:34.510
bit Excluded. So what you sometimes find in medical

110
00:06:34.510 --> 00:06:36.239
school, it's quite often you find in medical schools,

111
00:06:36.250 --> 00:06:39.200
and I've been hanging around medical schools for for

112
00:06:39.200 --> 00:06:43.220
over a decade now, working within them, but some

113
00:06:44.679 --> 00:06:47.799
of the sociology or their ethics or psychology that's

114
00:06:47.799 --> 00:06:51.239
to be taught, it's very often taught by a

115
00:06:51.239 --> 00:06:53.790
doctor, a medical doctor who has taken an interest

116
00:06:53.790 --> 00:06:56.290
in those fields rather than sort of domain experts

117
00:06:56.290 --> 00:07:00.190
across faculty coming in to teach but um. So

118
00:07:00.190 --> 00:07:04.709
that's a generality, but so even among those who

119
00:07:04.709 --> 00:07:09.390
say medicine is an information processing field, and there's

120
00:07:09.390 --> 00:07:14.429
some leading medical doctors who are advocates of information

121
00:07:14.429 --> 00:07:17.260
processing as if they don't really delve into what

122
00:07:17.260 --> 00:07:20.989
that actually means, which is interesting too, which is

123
00:07:20.989 --> 00:07:23.470
something I tried to cover a bit. But it's

124
00:07:23.470 --> 00:07:28.989
important because when you exclude other experts, you have

125
00:07:28.989 --> 00:07:32.790
a very narrow lens in which to understand the

126
00:07:32.790 --> 00:07:36.709
medical profession. And that I think has been a

127
00:07:36.709 --> 00:07:39.910
problem. So if you look at medical memoirs, medical

128
00:07:39.910 --> 00:07:44.839
books, memos and exception here, but they're mostly an

129
00:07:44.839 --> 00:07:49.609
assumption that only a doctor. Who who has practiced

130
00:07:49.609 --> 00:07:53.799
medicine has the real expertise to write about it,

131
00:07:54.049 --> 00:07:56.850
which is a bit again, it's overly narrow because

132
00:07:56.850 --> 00:08:01.250
we know sociology, psychology is relevant here, philosophy is

133
00:08:01.250 --> 00:08:06.505
relevant. Cognitive science is incredibly important in thinking about

134
00:08:06.505 --> 00:08:08.454
what it is the doctor do. So I feel

135
00:08:08.665 --> 00:08:13.015
that very often that gets omitted and we're left

136
00:08:13.015 --> 00:08:17.714
with a very there's a lot of blind spots

137
00:08:17.714 --> 00:08:21.529
and how to think about medical thinking. If it's

138
00:08:21.529 --> 00:08:25.000
only left to doctors and sometimes a kind of

139
00:08:25.000 --> 00:08:30.369
polyannaish response about how we improve medicine as in

140
00:08:30.369 --> 00:08:32.169
maybe they just need a bit doctors just need

141
00:08:32.169 --> 00:08:33.969
a bit more education. If you look at some

142
00:08:33.969 --> 00:08:37.090
of the educational. SOLUTIONS. I mean, I say it's

143
00:08:37.090 --> 00:08:40.570
a bit like homely chicken soup for, you know,

144
00:08:40.729 --> 00:08:43.409
conquers it's, you know, it's comforting, but it's not

145
00:08:43.409 --> 00:08:45.969
really going to do very much. And that's been

146
00:08:45.969 --> 00:08:50.320
a kind of a light motif throughout the history

147
00:08:50.320 --> 00:08:51.609
of medicine, I would say to which you can

148
00:08:51.609 --> 00:08:53.210
get into a little bit if you like.

149
00:08:53.739 --> 00:08:56.780
Mhm. Sure. Uh, SO tell us a little bit

150
00:08:56.780 --> 00:09:00.419
about the work you've done on, uh, sharing access

151
00:09:00.419 --> 00:09:04.619
to patients' health records. Why is that important and

152
00:09:04.979 --> 00:09:07.909
why is it sometimes so hard to access?

153
00:09:08.750 --> 00:09:12.989
Yeah, so access to one's health records online, a

154
00:09:12.989 --> 00:09:15.070
bit like online banking, you know, you get access

155
00:09:15.070 --> 00:09:17.950
to your own information in real time and and

156
00:09:17.950 --> 00:09:20.909
all the rest of it. Um, THIS is quite

157
00:09:20.909 --> 00:09:25.429
a recent innovation, um, but around 30 countries worldwide

158
00:09:25.429 --> 00:09:28.789
patients have at least some access to their online

159
00:09:28.789 --> 00:09:33.330
record um. America has the most robustly transparent. The

160
00:09:33.330 --> 00:09:36.909
United States is the most robustly transparent access. But

161
00:09:37.510 --> 00:09:40.750
what you find is there's been huge resistance wherever

162
00:09:40.750 --> 00:09:45.469
this is being proposed, and the medical profession gets

163
00:09:45.469 --> 00:09:49.109
very anxious about doctors about patients are poking their

164
00:09:49.109 --> 00:09:52.309
noses into their own records, um, which I think

165
00:09:52.309 --> 00:09:57.729
is an interesting thing in itself. Because it sort

166
00:09:57.729 --> 00:10:02.409
of is somewhat reflective of the sociologists talk about

167
00:10:02.409 --> 00:10:05.210
the power dynamics in medicine, and they very rarely

168
00:10:05.210 --> 00:10:07.929
explain where this why doctors want the power, where

169
00:10:07.929 --> 00:10:10.210
it comes from. That's a different story, but um

170
00:10:11.609 --> 00:10:15.104
so there is definitely this sort of. SUBORDINATION of

171
00:10:15.104 --> 00:10:19.405
the patient who after all has the greatest vested

172
00:10:19.405 --> 00:10:23.145
interest in accessing their own health information. And what's

173
00:10:23.145 --> 00:10:25.505
fascinating and I've done a lot of research in

174
00:10:25.505 --> 00:10:28.505
this with my colleagues, both at open notes in

175
00:10:28.505 --> 00:10:32.984
Beth Israel Deaconess and also in Uppsala University, but

176
00:10:33.385 --> 00:10:37.625
in Sweden, but patients derive wherever studies have been

177
00:10:37.625 --> 00:10:40.590
conducted, they derive a lot of benefit. Because they

178
00:10:40.590 --> 00:10:44.460
better remember what doctors said, they can look up

179
00:10:44.460 --> 00:10:48.710
medications, and they remember to follow up with test

180
00:10:48.710 --> 00:10:51.390
results. So there's a lot of benefits to the

181
00:10:51.390 --> 00:10:54.630
patient, but again, and I think somewhat for good

182
00:10:54.630 --> 00:10:57.830
reasons, when I say this, doctors are a bit

183
00:10:57.830 --> 00:11:01.510
resistant because, well, they express the fear that there's

184
00:11:01.510 --> 00:11:04.679
going to be more contact time from patients and

185
00:11:04.679 --> 00:11:07.919
very mixed findings of mostly patients. DON'T like to

186
00:11:07.919 --> 00:11:10.590
burden their doctors. Um, BUT those are the kinds

187
00:11:10.590 --> 00:11:14.070
of things that could be managed. So um it's

188
00:11:14.070 --> 00:11:17.669
reflective, I think of this this differential the status

189
00:11:17.669 --> 00:11:20.469
differential in medicine and the idea. I mean, the

190
00:11:20.469 --> 00:11:24.710
British Medical Association threatened to sue a couple of

191
00:11:24.710 --> 00:11:30.510
years ago, NHS England for opening the records didn't

192
00:11:30.510 --> 00:11:35.140
come to anything but enormous fear and resistance.

193
00:11:35.820 --> 00:11:40.099
Mhm. Do you think that doctors are essential in

194
00:11:40.099 --> 00:11:42.179
providing care to patients?

195
00:11:43.309 --> 00:11:47.599
Um, SO that's a great question. It's a provocative

196
00:11:47.599 --> 00:11:51.140
question. I, I think that yes, currently they are

197
00:11:51.140 --> 00:11:55.539
essential, but I suppose what the perspective that I

198
00:11:55.539 --> 00:11:57.799
take it would go back to this idea. If

199
00:11:57.799 --> 00:12:02.460
you look at the trajectory of technology, and I

200
00:12:02.460 --> 00:12:05.150
come back to this idea of Amara's law, Roy

201
00:12:05.150 --> 00:12:10.010
Amara was an American futurologist who said people tend

202
00:12:10.010 --> 00:12:13.700
to overestimate what technology can do in the short

203
00:12:13.700 --> 00:12:16.539
term, but underestimate what it can do in the

204
00:12:16.539 --> 00:12:25.250
long term. Um, AND, uh, uh, business analysts and

205
00:12:25.250 --> 00:12:31.330
again, other tech forecasters like um Christiansen, the late

206
00:12:31.330 --> 00:12:34.809
Clayton Christensen, but also Richard and Daniels have written

207
00:12:34.809 --> 00:12:39.789
about the future of the professions. I'm very sympathetic

208
00:12:40.039 --> 00:12:44.000
to their perspective, which is we are increasingly going

209
00:12:44.000 --> 00:12:49.000
to see a dismantling of tasks that are performed

210
00:12:49.000 --> 00:12:53.479
by white collar professionals that will be taken over.

211
00:12:53.880 --> 00:13:00.119
uh BY technology, somewhat in part, but then more

212
00:13:00.119 --> 00:13:04.559
than likely, largely taken over in the future. I

213
00:13:04.559 --> 00:13:06.380
don't think we're we're there yet at all, but

214
00:13:06.380 --> 00:13:09.039
we do see this incursion, which I don't think

215
00:13:09.039 --> 00:13:12.109
is going to go away. Um, SO in the

216
00:13:12.109 --> 00:13:14.929
longer term, I wouldn't wouldn't put forecasts on it,

217
00:13:14.960 --> 00:13:17.830
but I do think it predicts hard, hard timelines

218
00:13:17.830 --> 00:13:21.070
on it, but, but certainly I don't think doctors,

219
00:13:21.309 --> 00:13:24.260
as we know them will will always be. Uh,

220
00:13:26.039 --> 00:13:28.760
NECESSARY, which may be very creepy to a lot

221
00:13:28.760 --> 00:13:30.679
of people who are watching or listening.

222
00:13:31.469 --> 00:13:36.340
Yeah, so before we get into AI and how

223
00:13:36.340 --> 00:13:40.179
it can contribute to the health care system, let

224
00:13:40.179 --> 00:13:43.460
me just ask you a few questions about some

225
00:13:43.460 --> 00:13:46.419
of the issues that people might have or even

226
00:13:46.419 --> 00:13:51.260
frequently have when trying to access medicine and some

227
00:13:51.260 --> 00:13:53.460
of the issues they have to deal with also.

228
00:13:54.070 --> 00:13:59.369
When uh dealing with doctors directly. So, is medicine

229
00:13:59.369 --> 00:14:03.080
generally economically accessible to most people?

230
00:14:04.119 --> 00:14:08.330
The short answer is no. TAKE a system like

231
00:14:08.330 --> 00:14:11.250
the US and it's in some respects an outlier

232
00:14:11.250 --> 00:14:14.369
when it comes to rich countries, but let's even

233
00:14:14.369 --> 00:14:17.159
take a sort of principle of charity. Look at

234
00:14:17.159 --> 00:14:20.250
look at wealthier countries where people generally have more

235
00:14:20.250 --> 00:14:26.130
access to doctors. There are more doctors. In the

236
00:14:26.130 --> 00:14:31.479
US, 60% of bankruptcies are caused by medical costs,

237
00:14:32.369 --> 00:14:35.969
so there are massive challenges in paying for health

238
00:14:35.969 --> 00:14:40.530
coverage, but generally worldwide about 100 million people annually

239
00:14:40.530 --> 00:14:42.809
are propelled into poverty because of the cost of

240
00:14:42.809 --> 00:14:45.859
their healthcare. Um, BUT even if you look at

241
00:14:45.859 --> 00:14:49.859
the the the the EU where most patients have

242
00:14:49.859 --> 00:14:53.169
access to at least some services, still around the

243
00:14:53.169 --> 00:14:57.380
festive spending comes from patients' pockets, and that affects

244
00:14:57.380 --> 00:15:01.969
obviously poor patients more who have in the EU

245
00:15:01.969 --> 00:15:04.770
5 times more likely to have unmet health needs.

246
00:15:05.080 --> 00:15:07.549
Now, even if you have sort of free at

247
00:15:07.549 --> 00:15:10.109
the point of access care at the bricks and

248
00:15:10.109 --> 00:15:12.750
mortar hospital, and this is where medicine tends to

249
00:15:12.750 --> 00:15:15.710
focus on what happens when you actually enter the

250
00:15:15.710 --> 00:15:19.419
doctor's office. But before that, there are costs involved

251
00:15:19.419 --> 00:15:23.309
in getting to the doctor's office. Transportation costs are

252
00:15:23.309 --> 00:15:28.429
huge for patients. If you rely on public transport,

253
00:15:29.070 --> 00:15:31.659
you're more likely to miss your visit or to

254
00:15:31.669 --> 00:15:34.789
to delay going to the doctor. If you work

255
00:15:34.789 --> 00:15:37.190
in a gig gig economy job or you have

256
00:15:37.190 --> 00:15:40.380
parental duties, uh, or if you're you're a woman,

257
00:15:40.510 --> 00:15:43.580
you're you're more likely to forgo appointments as well.

258
00:15:43.750 --> 00:15:49.075
People with disabilities, the elderly, the actual physical challenge

259
00:15:49.075 --> 00:15:52.125
of getting to the doctor is huge. So these

260
00:15:52.125 --> 00:15:55.924
challenges are often referred to as the inverse care

261
00:15:55.924 --> 00:16:01.005
law. It's recognized that people with the greater greatest

262
00:16:01.005 --> 00:16:05.244
need are less likely to have access to healthcare.

263
00:16:07.989 --> 00:16:12.859
Uh, COSTS are huge, um, but also if you

264
00:16:12.859 --> 00:16:18.349
like physical barriers are massive challenges as well. Uh,

265
00:16:18.460 --> 00:16:21.580
PEOPLE with disabilities, if I mention them, they often

266
00:16:21.580 --> 00:16:25.200
tend to get overlooked in all of. But wherever

267
00:16:25.200 --> 00:16:28.359
you see studies, they've got their twice as likely

268
00:16:28.359 --> 00:16:32.719
to uh again for go seeing the doctor, because

269
00:16:32.719 --> 00:16:36.659
there's a lot of challenges and actually transporting yourself

270
00:16:36.659 --> 00:16:38.200
into the doctor's office.

271
00:16:39.770 --> 00:16:43.210
Apart from economic factors, what would you say are

272
00:16:43.210 --> 00:16:47.609
the other main barriers that people encounter in accessing

273
00:16:47.609 --> 00:16:48.359
medicine?

274
00:16:48.770 --> 00:16:51.630
So what I've said, I think transportation is a

275
00:16:51.630 --> 00:16:54.890
big one, taking the time out of your day,

276
00:16:54.969 --> 00:16:59.530
which is highly disruptive, um. Um, SO in American

277
00:16:59.530 --> 00:17:04.130
time use survey finds that for a 20 minute

278
00:17:04.130 --> 00:17:07.170
visit, patients take an average of 2 hours out

279
00:17:07.170 --> 00:17:11.010
of their day, to make the visit, that's higher

280
00:17:11.010 --> 00:17:14.050
if you're on a low income or you're unemployed

281
00:17:14.050 --> 00:17:17.530
because you're you're likely to have greater travel concerns

282
00:17:17.530 --> 00:17:21.459
um and it's more disruptive to your day. Um.

283
00:17:22.868 --> 00:17:25.890
Those sorts of issues again, the issues with disability,

284
00:17:25.910 --> 00:17:33.790
elderly patients in the states again, legacy structures, the

285
00:17:33.790 --> 00:17:36.510
effects of redlining and you know, laws that are

286
00:17:36.510 --> 00:17:39.910
that are not that we don't have to sort

287
00:17:39.910 --> 00:17:42.790
of create our next too far back in history

288
00:17:42.790 --> 00:17:45.900
that there was segregation and that's had a geographical

289
00:17:45.900 --> 00:17:51.040
effect on access. So, uh, primary care shortage areas

290
00:17:51.040 --> 00:17:56.000
areas tend to be uh they're 70% higher and

291
00:17:56.000 --> 00:18:00.329
predominantly black neighborhoods. So you have access buyers that

292
00:18:00.329 --> 00:18:03.359
can affect people for a variety of reasons. And

293
00:18:03.359 --> 00:18:07.050
then of course, not every the contingencies of history,

294
00:18:07.160 --> 00:18:10.199
not every city on earth. I mean, I lived

295
00:18:10.199 --> 00:18:14.469
in Boston, it's an epicenter for medicine. Uh, I'm

296
00:18:14.469 --> 00:18:19.319
from Belfast. It isn't, um, you know, so historical

297
00:18:19.319 --> 00:18:24.199
happenstance means not everybody actually has resources close to

298
00:18:24.199 --> 00:18:24.589
them.

299
00:18:25.640 --> 00:18:31.010
And when dealing with doctors themselves, what is symptom

300
00:18:31.010 --> 00:18:31.819
denial?

301
00:18:33.180 --> 00:18:40.189
So patients can sometimes patients delay seeking health seeking

302
00:18:40.189 --> 00:18:43.109
because they literally aren't quite sure what's wrong. There's

303
00:18:43.109 --> 00:18:46.489
this sort of conundrum where you have to decide,

304
00:18:46.500 --> 00:18:49.270
you as a patient are your first triage system.

305
00:18:49.630 --> 00:18:53.949
So you have to decide when your own symptoms

306
00:18:53.949 --> 00:18:57.040
reach a threshold. It's worthy of visiting the doctor

307
00:18:57.040 --> 00:18:59.550
in a sense. Um, EVEN though you lack that

308
00:18:59.550 --> 00:19:02.150
expertise, you've got to work it out. That's incidentally

309
00:19:02.150 --> 00:19:05.579
why many the internet as a resource, we get

310
00:19:05.579 --> 00:19:09.030
into sort of generative AI technologies to those sorts

311
00:19:09.030 --> 00:19:12.270
of tools, but they are useful to the extent

312
00:19:12.270 --> 00:19:14.699
they can of course they have limitations as well,

313
00:19:14.790 --> 00:19:20.130
but they can facilitate. Um, UNDERSTANDING. So symptom denial

314
00:19:20.130 --> 00:19:25.150
can arise as well. So there's this general uh

315
00:19:25.589 --> 00:19:27.949
challenge of knowing when your symptoms are right, but

316
00:19:27.949 --> 00:19:31.670
symptom denial can also arise in front of doctors.

317
00:19:32.030 --> 00:19:36.739
So, um, Uh, patients tend, and again this comes

318
00:19:36.739 --> 00:19:39.329
back to the psychology of status in the visit,

319
00:19:39.699 --> 00:19:43.300
where it's built in because you are sitting at

320
00:19:43.300 --> 00:19:44.979
the feet of an expert as a patient. That's

321
00:19:44.979 --> 00:19:46.979
the reason why you visit the doctor, you want

322
00:19:46.979 --> 00:19:51.439
to find out more. Um, BUT patients tend to

323
00:19:51.439 --> 00:19:55.854
see a face in those situations. Irving Goffman. The

324
00:19:55.854 --> 00:19:59.974
Canadian sociologists talk about presentation of self in everyday

325
00:19:59.974 --> 00:20:04.135
life and psychologists talk about social desirability biases and

326
00:20:04.135 --> 00:20:06.094
all the rest, but the idea that you tend

327
00:20:06.094 --> 00:20:10.415
to present yourself in a different way, particularly next

328
00:20:10.415 --> 00:20:14.494
to somebody who is comparatively higher status, meaning you're

329
00:20:14.494 --> 00:20:19.630
more likely to to uh Uh, studies in the

330
00:20:19.630 --> 00:20:23.469
states show about 85% of patients have lied or

331
00:20:23.469 --> 00:20:28.349
concealed clinically relevant information to a doctor, mostly about

332
00:20:28.349 --> 00:20:33.709
socially stigmatized conditions, mental health conditions, people delay health

333
00:20:33.709 --> 00:20:36.949
seeing, got lots of studies on that, but also

334
00:20:36.949 --> 00:20:41.564
embarrass. RED flag cancer symptoms as well. People may

335
00:20:41.564 --> 00:20:46.604
delay because they're afraid the doctor bother this currently

336
00:20:46.604 --> 00:20:50.415
comes up. They're afraid of being judged and they

337
00:20:50.415 --> 00:20:53.525
have a fear of embarrassment. So those are the

338
00:20:53.525 --> 00:20:57.484
kinds of psychological pitfalls that can interfere with the

339
00:20:57.484 --> 00:21:02.814
visit and amount to a kind of denial and

340
00:21:04.285 --> 00:21:05.964
vernacular kind of language.

341
00:21:07.280 --> 00:21:12.390
And and which are better interviewers, doctors or computers,

342
00:21:12.479 --> 00:21:14.119
I mean, do we know that?

343
00:21:16.189 --> 00:21:20.520
So, if you break, I'll say short answer, doctors,

344
00:21:20.770 --> 00:21:26.790
but if you break the task down, Patients disclose

345
00:21:26.790 --> 00:21:30.709
more to um not to they don't disclose as

346
00:21:30.709 --> 00:21:34.599
much to doctors. So even though a doctor is

347
00:21:34.599 --> 00:21:37.589
still probably the best we have now at having

348
00:21:37.589 --> 00:21:43.079
the dialogue to gather medical information and to to

349
00:21:43.079 --> 00:21:46.140
celebrate the task of diagnosis, but that's different from

350
00:21:46.140 --> 00:21:50.380
actually disclosing, divulging your problems, in which case uh

351
00:21:51.510 --> 00:21:56.300
computers and even pen and paper and just questionnaires

352
00:21:56.300 --> 00:21:58.859
are a better resource for doing. And we have

353
00:21:58.859 --> 00:22:02.890
decades of research on this. Um, AGAIN, the reasons

354
00:22:02.890 --> 00:22:07.250
why go back to the what we've just discussed

355
00:22:07.250 --> 00:22:12.260
this as psychology of status and and patients feeling

356
00:22:12.260 --> 00:22:16.229
a bit subordinate. They don't like to, um, Uh,

357
00:22:16.270 --> 00:22:20.800
divulge certain symptoms, they're embarrassed, but also in the

358
00:22:20.800 --> 00:22:24.430
medical visit, doctors do tend to dominate the dialogue.

359
00:22:24.530 --> 00:22:28.589
Again, lots of, of social science research on that.

360
00:22:28.839 --> 00:22:34.050
They interrupt. Um, THEY, they interrupt and they end

361
00:22:34.050 --> 00:22:37.489
up taking the floor, so to speak, which interferes

362
00:22:37.489 --> 00:22:41.410
with information gathering. So patients literally pour their hearts

363
00:22:41.410 --> 00:22:44.329
out literally it's not the right word, figuratively pour

364
00:22:44.329 --> 00:22:49.160
their hearts out to machines and um that again

365
00:22:49.160 --> 00:22:52.569
that goes back to the 1960s with Joseph Weizenbaum,

366
00:22:53.760 --> 00:22:58.040
Warner slack another doctor. VERY same year, 1966, they

367
00:22:58.040 --> 00:22:59.920
both recognize it published in the same as a

368
00:22:59.920 --> 00:23:02.599
great minds think alike kind of scenario and they

369
00:23:02.599 --> 00:23:05.839
recognized even when you put patients in front of

370
00:23:05.839 --> 00:23:08.680
these sort of monster sized computers, they could see

371
00:23:08.680 --> 00:23:12.640
them slagging off the computer uninhibited, saying, you know,

372
00:23:12.689 --> 00:23:14.790
all kinds of things that they would never say

373
00:23:14.790 --> 00:23:17.910
in front of a doctor in a white coat.

374
00:23:19.650 --> 00:23:23.439
When it comes to diagnosis, what should we consider

375
00:23:23.439 --> 00:23:27.329
when it comes to how doctors learn to diagnose

376
00:23:27.329 --> 00:23:29.560
and their human limitations?

377
00:23:31.430 --> 00:23:35.550
Lots to think about here. So, um, again, I

378
00:23:35.550 --> 00:23:38.359
come back to the idea that doctors are only

379
00:23:38.359 --> 00:23:40.910
human. Um, AND that's in some ways is the

380
00:23:40.910 --> 00:23:43.109
premise of the book. You know, let's take seriously

381
00:23:43.109 --> 00:23:47.790
the fact doctors aren't gods. Um, SO there's limitations

382
00:23:47.790 --> 00:23:53.560
with keeping up to date. Um, WITH biomedical knowledge,

383
00:23:54.619 --> 00:23:56.729
and what doctors are expected to do is, is,

384
00:23:56.780 --> 00:24:00.890
is just, it's colossal actually. I made a calculation

385
00:24:00.890 --> 00:24:05.900
a couple of years ago using PubMed, and a

386
00:24:05.900 --> 00:24:10.050
biomedical article is published every 39 seconds. Um, AND

387
00:24:10.050 --> 00:24:12.449
if doctors were to read only 2% of the

388
00:24:12.449 --> 00:24:16.319
relevant material there, they'd be spending 22.5 hours per

389
00:24:16.319 --> 00:24:18.489
day, and that's not even committing it to memory

390
00:24:18.489 --> 00:24:21.569
and making it sort of practically useful information, which

391
00:24:21.569 --> 00:24:25.170
is what you need for expertise. So you can

392
00:24:25.170 --> 00:24:28.329
see the massive challenges there and kind of this

393
00:24:28.329 --> 00:24:34.119
information treadmill. Um, BEYOND that, um, there are challenges

394
00:24:34.119 --> 00:24:38.000
with noise in medicine. Daniel Conneman has written about

395
00:24:38.000 --> 00:24:39.959
this is something I also focus on. There's a

396
00:24:39.959 --> 00:24:44.709
whole variety of noise that arises. There's slight gender

397
00:24:44.900 --> 00:24:49.369
differences in male and female doctors and how they

398
00:24:49.369 --> 00:24:53.930
practice with some fascinating ties to a slight incremental

399
00:24:54.060 --> 00:24:59.260
uh greater likelihood that female doctors practice evidence based

400
00:24:59.260 --> 00:25:02.119
medicine they're more likely to, which is interesting. But

401
00:25:02.119 --> 00:25:05.280
the other side, you've got huge variety in terms

402
00:25:05.280 --> 00:25:08.479
of say junior doctors on the one hand learning

403
00:25:08.479 --> 00:25:13.680
new things, uh, getting into the comfort zone of

404
00:25:13.680 --> 00:25:16.920
practicing medicine fluently, so to speak, and then you've

405
00:25:16.920 --> 00:25:19.640
got cognitive decline, which kicks in on our 40

406
00:25:19.640 --> 00:25:23.439
and 48 do really like to acknowledge this, but

407
00:25:23.439 --> 00:25:25.750
you know, you've got a cognitive decline kicking in,

408
00:25:25.760 --> 00:25:27.790
which makes it even more but that's kind of

409
00:25:27.790 --> 00:25:31.660
taboo actually. A greater challenge of keeping up to

410
00:25:31.660 --> 00:25:35.890
date. Um, AND, uh, you know, it's, it's very

411
00:25:35.890 --> 00:25:39.250
little wonder doctors practice evidence-based medicine only around half

412
00:25:39.250 --> 00:25:41.130
the time. We've got lots of studies to show.

413
00:25:41.560 --> 00:25:45.989
That's the case. Um, SO the diagnostic challenges here

414
00:25:45.989 --> 00:25:50.920
are enormous and even aside from the noise, you

415
00:25:50.920 --> 00:25:54.439
have the challenge of medicine itself is sort of

416
00:25:54.439 --> 00:25:59.119
an exercise in stereotyping, because it's just Robert McAuley

417
00:25:59.119 --> 00:26:02.959
would talk about practice naturalness, uh, a bit like.

418
00:26:03.099 --> 00:26:06.729
OF learned common system one, you learn to recognize

419
00:26:06.729 --> 00:26:11.089
patterns when it comes to symptoms and diseases, but

420
00:26:11.089 --> 00:26:17.050
the real challenge here is sometimes unwanted biases can

421
00:26:17.050 --> 00:26:19.890
kick in. How do you avoid those when you

422
00:26:19.890 --> 00:26:23.890
lack conscious access to to what how you're making

423
00:26:23.890 --> 00:26:28.849
the decision. Um, SO there, there are huge challenges

424
00:26:28.849 --> 00:26:31.170
and I mean the way I frame it is

425
00:26:31.170 --> 00:26:33.329
it's amazing doctors get it right as often as

426
00:26:33.329 --> 00:26:33.729
they do.

427
00:26:34.890 --> 00:26:39.530
Yeah. How about AI? I mean, how developed this

428
00:26:39.530 --> 00:26:43.650
AI in terms of being able to diagnose diseases

429
00:26:43.650 --> 00:26:45.599
at this point in time and do you think

430
00:26:45.599 --> 00:26:49.719
that it can overcome uh doctor's limitations when it

431
00:26:49.719 --> 00:26:51.130
comes to diagnosis?

432
00:26:53.199 --> 00:26:57.020
Really big question. So it depends on the AI

433
00:26:57.020 --> 00:27:01.880
tools, um, and it depends on how it is

434
00:27:01.880 --> 00:27:07.280
integrated into care as well. Um, SO, What we

435
00:27:07.280 --> 00:27:10.400
know is, so take sort of AI 2.0, which

436
00:27:10.400 --> 00:27:13.670
is machine learning, which is great at working out

437
00:27:13.670 --> 00:27:18.319
patterns, it can crunch through, you know, vast volumes

438
00:27:18.319 --> 00:27:21.839
of data to pick up, to discern patterns that

439
00:27:21.839 --> 00:27:24.650
we're not able to see. And that is an

440
00:27:24.650 --> 00:27:28.380
amazing tool. It's important in radiology. It can be

441
00:27:28.380 --> 00:27:32.380
very where it can uh it can be diag

442
00:27:32.380 --> 00:27:37.010
diagnostically more consistent and eliminate some of the noise

443
00:27:37.010 --> 00:27:41.199
and be more accurate. Um, BUT it's also that

444
00:27:41.199 --> 00:27:44.040
kind of uh of AI is also useful, for

445
00:27:44.040 --> 00:27:48.050
example, in electronic health records, sifting through and making

446
00:27:48.050 --> 00:27:53.670
prognostic predictions. Uh, AGAIN, coming back to to uh

447
00:27:53.670 --> 00:27:58.489
very interesting studies by by a team at Harvard

448
00:27:58.489 --> 00:28:04.160
Medical School, which found that from. HEALTH records, you

449
00:28:04.160 --> 00:28:09.839
can discern patients with, for example, victims of domestic

450
00:28:09.839 --> 00:28:15.040
violence or patients who are likely to take their

451
00:28:15.040 --> 00:28:18.079
own lives. And the studies show we're talking about,

452
00:28:18.160 --> 00:28:21.390
you know, from 10 to 30 months in advance

453
00:28:21.599 --> 00:28:24.319
for about 40% of patients of doctors who is

454
00:28:24.319 --> 00:28:26.079
the flip of a coin, they can't tell very

455
00:28:26.079 --> 00:28:29.479
often what's going to happen. So that kind of

456
00:28:29.479 --> 00:28:34.150
prediction is highly significant and could be incredibly important

457
00:28:34.400 --> 00:28:37.160
for doctors. Then you've got the kind of uh

458
00:28:37.959 --> 00:28:40.569
But the question is, are those tools useful? Well,

459
00:28:40.579 --> 00:28:43.180
it depends on how they're trained and are they

460
00:28:43.180 --> 00:28:45.660
valid for other populations. You've got a whole set

461
00:28:45.660 --> 00:28:50.609
of other concerns there. Then you have issues with

462
00:28:50.609 --> 00:28:54.010
uh you have the generative AI tools which are

463
00:28:54.010 --> 00:28:57.530
sort of AI 3.0 as we currently have, which

464
00:28:57.530 --> 00:29:01.609
are able to again use vast amounts of large

465
00:29:01.609 --> 00:29:05.849
language models these are of information that they that

466
00:29:05.849 --> 00:29:10.729
they can generate responses to. So we've all Anyone

467
00:29:10.729 --> 00:29:15.359
who's used tools like chat GPT or or Google's

468
00:29:15.359 --> 00:29:19.079
Gemini, that's a bit like talking to the internet

469
00:29:19.079 --> 00:29:21.780
on steroids. But again, these are very good at

470
00:29:21.780 --> 00:29:26.160
associative patterns, but they don't do counterfactual reasoning or

471
00:29:26.170 --> 00:29:28.729
or they don't have strengths and causality, but that

472
00:29:28.729 --> 00:29:31.329
they still can be very useful tools for picking

473
00:29:31.329 --> 00:29:35.969
out associations. And again, you've got fascinating and and

474
00:29:35.969 --> 00:29:39.010
you'd be cheerless not to to be impressed by

475
00:29:39.010 --> 00:29:44.609
a lot of these studies, which you give some

476
00:29:44.609 --> 00:29:48.209
examples, but um again, I would say the limitations

477
00:29:48.209 --> 00:29:51.699
here would be, again, we don't yet have all

478
00:29:51.699 --> 00:29:56.760
the evidence that they're superior to doctors, but they

479
00:29:56.770 --> 00:29:59.410
they they can be highly impressive and you've got

480
00:29:59.410 --> 00:30:02.479
challenges here with how these tools are trained. The

481
00:30:02.989 --> 00:30:06.310
how they hallucinate that is they make they make

482
00:30:06.310 --> 00:30:10.660
mistakes, which can be very compelling and confident, very

483
00:30:10.660 --> 00:30:16.469
confident. Um, THEY also can be succumbed to biases.

484
00:30:16.550 --> 00:30:21.050
They can replicate human biases, they can be obsequious

485
00:30:21.050 --> 00:30:22.800
sort of people. PLEASING in their own way and

486
00:30:22.800 --> 00:30:25.400
their responses. And so there are a whole host

487
00:30:25.400 --> 00:30:28.560
of challenges with those tools as well. And the

488
00:30:28.560 --> 00:30:31.079
real issue there, I think Ricardo in the long

489
00:30:31.079 --> 00:30:33.160
term is going to be, is it, is it

490
00:30:33.160 --> 00:30:34.920
better or worse because you're not going to have

491
00:30:34.920 --> 00:30:38.099
the perfect technology and we just don't know yet.

492
00:30:38.119 --> 00:30:41.910
We just don't have the research, but I would

493
00:30:41.910 --> 00:30:44.510
sort of pause here and say the technology is

494
00:30:44.510 --> 00:30:46.709
the worst it's ever going to be, so it

495
00:30:46.709 --> 00:30:48.930
would be a mistake only to focus on current

496
00:30:48.930 --> 00:30:52.150
technologies and say that that's sort of the end

497
00:30:52.150 --> 00:30:55.310
of the line here, you know, AI 75 years

498
00:30:55.310 --> 00:30:58.439
old, you know, your competition in the future is

499
00:30:58.439 --> 00:31:00.099
not going to look like if you're a doctor,

500
00:31:00.270 --> 00:31:04.140
but it currently doesn't, so that's um yeah.

501
00:31:05.550 --> 00:31:08.449
And how about treatment? What would you say are

502
00:31:08.449 --> 00:31:12.290
some of the biggest issues with human doctors doing

503
00:31:12.290 --> 00:31:12.609
it?

504
00:31:14.089 --> 00:31:18.569
Um, SO, Lots of ways to interpret this. We

505
00:31:18.569 --> 00:31:21.770
could be thinking about robotics. Uh, WE could be

506
00:31:21.770 --> 00:31:24.290
thinking about treatment in terms, not something I get

507
00:31:24.290 --> 00:31:26.890
into into in the book is robotics. I sort

508
00:31:26.890 --> 00:31:30.930
of think more about the primary care visit but

509
00:31:30.930 --> 00:31:36.369
treatment could be in terms of um treatment suggestions

510
00:31:36.369 --> 00:31:41.819
or prescribing or giving information to patients about what

511
00:31:41.819 --> 00:31:46.729
about their condition. And you've also got the issue

512
00:31:46.729 --> 00:31:50.079
of treatment is sort of bedside manner as well,

513
00:31:50.489 --> 00:31:51.920
which you think you can get into you think

514
00:31:51.920 --> 00:31:54.810
about them differently and I think it when it

515
00:31:54.810 --> 00:31:57.609
comes to each of those, there is, there are

516
00:31:57.609 --> 00:32:02.219
challenges for for doctors to get it right. By

517
00:32:02.219 --> 00:32:05.890
the way, these are humans who are multitasking, multitasking

518
00:32:05.890 --> 00:32:09.489
is, you know, people don't multitask, the task switch.

519
00:32:10.140 --> 00:32:13.510
So the toggle between tasks and the more tasks

520
00:32:13.510 --> 00:32:15.630
they have to do, the more likely they are

521
00:32:15.630 --> 00:32:20.589
to make mistakes. So lots of issues there were

522
00:32:20.589 --> 00:32:25.910
AI could come in and supplement or take over

523
00:32:25.910 --> 00:32:27.790
certain tasks. And if we if we take the

524
00:32:27.790 --> 00:32:35.540
example of um communicating the patients, treatment suggestions or

525
00:32:35.540 --> 00:32:39.310
recommendations. Look at the vehicle of the online record

526
00:32:39.310 --> 00:32:43.089
that we talked about earlier, um. It seems and

527
00:32:43.089 --> 00:32:45.319
I've done some research and it seems that doctors

528
00:32:45.319 --> 00:32:48.959
are using generative AI tools to help them with

529
00:32:48.959 --> 00:32:53.680
documentation, more than likely for communicating with patients. So

530
00:32:53.680 --> 00:32:57.630
I would say already there's an uptake in using

531
00:32:57.630 --> 00:33:02.650
these sorts of tools for treatment as a sort

532
00:33:02.650 --> 00:33:04.880
of as communication.

533
00:33:05.709 --> 00:33:08.630
Mhm. Uh, SO we talked about the limitations of

534
00:33:08.630 --> 00:33:12.109
doctors, but, uh, and we, you also mentioned some

535
00:33:12.109 --> 00:33:16.619
of the current limitations of AI tools, but, uh,

536
00:33:16.630 --> 00:33:21.109
can AI also be biased? And if so, why

537
00:33:21.109 --> 00:33:23.069
should we worry about that?

538
00:33:24.270 --> 00:33:27.579
AI can certainly be biased, and if we go

539
00:33:27.579 --> 00:33:33.469
back to Cathy O'Neill weapons of mass destruction 2016,

540
00:33:33.589 --> 00:33:37.589
her book, she sort of launched that into the

541
00:33:37.589 --> 00:33:41.150
conversation where there was a tendency to think algorithms

542
00:33:41.150 --> 00:33:43.630
are neutral and there's still a sort of human.

543
00:33:44.089 --> 00:33:46.719
TENDENCY to think that they they are more neutral

544
00:33:46.719 --> 00:33:50.280
than than humans, but certainly they can be biased.

545
00:33:52.170 --> 00:33:55.829
And in some sense, machine learning is a misnomer

546
00:33:55.829 --> 00:34:00.010
because humans can decide what they feed these machines,

547
00:34:00.209 --> 00:34:05.290
what they're trained on, and They can be it's

548
00:34:05.290 --> 00:34:08.330
the old slogan garbage in garbage out. So they

549
00:34:08.330 --> 00:34:14.010
can perpetuate if there's omissions and biases within information

550
00:34:14.010 --> 00:34:17.449
fed to these to whatever AI and machine learning

551
00:34:17.449 --> 00:34:20.725
tools, generative AI tools, they can still be. PERPETUATED.

552
00:34:21.014 --> 00:34:25.293
I give an example. Um, ONE study found that

553
00:34:25.293 --> 00:34:30.735
AI or health data that is used to train

554
00:34:30.735 --> 00:34:34.813
AI, but 50% of it comes from America, United

555
00:34:34.813 --> 00:34:38.375
States of America and China. Now that raises really

556
00:34:38.375 --> 00:34:43.510
grave issues about global representation. Similarly, generative AI tools

557
00:34:43.510 --> 00:34:47.510
are trained on open source on internet sources and

558
00:34:47.510 --> 00:34:50.168
all the rest 50% of the internet's in English.

559
00:34:50.830 --> 00:34:53.510
Fewer than 1 in 5 people speaks English where

560
00:34:53.510 --> 00:34:56.550
you've got a challenge there with the accuracy of

561
00:34:56.550 --> 00:34:59.870
these tools for marginalized languages. There's evidence that they're

562
00:34:59.870 --> 00:35:04.820
getting better for many languages Italian, Spanish is good.

563
00:35:05.560 --> 00:35:08.870
Uh, AS English for one study found for for

564
00:35:08.870 --> 00:35:14.550
medical responses. Uh, BUT, um, with all the limitations

565
00:35:14.550 --> 00:35:17.469
still, but for many languages, it's they're still not

566
00:35:17.469 --> 00:35:20.439
going to be there. So that's an example of

567
00:35:20.439 --> 00:35:23.350
the ways and we also know that these tools.

568
00:35:23.469 --> 00:35:28.250
AND perpetuate some of the diagnostic uh biases that

569
00:35:28.250 --> 00:35:31.100
exist and may even worsen them in some cases,

570
00:35:31.179 --> 00:35:33.860
some cases, some studies show worsen them. Some say

571
00:35:33.860 --> 00:35:37.100
that they're better and the fewer biases. It depends.

572
00:35:37.139 --> 00:35:39.729
So it's very hard to answer in the round,

573
00:35:39.860 --> 00:35:44.780
but certainly they can um they're susceptible to bias.

574
00:35:45.520 --> 00:35:47.360
Mhm. So, uh,

575
00:35:47.510 --> 00:35:48.439
what role

576
00:35:48.439 --> 00:35:53.669
would you say technology and specifically AI can play

577
00:35:53.669 --> 00:35:54.639
in medicine?

578
00:35:56.280 --> 00:36:01.540
I think it can play a it will play

579
00:36:01.540 --> 00:36:03.949
and it is already currently playing a role in

580
00:36:03.959 --> 00:36:10.580
in medicine. Um, I give some examples because AI

581
00:36:10.580 --> 00:36:12.989
tools have already, I mean, since the 1980s, there's

582
00:36:12.989 --> 00:36:18.550
been clinical decision support tools with expert AI's in

583
00:36:18.550 --> 00:36:26.469
the form of advice about um um. ALERTS about

584
00:36:26.469 --> 00:36:31.889
medications, for example, or um algorithmic support, you know

585
00:36:31.889 --> 00:36:35.149
that in the electronic health record that will say

586
00:36:35.149 --> 00:36:38.229
please ask this patient these questions about whatever their

587
00:36:38.229 --> 00:36:40.449
high risk for. But what tends to happen with

588
00:36:40.449 --> 00:36:43.550
those sorts of supports was they're just not well

589
00:36:43.550 --> 00:36:47.669
integrated into the workflow. So doctors tended to, they

590
00:36:47.669 --> 00:36:50.350
have very little evidence that these to actually improve

591
00:36:50.350 --> 00:36:55.219
patient outcomes because they tend to get overridden. DOCTORS

592
00:36:55.219 --> 00:36:58.500
are susceptible to alert fatigue as it's called. Um,

593
00:36:58.510 --> 00:37:03.909
BUT other tools are already being deployed in healthcare.

594
00:37:04.020 --> 00:37:07.979
I mean, I've just done a hold off the

595
00:37:07.979 --> 00:37:11.100
presses for you, Ricardo, we got survey data back

596
00:37:11.100 --> 00:37:14.750
on a UK study on what's called ambient AI.

597
00:37:15.060 --> 00:37:17.699
Now this is a form of AI that also

598
00:37:17.699 --> 00:37:21.540
uses large language models, but it listens to the

599
00:37:21.540 --> 00:37:27.000
medical visit. And it populates the conversation with into

600
00:37:27.000 --> 00:37:31.239
the electronic health record to make the administrative task

601
00:37:31.239 --> 00:37:34.040
of the doctor easier. So that they can actually

602
00:37:34.040 --> 00:37:37.320
just talk to the patient. What we find, we

603
00:37:37.320 --> 00:37:40.080
asked a UK GPs, you know, are you using

604
00:37:40.080 --> 00:37:43.149
it to 14% said that they were just 1

605
00:37:43.149 --> 00:37:46.959
in 7. Um, BUT there's mixed findings there about

606
00:37:46.959 --> 00:37:51.270
the error rate, how good these tools are for

607
00:37:51.600 --> 00:37:55.199
different kinds of patients. On the whole, they tended

608
00:37:55.199 --> 00:37:57.600
to think that the tools and very small sample

609
00:37:57.600 --> 00:38:02.030
size the tools are better for than their own

610
00:38:02.030 --> 00:38:05.030
note taking, but they need oversight. You can't, you

611
00:38:05.030 --> 00:38:08.080
know, there's errors there, the notes can so called

612
00:38:08.080 --> 00:38:13.169
hallucinate, the AI can hallucinate and For patients who

613
00:38:13.169 --> 00:38:15.370
don't speak English as their first language, there's still

614
00:38:15.370 --> 00:38:20.850
challenges. Um, SO AI is already within the visit

615
00:38:20.850 --> 00:38:25.879
increasingly so we can get into generative AI did

616
00:38:25.879 --> 00:38:28.090
survey research on this. I think I mentioned the

617
00:38:28.090 --> 00:38:33.560
other. Um, PODCAST we did, but we found 2024,

618
00:38:33.570 --> 00:38:37.580
1 in 5 doctors said they were using commercial

619
00:38:37.580 --> 00:38:41.169
genitive AI tools of UK doctors. This year it

620
00:38:41.169 --> 00:38:43.510
went up to 1 in 4 January we did

621
00:38:43.510 --> 00:38:46.235
the we did the survey. EXACT same survey, January

622
00:38:46.235 --> 00:38:49.415
25 quarter of doctors said they were using AI

623
00:38:49.415 --> 00:38:55.074
tools like CBT for clinical tasks. So it's already

624
00:38:55.074 --> 00:38:57.754
here, whether they're using them effectively is a whole

625
00:38:57.754 --> 00:39:02.675
different concern, but it's it's in the visit.

626
00:39:03.729 --> 00:39:06.709
And do we know if they're using it effectively?

627
00:39:07.800 --> 00:39:12.929
Um, So we here we get to the issue

628
00:39:12.929 --> 00:39:17.010
of uh this is this is the key issue

629
00:39:17.010 --> 00:39:20.159
actually, and again I overly problem with the book,

630
00:39:20.169 --> 00:39:22.090
but I do have a chapter on this this

631
00:39:22.090 --> 00:39:24.889
idea of man and machine working together and this

632
00:39:24.889 --> 00:39:26.969
is sort of the last mile problem as it's

633
00:39:26.969 --> 00:39:30.929
been called and actually it's very what what we

634
00:39:30.929 --> 00:39:36.330
do tend to see is that uh. So people

635
00:39:36.330 --> 00:39:40.419
tend to anthropomorphize, especially this later generation of AI

636
00:39:40.979 --> 00:39:42.810
because as they say, they feel like they're talking

637
00:39:42.810 --> 00:39:48.719
to another human, so to speak, and they because

638
00:39:48.719 --> 00:39:50.919
there's a back and forth, there's a dialogic nature

639
00:39:50.919 --> 00:39:54.389
of these tools, but there is a tendency among

640
00:39:54.389 --> 00:39:57.879
domain experts to be more likely to hold their

641
00:39:57.879 --> 00:40:00.620
nose to algorithmic output. And this goes back to

642
00:40:00.620 --> 00:40:05.000
the work of Paul Mill in the 50s. Berkeley

643
00:40:05.000 --> 00:40:08.620
De Forest in University of Chicago, you've got people

644
00:40:08.620 --> 00:40:11.639
like Jennifer Logg at Georgetown, they've worked on, but

645
00:40:11.639 --> 00:40:13.719
again this tends to be siloed and not much

646
00:40:13.719 --> 00:40:17.020
taken up with them in medical fields, but There

647
00:40:17.020 --> 00:40:21.610
is a tendency for experts to be more averse

648
00:40:21.610 --> 00:40:26.409
to algorithmic output, um, but the lay lay population

649
00:40:26.409 --> 00:40:30.060
tends to be in a particular domain, tends to

650
00:40:30.060 --> 00:40:33.219
be more deferential, more trusting, so that's called the

651
00:40:33.219 --> 00:40:39.379
algorithmic appreciation versus algorithmic aversion. But we do I'm

652
00:40:39.379 --> 00:40:47.879
seeing interesting studies where um there is where. Doctors

653
00:40:47.879 --> 00:40:51.310
are deferring sometimes, perhaps they are too trusting. So

654
00:40:51.310 --> 00:40:53.679
the issue here it's a really challenging one Carlo

655
00:40:53.679 --> 00:40:59.830
because do doctors sometimes overly trust or overly distrust

656
00:40:59.830 --> 00:41:03.639
and what we do see in some studies by

657
00:41:03.639 --> 00:41:06.625
Adam Rodman and Ethan. OTHERS is if you actually

658
00:41:06.625 --> 00:41:11.145
compare doctors working on with genitive AI tools or

659
00:41:11.145 --> 00:41:15.264
with other just internet tools versus the the generative

660
00:41:15.264 --> 00:41:19.385
AI on its own, like chat CPT or OpenAIO

661
00:41:19.385 --> 00:41:22.375
one or whatever, you do tend to still see

662
00:41:22.665 --> 00:41:27.679
that the In some studies that the doctors are

663
00:41:27.679 --> 00:41:30.669
hurting the accuracy of the AI and the AI

664
00:41:30.669 --> 00:41:34.479
is is quite significantly better. So that would suggest

665
00:41:34.479 --> 00:41:37.280
there is this continuation of a kind of algorithmic

666
00:41:37.280 --> 00:41:41.760
aversion, which which invites very interesting questions about what's

667
00:41:41.760 --> 00:41:45.949
the right configuration of expertise to work with AI

668
00:41:47.560 --> 00:41:51.000
and or indeed when humans should be in the

669
00:41:51.000 --> 00:41:53.469
loop and high. What, you know, do we need

670
00:41:53.469 --> 00:41:56.239
a medical professional at all, what does that mean?

671
00:41:56.399 --> 00:41:59.739
What, what is the training involved in order to

672
00:41:59.739 --> 00:42:04.080
be astute and critical, but also judicious. So it's

673
00:42:04.080 --> 00:42:06.209
not a very easy question to answer.

674
00:42:07.489 --> 00:42:10.330
OK, so I have one last question then. How

675
00:42:10.330 --> 00:42:13.010
do you look at the future of the sort

676
00:42:13.010 --> 00:42:18.459
of partnership between doctors and AI in providing healthcare?

677
00:42:18.489 --> 00:42:21.879
And do you think there's uh any version of

678
00:42:21.879 --> 00:42:26.969
the future where doctors would simply be replaced by

679
00:42:26.969 --> 00:42:27.489
AI?

680
00:42:28.979 --> 00:42:33.899
Um, I, I, I think that so that you're

681
00:42:33.899 --> 00:42:35.260
asking me to put my head above the parapet

682
00:42:35.260 --> 00:42:36.979
here very much, but look, I think in the

683
00:42:36.979 --> 00:42:40.100
future it will that's the direction of travel, but

684
00:42:40.100 --> 00:42:42.899
I don't think we're near it yet. Um, I

685
00:42:42.899 --> 00:42:45.209
think there's going to be a period of flux

686
00:42:45.209 --> 00:42:48.620
where we see, and we already see it's very

687
00:42:48.620 --> 00:42:53.100
hard to integrate these tools within workflow. It's very

688
00:42:53.100 --> 00:42:58.659
difficult for doctors to change their um. Uh, WORK

689
00:42:58.659 --> 00:43:02.840
habits because their day is so packed and busy

690
00:43:03.070 --> 00:43:06.830
and frenetic, so changing how you do things is

691
00:43:06.830 --> 00:43:13.550
challenging, um. But I do foresee that AI will

692
00:43:13.550 --> 00:43:17.790
take over some tasks, surveys I've conducted, doctors always

693
00:43:17.790 --> 00:43:19.469
think it's going to come for the task they

694
00:43:19.469 --> 00:43:25.550
least like, the administrative task, the documentation, they're divided

695
00:43:25.550 --> 00:43:29.860
about whether it will happen for diagnostics and prognostics,

696
00:43:29.949 --> 00:43:31.949
but the thing they least think it will come

697
00:43:31.949 --> 00:43:35.360
for is empathy. My answer to that is I

698
00:43:35.360 --> 00:43:37.959
don't think any doctor goes through 10 years of

699
00:43:37.959 --> 00:43:42.350
medical education to become a patient empathizer. So I

700
00:43:42.350 --> 00:43:44.939
think clinging to that is a particular, you know,

701
00:43:45.030 --> 00:43:47.719
will always be needed with or without AI because

702
00:43:47.719 --> 00:43:51.239
we empathize other people can empathize, we can deploy

703
00:43:51.239 --> 00:43:56.159
other people in the duty of care of patients

704
00:43:56.159 --> 00:43:59.929
and to facilitate that. So I guess it's it's

705
00:43:59.929 --> 00:44:03.610
um I think it will happen. I think that

706
00:44:03.610 --> 00:44:07.280
there will be increasing disintermediation or taking over technological

707
00:44:07.280 --> 00:44:09.090
task, but it's going to be much slower than

708
00:44:09.090 --> 00:44:12.449
people imagine. And there's wider societal questions that we

709
00:44:12.449 --> 00:44:15.770
need to ask about that as well. Privacy is

710
00:44:15.770 --> 00:44:19.169
huge issues with the disruption of of healthcare as

711
00:44:19.169 --> 00:44:19.669
we know it.

712
00:44:21.340 --> 00:44:24.280
Great. So the book is again Doctor Bo Why

713
00:44:24.280 --> 00:44:27.439
Doctors Can Fail Us and how AI could Save

714
00:44:27.439 --> 00:44:30.560
Lives. Uh, OF course, leaving a link to the

715
00:44:30.560 --> 00:44:34.280
book in the description down below and uh Charlotte,

716
00:44:34.360 --> 00:44:37.439
just before we go, where can people find you

717
00:44:37.439 --> 00:44:39.229
and your work on the internet?

718
00:44:39.760 --> 00:44:43.600
So you'll find me on uh scholar, Google Scholar,

719
00:44:43.760 --> 00:44:46.360
you'll find me on, I have my own website

720
00:44:46.360 --> 00:44:48.389
that I, I curate where I put all my,

721
00:44:48.439 --> 00:44:54.629
my articles and Uppsala University website as well. And

722
00:44:54.629 --> 00:44:57.169
I'm on I'm on Twitter and Blue Sky and

723
00:44:57.169 --> 00:44:57.649
LinkedIn.

724
00:44:58.459 --> 00:45:01.540
Yeah, great. So thank you so much for taking

725
00:45:01.540 --> 00:45:03.780
the time to come to come on the show

726
00:45:03.780 --> 00:45:05.689
again. It's always a pleasure to talk with you.

727
00:45:05.939 --> 00:45:07.689
It's great to be here. Thank you very much.

728
00:45:09.229 --> 00:45:11.750
Hi guys, thank you for watching this interview until

729
00:45:11.750 --> 00:45:13.879
the end. If you liked it, please share it,

730
00:45:14.070 --> 00:45:16.860
leave a like and hit the subscription button. The

731
00:45:16.860 --> 00:45:19.060
show is brought to you by Nights Learning and

732
00:45:19.060 --> 00:45:23.139
Development done differently, check their website at Nights.com and

733
00:45:23.139 --> 00:45:26.860
also please consider supporting the show on Patreon or

734
00:45:26.860 --> 00:45:29.340
PayPal. I would also like to give a huge

735
00:45:29.340 --> 00:45:32.770
thank you to my main patrons and PayPal supporters

736
00:45:32.770 --> 00:45:36.699
Pergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyche Olaf,

737
00:45:36.780 --> 00:45:40.020
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

738
00:45:40.100 --> 00:45:43.449
Erika Lenny, John Connors, Philip Fors Connolly. Then the

739
00:45:43.449 --> 00:45:47.250
Mari Robert Windegaruyasi Zu Mark Nes called in Holbrookfield

740
00:45:47.250 --> 00:45:52.040
governor, Michael Stormir Samuel Andrea, Francis Forti Agnseroro and

741
00:45:52.040 --> 00:45:56.370
Hal Herzognun Macha Joan Lays and the Samuel Corriere,

742
00:45:56.530 --> 00:46:00.199
Heinz, Mark Smith, Jore, Tom Hummel, Sardus Fran David

743
00:46:00.199 --> 00:46:04.060
Sloan Wilson, Asila dearraujoro and Roach Diego Londono Correa.

744
00:46:04.439 --> 00:46:09.750
Yannick Punteran Rosmani Charlotte blinikol Barbara Adamhn Pavlostaevskynaleb medicine,

745
00:46:10.479 --> 00:46:15.070
Gary Galman Samov Zaledrianei Poltonin John Barboza, Julian Price,

746
00:46:15.360 --> 00:46:19.810
Edward Hall Edin Bronner, Douglas Fry, Franca Bartolotti Gabrielon

747
00:46:19.810 --> 00:46:24.280
Scorteus Slelisky, Scott Zachary Fish Tim Duffyani Smith John

748
00:46:24.280 --> 00:46:29.219
Wieman. Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai

749
00:46:29.219 --> 00:46:33.719
Giorgio Theophanous, Chris Williamson, Peter Vozin, David Williams, the

750
00:46:33.719 --> 00:46:38.270
Augusta, Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez,

751
00:46:38.300 --> 00:46:42.520
Coralli Chevalier, bungalow atheists, Larry D. Lee Junior, Old

752
00:46:42.520 --> 00:46:47.429
Heringbo. Sterry Michael Bailey, then Sperber, Robert Grassy Zigoren,

753
00:46:47.590 --> 00:46:52.030
Jeff McMahon, Jake Zu, Barnabas radix, Mark Campbell, Thomas

754
00:46:52.030 --> 00:46:56.360
Dovner, Luke Neeson, Chris Stor, Kimberly Johnson, Benjamin Galbert,

755
00:46:56.510 --> 00:47:01.860
Jessica Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George

756
00:47:01.860 --> 00:47:07.090
Eoriatis, Valentin Steinman, Perkrolis, Kate van Goller, Alexander Aubert,

757
00:47:07.909 --> 00:47:13.750
Liam Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner,

758
00:47:13.870 --> 00:47:18.629
Ursula Gudinov, Gregory Hastings, David Pinsoff Sean Nelson, Mike

759
00:47:18.629 --> 00:47:22.290
Levine, and Jos Net. A special thanks to my

760
00:47:22.290 --> 00:47:25.129
producers. These are Webb, Jim, Frank Lucas Steffinik, Tom

761
00:47:25.129 --> 00:47:30.010
Venneden, Bernard Curtis Dixon, Benedic Muller, Thomas Trumbull, Catherine

762
00:47:30.010 --> 00:47:33.290
and Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and

763
00:47:33.290 --> 00:47:36.649
Nick Golden, and to my executive producers Matthew Levender,

764
00:47:36.770 --> 00:47:39.919
Sergio Quadrian, Bogdan Kanivets, and Rosie. Thank you for

765
00:47:39.919 --> 00:47:40.239
all.

