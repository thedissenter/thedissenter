WEBVTT

1
00:00:00.370 --> 00:00:03.109
Hello, everyone. Welcome to a new episode of the

2
00:00:03.109 --> 00:00:05.789
Dissenter. I'm your host, as always, Ricardo Lopez, and

3
00:00:05.789 --> 00:00:08.560
today I'm joined by Doctor Mark Sin. He works

4
00:00:08.560 --> 00:00:12.100
as a senior research scientist at the TNO, a

5
00:00:12.100 --> 00:00:16.158
leading research and technology organization in the Netherlands. And

6
00:00:16.158 --> 00:00:19.319
today we're talking about his book, Ethics for people

7
00:00:19.319 --> 00:00:22.719
who work in Tech. So, Doctor Sin, welcome to

8
00:00:22.719 --> 00:00:24.350
the show. It's a pleasure to everyone.

9
00:00:25.120 --> 00:00:26.719
Thanks for the invitation. It's a pleasure to be

10
00:00:26.719 --> 00:00:27.000
here.

11
00:00:28.479 --> 00:00:32.200
So tell us first, how do you approach ethics

12
00:00:32.200 --> 00:00:34.159
in your book? I mean, I guess that even

13
00:00:34.159 --> 00:00:37.880
the more basic question would be what is ethics

14
00:00:37.880 --> 00:00:43.040
exactly? Yeah, so, uh, uh, I've worked for 25

15
00:00:43.040 --> 00:00:47.580
years in research and development in ICT and what

16
00:00:47.580 --> 00:00:53.119
we would currently call uh AI, uh, data-driven uh

17
00:00:53.119 --> 00:00:59.180
innovation. And in that context, uh, uh, clients, partners,

18
00:00:59.419 --> 00:01:01.490
co-workers ask me whether I can help them with

19
00:01:01.490 --> 00:01:04.690
ethics, and that gives me a very practical perspective

20
00:01:04.690 --> 00:01:07.209
on ethics. So for me, it means those people

21
00:01:07.209 --> 00:01:10.720
involved in development and deployment of technologies, digital technologies,

22
00:01:10.970 --> 00:01:14.330
how they take into account various ethical questions during

23
00:01:14.330 --> 00:01:18.599
the process. So it's very much a process approach

24
00:01:18.599 --> 00:01:21.010
to ethics, and you can contrast it if you

25
00:01:21.010 --> 00:01:23.669
want, if you will, with the With more like,

26
00:01:23.680 --> 00:01:25.599
uh, yes, a bit of caricature but with a

27
00:01:25.599 --> 00:01:29.650
checkbox approach like did we do the privacy correctly?

28
00:01:29.760 --> 00:01:31.760
Did we do the bias correctly? All those topics

29
00:01:31.760 --> 00:01:34.839
appear also in this process approach of ethics, but

30
00:01:34.839 --> 00:01:39.419
much more in the In the form of conversations

31
00:01:39.419 --> 00:01:42.830
that people can have about these topics, deliberation, reflection,

32
00:01:42.860 --> 00:01:45.970
and then, yeah, changing the products that they're working

33
00:01:45.970 --> 00:01:49.180
on based on their findings from those dialogues and

34
00:01:49.180 --> 00:01:51.970
reflections. So it's an ethics as a process.

35
00:01:52.900 --> 00:01:55.650
Mhm. And in the book you mention or make

36
00:01:55.650 --> 00:02:00.050
reference to a humanistic approach to the ethics of

37
00:02:00.050 --> 00:02:01.290
technology. What is that?

38
00:02:02.110 --> 00:02:06.500
Um, I think humanistic, you can say it means

39
00:02:06.500 --> 00:02:09.850
putting people center stage that is also also being

40
00:02:09.850 --> 00:02:12.410
in the in the tradition of human centered design.

41
00:02:12.500 --> 00:02:15.860
So you're looking at the people, the ultimate beneficiaries

42
00:02:15.860 --> 00:02:19.940
of these systems where they can flourish better. You

43
00:02:19.940 --> 00:02:22.699
can also look at more level of society, whether

44
00:02:22.699 --> 00:02:26.714
these. This can help to create uh more equity,

45
00:02:26.884 --> 00:02:31.404
uh, uh, rule of law, fairness, uh, and also

46
00:02:31.404 --> 00:02:34.804
that's maybe another meaning of humanistic. It means involving

47
00:02:34.804 --> 00:02:38.315
the humanities much more than is often done in

48
00:02:38.315 --> 00:02:42.044
technology development deployment, so, uh, philosophy and, and culture

49
00:02:42.044 --> 00:02:43.035
and all those things.

50
00:02:43.990 --> 00:02:46.899
Mhm. So, since we're going to talk a lot

51
00:02:46.899 --> 00:02:50.669
about ethics in technology today, what is the distinction

52
00:02:50.669 --> 00:02:54.690
between ethics in people and ethics in machines?

53
00:02:55.220 --> 00:02:58.300
Yeah, that's, uh, I, I, I would say that

54
00:02:58.300 --> 00:03:02.529
for me, ethics only happens within people and, and,

55
00:03:02.580 --> 00:03:06.619
and between people. Uh, I don't think a computer

56
00:03:06.619 --> 00:03:10.419
can have ethics. I know that people, uh, also

57
00:03:10.419 --> 00:03:13.850
co-workers of mine, they make efforts to Uh, put

58
00:03:13.850 --> 00:03:17.770
a bit of ethical reasoning into robots. So the

59
00:03:17.770 --> 00:03:20.720
autonomous system will then have a rule like, uh,

60
00:03:21.130 --> 00:03:24.270
uh, don't do this because it is dangerous, uh,

61
00:03:24.410 --> 00:03:26.279
for people, so don't do it. So you can

62
00:03:26.279 --> 00:03:29.089
say, well, it is programming a bit of basic

63
00:03:29.089 --> 00:03:32.929
ethics into the robot. Yeah, in that sense, the

64
00:03:32.929 --> 00:03:35.410
machine can be more or less, behave more or

65
00:03:35.410 --> 00:03:38.619
less ethical, but I mean, the ethics happened in

66
00:03:38.619 --> 00:03:41.570
the programmers who then wrote the program and then

67
00:03:41.570 --> 00:03:45.179
the computer, the robot executes. So yeah, I would

68
00:03:45.179 --> 00:03:47.350
say that ethics happens within people and, and in

69
00:03:47.350 --> 00:03:49.100
between people, how we, how we engage with each

70
00:03:49.100 --> 00:03:51.100
other, how we treat each other, of course, but

71
00:03:51.100 --> 00:03:52.179
not so much in robots.

72
00:03:53.029 --> 00:03:55.990
Mhm. So, since you've been working a lot on

73
00:03:55.990 --> 00:04:01.589
the design and application of technology, are there similarities

74
00:04:01.589 --> 00:04:05.270
between normative ethics and that that is the design

75
00:04:05.270 --> 00:04:06.750
and application of technology?

76
00:04:07.419 --> 00:04:11.399
Yeah, that's, that's a, that's an interesting question. In

77
00:04:11.399 --> 00:04:14.520
my experience, they have lots, lots in common, so

78
00:04:14.520 --> 00:04:18.358
normative ethics understood as that branch of ethics that

79
00:04:18.358 --> 00:04:20.600
looks at the world and then says, hey, we

80
00:04:20.600 --> 00:04:24.410
can do this better, more towards fairness, more towards

81
00:04:24.920 --> 00:04:30.320
equity. It bears resemblance to the work in my

82
00:04:30.320 --> 00:04:32.720
experience that engineers or designers can do. They also

83
00:04:32.720 --> 00:04:35.200
look at the world and say, hey, there's something

84
00:04:35.200 --> 00:04:37.440
not quite right here. Something can be improved, and

85
00:04:37.440 --> 00:04:39.980
then they go about it. So in that sense,

86
00:04:40.070 --> 00:04:43.529
they share a similar worldview looking at the world,

87
00:04:43.690 --> 00:04:46.470
finding some elements of it problematic, and then making

88
00:04:46.470 --> 00:04:50.299
efforts to to improve those situations. And of course,

89
00:04:50.329 --> 00:04:54.089
the engineer will use technology and a normative ethicist

90
00:04:54.089 --> 00:04:57.209
will use uh ethics, but they have, they have

91
00:04:57.209 --> 00:04:58.730
lots of things in common in that sense, and

92
00:04:58.730 --> 00:05:01.170
I I tried to include that, I, I, I

93
00:05:01.170 --> 00:05:02.980
included that in the book in the introduction just

94
00:05:02.980 --> 00:05:07.209
to, to get um engineers or developers or technologies

95
00:05:07.209 --> 00:05:09.440
more broadly to get them more on board like

96
00:05:09.769 --> 00:05:12.250
the work of ethics is not weird. You have

97
00:05:12.250 --> 00:05:14.290
something in common already. You look at the world,

98
00:05:14.369 --> 00:05:16.929
you want to improve things, which is good. So,

99
00:05:17.170 --> 00:05:20.799
yeah, it's, it's making ethics more attractive, more, more,

100
00:05:20.970 --> 00:05:22.959
more, more accessible to them.

101
00:05:23.890 --> 00:05:27.890
But do you think that uh philoso moral philosophers

102
00:05:27.890 --> 00:05:32.730
and ethicists would be open to approaching the ethics

103
00:05:32.730 --> 00:05:37.369
of technology in this more sort of design and

104
00:05:37.369 --> 00:05:38.799
application way or?

105
00:05:38.970 --> 00:05:41.130
Oh yeah, I mean, uh, where I live in

106
00:05:41.130 --> 00:05:45.440
the Netherlands, uh we have uh 4 universities of

107
00:05:45.440 --> 00:05:49.149
technologies, and they, they all have great departments of

108
00:05:49.149 --> 00:05:53.399
applied philosophy. Uh, APPLIED ethics, ethics of technology. So,

109
00:05:53.519 --> 00:05:57.239
yeah, that, that combination is, uh, is almost normal,

110
00:05:57.279 --> 00:06:00.790
you can say in, in the Netherlands. Mhm.

111
00:06:01.480 --> 00:06:04.119
So tell us then about what in the book

112
00:06:04.119 --> 00:06:09.079
you present as a three-step approach to facilitate ethical

113
00:06:09.079 --> 00:06:13.420
reflection, inquiry, and deliberation. So what are the three

114
00:06:13.420 --> 00:06:14.079
steps here?

115
00:06:14.350 --> 00:06:16.890
Yeah, thanks. So that, that ties back to what

116
00:06:16.890 --> 00:06:20.170
we said in the introduction, ethics as a process

117
00:06:20.459 --> 00:06:25.140
to make it a bit more systematic. I'm introducing

118
00:06:25.140 --> 00:06:28.579
this three-step progress process and I'm also emphasizing this

119
00:06:28.579 --> 00:06:31.660
is an iterative process. So it's not like you

120
00:06:31.660 --> 00:06:33.980
go 123, then you're good. You can do 12,

121
00:06:34.019 --> 00:06:35.459
you can do 1 again, you can 2, you

122
00:06:35.459 --> 00:06:37.820
can, so you, you can switch between, but it's

123
00:06:37.820 --> 00:06:41.720
also useful to distinguish these phases, so. Uh, STEP

124
00:06:41.720 --> 00:06:45.760
one, phase one is identifying those aspects, those topics,

125
00:06:45.799 --> 00:06:49.040
those issues that are or may become problematic in

126
00:06:49.040 --> 00:06:52.200
your project. So say you're working, a project team

127
00:06:52.200 --> 00:06:58.190
is working on fraud detection algorithm, uh, well, Then

128
00:06:58.190 --> 00:07:00.750
they can have a bit of a reflection of

129
00:07:00.750 --> 00:07:04.459
brainstorming even what could go wrong? Well, bias, discrimination,

130
00:07:04.790 --> 00:07:07.839
uh, the problem of transparency, uh, the problem of

131
00:07:07.839 --> 00:07:12.029
false positives where you accuse somebody and then they

132
00:07:12.029 --> 00:07:15.220
did not commit fraud. So there's discrepancy between the

133
00:07:15.220 --> 00:07:19.209
algorithm and reality. So it's, it's, it's putting those

134
00:07:19.209 --> 00:07:22.220
issues on the table to then go to step

135
00:07:22.220 --> 00:07:25.480
2 that is have conversations about these issues. And

136
00:07:25.480 --> 00:07:28.450
first, these conversations can be between uh the people

137
00:07:28.450 --> 00:07:31.700
in the project team, but I'm also advocating that

138
00:07:31.700 --> 00:07:35.040
um bringing in the clients, bringing partners, maybe bringing

139
00:07:35.040 --> 00:07:38.619
in uh uh NGOs or people who know about

140
00:07:38.619 --> 00:07:41.820
human rights or know about technology more. uh, AND

141
00:07:41.820 --> 00:07:44.869
then, and then having More depth to the conversation.

142
00:07:44.989 --> 00:07:47.470
So what is exactly the problem and what are

143
00:07:47.470 --> 00:07:50.230
ways to, to, to address and solve these problems?

144
00:07:50.589 --> 00:07:52.980
And then thirdly, and that is I think um

145
00:07:53.309 --> 00:07:56.510
where the, where the normative ethics and the engineering

146
00:07:56.510 --> 00:08:01.709
uh mindset, uh, yeah, go together best. It's essential

147
00:08:01.709 --> 00:08:04.790
step 3 is do something with those findings. So

148
00:08:05.109 --> 00:08:06.630
you have the issues on the table, you had

149
00:08:06.630 --> 00:08:08.910
good conversations about it within the team and outside

150
00:08:08.910 --> 00:08:11.700
the team, and then do something with it and

151
00:08:11.910 --> 00:08:14.829
typically you can You, you need also, of course,

152
00:08:14.869 --> 00:08:17.739
the commitment of project management or of the customer

153
00:08:17.739 --> 00:08:20.269
or the commissioner of the project. Like, OK, OK,

154
00:08:20.390 --> 00:08:23.130
I see this problem. I've heard the solution, let's

155
00:08:23.130 --> 00:08:25.700
just implement this and then it's always a good

156
00:08:25.700 --> 00:08:28.910
uh uh uh practice to, to make this into

157
00:08:28.910 --> 00:08:32.270
an iterative process again, like after 3 months, we'll

158
00:08:32.270 --> 00:08:34.789
meet again and then we'll evaluate how the, how

159
00:08:34.789 --> 00:08:37.308
the improvements have worked out or have not worked

160
00:08:37.308 --> 00:08:40.440
out, whether we need to adjust more. So, uh,

161
00:08:40.450 --> 00:08:44.388
identify issues, have conversations about this and act. That's

162
00:08:44.388 --> 00:08:45.948
really just the 3 steps.

163
00:08:46.729 --> 00:08:50.320
Mhm. And uh what, what do you reply to

164
00:08:50.320 --> 00:08:54.559
a question such as is technology neutral? Because this

165
00:08:54.559 --> 00:08:58.030
is very highly debated, a very highly debated topic.

166
00:08:58.200 --> 00:09:00.200
Uh, WHAT is your answer?

167
00:09:00.320 --> 00:09:02.719
Yeah, I think I quote somebody, I forgot who

168
00:09:02.719 --> 00:09:07.140
said it, but Technology is nor good nor bad,

169
00:09:07.739 --> 00:09:11.260
nor is it neutral, so you can't say the

170
00:09:11.260 --> 00:09:13.549
technology, this technology that is, is good, you can't

171
00:09:13.549 --> 00:09:15.070
say it, you can say it's bad, you can

172
00:09:15.070 --> 00:09:17.950
also say not say it's neutral. It is what

173
00:09:17.950 --> 00:09:21.469
is it then? And I I think there's things

174
00:09:21.469 --> 00:09:24.419
to be. There are various ways to approach this.

175
00:09:24.640 --> 00:09:27.200
One could say, well, it's, it always depends upon

176
00:09:27.200 --> 00:09:29.400
what people do with the technology. That's of course

177
00:09:29.400 --> 00:09:33.150
the first way of understanding this. So this technology,

178
00:09:33.280 --> 00:09:35.479
which is not good, not bad, nor neutral, but

179
00:09:35.479 --> 00:09:37.559
it can be applied to good purposes or to

180
00:09:37.559 --> 00:09:40.390
good ends or the other way around to evil

181
00:09:40.390 --> 00:09:47.340
purposes. And to complicate matters further, often technologies have

182
00:09:47.340 --> 00:09:50.750
a history. They have like a path dependency. Say

183
00:09:50.750 --> 00:09:54.690
your, your, your gasoline car, uh, can you just

184
00:09:54.690 --> 00:09:57.609
change it into an electric car? Well yes, you

185
00:09:57.609 --> 00:10:00.270
can, but then, yeah, you can only do that

186
00:10:00.270 --> 00:10:03.330
at the moment when, when your car is old

187
00:10:03.330 --> 00:10:05.250
enough that you can afford to buy. NEW one

188
00:10:05.250 --> 00:10:07.700
and then you must also take into account how

189
00:10:07.700 --> 00:10:10.119
does it fit into the situation where there are

190
00:10:10.119 --> 00:10:12.599
more gas stations and electricals, well, that's not the

191
00:10:12.599 --> 00:10:17.039
case anymore because there are many electrical charging stations

192
00:10:17.039 --> 00:10:20.159
nowadays. So this, this, this examples maybe not a

193
00:10:20.159 --> 00:10:23.599
good one anymore, but you can't also not change

194
00:10:23.599 --> 00:10:26.520
something because there's a past dependency, the choices that

195
00:10:26.520 --> 00:10:30.070
you've made in the past. Sort of restrict, not

196
00:10:30.070 --> 00:10:33.229
totally restrict, but they, yeah, they modify the choices

197
00:10:33.229 --> 00:10:36.109
that you have. So a technology can therefore, if

198
00:10:36.109 --> 00:10:39.109
it has already been used in certain applications, if

199
00:10:39.109 --> 00:10:41.719
it is already embedded in certain practices, it tends

200
00:10:41.719 --> 00:10:43.789
to do something more than it can do something

201
00:10:43.789 --> 00:10:45.859
else. So at that moment, you can't say, well,

202
00:10:45.869 --> 00:10:47.549
you can use it for, no, because you can

203
00:10:47.549 --> 00:10:50.270
only really practically use it for that purpose for

204
00:10:50.270 --> 00:10:54.109
which it, yeah, has been embedded uh in, in

205
00:10:54.109 --> 00:10:57.580
all those practices and structures already. So, Yeah, I

206
00:10:57.580 --> 00:11:00.020
think that like you're saying exactly the debate can

207
00:11:00.020 --> 00:11:02.659
go on, but it's not neutral. That is, uh,

208
00:11:02.700 --> 00:11:03.690
it's more complex.

209
00:11:04.340 --> 00:11:07.969
Mhm. And how do you look at the relationship

210
00:11:07.969 --> 00:11:10.250
between people and technology?

211
00:11:12.710 --> 00:11:18.630
Um, THAT'S a difficult question. WHAT exactly?

212
00:11:20.229 --> 00:11:20.630
Uh,

213
00:11:20.710 --> 00:11:21.390
I'm, I'm.

214
00:11:23.169 --> 00:11:24.929
I, I mean, I guess that we could put

215
00:11:24.929 --> 00:11:26.809
it in two different ways. I mean, one of

216
00:11:26.809 --> 00:11:30.760
them is since technology is created by people and

217
00:11:30.760 --> 00:11:33.609
uh the the kinds of technologies that you're interested

218
00:11:33.609 --> 00:11:37.130
in in terms of the ethics behind them, it,

219
00:11:37.210 --> 00:11:40.369
it's people who need to program them in certain

220
00:11:40.369 --> 00:11:43.260
ways to do this or do that or act

221
00:11:43.820 --> 00:11:46.539
in the ways that we want them to. Uh,

222
00:11:46.619 --> 00:11:48.900
I mean, I guess that, that, that would be

223
00:11:48.900 --> 00:11:50.780
one way of putting it out, how do you

224
00:11:50.780 --> 00:11:55.380
look at the relationship between people and technology in

225
00:11:55.380 --> 00:11:56.700
that sense, so.

226
00:11:58.169 --> 00:12:00.940
Yeah, yeah, I think I'll approach it this way.

227
00:12:01.510 --> 00:12:04.070
This is taking into account like the last 1

228
00:12:04.070 --> 00:12:06.070
or 2 years of Jet GBT and all the

229
00:12:06.070 --> 00:12:10.479
AI hype and You've talked about artificial general intelligence

230
00:12:10.479 --> 00:12:14.330
or artificial superintelligence, all these things. This may be

231
00:12:14.330 --> 00:12:16.369
a context in which I can, can say something

232
00:12:16.369 --> 00:12:20.260
that can be, can be useful, um. Here it

233
00:12:20.260 --> 00:12:22.900
occurs to me and I'm, I'm not finished thinking

234
00:12:22.900 --> 00:12:25.580
about this, but I often get the sense that

235
00:12:25.580 --> 00:12:30.780
people confuse whether technology is a tool for people

236
00:12:30.780 --> 00:12:33.059
to be used and then that that back to

237
00:12:33.059 --> 00:12:36.099
your previous question, not neutral, but bad dependent, etc.

238
00:12:37.059 --> 00:12:40.760
OR whether this technology, the AI or the ASI

239
00:12:40.760 --> 00:12:44.460
or the AGI. Whether it, it can be or

240
00:12:44.460 --> 00:12:47.169
can become an agent, uh, maybe not a moral

241
00:12:47.169 --> 00:12:49.010
agent, but then an agent in the sense that

242
00:12:49.010 --> 00:12:52.210
it acts more or less autonomously. And this really,

243
00:12:52.330 --> 00:12:54.969
this really makes me think and like I said,

244
00:12:55.010 --> 00:12:59.200
I'm not, I'm not. I'm not decided yet, but

245
00:12:59.520 --> 00:13:03.229
I don't like the idea of a machine being

246
00:13:03.229 --> 00:13:06.960
autonomous in the sense that as if it is

247
00:13:06.960 --> 00:13:09.479
totally autonomous, there will always be a dependency like

248
00:13:09.479 --> 00:13:13.000
you're saying, there's maintenance, there's programming, uh, it needs

249
00:13:13.000 --> 00:13:15.159
to change its batteries and people need to do

250
00:13:15.159 --> 00:13:17.919
that. So, but there, there will be moments or

251
00:13:17.919 --> 00:13:20.440
periods of time where it acts autonomously and at

252
00:13:20.440 --> 00:13:23.520
that moment, yeah, it, the machine, the robot can

253
00:13:23.520 --> 00:13:26.729
behave almost. As if it is like a fellow

254
00:13:26.729 --> 00:13:30.169
worker, a co-worker, maybe helper, assistant. These are, of

255
00:13:30.169 --> 00:13:32.570
course, the words that people often use. And at

256
00:13:32.570 --> 00:13:34.989
that moment, it's not really a tool anymore, not,

257
00:13:35.049 --> 00:13:37.130
not, not a tool like a shovel that you

258
00:13:37.130 --> 00:13:38.919
use to dig a hole. I mean, it's much,

259
00:13:38.929 --> 00:13:42.849
it's much more than that. Um, SO relationship between

260
00:13:42.849 --> 00:13:46.330
people and technology and what I, in this, I

261
00:13:46.330 --> 00:13:50.169
hope that uh people will, will remain like in

262
00:13:50.169 --> 00:13:55.059
charge, in control of the technology. And then also,

263
00:13:55.179 --> 00:13:57.500
but that's entirely different question that we may or

264
00:13:57.500 --> 00:13:59.619
may not go to like uh that who owns

265
00:13:59.619 --> 00:14:02.820
the technology, uh, who controls it, is it, is

266
00:14:02.820 --> 00:14:05.099
it big tech companies in the US? Is it

267
00:14:05.099 --> 00:14:08.539
big tech companies in China? Is it the European

268
00:14:08.539 --> 00:14:12.190
Union who can do something uh uh besides regulating,

269
00:14:12.340 --> 00:14:16.169
can we make technology? That works for people. I

270
00:14:16.169 --> 00:14:18.929
think that would be my uh my hope for

271
00:14:18.929 --> 00:14:21.530
this relationship, that technology is there for people and

272
00:14:21.530 --> 00:14:22.690
not the other way around.

273
00:14:23.549 --> 00:14:26.190
Mhm. And I guess that is also at least

274
00:14:26.190 --> 00:14:29.789
to some extent raises the question of if technology

275
00:14:29.789 --> 00:14:33.429
behaves in ways that we find to be bad,

276
00:14:33.679 --> 00:14:36.590
who is to be held responsible, right?

277
00:14:38.820 --> 00:14:41.919
Yeah, and now you're, maybe you're using or moving

278
00:14:41.919 --> 00:14:44.840
towards the, the vocabulary of uh of of law

279
00:14:44.840 --> 00:14:49.880
of uh uh accountable, responsible, liable. Oh, that's a

280
00:14:49.880 --> 00:14:54.979
whole different uh uh field of, uh, yeah. Uh,

281
00:14:55.049 --> 00:14:58.979
LEGAL personality of those robots, yes or no, and

282
00:14:58.979 --> 00:15:02.650
etc. I'm not an expert on that, so, uh,

283
00:15:02.700 --> 00:15:04.690
we should park that topic.

284
00:15:05.260 --> 00:15:09.090
OK. So, and in the book you also question

285
00:15:09.090 --> 00:15:11.859
what is value at a certain point. So what

286
00:15:11.859 --> 00:15:17.210
is value actually and what is technology specifically for?

287
00:15:17.340 --> 00:15:19.780
What do we expect to get from technology?

288
00:15:20.049 --> 00:15:22.599
Yeah, that's a nice question. So, while writing the

289
00:15:22.599 --> 00:15:25.080
book, uh, it occurred to me, uh, that people

290
00:15:25.080 --> 00:15:28.320
use value in various ways, and some people will

291
00:15:28.320 --> 00:15:32.159
associate value with value creation and associate that with,

292
00:15:32.169 --> 00:15:36.679
uh, market share and profits and, and, uh, like

293
00:15:36.679 --> 00:15:40.080
the financial or economic side of it. Uh, THERE'S

294
00:15:40.080 --> 00:15:42.000
something to be said for that, but you can

295
00:15:42.000 --> 00:15:45.200
also understand the value in a sense of uh

296
00:15:45.200 --> 00:15:47.760
what value does it have for society or for

297
00:15:47.760 --> 00:15:52.940
the public goods. And so that's a whole a

298
00:15:52.940 --> 00:15:57.849
whole different vocabulary where it's more about um. Uh,

299
00:15:57.890 --> 00:16:03.849
JUST this, um. Democracy, even it can have or

300
00:16:03.849 --> 00:16:10.169
undermine a democracy. Um, So I just think it

301
00:16:10.169 --> 00:16:13.729
is important when people use words like value, let's

302
00:16:13.729 --> 00:16:15.739
create value. Is it valuable or not, that they,

303
00:16:15.849 --> 00:16:18.530
that they make explicit. So what are we talking

304
00:16:18.530 --> 00:16:20.609
about? Are we talking about the financial side, economic

305
00:16:20.609 --> 00:16:23.669
side of it? Are we talking about societal, the,

306
00:16:23.690 --> 00:16:28.380
the, yeah, the public good, uh. Or the, or

307
00:16:28.380 --> 00:16:31.690
the, or the well-being uh side of, of value.

308
00:16:31.780 --> 00:16:35.900
And yeah, in that sense, both are, are, are

309
00:16:35.900 --> 00:16:38.179
options. I just think it's, it needs to be

310
00:16:38.179 --> 00:16:42.830
clear, uh. What people are discussing.

311
00:16:44.080 --> 00:16:47.359
So it's not so much that one meaning of

312
00:16:47.359 --> 00:16:50.520
value from the ones you mentioned there would be

313
00:16:50.520 --> 00:16:55.119
better than the others in designing technology, but what's

314
00:16:55.119 --> 00:16:58.320
more important is for people to agree on what

315
00:16:58.320 --> 00:16:59.030
they want.

316
00:16:59.479 --> 00:17:03.520
Yeah, that's maybe that's maybe a sort of Uh,

317
00:17:03.609 --> 00:17:06.930
diplomatic answer and indeed I gave that answer, but

318
00:17:06.930 --> 00:17:09.520
then if you, if, if, if we associated with,

319
00:17:09.569 --> 00:17:11.560
with what I said earlier on the humanistic approach

320
00:17:11.560 --> 00:17:15.290
and uh Uh, the serving people rather than the

321
00:17:15.290 --> 00:17:17.729
other way around. I think I can say that

322
00:17:17.729 --> 00:17:20.719
I, I would prioritize when you're talking to me.

323
00:17:20.890 --> 00:17:23.319
I would, I would just find it more interesting,

324
00:17:23.689 --> 00:17:25.569
uh, to, to understand value in the, in those,

325
00:17:25.689 --> 00:17:27.979
in those second, uh, way like, uh, what does

326
00:17:27.979 --> 00:17:31.569
it add to society, to democracy, to fairness, to

327
00:17:31.569 --> 00:17:35.079
people's daily lives, their well-being, uh, and I know

328
00:17:35.079 --> 00:17:38.569
that companies must make a dose of profit. There

329
00:17:38.569 --> 00:17:41.959
must be a level of profitability. Uh, FOR it

330
00:17:41.959 --> 00:17:45.040
to, uh, maintain its, uh, to, to continue to

331
00:17:45.040 --> 00:17:48.439
maintain its services and products. But it does not

332
00:17:48.439 --> 00:17:50.260
need to be, and this is maybe, uh, uh,

333
00:17:50.280 --> 00:17:53.959
a political orientation of mine, uh, to that extent

334
00:17:53.959 --> 00:17:57.910
where, uh, the big tech companies are just immensely

335
00:17:58.089 --> 00:18:01.280
incomprehensibly, uh, I mean, how many, how many zeros

336
00:18:01.280 --> 00:18:05.770
are there in those. Market cap valuations, there's like

337
00:18:05.770 --> 00:18:09.849
a billion, trillion, I mean, there's these are. Yeah.

338
00:18:10.699 --> 00:18:14.180
Hard to comprehend, so yeah, value for me means

339
00:18:14.180 --> 00:18:15.930
more like uh the other way.

340
00:18:16.810 --> 00:18:20.369
Mhm. So, in the book, you get into 3

341
00:18:20.369 --> 00:18:24.689
topics that you say regularly featuring discussions about ethics

342
00:18:24.689 --> 00:18:28.170
and technology or the ethics of technology. Let's go

343
00:18:28.170 --> 00:18:30.969
through them all. So, the first one is the

344
00:18:30.969 --> 00:18:34.530
trolley problem. So, would you like to introduce what

345
00:18:34.530 --> 00:18:39.109
the trolley problem is and then why it's important

346
00:18:39.109 --> 00:18:40.280
in these discussions?

347
00:18:40.770 --> 00:18:45.280
Yeah, so it is a, it is from 1967.

348
00:18:45.439 --> 00:18:49.119
Philippa Fo introduced the trolley problem. It was not

349
00:18:49.119 --> 00:18:51.640
called the trolley problem. It later became known as

350
00:18:51.640 --> 00:18:53.880
the trolley problem, but it goes like this. You're

351
00:18:53.880 --> 00:18:56.839
standing near a railroad or a trolley or a

352
00:18:56.839 --> 00:19:00.359
train, but that's, yeah, a trolley. It was, of

353
00:19:00.359 --> 00:19:07.849
course, and It goes Uh, if it continues to

354
00:19:07.849 --> 00:19:10.770
run, it will run off of 4 people. However,

355
00:19:10.849 --> 00:19:13.650
there's also a switch in the track that you

356
00:19:13.650 --> 00:19:16.699
can control, and if you apply that switch in

357
00:19:16.699 --> 00:19:20.410
the track, it goes to an alternative track and

358
00:19:20.410 --> 00:19:24.760
there it will kill one person. Mhm. So then

359
00:19:24.760 --> 00:19:28.160
the question is, what would you do? Well, utilitarianism

360
00:19:28.160 --> 00:19:31.760
or consequentialism would say, well, uh, one death is

361
00:19:31.760 --> 00:19:35.280
less evil than 4 deaths, so must apply the

362
00:19:35.280 --> 00:19:38.280
switch so that it only kills one. And then

363
00:19:38.280 --> 00:19:41.150
they make it more complex. They say, well, there's

364
00:19:41.150 --> 00:19:44.349
also the same situation, but there's not a switch,

365
00:19:44.430 --> 00:19:48.459
but there's a bridge over the railroad, the track,

366
00:19:48.750 --> 00:19:51.949
and there's a person who is like a heavy

367
00:19:51.949 --> 00:19:54.880
body weight. So if you throw him down from

368
00:19:54.880 --> 00:19:58.140
the bridge onto the track, it will also he,

369
00:19:58.189 --> 00:20:01.270
the person will also stop the train. But that

370
00:20:01.270 --> 00:20:04.290
of course, feels much more like murdering because you're

371
00:20:04.290 --> 00:20:07.170
pushing, whereas the switching of the track, the first

372
00:20:07.170 --> 00:20:09.689
scenario is, is like the train killing it and

373
00:20:09.689 --> 00:20:12.609
you, uh, yeah, you're not really killing. So all

374
00:20:12.609 --> 00:20:16.569
of that is psychology, it's philosophy. People know this

375
00:20:16.569 --> 00:20:20.050
example very often when people talk about ethics and

376
00:20:20.050 --> 00:20:22.810
machines, machine ethics and programming the ethics in the

377
00:20:22.810 --> 00:20:24.890
machine, what we had at the beginning of our

378
00:20:24.890 --> 00:20:27.489
conversation, people will say, Well, the trolley problem, and

379
00:20:27.489 --> 00:20:29.930
I just got a little bit tired of that

380
00:20:29.930 --> 00:20:36.689
problem because it, it assumes. That these questions can

381
00:20:36.689 --> 00:20:41.050
be solved mathematically by saying 1 is less than

382
00:20:41.050 --> 00:20:44.199
4, so that option is better, and it is,

383
00:20:44.329 --> 00:20:46.569
it is not taking into account so much. Well,

384
00:20:46.609 --> 00:20:48.760
it does take into account the psychology of it,

385
00:20:49.010 --> 00:20:51.449
but there's also human rights, there's the ontology, there's

386
00:20:51.449 --> 00:20:53.810
relational ethics, there's virtue ethics. So all of these

387
00:20:53.810 --> 00:20:56.089
other types of ethics that will go into, I

388
00:20:56.089 --> 00:20:58.614
guess. Later on, are out of the picture and

389
00:20:58.614 --> 00:21:01.805
it's just simplifying all of it and even more,

390
00:21:01.935 --> 00:21:03.415
if I look at that thought, I know it's

391
00:21:03.415 --> 00:21:05.535
only a thought experiment, but if people treat it

392
00:21:05.535 --> 00:21:07.844
not as a thought experiment, but as a realistic

393
00:21:07.844 --> 00:21:10.094
situation that you can actually solve through computation and

394
00:21:10.094 --> 00:21:14.369
calculation, just do the numbers. Then they're missing out

395
00:21:14.369 --> 00:21:16.819
the picture and I have a background in industrial

396
00:21:16.819 --> 00:21:19.180
design engineering, so I'm a practical person. I can

397
00:21:19.180 --> 00:21:21.780
also think very much and easily in in solutions,

398
00:21:21.819 --> 00:21:24.030
alternatives, creativity, all of that. So I would say,

399
00:21:24.040 --> 00:21:27.329
well, can you, can you, can you also do

400
00:21:27.329 --> 00:21:30.579
another analysis where you ask why did the brakes

401
00:21:30.579 --> 00:21:34.569
malfunction and how is maintenance of the train organized?

402
00:21:34.859 --> 00:21:37.180
And is it possible to do something else? Can

403
00:21:37.180 --> 00:21:39.939
you just yell at the people on the track?

404
00:21:40.510 --> 00:21:43.010
Uh, LIKE, hey, watch out, OK. There are there

405
00:21:43.010 --> 00:21:46.000
are like 1000 other things that you can ask

406
00:21:46.290 --> 00:21:48.729
on the problem definition site or that you can

407
00:21:48.729 --> 00:21:53.609
explore on the solution finding site. Around that very

408
00:21:53.609 --> 00:21:57.329
much too simplistic thought experiment and um yeah, I

409
00:21:57.329 --> 00:21:59.949
just like to draw in all of that complexity

410
00:21:59.949 --> 00:22:02.290
and say, well, the trolley problem is, is what

411
00:22:02.290 --> 00:22:04.930
it is. It's a thought experiment, but you must

412
00:22:04.930 --> 00:22:07.969
not use it as a realistic situation as if

413
00:22:07.969 --> 00:22:10.290
the computer can solve it because reality is more

414
00:22:10.290 --> 00:22:13.609
difficult and you must treat reality as difficult and

415
00:22:13.609 --> 00:22:15.800
as complex as it is. And later on I

416
00:22:15.800 --> 00:22:17.479
published something that maybe if you have shown notes

417
00:22:17.479 --> 00:22:20.599
I can put it in where I combine this

418
00:22:20.599 --> 00:22:25.579
more explicitly uh with systems thinking, the need to

419
00:22:25.579 --> 00:22:28.589
understand a problem and a situation in its complexity

420
00:22:29.359 --> 00:22:31.040
and then uh address it.

421
00:22:32.069 --> 00:22:35.819
Mhm. But then, do you think that the really

422
00:22:35.819 --> 00:22:40.219
problem with uh all of those caveats and perhaps

423
00:22:40.219 --> 00:22:43.420
adding some of those other considerations that you mentioned

424
00:22:43.420 --> 00:22:47.219
there is still a useful thought experiment in the

425
00:22:47.219 --> 00:22:48.939
ethics of technology or

426
00:22:48.939 --> 00:22:51.619
not? Yes, yes, it. IT is, it can be

427
00:22:51.619 --> 00:22:54.260
used, so we'll later go on into like 4

428
00:22:54.260 --> 00:22:56.619
ethical perspectives that I used that that that are

429
00:22:56.619 --> 00:23:00.209
that often people in in ethics of technology use.

430
00:23:00.380 --> 00:23:03.660
And one of them is this consequentialism where you

431
00:23:03.660 --> 00:23:06.260
look at the positives and the negative consequences, you

432
00:23:06.260 --> 00:23:09.780
compare them, you evaluate them, you choose. For that

433
00:23:09.780 --> 00:23:13.380
perspective, it makes sense to understand the basics of

434
00:23:13.380 --> 00:23:16.530
the trolley problem. Like sometimes you must choose between

435
00:23:16.530 --> 00:23:19.920
something that's bad and something that's less bad. And

436
00:23:19.920 --> 00:23:21.880
then if you take into account the agency that

437
00:23:21.880 --> 00:23:23.959
you have, and if you take into account the

438
00:23:23.959 --> 00:23:27.119
moral implications of that agency, like switching is different

439
00:23:27.119 --> 00:23:30.560
from pushing the, the, the, the, the, the, it's

440
00:23:30.560 --> 00:23:32.170
called the fat man, but I think it's ugly

441
00:23:32.170 --> 00:23:34.479
title, the, the, the, the larger person from the

442
00:23:34.479 --> 00:23:38.270
bridge. Um If you take that into account, I

443
00:23:38.270 --> 00:23:40.229
mean, that, that, that's what the thought experiment can

444
00:23:40.229 --> 00:23:43.060
help you with. Mhm. But not, but not more

445
00:23:43.060 --> 00:23:43.689
than that.

446
00:23:44.339 --> 00:23:47.930
OK, so the second topic that you mentioned in

447
00:23:47.930 --> 00:23:50.729
the book has been regularly featured in these kinds

448
00:23:50.729 --> 00:23:54.099
of discussions has to do with privacy. So, in

449
00:23:54.099 --> 00:23:58.060
the realm of technology, what is privacy, what does

450
00:23:58.060 --> 00:24:01.060
it entail, and what are perhaps some of the

451
00:24:01.310 --> 00:24:04.859
biggest questions we have to tackle in this domain?

452
00:24:05.829 --> 00:24:09.729
Yeah, so, uh, uh, another topic like you're saying

453
00:24:10.060 --> 00:24:13.890
that often pops up in conversations about ethics and

454
00:24:13.890 --> 00:24:17.609
technology, ethics and AI is privacy that has been

455
00:24:17.609 --> 00:24:22.180
a topic since at least since Cambridge Analytica or

456
00:24:22.180 --> 00:24:27.420
Facebook, uh, in, in an unfair way, barely legal

457
00:24:27.420 --> 00:24:31.500
way, illegal way, uh, scraped all those data from

458
00:24:31.500 --> 00:24:34.140
all those Facebook friends of friends and then used

459
00:24:34.140 --> 00:24:38.270
it to, uh. Uh, WEAPONIZED the advertisements on Facebook

460
00:24:38.270 --> 00:24:41.390
and then skewed the presidential elections in the US

461
00:24:41.390 --> 00:24:43.630
at that time and the Brexit vote in the

462
00:24:43.630 --> 00:24:45.390
UK around the same time. So all of that.

463
00:24:45.880 --> 00:24:49.609
That has drawn enormous attention to privacy, but privacy

464
00:24:49.609 --> 00:24:53.920
then means who owns the personal data and who

465
00:24:53.920 --> 00:24:57.060
can just or can or cannot just scrape it

466
00:24:57.060 --> 00:25:00.180
and use it for other purposes. So it's an

467
00:25:00.180 --> 00:25:04.209
interesting topic, but then people, uh, sometimes if you

468
00:25:04.209 --> 00:25:07.810
combine it with the context of a company who

469
00:25:07.810 --> 00:25:10.609
uh develops or deploys technology, they will have a

470
00:25:10.609 --> 00:25:15.739
legal officer and they will say privacy. And then

471
00:25:15.739 --> 00:25:18.660
nothing else as if ethics is only privacy or

472
00:25:18.660 --> 00:25:21.780
they'll say privacy compliance to the GDPR, the General

473
00:25:21.780 --> 00:25:25.300
Data Protection Act of the, of the EU, uh.

474
00:25:26.290 --> 00:25:30.329
As if, if you do the compliance of privacy

475
00:25:30.329 --> 00:25:32.910
in terms of GDPR, then you're all good, whereas

476
00:25:32.910 --> 00:25:35.400
privacy can be much more and even more so,

477
00:25:35.609 --> 00:25:38.640
there's more topics than only privacy, as I mentioned

478
00:25:38.640 --> 00:25:41.170
before, that's, that's really a key topic for me.

479
00:25:41.209 --> 00:25:44.969
There's, there's, there's fairness, justice, and non-discrimination bias, all

480
00:25:44.969 --> 00:25:48.680
of that. There's transparency, accountability, responsibility, all of that,

481
00:25:49.250 --> 00:25:53.359
um. Uh, YEAH, privacy is only one thing and

482
00:25:53.359 --> 00:25:56.000
then this narrow legal interpretation of privacy is yet

483
00:25:56.000 --> 00:25:59.479
only one other thing. And if people want to

484
00:25:59.479 --> 00:26:02.079
know more about this, Karissa Felli wrote a great

485
00:26:02.079 --> 00:26:04.920
book on it. Privacy is Power. Well, it's already

486
00:26:04.920 --> 00:26:09.430
in the title where she sets up this argument

487
00:26:09.430 --> 00:26:14.560
that Who owns the data has power. And then

488
00:26:14.609 --> 00:26:17.489
uh you can, uh, a big company can then

489
00:26:17.489 --> 00:26:20.650
even transform its economic power into political power. We've

490
00:26:20.650 --> 00:26:24.130
seen that recently in uh the activities of Elon

491
00:26:24.130 --> 00:26:26.609
Musk in the presidential elections in the US where

492
00:26:26.609 --> 00:26:30.130
like economic power sort of equals financial power, sort

493
00:26:30.130 --> 00:26:33.689
of equals political power. And well, in, in Elon

494
00:26:33.689 --> 00:26:38.500
Musk's case, that is not necessarily directly. Tied to

495
00:26:38.500 --> 00:26:40.699
this privacy thing, but there are many other big

496
00:26:40.699 --> 00:26:42.979
tech companies who do this privacy thing, all of

497
00:26:42.979 --> 00:26:45.900
those businesses who, who thrive on ad revenues, of

498
00:26:45.900 --> 00:26:49.979
course, like Google and Facebook or Me I should

499
00:26:49.979 --> 00:26:50.530
say, of course.

500
00:26:51.140 --> 00:26:54.530
Right. So the first topic has to do with

501
00:26:54.530 --> 00:26:59.130
responsibility. So how does responsibility apply to the ethics

502
00:26:59.130 --> 00:27:00.410
of technology.

503
00:27:01.250 --> 00:27:04.969
That's really the third term that, that, that often

504
00:27:04.969 --> 00:27:07.089
pops up even in, in, in, in, in, in

505
00:27:07.089 --> 00:27:10.670
vocabulary like responsible innovation, uh, I call my area

506
00:27:10.670 --> 00:27:13.569
of work responsible innovation and now people say, well,

507
00:27:13.729 --> 00:27:15.890
can we do it responsibly? We can do this

508
00:27:15.890 --> 00:27:19.875
responsible. RESPONSIBLE AI. So this word just pops up

509
00:27:20.175 --> 00:27:21.574
and I was just looking for what does it

510
00:27:21.574 --> 00:27:24.334
mean? And then I found in the literature of

511
00:27:24.334 --> 00:27:28.295
ethics of technology, uh, it, it is often understood

512
00:27:28.295 --> 00:27:32.415
as having two components like a knowledge component and

513
00:27:32.415 --> 00:27:34.935
an action or control component. And what it means

514
00:27:34.935 --> 00:27:39.665
is a person, imagine a developer and a team

515
00:27:39.665 --> 00:27:42.535
working on this algorithm for fraud detection. He or

516
00:27:42.535 --> 00:27:46.439
she. Can be responsible, but can only be responsible

517
00:27:46.439 --> 00:27:48.880
to that level or to that extent or to

518
00:27:48.880 --> 00:27:51.640
that quality that they know about the, the, the

519
00:27:51.640 --> 00:27:55.599
knowledge component. So if they know about the current

520
00:27:55.599 --> 00:27:59.040
situation, uh, how bad is the problem they're trying

521
00:27:59.040 --> 00:28:02.760
to solve if they know about possible future situations

522
00:28:02.760 --> 00:28:05.239
like how good is the algorithm? Can they really

523
00:28:05.239 --> 00:28:08.709
improve the situation? So, yeah. Knowledge about it. If

524
00:28:08.709 --> 00:28:10.510
they don't have the knowledge, they cannot be really

525
00:28:10.510 --> 00:28:13.310
held responsible or and even more so not not

526
00:28:13.310 --> 00:28:16.020
accountable for it. The other, but if they know,

527
00:28:16.189 --> 00:28:19.020
then, then the responsibility sort of grows with it.

528
00:28:19.109 --> 00:28:21.229
So the more knowledge you have, the more responsible

529
00:28:21.229 --> 00:28:23.589
you are. The same for the, for the control

530
00:28:23.589 --> 00:28:27.569
or the Uh, what's that the word control or

531
00:28:27.569 --> 00:28:29.949
action? What you do it? I'm forgetting it. But

532
00:28:29.949 --> 00:28:32.750
anyways, the amount or type of control that this

533
00:28:32.750 --> 00:28:36.219
person has over the situation, if they can influence

534
00:28:36.219 --> 00:28:38.510
the project a lot, maybe because they're a senior

535
00:28:38.510 --> 00:28:41.349
developer, maybe they're the project manager, that means that

536
00:28:41.349 --> 00:28:44.910
their responsibility also grows proportionally to that. Whereas if

537
00:28:44.910 --> 00:28:48.189
they're only, and some people are only a smaller

538
00:28:48.189 --> 00:28:50.619
player in the project team, they may be junior,

539
00:28:50.910 --> 00:28:54.189
maybe the budget is not too big. So then,

540
00:28:54.819 --> 00:28:57.369
And this may be harder to to to claim,

541
00:28:57.400 --> 00:29:01.819
but still it sounds reasonable. That they are can

542
00:29:01.819 --> 00:29:05.500
then be held less responsible for it. But now

543
00:29:05.500 --> 00:29:07.660
that I'm saying it, it sounds like you can

544
00:29:07.660 --> 00:29:11.349
buy your uh uh get out of jail card

545
00:29:11.349 --> 00:29:13.900
very easily by just claiming, but I'm only a

546
00:29:13.900 --> 00:29:17.020
small player. That's not, it's, it's really meant the

547
00:29:17.020 --> 00:29:21.339
other way around, um. That uh you can take

548
00:29:21.339 --> 00:29:24.660
you, there's an invitation to, to grab more control

549
00:29:24.660 --> 00:29:27.099
of the situation that you would normally do, which

550
00:29:27.099 --> 00:29:29.780
can require courage like raising your hand, asking a

551
00:29:29.780 --> 00:29:32.540
question, because that, that goes hand in hand with

552
00:29:32.540 --> 00:29:35.020
being responsible. And then in the book, I have

553
00:29:35.020 --> 00:29:38.420
a drawing um that was uh I took it,

554
00:29:38.459 --> 00:29:42.030
I think from um What's his name? You can

555
00:29:42.030 --> 00:29:43.910
look it up in the book, of course. It's

556
00:29:43.910 --> 00:29:46.500
not my metaphor, but I looked it up, uh,

557
00:29:46.589 --> 00:29:50.250
climbers, rock climbers, they, they do it where they,

558
00:29:50.430 --> 00:29:54.810
where they go up climbing, um. Uh, AND this

559
00:29:54.810 --> 00:29:58.689
is also. They go from knowledge to control to

560
00:29:58.689 --> 00:30:01.400
knowledge to control, so it's an invitation for people

561
00:30:01.400 --> 00:30:05.689
to, to improve their responsibility first by raising their

562
00:30:05.689 --> 00:30:09.449
knowledge about the situation, secondly, by uh trying to

563
00:30:09.449 --> 00:30:12.760
get more agency, raising their hands, applying courage, uh,

564
00:30:13.329 --> 00:30:15.369
and then, and then that will give them more

565
00:30:15.369 --> 00:30:17.329
knowledge and that will give them more control. So

566
00:30:17.329 --> 00:30:21.339
it's an invitation to To be more responsible in

567
00:30:21.339 --> 00:30:24.140
that sense, it's certainly not what I just said

568
00:30:24.140 --> 00:30:26.699
a couple of minutes ago, uh uh invitation to

569
00:30:26.699 --> 00:30:28.660
claim the other way around. I know a little.

570
00:30:28.939 --> 00:30:30.819
I'm just a small player. I can't be held

571
00:30:30.819 --> 00:30:33.640
responsible. It's really the other way around. Mhm.

572
00:30:35.020 --> 00:30:37.500
So, let's get then into the 4 different ethical

573
00:30:37.500 --> 00:30:40.339
perspectives that you present in the book. I mean,

574
00:30:40.459 --> 00:30:44.260
you talk about consequentialism, the ontology, relational ethics, and

575
00:30:44.260 --> 00:30:47.010
virtue ethics. Uh, TELL us about each of them.

576
00:30:47.219 --> 00:30:48.939
Let's start with consequentialism.

577
00:30:49.780 --> 00:30:54.650
Yes, thanks. So, um, The first two are relatively

578
00:30:54.650 --> 00:30:58.489
easily. I found out this 3rd and 4th 1

579
00:30:58.489 --> 00:31:03.930
are more innovative for some people's expectations. The context

580
00:31:03.930 --> 00:31:06.530
in which I use this is I have developed

581
00:31:06.530 --> 00:31:09.410
a workshop format, rapid ethical deliberation. I call it

582
00:31:09.410 --> 00:31:12.130
rapid because it just sounds like it doesn't take

583
00:31:12.130 --> 00:31:14.010
ages. That's correct. You can do it in 2

584
00:31:14.010 --> 00:31:16.410
hours. Of course, after 2 hours, you don't know

585
00:31:16.410 --> 00:31:18.650
everything. You are invited to do it again and

586
00:31:18.650 --> 00:31:22.680
again, it's an iterative process again. Um. And I

587
00:31:22.680 --> 00:31:26.359
found that uh this, this first perspective, consequentialism is

588
00:31:26.359 --> 00:31:30.239
relatively easy for people to, to get into. It's

589
00:31:30.239 --> 00:31:34.160
the mindset of uh imagining the product that you're

590
00:31:34.160 --> 00:31:36.920
working on, it being out in the world, it

591
00:31:36.920 --> 00:31:41.920
having effects, it's having consequences, and then relatively simply

592
00:31:42.239 --> 00:31:44.349
making a list of the good things that can

593
00:31:44.349 --> 00:31:47.199
happen and the bad things that can happen. Uh,

594
00:31:47.239 --> 00:31:50.849
IF you do this. Option A and for option

595
00:31:50.849 --> 00:31:54.050
B, uh, which can also be not doing option

596
00:31:54.050 --> 00:31:55.709
A, but let's say you have A and B

597
00:31:55.709 --> 00:31:59.040
and therefore B also the making the, the pluses

598
00:31:59.209 --> 00:32:01.930
and the minuses. And then out of it will

599
00:32:01.930 --> 00:32:07.369
come uh often uh Yeah, a direction like B

600
00:32:07.369 --> 00:32:11.640
is better because it has less or, or smaller

601
00:32:11.640 --> 00:32:17.390
disadvantages or it has more or larger advantages. So

602
00:32:17.640 --> 00:32:20.680
that appeals very much to anybody with a technology

603
00:32:20.680 --> 00:32:23.359
background because it's, it's, it's, it's very comparable to

604
00:32:23.359 --> 00:32:25.479
an engineering mindset where you have two options. You

605
00:32:25.479 --> 00:32:27.160
just look at how good are they? Well, that

606
00:32:27.160 --> 00:32:29.079
one is better, so we do it. So that

607
00:32:29.079 --> 00:32:33.910
first one is uh consequential relatively straightforward. It has

608
00:32:33.910 --> 00:32:38.550
its drawbacks. Uh, YOU can't compare apples and pears,

609
00:32:38.709 --> 00:32:44.069
so, uh. Um, The plus here is for that

610
00:32:44.069 --> 00:32:46.949
group, but the minus is for another group. Say

611
00:32:46.949 --> 00:32:49.750
you have your self-driving car, it's a good thing

612
00:32:49.750 --> 00:32:51.349
that you don't have to drive. It saves you

613
00:32:51.349 --> 00:32:54.229
time and energy. The bad thing is that maybe

614
00:32:54.229 --> 00:32:56.910
it will be a threat to pedestrians or people

615
00:32:56.910 --> 00:32:59.790
on bikes if they, etc. So it puts the

616
00:32:59.790 --> 00:33:02.270
pluses for the car owners. It puts the minuses

617
00:33:02.270 --> 00:33:07.780
for the non-car owners. And consequentialism does not have

618
00:33:07.780 --> 00:33:10.209
a really easy way to solve that. So that's,

619
00:33:10.420 --> 00:33:12.229
that's where you go to the second perspective, the

620
00:33:12.229 --> 00:33:16.380
ontology or duty ethics. It puts center stages, the

621
00:33:16.380 --> 00:33:18.859
duties that people have and the rights that people

622
00:33:18.859 --> 00:33:22.739
have, and typically schematically, the duties will lie with

623
00:33:22.739 --> 00:33:26.530
the people involved in developing the deployment of technology,

624
00:33:27.050 --> 00:33:28.859
the people who make the algorithm, the people who

625
00:33:28.859 --> 00:33:32.619
deploy the algorithm or the electric car. And the

626
00:33:32.619 --> 00:33:34.900
rights are often on the side of the people

627
00:33:34.900 --> 00:33:39.099
uh implied suffering the consequences on the receiving end

628
00:33:39.099 --> 00:33:41.099
of it, and they will have fundamental rights, human

629
00:33:41.099 --> 00:33:44.939
rights, and that ethical perspective gives you a much

630
00:33:44.939 --> 00:33:48.020
more principled outlook like, no, no, no, I don't

631
00:33:48.020 --> 00:33:50.619
care so much about the pluses and minuses. There's

632
00:33:50.619 --> 00:33:54.050
just this fundamental rights and you can't violate that

633
00:33:54.300 --> 00:33:59.369
no matter how many pluses for that actor. In

634
00:33:59.369 --> 00:34:02.369
financial sense that are there because in the vocabulary

635
00:34:02.369 --> 00:34:04.849
of duty ethics you're not looking at plus minuses

636
00:34:04.849 --> 00:34:07.689
there something else you're looking at people's rights and

637
00:34:07.689 --> 00:34:11.668
they, and they will trump other things. So that's

638
00:34:11.668 --> 00:34:15.080
um that's that perspective appeals very much to people

639
00:34:15.080 --> 00:34:17.290
with a, with a legal background because of course

640
00:34:17.290 --> 00:34:20.438
they, they know about compliance, which is an obligation

641
00:34:20.438 --> 00:34:22.580
and know about human rights, which is the right

642
00:34:22.580 --> 00:34:26.590
side. So those, those two perspectives are relatively easy

643
00:34:26.590 --> 00:34:28.679
for people and then I go to the third

644
00:34:28.679 --> 00:34:33.030
one, which is relational ethics. Um, AND in the

645
00:34:33.030 --> 00:34:34.989
book, I explained that I use it as an

646
00:34:34.989 --> 00:34:39.510
umbrella term, also ethics of care or care ethics

647
00:34:39.510 --> 00:34:43.110
of feminist ethics. Uh, WHAT all of it comes

648
00:34:43.110 --> 00:34:45.629
down together is often when I use it in

649
00:34:45.629 --> 00:34:48.270
the workshop, I'm, I'm using like two questions. First

650
00:34:48.270 --> 00:34:51.639
question is Often easy for people to get into

651
00:34:51.639 --> 00:34:54.958
like, suppose this algorithm is implemented in the world

652
00:34:54.958 --> 00:34:57.120
and it is being used in practice. So again,

653
00:34:57.159 --> 00:35:03.379
this fraud detection algorithm. Um. How will it change

654
00:35:03.379 --> 00:35:06.139
the way that people engage with each other, and

655
00:35:06.139 --> 00:35:08.139
look into each other in the eyes, or cannot

656
00:35:08.139 --> 00:35:11.729
look each other in the eyes? Will it, will

657
00:35:11.729 --> 00:35:14.699
the, the, the tax inspector use the algorithm? Will

658
00:35:14.699 --> 00:35:18.090
they, will they view the person as a number

659
00:35:18.090 --> 00:35:20.590
or as a person with a, well, of, of,

660
00:35:20.699 --> 00:35:22.870
of, of flesh and blood so to say. So

661
00:35:22.870 --> 00:35:27.500
this, this, this looks at The way that technology

662
00:35:27.919 --> 00:35:31.879
can be like very machine like or the other

663
00:35:31.879 --> 00:35:34.830
way around if you take relational ethics more seriously,

664
00:35:35.040 --> 00:35:37.419
where technology stays a little bit more to the

665
00:35:37.419 --> 00:35:41.360
background and there's room enough for person to person

666
00:35:41.360 --> 00:35:47.320
engagement and uh uh yeah, discretion, uh professional discretion

667
00:35:47.320 --> 00:35:50.120
like I know that the algorithm says this, it

668
00:35:50.120 --> 00:35:52.949
has a flag for your name. But I'm looking

669
00:35:52.949 --> 00:35:57.989
at you and I'm using my human senses. So

670
00:35:58.000 --> 00:36:00.239
this is again ties back to what we said

671
00:36:00.239 --> 00:36:02.449
earlier like then the technology is only a tool.

672
00:36:03.310 --> 00:36:06.030
Uh, FOR me, but I can, if I'm the

673
00:36:06.030 --> 00:36:08.419
tax inspector using the fraud to text and algorithm,

674
00:36:09.229 --> 00:36:11.739
I pay more attention to my, to my human

675
00:36:11.739 --> 00:36:15.550
sensitivity and capabilities to do discretion. Uh, SO that,

676
00:36:15.610 --> 00:36:17.780
that's one question in relation to ethics. Second one

677
00:36:17.780 --> 00:36:20.510
is, uh, that's more abstract, but people also get

678
00:36:20.510 --> 00:36:25.780
it easily, um. But it's abstract it goes up

679
00:36:25.780 --> 00:36:28.389
a couple of levels, it looks at power. So

680
00:36:28.389 --> 00:36:31.290
if the system is there, what happens to power?

681
00:36:31.429 --> 00:36:34.270
Do the bureaucrats become more powerful or do the

682
00:36:34.270 --> 00:36:38.149
citizens become more powerful? And in that sense, that's

683
00:36:38.149 --> 00:36:40.709
why I mentioned it, it has uh uh connections

684
00:36:40.709 --> 00:36:44.840
to feminist uh critique, uh, so it's feminist ethics

685
00:36:45.110 --> 00:36:48.229
in that sense. And lastly for relational ethics, it

686
00:36:48.229 --> 00:36:53.070
looks at um At care and justice, and justice

687
00:36:53.070 --> 00:36:54.639
is the thing that's very much in this duty

688
00:36:54.639 --> 00:36:58.520
ethics like. Uh, RIGHTS and human rights are very

689
00:36:58.520 --> 00:37:01.360
much associated with justice, but there's also care and

690
00:37:01.360 --> 00:37:04.879
then relational ethics will say care cannot do without

691
00:37:04.879 --> 00:37:07.840
justice, just like justice cannot do without care. So

692
00:37:07.840 --> 00:37:13.510
it's drawing attention to those dimensions. Lastly, yeah, virtual

693
00:37:13.510 --> 00:37:15.459
ethics, yeah, yeah, yeah, yeah, I, I know, I

694
00:37:15.459 --> 00:37:17.620
know, but, uh, I, I am treating this with

695
00:37:17.620 --> 00:37:19.909
some depth because I consider it really the, the

696
00:37:19.909 --> 00:37:22.300
heart of the book like what are these ethical

697
00:37:22.300 --> 00:37:24.780
perspectives? How can you make them practical? So fourth

698
00:37:24.780 --> 00:37:28.229
one is, um, virtual. Ethics. Uh, I'm very much

699
00:37:28.229 --> 00:37:31.479
inspired, informed by Shannon Veler's work, uh, the book

700
00:37:31.479 --> 00:37:34.199
of hers of 2016, I think it was technology

701
00:37:34.199 --> 00:37:37.469
and the virtues. She makes a great argument, uh,

702
00:37:37.510 --> 00:37:42.189
drawing from various virtue ethics, um, uh, traditions. Uh,

703
00:37:42.320 --> 00:37:43.840
THE way that she looks at it and the

704
00:37:43.840 --> 00:37:47.929
way also apply it is um. You can look

705
00:37:47.929 --> 00:37:50.250
at a certain technology, say this mobile phone with

706
00:37:50.250 --> 00:37:54.320
a social uh social uh. Uh, SOCIAL media app

707
00:37:54.320 --> 00:37:57.500
on it, um, does it help or does it

708
00:37:57.500 --> 00:38:01.959
hinder? People to cultivate certain values, sorry, um I'm

709
00:38:01.959 --> 00:38:08.409
doing this, certain virtues, um. Uh, THE self-control, understood

710
00:38:08.409 --> 00:38:13.870
as one of those, uh, classical card cardinal virtues,

711
00:38:14.250 --> 00:38:18.169
uh, the virtue self-control, it means that I command

712
00:38:18.169 --> 00:38:21.409
my own actions, uh, and I can stop when

713
00:38:21.409 --> 00:38:23.010
I can, when I want to, I can start

714
00:38:23.010 --> 00:38:25.159
when I want to. Well, Of course that virtue

715
00:38:25.159 --> 00:38:29.580
of self-control is very easily undermined by social media

716
00:38:29.580 --> 00:38:34.030
apps because they have like 500 psychologists on the

717
00:38:34.030 --> 00:38:38.469
payroll making the app as addictive as possible because

718
00:38:38.469 --> 00:38:40.750
of the ad revenue, they grab, they hold the

719
00:38:40.750 --> 00:38:46.320
monetize your attention. Um, And then you can use

720
00:38:46.320 --> 00:38:49.040
virtue ethics creatively and think of, OK, can you

721
00:38:49.040 --> 00:38:51.760
think of a social media app that does not

722
00:38:51.760 --> 00:38:54.439
erode your self-control, but that can help you to

723
00:38:54.439 --> 00:38:57.959
cultivate self control? Yeah, that can be the social

724
00:38:57.959 --> 00:39:00.209
media app where if you, if you start up

725
00:39:00.209 --> 00:39:02.129
the app, it says how long do you want

726
00:39:02.129 --> 00:39:04.429
to spend? I say, Well, 5 minutes is good.

727
00:39:04.560 --> 00:39:06.600
I just want to be updated on LinkedIn. I

728
00:39:06.600 --> 00:39:08.629
just want to be updated on my friends on

729
00:39:08.639 --> 00:39:11.429
on somewhere else. And then after 5 minutes it

730
00:39:11.429 --> 00:39:15.449
goes beep. This is a reminder to cultivate your

731
00:39:15.449 --> 00:39:18.129
self-control. It's time to think of doing something else,

732
00:39:18.209 --> 00:39:21.810
more useful, more creative, or whatever, and thanks. So

733
00:39:21.810 --> 00:39:24.979
it can be done. If you have another business

734
00:39:24.979 --> 00:39:27.739
model or a revenue model. So this is virtue

735
00:39:27.739 --> 00:39:30.219
ethics in the sense that Shanveer is often applying

736
00:39:30.219 --> 00:39:32.219
it and many others as well. And I'm adding

737
00:39:32.219 --> 00:39:35.739
like uh 11 sort of layer to it, looking

738
00:39:35.739 --> 00:39:38.820
at the virtues that the developers and the people

739
00:39:38.820 --> 00:39:41.580
involved in developing deployment of technology will need. So

740
00:39:41.979 --> 00:39:45.370
um if you want as a development team, your

741
00:39:45.370 --> 00:39:48.389
algorithm. To not discriminate if you want it to

742
00:39:48.389 --> 00:39:52.639
be fair with bias, then, well, very easily you

743
00:39:52.639 --> 00:39:55.120
can you can imagine that those people will then

744
00:39:55.120 --> 00:39:58.590
have to cultivate within themselves the virtue of, of

745
00:39:58.590 --> 00:40:02.250
fairness or of justice, uh, being sensitive to things

746
00:40:02.250 --> 00:40:05.770
going like in a weird direction, combined often with

747
00:40:05.770 --> 00:40:10.679
courage, raising your hand, asking questions combined with wisdom,

748
00:40:10.699 --> 00:40:14.610
which is also a cardinal virtue, um, seeing things

749
00:40:14.610 --> 00:40:20.439
more clearly. Uh, USING discretion. So, uh, those are

750
00:40:20.439 --> 00:40:25.280
the those are the four, ethical perspectives that, that,

751
00:40:25.320 --> 00:40:27.239
that are often used and I make them very

752
00:40:27.239 --> 00:40:28.159
practical in the book.

753
00:40:29.159 --> 00:40:31.750
OK, so let me ask you a few follow-up

754
00:40:31.750 --> 00:40:37.229
questions about these for ethical perspectives. So, um, do

755
00:40:37.229 --> 00:40:39.810
you think that they are, I, I mean, what,

756
00:40:39.870 --> 00:40:42.699
how should we approach them when it comes to

757
00:40:42.699 --> 00:40:47.030
designing technology? Are they all viable? Should They should

758
00:40:47.030 --> 00:40:50.350
the four of them always be considered? Do you

759
00:40:50.350 --> 00:40:52.810
think that one of them is better than the

760
00:40:52.810 --> 00:40:57.070
others? How basically should people who work in ethics

761
00:40:57.070 --> 00:41:01.739
think about and apply these four different perspectives?

762
00:41:01.750 --> 00:41:07.320
Yeah, it's an excellent question. Thanks. My invitation would

763
00:41:07.320 --> 00:41:10.010
be to use all four of them and to

764
00:41:10.010 --> 00:41:12.370
use them like in parallel, but not really in

765
00:41:12.370 --> 00:41:15.770
parallel, but like serial like uh 20 minutes this,

766
00:41:15.850 --> 00:41:18.250
20 minutes that 20, so you can do it

767
00:41:18.250 --> 00:41:21.479
within 1 or 2 hours, 2 hours more likely.

768
00:41:21.850 --> 00:41:26.159
Um, WHY am I inviting people to look at

769
00:41:26.159 --> 00:41:29.209
all four perspectives? It's just because they, they, they

770
00:41:29.209 --> 00:41:32.500
complement each other so beautifully. It would be silly

771
00:41:32.500 --> 00:41:34.500
not to look at the pluses and minuses. Yeah,

772
00:41:34.739 --> 00:41:36.419
but it would also be silly to not look

773
00:41:36.419 --> 00:41:38.060
at human rights. Yeah, but it would also be

774
00:41:38.060 --> 00:41:40.219
silly not to look at power and how it

775
00:41:40.219 --> 00:41:42.620
changed relationships. Yeah. So all of them are viable.

776
00:41:43.729 --> 00:41:49.330
Uh, HAVING said that, um, uh, in my experience,

777
00:41:49.370 --> 00:41:54.129
it works best if you also isolate them because

778
00:41:54.129 --> 00:41:59.120
it's like the language, uh, uh, you're Portuguese speaking,

779
00:41:59.520 --> 00:42:02.449
uh, I'm Dutch speaking, we're now talking English, so

780
00:42:02.449 --> 00:42:05.209
we're finding ways to communicate and if in the

781
00:42:05.209 --> 00:42:07.479
middle of the sentence I will change the language,

782
00:42:07.649 --> 00:42:10.219
it will be harder for you to follow. So

783
00:42:10.389 --> 00:42:12.830
in the midst of a discussion of pluses and

784
00:42:12.830 --> 00:42:15.689
minuses, it's good to, to remain in that mindset,

785
00:42:15.860 --> 00:42:19.300
the pluses and minuses mindset of consequentialism and then

786
00:42:19.300 --> 00:42:21.949
people can say human rights something and then I

787
00:42:21.949 --> 00:42:24.989
found out that's a good topic. We'll park it

788
00:42:24.989 --> 00:42:27.629
for later for the next uh part of the

789
00:42:27.629 --> 00:42:31.360
agenda and then we do fully all of the

790
00:42:31.360 --> 00:42:36.790
obligations, the law, uh, compliance, human rights, all of

791
00:42:36.790 --> 00:42:40.070
that. And then somebody says plus the m, yeah,

792
00:42:40.149 --> 00:42:42.020
it's a good idea, but we, we don't let,

793
00:42:42.429 --> 00:42:45.629
we give the full floor to that perspective for

794
00:42:45.629 --> 00:42:47.409
as long as we want, let's say 20 minutes,

795
00:42:47.590 --> 00:42:51.270
so we can really dive into that perspective, uh,

796
00:42:51.550 --> 00:42:55.370
and this is again my, yeah. My recommendation to

797
00:42:55.370 --> 00:42:59.760
make all of these conversations more explicit. So what

798
00:42:59.760 --> 00:43:02.439
are we talking about? Is it pluses and minuses?

799
00:43:03.239 --> 00:43:05.179
Are you now saying it's good for profit? I

800
00:43:05.179 --> 00:43:08.939
mean, it's, it's, it's, yeah, it's perfectly practical if

801
00:43:08.939 --> 00:43:11.260
somebody says, well, this option is better because it

802
00:43:11.260 --> 00:43:14.419
gives us profitability economic, you can do that. But

803
00:43:14.419 --> 00:43:19.050
if somebody, somebody says human rights, that's also a

804
00:43:19.050 --> 00:43:21.659
great topic, but a different topic with a different

805
00:43:21.659 --> 00:43:25.370
perspective. So, uh, making things explicit and also making,

806
00:43:25.459 --> 00:43:31.139
making, making careful in the sense that Don't confuse

807
00:43:31.139 --> 00:43:34.379
those perspectives, uh, because then it will be a

808
00:43:34.379 --> 00:43:38.550
messy dialogue. Uh, HAVING said that, the first two

809
00:43:39.290 --> 00:43:42.340
So consequentialism and duty ethics are, as I, as

810
00:43:42.340 --> 00:43:45.739
I suggested before, relatively easy for people to get

811
00:43:45.739 --> 00:43:48.340
into. That's also why I start uh workshops with

812
00:43:48.340 --> 00:43:51.139
those and they were good, then we're halfway, and

813
00:43:51.139 --> 00:43:54.020
then I make somewhat more complex ones like the

814
00:43:54.020 --> 00:43:56.750
relational ethics. By the way, I don't say feminist,

815
00:43:56.820 --> 00:43:59.179
I don't say care, I just pose the questions

816
00:43:59.179 --> 00:44:02.260
like how would it change interactions between people, what

817
00:44:02.260 --> 00:44:05.090
does it do to power? So I, they don't

818
00:44:05.090 --> 00:44:11.139
need the theoretical backgrounds. Uh, SO you were saying,

819
00:44:11.219 --> 00:44:13.419
are they all needed? Yes, they're all needed. Do

820
00:44:13.419 --> 00:44:16.830
I have a preference? Not really. However, given the

821
00:44:16.830 --> 00:44:21.159
situation that people are already very often doing consequentialism

822
00:44:21.159 --> 00:44:25.010
duty ethics. Uh, MY favorite ones are the relational

823
00:44:25.010 --> 00:44:27.969
ethics and the virtue ethics because they're often relatively

824
00:44:27.969 --> 00:44:30.399
new to people and it adds something to their,

825
00:44:30.689 --> 00:44:33.810
to their vocabulary and to their sensitivity. So next

826
00:44:33.810 --> 00:44:37.780
workshop they can say, Yeah, but we can also

827
00:44:37.780 --> 00:44:41.860
imagine it's having influence on people's virtues and they're

828
00:44:41.860 --> 00:44:44.379
flourishing and then I think, well, that's great. You

829
00:44:44.379 --> 00:44:46.820
would not have been able to say that if

830
00:44:46.820 --> 00:44:50.139
we had not also spent some time on that

831
00:44:50.139 --> 00:44:51.370
ethical perspective.

832
00:44:52.250 --> 00:44:56.199
Right. But uh for example, if we're dealing with

833
00:44:56.199 --> 00:45:01.179
different types of technologies that someone is designing, would,

834
00:45:01.300 --> 00:45:05.719
uh, different ethical perspectives apply to different kinds of

835
00:45:05.719 --> 00:45:09.250
technologies better than the others or, for example, For

836
00:45:09.250 --> 00:45:12.570
example, if we're dealing with a specific ethical problem

837
00:45:12.570 --> 00:45:15.580
that we are trying to solve in designing a

838
00:45:15.580 --> 00:45:19.050
specific kind of technology, is it that one of

839
00:45:19.050 --> 00:45:22.260
them is better than the others, or should we

840
00:45:22.260 --> 00:45:25.449
all, uh, should we always try to apply all

841
00:45:25.449 --> 00:45:26.090
four of them?

842
00:45:26.439 --> 00:45:28.229
Yeah, I think, I think it's a, it's a,

843
00:45:28.310 --> 00:45:30.909
it's a relevant question. People have asked me also,

844
00:45:30.949 --> 00:45:32.149
do we need all 4? Yeah, we need all

845
00:45:32.149 --> 00:45:35.909
4, but then in practice, it often happens that

846
00:45:36.270 --> 00:45:38.389
one or two of them get most of the

847
00:45:38.389 --> 00:45:43.949
attention, uh, often because that's just the application or

848
00:45:43.949 --> 00:45:46.030
the system or the product or service they're working

849
00:45:46.030 --> 00:45:50.459
on. It just raises more questions in that perspective.

850
00:45:51.139 --> 00:45:55.110
So, uh, an, an app that facilitates human to

851
00:45:55.110 --> 00:45:57.550
human communication, you will spend a bit more on

852
00:45:57.550 --> 00:46:01.060
the relational ethics side of it. An app that

853
00:46:01.060 --> 00:46:04.179
uh or a system that people use all the

854
00:46:04.179 --> 00:46:06.659
time that really ties into their habits. Habits is

855
00:46:06.659 --> 00:46:10.280
also a vocabulary of virtue ethics, uh, to develop

856
00:46:10.280 --> 00:46:13.300
virtuous habits is like the whole thing of virtue

857
00:46:13.300 --> 00:46:16.409
ethics. So for those systems where they become part

858
00:46:16.409 --> 00:46:19.060
of a behavior of a habit, their virtue ethics

859
00:46:19.060 --> 00:46:21.260
is a, is a, is a, is a good

860
00:46:21.260 --> 00:46:23.419
place to spend some extra time, like how does

861
00:46:23.419 --> 00:46:29.429
it change people's, uh, attitudes, behaviors, habits, virtues. Uh,

862
00:46:29.629 --> 00:46:31.370
SO yeah, just, just do all four of them

863
00:46:31.370 --> 00:46:34.060
and it's also great if uh two of them

864
00:46:34.060 --> 00:46:36.540
get some more attention because, well, that's just like

865
00:46:36.540 --> 00:46:40.219
you're saying, uh, projects are different and will require

866
00:46:40.219 --> 00:46:41.419
different emphasis.

867
00:46:42.639 --> 00:46:45.899
Mhm. So to get a little bit more practical

868
00:46:45.899 --> 00:46:48.429
now that we're we're reaching the end of our

869
00:46:48.429 --> 00:46:52.350
conversation, if someone is thinking about developing a new

870
00:46:52.350 --> 00:46:57.350
kind of technology where ethical considerations might apply, how

871
00:46:57.350 --> 00:46:59.709
do you think they should proceed? I mean, would

872
00:46:59.709 --> 00:47:03.669
there be a set of principles or guidelines for

873
00:47:03.669 --> 00:47:04.469
them to follow?

874
00:47:06.689 --> 00:47:08.919
If they, if they just want to get started,

875
00:47:09.290 --> 00:47:13.209
uh, uh, I've made a canvas, so that's a

876
00:47:13.209 --> 00:47:16.969
big piece of paper with a handful of questions

877
00:47:16.969 --> 00:47:19.600
for each of those four perspectives written on them,

878
00:47:19.969 --> 00:47:22.969
and I've used it often in workshops. It starts

879
00:47:22.969 --> 00:47:25.530
with, um, in the middle of the, the drawing

880
00:47:25.530 --> 00:47:28.250
of the canvas is like a blank circle, and

881
00:47:28.250 --> 00:47:30.820
I think it's good to Because you're saying, where

882
00:47:30.820 --> 00:47:33.330
do you start? I always advise people to start

883
00:47:33.330 --> 00:47:35.419
with the thing they're working on to put that

884
00:47:35.419 --> 00:47:38.649
center stage. Uh, SO, OK, what is that the

885
00:47:38.649 --> 00:47:42.020
algorithm that you're working on? Tell me in somewhat

886
00:47:42.020 --> 00:47:46.969
practical detail with some specificity, who uses it, how

887
00:47:46.969 --> 00:47:50.929
do they use it, what then happens, who then

888
00:47:50.929 --> 00:47:54.239
gets the consequences from what happens? Just tell me,

889
00:47:54.449 --> 00:47:57.330
what does it do? And that's the starting point

890
00:47:57.330 --> 00:48:01.600
for the discussion and uh uh technology-oriented people love

891
00:48:01.600 --> 00:48:04.919
that, I found because it makes it practical because

892
00:48:04.919 --> 00:48:07.360
the 1st 5 or 10 minutes of each workshop

893
00:48:07.360 --> 00:48:09.879
is just filled with their project, but it is

894
00:48:09.879 --> 00:48:12.800
their project, also the workshop is their project, but

895
00:48:12.800 --> 00:48:15.669
it's just a good starting point and what it

896
00:48:16.040 --> 00:48:20.000
helps people to do, uh, critically is that it

897
00:48:20.000 --> 00:48:23.449
makes all the conversations after that much more practical.

898
00:48:24.639 --> 00:48:27.639
Because you can't say, OK, suppose that we don't

899
00:48:27.639 --> 00:48:29.489
do it, uh, the first step, but we just

900
00:48:29.489 --> 00:48:32.360
say, well, there's the project, fraud detection something. Well,

901
00:48:32.449 --> 00:48:34.510
it doesn't matter the details, we'll just talk about

902
00:48:34.510 --> 00:48:38.489
ethics. Yeah, then people will stare at each other

903
00:48:38.489 --> 00:48:45.010
like, yeah, something fairness, something biased, but then, then

904
00:48:45.010 --> 00:48:46.989
the then the conversation just drops on the, on

905
00:48:46.989 --> 00:48:49.250
the ground, it falls flat. Whereas if they had

906
00:48:49.250 --> 00:48:51.239
spent 5 or 10 minutes at the start, OK,

907
00:48:51.330 --> 00:48:53.889
this is data that go into it. These are

908
00:48:53.889 --> 00:48:56.850
the databases that we're using. So these are the

909
00:48:56.850 --> 00:48:58.610
biases that are in there. This is how the

910
00:48:58.610 --> 00:49:01.010
algorithm works. These are the false positives, these are

911
00:49:01.010 --> 00:49:02.925
the false negatives. This is what then happens. This

912
00:49:02.925 --> 00:49:06.655
is the process organized around it? Is there, is

913
00:49:06.655 --> 00:49:09.885
there slack? Is there maneuver room for the inspector

914
00:49:09.885 --> 00:49:13.104
person to use their own discretion against the algorithm?

915
00:49:13.125 --> 00:49:16.165
Can they do pushback? Is there a feedback mechanism

916
00:49:16.165 --> 00:49:19.715
in uh uh envisioned? Is it being evaluated, so

917
00:49:19.715 --> 00:49:21.504
all of it. Tell me all the details. OK.

918
00:49:21.885 --> 00:49:23.554
Now we're good. Now we can start. And then,

919
00:49:24.629 --> 00:49:27.070
And then the, all the dialogues after that are

920
00:49:27.070 --> 00:49:30.510
much more informed and much more, much more detailed.

921
00:49:31.350 --> 00:49:33.489
So your question, where do you start? Well, with

922
00:49:33.489 --> 00:49:36.919
the project and its details and specificity, and then

923
00:49:37.169 --> 00:49:38.889
you let loose the four perspectives.

924
00:49:39.830 --> 00:49:43.429
Mhm. And then, uh, finally, I wanted to ask

925
00:49:43.429 --> 00:49:47.389
you about, uh, 3 different methods that you present

926
00:49:47.389 --> 00:49:51.080
toward the end of your book. One is human-centered

927
00:49:51.080 --> 00:49:54.550
design, the second one value sensitive design, and the

928
00:49:54.550 --> 00:49:58.350
third one, responsible innovation. So, uh, tell us a

929
00:49:58.350 --> 00:50:01.870
little bit about them. What are these 3 different

930
00:50:01.870 --> 00:50:04.870
methods, how they apply and things like that?

931
00:50:05.250 --> 00:50:07.479
Yeah, yeah, great. Yeah, that's indeed the last bit

932
00:50:07.479 --> 00:50:09.760
of the book. What I do there is I

933
00:50:09.760 --> 00:50:13.389
look at three, traditions, if you like, that are

934
00:50:13.389 --> 00:50:17.159
already there. And so my argument is then don't

935
00:50:17.159 --> 00:50:20.310
waste your time on setting up something, something ethical.

936
00:50:20.750 --> 00:50:22.600
Just look at what is already there. Maybe people

937
00:50:22.600 --> 00:50:25.570
are doing human centered design already or value center

938
00:50:25.570 --> 00:50:29.139
design already or responsible innovation already. And I'll go

939
00:50:29.139 --> 00:50:30.810
into all of three of them, but the, the

940
00:50:30.810 --> 00:50:33.810
mechanism that I want to uh recommend is just

941
00:50:33.810 --> 00:50:36.540
look at the practices that are already there and

942
00:50:36.540 --> 00:50:41.090
then add ethics just to it. Uh, BECAUSE otherwise

943
00:50:41.090 --> 00:50:44.360
it, it puts much too much pressure on the,

944
00:50:44.889 --> 00:50:48.409
on, on, on having to organize an entirely new

945
00:50:48.409 --> 00:50:51.370
and different pro, and no, don't do that. Just

946
00:50:51.729 --> 00:50:54.489
take what's there. So human centered design is broadly

947
00:50:54.489 --> 00:50:59.399
the approach where the developers uh talk to potential

948
00:50:59.399 --> 00:51:02.489
future end users, maybe in focus groups, maybe usability

949
00:51:02.489 --> 00:51:06.070
testing, maybe user experience testing, uh, all of that,

950
00:51:06.370 --> 00:51:09.330
uh. And it's very, uh, not, not fair, but

951
00:51:09.330 --> 00:51:12.770
it's relatively easy to just add to that the

952
00:51:12.770 --> 00:51:15.489
four types of questions from the four ethical perspectives

953
00:51:15.770 --> 00:51:19.429
in the focus group in the usability testing, uh,

954
00:51:19.810 --> 00:51:25.300
and then, uh, more practically. Uh, HUMAN centered design,

955
00:51:25.340 --> 00:51:28.780
if you look at its, uh, there's an ISO

956
00:51:28.780 --> 00:51:32.899
norm for it. Uh, IT is already iterative, so

957
00:51:32.899 --> 00:51:36.060
that's good. And it is already participative in the

958
00:51:36.060 --> 00:51:39.260
sense that you invite the potential future end users

959
00:51:39.260 --> 00:51:43.300
to collaborate, but also you're asking experts, uh, and

960
00:51:43.300 --> 00:51:45.979
then you would now add also maybe a human

961
00:51:45.979 --> 00:51:49.340
rights expert or an NGO person to talk about

962
00:51:49.340 --> 00:51:52.959
human rights just to the team. So it's acknowledging

963
00:51:52.959 --> 00:51:56.320
human centered design is already halfway great. You're already

964
00:51:56.320 --> 00:51:58.959
putting people and their experiences center stage. It is

965
00:51:58.959 --> 00:52:02.020
already iterative, it is already participatory. So just add

966
00:52:02.020 --> 00:52:06.209
ethics to it and you're good. Another approach that

967
00:52:06.209 --> 00:52:09.550
I mentioned is value sensit design. Uh, IT came,

968
00:52:09.929 --> 00:52:20.850
uh, also from industry. Uh, It recommends. Identifying those

969
00:52:20.850 --> 00:52:24.570
values that are are potentially at stake in this

970
00:52:24.570 --> 00:52:28.610
project and then inviting stakeholders around the table to

971
00:52:28.610 --> 00:52:33.090
talk about these values and then having a decision

972
00:52:33.090 --> 00:52:37.570
mechanism to yeah, to make choices like, OK, so

973
00:52:37.570 --> 00:52:39.729
this is the value that we will prioritize and

974
00:52:39.729 --> 00:52:42.139
we want to prioritize and then secondly, how do

975
00:52:42.139 --> 00:52:45.129
we do that? How do we develop the technology

976
00:52:45.129 --> 00:52:47.370
or modify the technology or the airport system in

977
00:52:47.370 --> 00:52:50.790
such ways that indeed that That value that we

978
00:52:50.790 --> 00:52:55.360
want to improve is improved. Uh, SO that's a

979
00:52:55.360 --> 00:52:58.800
great way already, uh, to do ethics, and many

980
00:52:58.800 --> 00:53:01.810
people, uh, especially, I think in the Netherlands, it's

981
00:53:01.810 --> 00:53:05.560
like a very common approach in the four, universities

982
00:53:05.560 --> 00:53:07.879
of technology that I mentioned. So that's Delft aint

983
00:53:07.879 --> 00:53:11.830
of tre and Wageningen, those were Dutch words, uh,

984
00:53:11.840 --> 00:53:14.840
but people maybe know one of them, uh, um.

985
00:53:15.770 --> 00:53:18.590
They do often something very akin to value sense

986
00:53:18.590 --> 00:53:20.649
of design. They can also do it designed for

987
00:53:20.649 --> 00:53:23.830
values, which is acknowledging that during the design process,

988
00:53:23.870 --> 00:53:25.510
you can do it such and such that the

989
00:53:25.510 --> 00:53:31.300
values are uh uh promoted rather than corroded. Now

990
00:53:31.300 --> 00:53:34.310
between parenthesis, I can add a bit of critique

991
00:53:34.790 --> 00:53:38.270
on value sensitive design because uh sometimes I've seen

992
00:53:38.270 --> 00:53:42.310
that it is difficult for people to, to, to

993
00:53:42.310 --> 00:53:45.750
become practical with the value. Say we're, we're having

994
00:53:45.750 --> 00:53:48.310
a discussion. Well, justice, justice is good, right? Fairness

995
00:53:48.310 --> 00:53:54.229
is good, right? Yeah, but We don't nobody will

996
00:53:54.229 --> 00:53:56.919
very few people will disagree on that. So how

997
00:53:56.919 --> 00:54:00.040
do you then decide uh what it means very

998
00:54:00.040 --> 00:54:03.520
practically in the project. So, um I found that

999
00:54:03.520 --> 00:54:10.389
talking about virtues. Can sometimes help people uh to

1000
00:54:10.389 --> 00:54:13.469
become more, to have a more practical approach to

1001
00:54:13.469 --> 00:54:17.350
the system. So fraud detection thing again or the

1002
00:54:17.350 --> 00:54:20.820
social media up again, uh, the values are like,

1003
00:54:20.909 --> 00:54:24.189
yeah, we, we agree on them relatively easily, but

1004
00:54:24.189 --> 00:54:27.270
then what happens practically in terms of what people's

1005
00:54:27.270 --> 00:54:30.189
habits and, and, and, and traits and, and, and

1006
00:54:30.189 --> 00:54:32.709
virtues, how do they, how do they change and

1007
00:54:32.709 --> 00:54:35.550
that's Yeah, I'm not the only one. There are

1008
00:54:35.550 --> 00:54:37.790
a couple of other authors also they're saying like

1009
00:54:37.790 --> 00:54:40.790
something like, like virtue centered design or people are

1010
00:54:40.790 --> 00:54:43.550
saying capability, sensitive design. I will not go into

1011
00:54:43.550 --> 00:54:46.469
that, but the capability approach is akin to virtue

1012
00:54:46.469 --> 00:54:49.389
ethics. So uh it, it's a way of making

1013
00:54:49.389 --> 00:54:51.669
the values which have a risk of remaining a

1014
00:54:51.669 --> 00:54:57.250
bit abstract of making them more practical. And thirdly,

1015
00:54:57.290 --> 00:55:00.850
responsible innovation. I'm not sure whether people in industry

1016
00:55:00.850 --> 00:55:05.129
will recognize all of the vocabulary that goes together

1017
00:55:05.129 --> 00:55:08.260
with responsible innovation. Maybe responsible innovation is more like

1018
00:55:08.260 --> 00:55:10.929
an academic term, but some of the elements I

1019
00:55:10.929 --> 00:55:14.050
think will be, will be uh uh uh yeah,

1020
00:55:14.330 --> 00:55:18.120
very, uh. Uh, PEOPLE will be familiar with them

1021
00:55:18.120 --> 00:55:21.719
in industry. So there are 4 of them often,

1022
00:55:21.830 --> 00:55:29.090
um, anticipation, responsiveness, inclusion, and reflexivity, and I can

1023
00:55:29.370 --> 00:55:31.590
say just a little bit about them, but, uh,

1024
00:55:31.600 --> 00:55:37.419
the thing is that If in industry. People are

1025
00:55:37.419 --> 00:55:42.919
imagining future consequences or outcomes. They're doing anticipation and

1026
00:55:42.919 --> 00:55:45.379
then if they want to act upon it, they're

1027
00:55:45.379 --> 00:55:51.820
doing responsiveness. So, uh. It's really a recommendation to,

1028
00:55:51.929 --> 00:55:55.139
to, to bake some bit more of that into

1029
00:55:55.139 --> 00:55:58.129
your projects. Just think about what could go wrong,

1030
00:55:58.340 --> 00:56:00.419
but also the other way around, what could, what

1031
00:56:00.419 --> 00:56:03.659
could go enormously successful, which, which can cause its

1032
00:56:03.659 --> 00:56:08.870
own problems, uh, weirdly enough, um. Third one of

1033
00:56:08.870 --> 00:56:11.750
those is diversity and inclusion and I like that

1034
00:56:11.750 --> 00:56:16.389
uh that uh dimension a lot. Uh, I'm associating

1035
00:56:16.389 --> 00:56:19.350
it also with, uh with the participatory aspect of

1036
00:56:19.350 --> 00:56:23.110
it, uh, what I said before. So rather than

1037
00:56:23.110 --> 00:56:29.429
having only the technology-oriented project team members discussed those

1038
00:56:29.429 --> 00:56:32.669
topics, uh, doing the ethics, uh, practical workshops, what

1039
00:56:32.669 --> 00:56:37.350
I said, it's, it's, it's just more advantage. To

1040
00:56:37.350 --> 00:56:40.870
advantageous to, to, to invite other people with other

1041
00:56:40.870 --> 00:56:45.429
backgrounds. Notoriously, uh, many of the products that come

1042
00:56:45.429 --> 00:56:48.389
out of Silicon Valley look like they've been designed

1043
00:56:48.389 --> 00:56:54.600
by White rich, 30-ish people and maybe they are,

1044
00:56:54.770 --> 00:56:57.969
I think they are. Had they included in their

1045
00:56:57.969 --> 00:57:01.370
project teams, I don't know, older people, people of

1046
00:57:01.370 --> 00:57:07.169
color, people who are not affluent, uh, other concerns

1047
00:57:07.169 --> 00:57:10.370
would have been on the table, uh potentially and

1048
00:57:10.370 --> 00:57:13.229
would have been taken into account. So this is

1049
00:57:13.229 --> 00:57:17.719
just a big recommendation to uh if possible, if

1050
00:57:17.719 --> 00:57:21.159
relevant, uh make the project team and interactions that

1051
00:57:21.159 --> 00:57:23.060
you have just a bit more or a lot

1052
00:57:23.060 --> 00:57:27.479
more uh diverse and inclusive of, yeah, those concerns

1053
00:57:27.479 --> 00:57:30.439
and those perspectives that otherwise remain out of sight.

1054
00:57:31.419 --> 00:57:37.020
Uh, 4th 1 of responsible innovation is reflexivity, um.

1055
00:57:38.050 --> 00:57:40.929
Yeah. In short, it just means be aware of

1056
00:57:40.929 --> 00:57:46.199
what you're doing. Mhm. Don't think that uh other

1057
00:57:46.199 --> 00:57:48.120
people will solve the problem that you're looking at

1058
00:57:48.120 --> 00:57:50.189
because no, you're the person who's looking at it,

1059
00:57:50.270 --> 00:57:53.590
so beware of that and just try to improve

1060
00:57:53.590 --> 00:57:53.989
it.

1061
00:57:56.280 --> 00:57:59.250
Great. So, the book is again, ethics for people

1062
00:57:59.250 --> 00:58:01.300
who work in tech. I'm leaving a link to

1063
00:58:01.300 --> 00:58:03.969
it in the description of the interview. And Doctor

1064
00:58:03.969 --> 00:58:07.120
Stein, just before we go apart from the book,

1065
00:58:07.330 --> 00:58:09.929
are there any places on the internet you would

1066
00:58:09.929 --> 00:58:12.250
like to mention where people can find you and

1067
00:58:12.250 --> 00:58:12.969
your work?

1068
00:58:13.659 --> 00:58:15.790
Yeah, thanks for the question. Yeah, uh, so the

1069
00:58:15.790 --> 00:58:19.270
book has a website associated to it. It's simply

1070
00:58:19.270 --> 00:58:23.260
called Ethics for people who work in tech.com. And

1071
00:58:23.260 --> 00:58:25.189
the great thing of that website is you can

1072
00:58:25.189 --> 00:58:28.629
find uh lots of resources there. So the, the

1073
00:58:28.629 --> 00:58:30.500
good thing is by the book, but the less

1074
00:58:30.500 --> 00:58:32.389
good thing, but still great if, if you just

1075
00:58:32.389 --> 00:58:37.110
go there and look at shorter articles, essays, uh,

1076
00:58:37.350 --> 00:58:40.820
I have a sort of annotated, uh. Uh, ONLINE

1077
00:58:40.820 --> 00:58:44.669
resources for each chapter. So, uh, on the go,

1078
00:58:44.709 --> 00:58:46.989
you can just have a listen to a podcast

1079
00:58:46.989 --> 00:58:50.189
here or there or a short YouTube, uh, a

1080
00:58:50.189 --> 00:58:52.709
video to just also explain the things that are

1081
00:58:52.709 --> 00:58:55.030
in the book. It's just to make it more

1082
00:58:55.030 --> 00:58:59.600
accessible to people. Oh, and Mark Steyn, Mark M

1083
00:58:59.600 --> 00:59:03.879
A R C Steyn at sorry dot NL. That's

1084
00:59:03.879 --> 00:59:06.879
the Netherlands. That's my personal uh where all my

1085
00:59:06.879 --> 00:59:07.949
academic stuff is.

1086
00:59:08.870 --> 00:59:11.590
OK, great. So thank you so much again for

1087
00:59:11.590 --> 00:59:13.649
coming on the show. It's been a real pleasure

1088
00:59:13.649 --> 00:59:14.419
to talk with you.

1089
00:59:15.239 --> 00:59:16.719
Thanks a lot for the invitation. I enjoyed a

1090
00:59:16.719 --> 00:59:17.639
lot. Thanks, Ricardo.

1091
00:59:19.070 --> 00:59:21.560
Hi guys, thank you for watching this interview until

1092
00:59:21.560 --> 00:59:23.739
the end. If you liked it, please share it,

1093
00:59:23.909 --> 00:59:26.699
leave a like and hit the subscription button. The

1094
00:59:26.699 --> 00:59:28.899
show is brought to you by Nights Learning and

1095
00:59:28.899 --> 00:59:32.979
Development done differently, check their website at Nights.com and

1096
00:59:32.979 --> 00:59:36.699
also please consider supporting the show on Patreon or

1097
00:59:36.699 --> 00:59:39.179
PayPal. I would also like to give a huge

1098
00:59:39.179 --> 00:59:42.570
thank you to my main patrons and PayPal supporters

1099
00:59:42.570 --> 00:59:46.500
Pergo Larsson, Jerry Mullern, Fredrik Sundo, Bernard Seyches Olaf,

1100
00:59:46.620 --> 00:59:50.340
Alexandam Castle, Matthew Whitting Berarna Wolf, Tim Hollis, Erika

1101
00:59:50.340 --> 00:59:53.550
Lenny, John Connors, Philip Fors Connolly. Then the Matter

1102
00:59:53.550 --> 00:59:58.370
Robert Windegaruyasi Zu Mark Neevs called Holbrookfield governor Michael

1103
00:59:58.370 --> 01:00:02.770
Stormir, Samuel Andre, Francis Forti Agnseroro and Hal Herzognun

1104
01:00:02.770 --> 01:00:06.610
Macha Joan Labrant John Jasent and Samuel Corriere, Heinz,

1105
01:00:06.659 --> 01:00:10.330
Mark Smith, Jore, Tom Hummel, Sardus Fran David Sloan

1106
01:00:10.330 --> 01:00:15.939
Wilson, Asila dearraujurumen ro Diego Londono Correa. Yannick Punterrumani

1107
01:00:15.939 --> 01:00:21.120
Charlotte blinikolbar Adamhn Pavlostaevsky nale back medicine, Gary Galman

1108
01:00:21.120 --> 01:00:26.159
Samovallidrianei Poltonin John Barboza, Julian Price, Edward Hall Edin

1109
01:00:26.159 --> 01:00:31.840
Bronner, Douglas Fry, Franco Bartolotti Gabrielon Corteseus Slelitsky, Scott

1110
01:00:31.840 --> 01:00:36.020
Zachary Fish Tim Duffyani Smith John Wieman. Daniel Friedman,

1111
01:00:36.070 --> 01:00:40.429
William Buckner, Paul Georgianeau, Luke Lovai Giorgio Theophanous, Chris

1112
01:00:40.429 --> 01:00:45.149
Williamson, Peter Vozin, David Williams, the Augusta, Anton Eriksson,

1113
01:00:45.310 --> 01:00:49.870
Charles Murray, Alex Shaw, Marie Martinez, Corale Chevalier, bungalow

1114
01:00:49.870 --> 01:00:54.709
atheists, Larry D. Lee Junior, old Erringbo. Sterry Michael

1115
01:00:54.709 --> 01:00:59.070
Bailey, then Sperber, Robert Grayigoren, Jeff McMann, Jake Zu,

1116
01:00:59.510 --> 01:01:03.419
Barnabas radix, Mark Campbell, Thomas Dovner, Luke Neeson, Chris

1117
01:01:03.419 --> 01:01:07.870
Storry, Kimberly Johnson, Benjamin Gilbert, Jessica Nowicki, Linda Brandon,

1118
01:01:07.949 --> 01:01:14.189
Nicholas Carlsson, Ismael Bensleyman. George Eoriatis, Valentin Steinman, Perkrolis,

1119
01:01:14.280 --> 01:01:20.459
Kate van Goller, Alexander Hubbert, Liam Dunaway, BR Masoud

1120
01:01:20.459 --> 01:01:26.340
Ali Mohammadi, Perpendicular John Nertner, Ursulauddinov, Gregory Hastings, David

1121
01:01:26.340 --> 01:01:31.000
Pinsoff Sean Nelson, Mike Levine, and Jos Net. A

1122
01:01:31.000 --> 01:01:33.550
special thanks to my producers. These are Webb, Jim,

1123
01:01:33.600 --> 01:01:38.000
Frank Lucas Steffinik, Tom Venneden, Bernard Curtis Dixon, Benedict

1124
01:01:38.000 --> 01:01:41.520
Muller, Thomas Trumbull, Catherine and Patrick Tobin, Gian Carlo

1125
01:01:41.520 --> 01:01:44.479
Montenegroal Ni Cortiz and Nick Golden, and to my

1126
01:01:44.479 --> 01:01:48.649
executive producers, Matthew Levender, Sergio Quadrian, Bogdan Kanivets, and

1127
01:01:48.649 --> 01:01:50.070
Rosie. Thank you for all.

