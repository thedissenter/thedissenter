WEBVTT

1
00:00:00.140 --> 00:00:02.839
Hello everybody. Welcome to a new episode of the,

2
00:00:02.849 --> 00:00:05.699
the Center. I'm your host, Ricardo Loops. And today

3
00:00:05.710 --> 00:00:08.460
I'm joined by Doctor Paul Molde. He is an

4
00:00:08.470 --> 00:00:12.590
Associate Professor of Cognitive and Information Sciences and Faculty

5
00:00:12.600 --> 00:00:16.469
in the Quantitative and Systems Biology graduate program at

6
00:00:16.479 --> 00:00:19.520
the University of California Merced. And he's also an

7
00:00:19.530 --> 00:00:24.110
external professor at the Santa Fe Institute. And today

8
00:00:24.120 --> 00:00:28.799
we're talking about his book Modeling, Social Behavior, Mathematical

9
00:00:28.809 --> 00:00:32.459
and Agent Based Models of Social Dynamics and Cultural

10
00:00:32.470 --> 00:00:35.930
Evolution. So Doctor Malden, welcome to the show. It's

11
00:00:35.939 --> 00:00:37.409
a huge pleasure to everyone.

12
00:00:38.360 --> 00:00:40.560
Oh, very nice to be here. Thanks.

13
00:00:41.689 --> 00:00:46.349
So, just to introduce the topic here. What is

14
00:00:46.360 --> 00:00:50.330
modeling in science exactly? I mean, what is the

15
00:00:50.340 --> 00:00:51.110
model?

16
00:00:51.610 --> 00:00:57.729
Yeah. So uh most science, most of it depends

17
00:00:57.740 --> 00:01:00.909
no matter what field you're using, whether it's biology

18
00:01:00.919 --> 00:01:05.480
or the social sciences or chemistry or physics. Very

19
00:01:05.489 --> 00:01:09.239
often use models. Uh And you're using a model

20
00:01:09.250 --> 00:01:15.580
really, anytime you're studying something kind of indirectly, um

21
00:01:15.589 --> 00:01:17.800
if we want to know about a rock and

22
00:01:17.809 --> 00:01:19.599
I wanna know about this, I have a rock

23
00:01:19.610 --> 00:01:22.099
in my hand and I wanna study this specific

24
00:01:22.110 --> 00:01:24.209
rock. I don't need a model because I'm studying

25
00:01:24.220 --> 00:01:27.930
the rock. Um If I wanna study rocks in

26
00:01:27.940 --> 00:01:31.300
general or if I wanna study people in general,

27
00:01:31.690 --> 00:01:35.500
I, I need a model. A model is something

28
00:01:35.510 --> 00:01:40.959
that I study that represents, I usually a broader

29
00:01:40.970 --> 00:01:45.970
class of systems or similar systems or phenomena. So

30
00:01:46.769 --> 00:01:49.279
in, in, we use model organisms and most people

31
00:01:49.290 --> 00:01:52.849
who study fruit flies or rats, some of them

32
00:01:52.860 --> 00:01:55.360
are, but most of them are not interested directly

33
00:01:55.370 --> 00:01:57.569
in. I really want to know about fruit flies

34
00:01:57.580 --> 00:02:00.599
per se or rats per se. They're using those

35
00:02:00.610 --> 00:02:05.449
organisms as models for, let's say any organism with

36
00:02:05.459 --> 00:02:10.410
genes or all mammals or even, you know, representing

37
00:02:11.220 --> 00:02:16.119
social organisms, uh et cetera. Um And experiments are

38
00:02:16.130 --> 00:02:19.679
often models, let's say in, in psychology, if you're

39
00:02:19.690 --> 00:02:22.679
studying, you know, if you have a kid and

40
00:02:22.690 --> 00:02:23.979
you sit them down at a table and you

41
00:02:23.990 --> 00:02:26.259
put a marshmallow in front of them, this is

42
00:02:26.270 --> 00:02:28.929
like a famous psychology study. They, they torch for

43
00:02:28.940 --> 00:02:31.389
these kids by giving, putting a marshmallow in front

44
00:02:31.399 --> 00:02:33.429
of them and then saying, all right, I'm gonna

45
00:02:33.440 --> 00:02:35.710
leave the room. But, and if that marshmallow is

46
00:02:35.720 --> 00:02:37.929
still there, when I come back, I'll give you

47
00:02:37.940 --> 00:02:40.440
a second marshmallow and they'll test how long kids

48
00:02:40.449 --> 00:02:42.710
can wait and they'll try to correlate this with

49
00:02:42.720 --> 00:02:47.410
other things. And aside from maybe some very strange

50
00:02:47.419 --> 00:02:49.100
people, most people who do this kind of thing

51
00:02:49.110 --> 00:02:52.449
are not interested per se in whether in how

52
00:02:52.460 --> 00:02:54.839
long kids can wait to eat a marshmallow, right?

53
00:02:54.850 --> 00:02:57.479
They're interested in a broader class of phenomena like

54
00:02:57.500 --> 00:03:01.470
willpower or trust and authority or et cetera. Um

55
00:03:02.169 --> 00:03:06.020
And so, uh a type of model that's very

56
00:03:06.029 --> 00:03:09.220
common in, in some sciences and slightly less common

57
00:03:09.229 --> 00:03:12.600
in other sciences are formal models. And a formal

58
00:03:12.610 --> 00:03:19.070
model is usually a mathematical or computational representation of

59
00:03:19.080 --> 00:03:23.009
some sort of decomposition of a system where you

60
00:03:23.020 --> 00:03:26.479
say OK, here are the relationships I think are

61
00:03:26.490 --> 00:03:29.380
there and these are the properties I think matter.

62
00:03:29.779 --> 00:03:35.630
And we can then use the formalism to examine

63
00:03:35.639 --> 00:03:40.369
the logically necessary consequences of our assumptions. And this

64
00:03:40.380 --> 00:03:43.399
allows us to test all sorts of things and,

65
00:03:43.410 --> 00:03:47.470
and really get a handle on how to describe

66
00:03:47.479 --> 00:03:51.110
theories and how to classify certain phenomena, how to

67
00:03:51.119 --> 00:03:55.229
find edge cases and, and when certain ideas or

68
00:03:55.240 --> 00:03:58.190
phenomena are likely to happen or unlikely to happen.

69
00:04:00.070 --> 00:04:03.509
So, I mean, of course, you, your book is

70
00:04:03.520 --> 00:04:06.960
focused on social phenomena. But just before we get

71
00:04:06.970 --> 00:04:12.039
into depth, when it comes to science more generally,

72
00:04:12.050 --> 00:04:14.050
I know that that this is a very broad

73
00:04:14.059 --> 00:04:18.399
question, but how do people know what they should

74
00:04:18.410 --> 00:04:22.309
include in the model? What they should exclude if

75
00:04:22.320 --> 00:04:26.684
they're studying a multifactorial food phenomenon? Which we're all,

76
00:04:26.695 --> 00:04:30.834
we're, I mean, almost always studying in social science,

77
00:04:31.135 --> 00:04:35.114
the factors that they should live in or the

78
00:04:35.125 --> 00:04:37.674
factors that they should consider the ones they should

79
00:04:37.684 --> 00:04:41.315
live out. I mean, how does this process go?

80
00:04:42.059 --> 00:04:44.869
Yeah, it's a really good question. Uh And it's

81
00:04:44.880 --> 00:04:48.040
a, it's a really important one, not just for

82
00:04:48.049 --> 00:04:51.079
building models, but just for, in generally, in general

83
00:04:51.089 --> 00:04:55.440
developing any sort of theory or hypothesis uh for,

84
00:04:55.450 --> 00:04:59.640
for what to test. Because ultimately any hypothesis is

85
00:04:59.649 --> 00:05:03.809
not about the whole world. It's about uh a

86
00:05:03.820 --> 00:05:08.859
deconstruction where we think certain parts of a system

87
00:05:08.869 --> 00:05:11.880
or a phenomenon are really important and it's about

88
00:05:11.890 --> 00:05:15.410
the relationships between those important things. And so any

89
00:05:15.420 --> 00:05:20.779
model is ultimately in service of a particular kind

90
00:05:20.790 --> 00:05:23.910
of question or idea. Um And I think this

91
00:05:23.920 --> 00:05:27.410
is something that's often missed. You can't just have

92
00:05:27.420 --> 00:05:31.029
a model of a system and say, oh, this

93
00:05:31.040 --> 00:05:33.630
is a model of a system, whether that model

94
00:05:33.640 --> 00:05:36.140
is a system, the system is like a city

95
00:05:36.149 --> 00:05:41.299
or a relationship or a group or an ecology

96
00:05:41.359 --> 00:05:45.100
or a species, uh you know, or, or, or,

97
00:05:45.109 --> 00:05:47.890
you know, a food web, you need to have

98
00:05:47.899 --> 00:05:55.440
a specific question or theory about some, some specific

99
00:05:55.450 --> 00:05:59.100
type of phenomenon or relationship. And you need to

100
00:05:59.109 --> 00:06:01.959
say these are the things in the world in

101
00:06:01.970 --> 00:06:04.000
this system that I think are important to my

102
00:06:04.010 --> 00:06:08.049
idea. I need to represent the people because my

103
00:06:08.059 --> 00:06:10.480
theory is about the people I need to represent

104
00:06:10.779 --> 00:06:14.279
uh their co-operative behavior because my theory is about

105
00:06:14.290 --> 00:06:16.559
their co-operative behavior. Do I need to represent their

106
00:06:16.570 --> 00:06:20.320
age or their sex or their wealth or their

107
00:06:20.329 --> 00:06:24.390
geographical location? Maybe if that's what my hypothesis is

108
00:06:24.399 --> 00:06:27.640
about or theory is about or I think that

109
00:06:28.140 --> 00:06:32.440
differences in those factors are gonna be really important

110
00:06:33.160 --> 00:06:36.470
and I have some idea about that. But if

111
00:06:36.480 --> 00:06:39.640
not, I, I'm probably gonna exclude them because I

112
00:06:39.649 --> 00:06:42.369
can't in include everything. A model is the, the,

113
00:06:42.380 --> 00:06:45.970
the value of a model is often about abstracting

114
00:06:45.980 --> 00:06:48.950
away all of the messiness of the real world

115
00:06:48.959 --> 00:06:51.119
and saying, OK, if we could ignore all those

116
00:06:51.130 --> 00:06:55.309
other sources of noise and interference and other factors,

117
00:06:55.470 --> 00:07:00.559
all things being equal. If this relationship and these

118
00:07:00.570 --> 00:07:02.260
properties that I'm including in the model were all

119
00:07:02.269 --> 00:07:08.420
there were what would happen. And uh this so,

120
00:07:08.529 --> 00:07:11.390
so this problem of this is sometimes called decomposition

121
00:07:11.399 --> 00:07:15.640
or sometimes called articulation of parts about figuring out

122
00:07:15.649 --> 00:07:17.170
what are the parts of the system that are

123
00:07:17.179 --> 00:07:19.940
in the model? And what are the properties of

124
00:07:19.950 --> 00:07:22.190
those parts that are important for the model is,

125
00:07:22.200 --> 00:07:25.660
is really at the core of model development. Um

126
00:07:26.149 --> 00:07:27.920
And for any given system, you can have many,

127
00:07:27.929 --> 00:07:30.779
many models that might apply because they apply to

128
00:07:30.790 --> 00:07:34.630
different kinds of questions and a correlator to that,

129
00:07:34.640 --> 00:07:37.670
is that different kinds of models or different kinds

130
00:07:37.679 --> 00:07:40.500
of decompositions allow you to ask different kinds of

131
00:07:40.510 --> 00:07:42.619
questions about a system.

132
00:07:44.290 --> 00:07:47.940
Yeah. And you mentioned the importance of theory there.

133
00:07:47.950 --> 00:07:51.640
Uh Since we're talking here about social science or

134
00:07:51.649 --> 00:07:54.339
we're going to talk about some social phenomena here.

135
00:07:54.350 --> 00:07:59.170
As I've already mentioned the fact that apparently in

136
00:07:59.179 --> 00:08:05.140
the social science there's no broadly agreed theoretical foundation,

137
00:08:05.329 --> 00:08:07.459
is that a problem here or

138
00:08:07.470 --> 00:08:13.700
not? I think it's a problem. Uh II I

139
00:08:13.799 --> 00:08:17.140
think the caveat to that is we should be

140
00:08:17.149 --> 00:08:20.769
careful about rushing into committing to any specific theory

141
00:08:21.109 --> 00:08:25.450
because of how hard the social sciences are. Um

142
00:08:25.940 --> 00:08:29.670
YOU know, uh there's a reason why some of

143
00:08:29.679 --> 00:08:33.058
the, the, the earliest formal sciences were things like

144
00:08:33.179 --> 00:08:36.450
the physics of, you know, moving bodies and, and

145
00:08:36.460 --> 00:08:40.450
astronomy where the number of moving parts is is

146
00:08:40.460 --> 00:08:45.429
very few. And the number of uh interferences can

147
00:08:45.440 --> 00:08:49.099
be small. And also the the different ways to

148
00:08:49.109 --> 00:08:52.039
conceptualize a system are fairly limited. Like if you

149
00:08:52.049 --> 00:08:55.729
have rock falling and you're just trying to say

150
00:08:55.739 --> 00:08:59.070
like, OK, uh falling on a on the surface

151
00:08:59.080 --> 00:09:01.729
of an earth of a planet, you have a

152
00:09:01.739 --> 00:09:03.619
big thing and a little thing that are attracted

153
00:09:03.630 --> 00:09:07.950
to each other with gravity done. And you have

154
00:09:07.960 --> 00:09:10.679
a theory about how forces work and gravitation work.

155
00:09:10.690 --> 00:09:12.630
Now that's not trivial. Those are hard things to

156
00:09:12.640 --> 00:09:14.830
do and it's a, it's a real achievement. But

157
00:09:15.840 --> 00:09:18.140
you think about social science and you think about

158
00:09:18.150 --> 00:09:22.460
how complicated an individual is and you can describe

159
00:09:22.469 --> 00:09:25.719
them at the, you know, molecular level, the physiological

160
00:09:25.729 --> 00:09:28.969
level, the cognitive level, the developmental level, they have

161
00:09:28.979 --> 00:09:33.210
goals and beliefs and motivations and social relationships and

162
00:09:33.219 --> 00:09:39.169
entanglements, they get cold, they get hungry and then

163
00:09:39.179 --> 00:09:41.469
you put them in a network of other people

164
00:09:42.340 --> 00:09:45.200
and then you realize that, you know, they're bound

165
00:09:45.210 --> 00:09:50.229
by norms and culture and physical infrastructure and ecology

166
00:09:50.770 --> 00:09:58.469
and expectations and risk management. And it's just crazy

167
00:09:58.479 --> 00:10:01.039
complicated just in terms of the number of ways

168
00:10:01.049 --> 00:10:06.770
you can describe anything. Um But a problem when

169
00:10:06.780 --> 00:10:09.820
we don't have any formal theory, we have lots

170
00:10:09.830 --> 00:10:12.409
of theories. So theory, this is a well worn

171
00:10:12.419 --> 00:10:14.429
thing like uh I, I feel like I don't

172
00:10:14.440 --> 00:10:16.489
even, I almost don't need to repeat this because

173
00:10:16.500 --> 00:10:18.059
it's been said so many times. Or maybe I

174
00:10:18.070 --> 00:10:20.369
do that, you know, theory, the word theory in

175
00:10:20.380 --> 00:10:24.179
the sciences means something different than the colloquial meaning

176
00:10:24.190 --> 00:10:26.179
of the word theory. It's not just sort of

177
00:10:26.190 --> 00:10:31.179
any idea, but it's, it's usually a set of

178
00:10:31.190 --> 00:10:42.229
constraints on hypothesis generation that is well justified. So

179
00:10:43.880 --> 00:10:46.369
why do I make a theory? Well, I have

180
00:10:46.380 --> 00:10:50.619
a theory and I can therefore many hypotheses or

181
00:10:50.630 --> 00:10:54.140
predictions follow from that theory because what it is

182
00:10:54.150 --> 00:10:57.039
is the set of assumptions that will constrain all

183
00:10:57.049 --> 00:11:02.010
hypotheses that I generate about some system. And when

184
00:11:02.020 --> 00:11:05.500
we don't have a clear theory, so there's many,

185
00:11:05.510 --> 00:11:10.539
many, there everyone's, there's a joke among social psychologists

186
00:11:10.549 --> 00:11:12.479
which is that in that field, theories are like

187
00:11:12.489 --> 00:11:15.700
toothbrushes, right? Everyone's got to have one but you

188
00:11:15.710 --> 00:11:21.299
don't want to use anyone else's. Um This is

189
00:11:21.309 --> 00:11:24.400
a AAA large problem for several reasons. One it

190
00:11:24.409 --> 00:11:29.580
makes collaboration and coordinating between uh researchers really hard.

191
00:11:29.590 --> 00:11:33.359
It means um that theories stated this way are

192
00:11:33.369 --> 00:11:36.179
really hard to falsify. If they're just stated in

193
00:11:36.190 --> 00:11:40.760
words, words are ambiguous, they're vague. You can say,

194
00:11:40.770 --> 00:11:43.799
well, that's not what I mean. If somebody says,

195
00:11:43.809 --> 00:11:45.599
oh, it seems like this result doesn't fit your

196
00:11:45.609 --> 00:11:47.070
theory and you can say, well, that's not what

197
00:11:47.080 --> 00:11:50.320
I meant. Um And it means that the theories

198
00:11:50.330 --> 00:11:54.169
are kind of either they're an up or down

199
00:11:54.179 --> 00:11:57.119
situation, right? If I just say a theory in

200
00:11:57.130 --> 00:12:02.340
words and say, I think that the reason that

201
00:12:03.200 --> 00:12:07.020
uh people have social identities is because they want

202
00:12:07.030 --> 00:12:10.039
a feeling of belongingness, but also they need to

203
00:12:10.049 --> 00:12:13.859
stand out and these things oppose each other. Um

204
00:12:14.510 --> 00:12:19.700
There's a OK, a the there's a million ways

205
00:12:19.710 --> 00:12:23.500
to formalize that to say, you know, what does

206
00:12:23.510 --> 00:12:27.000
or doesn't count as fitting that theory. And two,

207
00:12:28.440 --> 00:12:29.909
I think I just said A and two, but

208
00:12:29.919 --> 00:12:34.489
it's fine. Um The second point is that it

209
00:12:34.500 --> 00:12:36.270
makes it really hard to build on the theory

210
00:12:36.280 --> 00:12:40.169
to extend it to say, well, OK, what about

211
00:12:40.179 --> 00:12:42.590
uh maybe that there are versions of this theory

212
00:12:42.599 --> 00:12:45.250
that hold only in certain conditions and not others.

213
00:12:45.390 --> 00:12:48.479
This goes along with, let's say another theory about

214
00:12:48.489 --> 00:12:52.690
social organization or network structure or cooper operation. And

215
00:12:52.700 --> 00:12:57.309
how do these different factors fit together and when

216
00:12:57.320 --> 00:12:59.630
they're verbal and they just sort of written out

217
00:12:59.640 --> 00:13:02.320
and talked about as the way that sort of

218
00:13:02.330 --> 00:13:06.859
classical social science theory often does. It makes them

219
00:13:06.869 --> 00:13:09.619
hard to not only falsify but also extend and

220
00:13:09.630 --> 00:13:14.200
build upon and so formalizing things by saying, all

221
00:13:14.210 --> 00:13:16.799
right, we gotta write down some math, write down

222
00:13:16.809 --> 00:13:20.640
some computational algorithms to say here's what we're talking

223
00:13:20.650 --> 00:13:26.809
about when I'm talking about co-operation or norms or

224
00:13:26.820 --> 00:13:31.789
contagion or you know, identity, whatever it is, I'm

225
00:13:31.799 --> 00:13:35.520
gonna talk about it like this, exactly this relationship.

226
00:13:35.530 --> 00:13:38.419
And therefore, I can say I can a, I

227
00:13:38.429 --> 00:13:42.010
can derive exact conditions when some phenomenon will or

228
00:13:42.020 --> 00:13:45.989
won't happen. And also if you object, you can

229
00:13:46.000 --> 00:13:51.450
say exactly whether or not the real world has

230
00:13:51.460 --> 00:13:55.659
uh conditions that meet my assumptions or don't because

231
00:13:55.669 --> 00:13:58.049
I've specified my assumptions really precisely.

232
00:14:00.159 --> 00:14:04.299
So in the book, you focus on two different

233
00:14:04.309 --> 00:14:10.320
kinds of models, mathematical models and agent based models.

234
00:14:10.330 --> 00:14:12.500
But could you tell us the difference there?

235
00:14:13.940 --> 00:14:20.549
Sure. Um I mean, the difference is, is not

236
00:14:20.559 --> 00:14:24.059
so stark as some people like to think. So

237
00:14:24.070 --> 00:14:27.869
mathematical models is a model that you can write

238
00:14:27.880 --> 00:14:31.340
down with mathematical equations. So if I have a

239
00:14:31.349 --> 00:14:35.900
model of um you know, uh predator prey dynamics,

240
00:14:35.909 --> 00:14:38.510
and I can write down an equation for the,

241
00:14:39.400 --> 00:14:42.190
the ch the rate of change in the population

242
00:14:42.200 --> 00:14:45.119
of the prey species as a function of its

243
00:14:45.130 --> 00:14:49.580
current population and also the population of the predators.

244
00:14:49.750 --> 00:14:51.109
And I can do the same thing for the

245
00:14:51.119 --> 00:14:54.429
rate of change of the predators. I can therefore

246
00:14:54.609 --> 00:14:57.929
work out the dynamics of a system, how it's

247
00:14:57.940 --> 00:15:02.270
cycling, whether it's likely to collapse, et cetera. And

248
00:15:02.280 --> 00:15:05.229
these, and I can work out these equations, equations

249
00:15:05.239 --> 00:15:08.479
uh can often be for uh solved in a

250
00:15:08.489 --> 00:15:11.570
sort of closed form case, which just means I

251
00:15:11.580 --> 00:15:15.429
can get exact conditions for what will happen at

252
00:15:15.440 --> 00:15:17.429
any time in the future. Or I can get

253
00:15:17.440 --> 00:15:20.869
exact conditions when such a result will or won't

254
00:15:20.880 --> 00:15:25.549
happen. So like when will one ST behavioral strategy

255
00:15:25.559 --> 00:15:32.739
outcompete another strategy? Um AND they're really powerful, but

256
00:15:32.770 --> 00:15:37.539
sometimes they're limiting because they tend to treat um

257
00:15:37.549 --> 00:15:41.679
they, they'll use terms mathematical variables to represent, let's

258
00:15:41.690 --> 00:15:45.140
say entire classes of individuals or entire species or

259
00:15:45.150 --> 00:15:49.580
subspecies or types or strategies or whatever. So um

260
00:15:50.169 --> 00:15:53.469
they force you to assume a certain amount of

261
00:15:53.479 --> 00:15:57.770
homogeneity or sameness in a population. Um One way

262
00:15:57.780 --> 00:16:01.000
to get around this is to build agent based

263
00:16:01.010 --> 00:16:05.169
models. Now, these are computational models. They're like little

264
00:16:05.179 --> 00:16:08.890
computer simulations where instead of having one term for

265
00:16:08.900 --> 00:16:11.570
a whole, every member of a population just saying

266
00:16:11.580 --> 00:16:14.390
like how many of them are there? Um OR

267
00:16:14.400 --> 00:16:18.989
what's their average resources, et cetera, you can stimulate

268
00:16:19.409 --> 00:16:24.650
every single individual in a population independently and you

269
00:16:24.659 --> 00:16:27.969
can put them in a spatial a spatial layout

270
00:16:27.979 --> 00:16:29.729
where they can move around. You could put them

271
00:16:29.739 --> 00:16:32.479
in a network where they can interact with their

272
00:16:32.489 --> 00:16:35.690
network neighbors, you could put them in a geographical

273
00:16:35.700 --> 00:16:38.530
space, you could upload a map and have them

274
00:16:38.539 --> 00:16:41.820
literally move around a map and response to that.

275
00:16:41.830 --> 00:16:44.190
You could have them have vision where they can

276
00:16:44.200 --> 00:16:47.530
see each other and run away. Um There's a

277
00:16:47.539 --> 00:16:51.679
lot of potential for this and um the downside

278
00:16:51.869 --> 00:16:55.169
relative to mathematical models is that you lose a

279
00:16:55.179 --> 00:16:58.929
certain amount of simple tractability with the math model.

280
00:16:58.940 --> 00:17:00.789
If you can solve it correctly, all you have

281
00:17:00.799 --> 00:17:03.630
to do is plug in new numbers for your

282
00:17:03.640 --> 00:17:06.400
variables. And you've got a solution right away with

283
00:17:06.410 --> 00:17:08.880
the agent based models. You, you literally have to

284
00:17:08.890 --> 00:17:12.410
simulate the whole thing for every, every time, for

285
00:17:12.420 --> 00:17:14.739
every new value you're testing and often you have

286
00:17:14.750 --> 00:17:16.959
to do it many times because there's often a

287
00:17:16.969 --> 00:17:21.239
lot of stochastic or randomness involved. So the same

288
00:17:21.329 --> 00:17:24.989
sort of parameters might lead you to different outcomes

289
00:17:25.000 --> 00:17:28.479
just based on like exactly where the agents started

290
00:17:28.489 --> 00:17:30.709
or where they are in relationship to each other

291
00:17:30.719 --> 00:17:35.449
or what properties they have. Um But I would

292
00:17:35.459 --> 00:17:38.949
say they're not, they're not competing approaches, they're very

293
00:17:38.959 --> 00:17:43.479
much complementary approaches. Um WHERE it's nice you get

294
00:17:43.489 --> 00:17:45.609
as far as you can with math because it's,

295
00:17:45.619 --> 00:17:49.150
it's nice to prove things. Um And there's, it's

296
00:17:49.160 --> 00:17:51.170
also just, there's a certain elegance for those of

297
00:17:51.180 --> 00:17:54.469
us who like math. Um But it's also you

298
00:17:54.479 --> 00:17:56.359
can, you can do a lot of exploration with

299
00:17:56.369 --> 00:17:58.869
agent based models. So they're pretty, pretty cool. They

300
00:17:58.880 --> 00:18:02.380
often also can generate pretty neat visualizations. So they're

301
00:18:02.390 --> 00:18:06.839
fun to watch. So like scientists in all uh

302
00:18:06.849 --> 00:18:09.589
walks away of use agent based models. But of

303
00:18:09.599 --> 00:18:12.109
course, like if you don't or if you're not

304
00:18:12.119 --> 00:18:14.290
familiar with their use in science. I'm sure you're

305
00:18:14.300 --> 00:18:17.160
familiar with their use in, let's say video games

306
00:18:17.170 --> 00:18:20.829
or movies, right? Like any time you see like

307
00:18:20.839 --> 00:18:24.319
a flock of birds or a school of fish

308
00:18:24.329 --> 00:18:26.979
or you know, a stampeding herd in a, in

309
00:18:26.989 --> 00:18:31.089
a video game or a movie. Um The dynamics

310
00:18:31.099 --> 00:18:35.410
of those individuals are probably an come from an

311
00:18:35.420 --> 00:18:37.890
aging based model that then this have pictures drawn

312
00:18:37.900 --> 00:18:40.250
on top or, or imposed on top. So like

313
00:18:40.439 --> 00:18:42.810
um like the Lord of the Rings, those giant

314
00:18:42.819 --> 00:18:45.489
battle scenes with like 10,000 orcs and stuff like

315
00:18:45.500 --> 00:18:49.880
those long shots, they're not, they didn't have 10,000

316
00:18:49.890 --> 00:18:53.709
extras in costumes and they also didn't have computer

317
00:18:53.719 --> 00:18:57.709
programmers program every single individual for what they did.

318
00:18:57.800 --> 00:18:59.869
They built an agent based model and they coded

319
00:18:59.880 --> 00:19:01.530
in some rules for behavior and they let it

320
00:19:01.540 --> 00:19:02.020
go.

321
00:19:03.839 --> 00:19:07.660
So in modeling, I hear people talk a lot

322
00:19:07.670 --> 00:19:13.109
about fine grained and coarse grained models. What are

323
00:19:13.119 --> 00:19:16.979
the main differences there? And is it that one

324
00:19:16.989 --> 00:19:20.099
is better than the other or does it depend

325
00:19:20.109 --> 00:19:24.819
on the context or the specific case we're studying?

326
00:19:28.109 --> 00:19:31.229
OK. Yeah. So I would say this is a,

327
00:19:31.239 --> 00:19:34.560
the distinction between fine grain and, and coarse grains

328
00:19:34.650 --> 00:19:37.199
and modeling is, is very much sort of a

329
00:19:37.209 --> 00:19:41.510
continuum rather than a stark difference. And sort of

330
00:19:41.520 --> 00:19:43.130
you could think of it as like the coarseness

331
00:19:43.140 --> 00:19:46.449
or fineness of, of the model. And, and this

332
00:19:46.459 --> 00:19:48.560
just has to do with the kind of precision

333
00:19:48.569 --> 00:19:54.180
of the representation. So for example, um in a

334
00:19:54.189 --> 00:19:57.040
lot of the, the physical sciences, we can have

335
00:19:57.050 --> 00:19:58.930
really, it's very easy to have very fine green

336
00:19:58.939 --> 00:20:02.349
models. Uh If you're modeling, you know, a pendulum

337
00:20:02.359 --> 00:20:06.040
or you're modeling, uh I don't know the, the

338
00:20:06.050 --> 00:20:09.339
structural integrity of a, of a bridge or you're

339
00:20:09.349 --> 00:20:17.260
modeling. Um The uh let's say that the rate

340
00:20:17.270 --> 00:20:21.910
of neural spike trains, these can be really precise,

341
00:20:21.920 --> 00:20:24.930
they're fine grain in the sense that the mapping

342
00:20:24.939 --> 00:20:28.209
between the real world, the phenomenon that you're measuring.

343
00:20:28.550 --> 00:20:33.780
And uh the model itself is al almost perfectly

344
00:20:33.790 --> 00:20:36.869
1 to 1. And you know, I I like

345
00:20:36.880 --> 00:20:39.319
to say like in a lot of physics models,

346
00:20:39.329 --> 00:20:41.170
one of the things that's noteworthy is like the

347
00:20:41.180 --> 00:20:44.079
terms in the models, all have units, they are

348
00:20:44.089 --> 00:20:47.930
all quantities that are, that are the measurements. So

349
00:20:47.939 --> 00:20:51.859
you are measuring charge, you're measuring voltage, you're measuring

350
00:20:51.869 --> 00:20:58.489
mass. Um And so your model is about the

351
00:20:58.500 --> 00:21:04.849
predicted relationships between these measured quantities. Um That's a

352
00:21:04.859 --> 00:21:07.030
fine grained model. And we can sometimes do that

353
00:21:07.040 --> 00:21:10.670
in the social sciences for um for situations where

354
00:21:10.680 --> 00:21:14.699
we have a strong enough theory about the kinds

355
00:21:14.709 --> 00:21:17.430
of things that are likely to happen and b

356
00:21:17.439 --> 00:21:22.150
good enough data so that we can plug in

357
00:21:22.160 --> 00:21:28.680
enough um constraints for the system. So in systems

358
00:21:28.689 --> 00:21:34.329
like urban dynamics, traffic models or um evacuation models,

359
00:21:34.339 --> 00:21:36.829
so people will put in like a map of

360
00:21:36.839 --> 00:21:38.930
a of a, of a street grid or a

361
00:21:38.939 --> 00:21:42.479
building and they'll put in car agents or human

362
00:21:42.489 --> 00:21:45.760
agents trying to navigate the streets or get out

363
00:21:45.770 --> 00:21:49.449
of the building so that they're, they're really trying

364
00:21:49.459 --> 00:21:51.790
to see, you know, how people are moving and,

365
00:21:51.800 --> 00:21:54.150
and what different assumptions about the layouts of the,

366
00:21:54.160 --> 00:21:57.459
this exact street or building, how they change things.

367
00:21:57.469 --> 00:21:59.839
What can allow people to escape the building more

368
00:21:59.849 --> 00:22:02.640
easily? What allows traffic flow to, to happen more

369
00:22:02.650 --> 00:22:08.030
easily? Um Certain epidemic models are like this. If

370
00:22:08.040 --> 00:22:11.750
um you have really good information about how a

371
00:22:11.760 --> 00:22:15.329
disease works, you know how contagious it is, you

372
00:22:15.339 --> 00:22:19.160
know how uh what the time course is between

373
00:22:19.170 --> 00:22:21.515
when you get infected and when you're contagious and

374
00:22:21.525 --> 00:22:23.994
how likely you are to recover at a given

375
00:22:24.005 --> 00:22:25.875
time or, or what the death rate is, you

376
00:22:25.885 --> 00:22:30.474
know, where people are in physical space and how

377
00:22:30.484 --> 00:22:32.935
often they tend to interact with each other depending

378
00:22:32.944 --> 00:22:34.484
on where they are. You have to know a

379
00:22:34.494 --> 00:22:38.224
lot to have a really fine grained model. Um

380
00:22:39.089 --> 00:22:41.589
And often in the social sciences, we don't have

381
00:22:41.599 --> 00:22:44.530
that level of fine grain information both in terms

382
00:22:44.540 --> 00:22:48.709
of data and in terms of theory. Um BUT

383
00:22:48.719 --> 00:22:50.790
that doesn't mean models aren't useful. We can have

384
00:22:50.800 --> 00:22:53.680
more coarse grain models where instead of saying we

385
00:22:53.689 --> 00:22:55.819
want to know exactly what is going to happen

386
00:22:55.829 --> 00:22:59.050
in this exact system, we can pull back and

387
00:22:59.060 --> 00:23:01.400
say become a little more abstract and say, what

388
00:23:01.410 --> 00:23:06.060
kinds of things will tend to happen when systems

389
00:23:06.069 --> 00:23:11.250
are kind of like this? And that's actually extremely

390
00:23:11.260 --> 00:23:13.920
useful. And and I would say most modeling in

391
00:23:13.930 --> 00:23:15.920
the social sciences of this latter type.

392
00:23:17.209 --> 00:23:20.750
So before we get into some of the specific

393
00:23:20.760 --> 00:23:24.540
examples of social dynamics that you explore in your

394
00:23:24.550 --> 00:23:28.500
book, let me just ask you one more general

395
00:23:28.510 --> 00:23:34.310
question. So are there any general assumptions about human

396
00:23:34.319 --> 00:23:38.229
psychology that we should keep in mind when studying

397
00:23:38.290 --> 00:23:43.079
social phenomena? And if so where would they come

398
00:23:43.089 --> 00:23:46.119
from? I mean, because there are many different branches

399
00:23:46.130 --> 00:23:49.400
of psychology, would it be, I don't know, social

400
00:23:49.410 --> 00:23:56.650
psychology, developmental, psychology, evolutionary psychology. Uh How does this

401
00:23:56.660 --> 00:23:57.599
work exactly?

402
00:23:59.270 --> 00:24:03.209
I like this question. Um I, so, you know,

403
00:24:03.219 --> 00:24:06.890
my background like in academia is, is kind of

404
00:24:06.900 --> 00:24:10.109
varied and I, I started out as an undergrad

405
00:24:10.119 --> 00:24:13.229
in physics and then I went and studied psychology

406
00:24:13.239 --> 00:24:16.479
and my graduate degrees are on psychology. And then

407
00:24:16.560 --> 00:24:19.170
uh I did um three postdocs all in the

408
00:24:19.180 --> 00:24:23.770
social sciences working with economists and political scientists and,

409
00:24:23.780 --> 00:24:30.660
and anthropologists. And um and I think that this

410
00:24:30.670 --> 00:24:33.250
is the reason III I bring this up is

411
00:24:33.260 --> 00:24:37.329
because um I, I think that a lot of

412
00:24:38.510 --> 00:24:46.189
caution should be exercised in um taking theories of

413
00:24:46.199 --> 00:24:52.420
psychology at face value. Um Because what I found

414
00:24:52.430 --> 00:24:55.380
is is a lot of psychological theories, especially ones

415
00:24:55.390 --> 00:24:58.359
that, that haven't engaged with the uh the social

416
00:24:58.369 --> 00:25:01.920
sciences or sciences that involve populations often tend to

417
00:25:01.930 --> 00:25:05.680
focus on kind of phenomenology, the inner life of

418
00:25:05.689 --> 00:25:09.459
an individual, what it feels like what it, what

419
00:25:09.469 --> 00:25:13.520
would make one happy or satisfied or, or uh

420
00:25:13.530 --> 00:25:16.280
unhappy and these things are important and they're valuable

421
00:25:16.930 --> 00:25:20.310
and they're important to think about. Um But in

422
00:25:20.319 --> 00:25:27.520
the social world, ultimately, what psychology is for is

423
00:25:28.329 --> 00:25:34.400
to produce adaptive behavior and to inhibit non adaptive

424
00:25:34.410 --> 00:25:41.050
behavior. And uh you know, not every single behavior

425
00:25:41.060 --> 00:25:43.949
is obviously adaptive, right? Because we, we all make

426
00:25:43.959 --> 00:25:46.160
mistakes. The world is complicated. There's a ton of

427
00:25:46.170 --> 00:25:50.270
uncertainty, we make a million, uh we we there

428
00:25:50.540 --> 00:25:54.199
million behavioral choices every day, day to day. Um

429
00:25:54.209 --> 00:25:58.829
But in overall, I think the features of psychology

430
00:25:59.290 --> 00:26:03.500
that um are involved in behavior, which is all

431
00:26:03.510 --> 00:26:07.229
features of psychology are involved in behavior, um are

432
00:26:07.239 --> 00:26:11.410
for something, right? They, they have an instrumental purpose.

433
00:26:11.849 --> 00:26:16.739
And so to me, the, I think about psychology

434
00:26:16.750 --> 00:26:19.209
a lot, I mean, I'm in the cognitive science

435
00:26:19.219 --> 00:26:21.689
department. We, we think about psychology quite a bit,

436
00:26:21.699 --> 00:26:26.239
but I think ultimately I'm interested in, in adaptive

437
00:26:26.250 --> 00:26:29.000
behavior and in and in non adaptive behavior. I'm

438
00:26:29.010 --> 00:26:32.199
interested in, in social phenomenon. And so I guess,

439
00:26:32.209 --> 00:26:36.150
I think of psychology in terms of the kinds

440
00:26:36.160 --> 00:26:44.170
of behavioral strategies that um psychology will produce. And

441
00:26:44.180 --> 00:26:46.380
I guess maybe I'm, I'm like a little bit

442
00:26:46.390 --> 00:26:49.280
of a behaviorist in this uh sense, although I'm

443
00:26:49.290 --> 00:26:52.520
not uh like a radical behaviorist denying uh that

444
00:26:52.530 --> 00:26:55.550
we can study cognitive processes. But I think ultimately

445
00:26:55.560 --> 00:26:58.989
what matters is the behavior and s and study

446
00:26:59.000 --> 00:27:02.800
of cognition and psychology is ultimately in service of

447
00:27:02.810 --> 00:27:04.109
understanding behavior,

448
00:27:05.229 --> 00:27:10.650
right? So what is a contagion? I mean, o

449
00:27:10.660 --> 00:27:13.520
of course, uh I'm not asking you here to

450
00:27:13.530 --> 00:27:16.300
tell us about contagion in the sort of public

451
00:27:16.310 --> 00:27:22.640
health sense, but when studying social dynamics specifically, what

452
00:27:22.650 --> 00:27:24.609
does contagion mean?

453
00:27:25.989 --> 00:27:30.630
Yeah. Um So in the book, um after a

454
00:27:30.640 --> 00:27:33.310
couple chapters of sort of philosophy of modeling and

455
00:27:33.319 --> 00:27:36.589
introduction to modeling and, and um playing around with

456
00:27:36.599 --> 00:27:39.430
tools and learning some software techniques and, and making

457
00:27:39.439 --> 00:27:42.489
some, some agents move around. Uh I have a

458
00:27:42.500 --> 00:27:45.859
number of chapters, each of which goes through kind

459
00:27:45.869 --> 00:27:50.510
of AAA modeling topic or, or tradition to study

460
00:27:50.520 --> 00:27:52.599
a sort of broad topic. And, and yeah, one

461
00:27:52.609 --> 00:27:58.130
of these topics is uh contagion models. Um So

462
00:27:58.140 --> 00:28:01.750
you said you're not talking about disease dynamics and,

463
00:28:01.760 --> 00:28:07.719
and epidemiology, but um contagion models really come from

464
00:28:07.729 --> 00:28:10.890
that and, and uh they come out of a,

465
00:28:10.900 --> 00:28:14.420
of an analogy that's been drawn between the way

466
00:28:14.430 --> 00:28:17.989
that a disease is transmitted from one person to

467
00:28:18.000 --> 00:28:24.300
another by physical contact and the way information is

468
00:28:24.310 --> 00:28:27.589
transmitted from one person to another through either direct

469
00:28:27.599 --> 00:28:30.689
contact, face to face or some other kind of

470
00:28:30.699 --> 00:28:33.800
communication. Like now we have phones and internets and

471
00:28:33.810 --> 00:28:37.699
et cetera. Um But in general, it's the idea

472
00:28:37.709 --> 00:28:42.099
that you have something and the fact that you

473
00:28:42.109 --> 00:28:45.339
have something means you can give that thing to

474
00:28:45.349 --> 00:28:49.729
someone else without you losing the thing. And that's,

475
00:28:49.739 --> 00:28:55.579
that's contagion, right? Um And so, um behavioral scientists

476
00:28:55.589 --> 00:28:58.380
and social scientists since the 19 sixties have been

477
00:28:58.390 --> 00:29:05.699
using contagion models to think about how innovations and

478
00:29:05.709 --> 00:29:12.699
products and behaviors and norms uh diffused through populations.

479
00:29:13.390 --> 00:29:15.719
And we find, you know, for a lot of

480
00:29:15.729 --> 00:29:20.699
these behaviors and norms, products, et cetera, the trajectories

481
00:29:20.780 --> 00:29:24.050
um in a population look very much like the

482
00:29:24.060 --> 00:29:27.099
trajectory of a disease dynamic, especially diseases that are

483
00:29:27.109 --> 00:29:32.099
transmitted through direct contact. And so we can think

484
00:29:32.109 --> 00:29:35.160
about it in this way and therefore build models

485
00:29:35.170 --> 00:29:39.880
that involve things like um how transmissible is a

486
00:29:39.890 --> 00:29:42.780
a contagion. How likely if you meet someone who's

487
00:29:42.790 --> 00:29:46.250
got someone who's adopted a product or behavior or

488
00:29:46.260 --> 00:29:48.550
a disease, how likely are they to transmit the

489
00:29:48.560 --> 00:29:51.660
thing to you? What's the contact rates or how

490
00:29:51.670 --> 00:29:54.819
often are people interacting with the same people or

491
00:29:54.829 --> 00:29:58.760
different people? Um Is there a kind of recovery

492
00:29:58.770 --> 00:30:01.430
or dis adoption if you adopt the thing? Like

493
00:30:01.439 --> 00:30:02.890
is that just it, do you just have it

494
00:30:02.900 --> 00:30:05.959
forever? Do you give it up? Do you get,

495
00:30:05.969 --> 00:30:07.839
you know, just like if you get better from

496
00:30:07.849 --> 00:30:11.540
a sickness, um if you get better, can you

497
00:30:11.550 --> 00:30:16.119
get it again? And so depending on different, there

498
00:30:16.130 --> 00:30:18.250
are different dynamics depending on your answers to these

499
00:30:18.260 --> 00:30:20.969
questions. But we can look at things like OK,

500
00:30:20.979 --> 00:30:23.119
well, what about like the structure of a population,

501
00:30:23.130 --> 00:30:27.369
how clustered a network is how um how much

502
00:30:27.380 --> 00:30:31.635
do people interact? Are there, you know, uh certain

503
00:30:31.645 --> 00:30:36.494
social structures like identity or a version to adopting

504
00:30:36.505 --> 00:30:40.224
something associated with an out group that might affect

505
00:30:40.234 --> 00:30:44.074
the contagion dynamics. Um There are certain contagions that

506
00:30:44.084 --> 00:30:49.594
maybe uh don't work just like diseases. Um In

507
00:30:49.645 --> 00:30:53.405
sociology, this is sometimes called complex contagion. And in

508
00:30:53.415 --> 00:30:56.474
cultural evolution is just called frequency dependence, uh social

509
00:30:56.484 --> 00:30:59.589
learning. But the idea is the same, which is

510
00:30:59.599 --> 00:31:04.069
that um I'm more likely to adopt something if

511
00:31:04.079 --> 00:31:09.329
it's uh if it's exhibited by multiple people. So

512
00:31:09.469 --> 00:31:15.430
for disease, there's no difference between interacting with one

513
00:31:15.439 --> 00:31:18.239
sick person, 10 times and 10 sick people one

514
00:31:18.250 --> 00:31:21.900
time each, right? That's for disease transmission. That would

515
00:31:21.910 --> 00:31:25.619
be the same. But in social contagion, that's not

516
00:31:25.630 --> 00:31:29.260
the same, right? Because you can, you have one

517
00:31:29.270 --> 00:31:32.640
friend who's doing something crazy and they're just like,

518
00:31:32.650 --> 00:31:35.560
I've got this new con like I, I've been

519
00:31:35.569 --> 00:31:38.359
smoking banana peels and it makes me feel awesome.

520
00:31:38.449 --> 00:31:40.640
You gotta try it and you could be like,

521
00:31:40.650 --> 00:31:43.680
that's great. I'm really happy for you. But like

522
00:31:43.689 --> 00:31:46.219
you go, you do you but I'm not gonna

523
00:31:46.229 --> 00:31:48.520
do that and they could visit you every day

524
00:31:48.530 --> 00:31:51.160
with their pipe full of dried banana peels, smoking

525
00:31:51.170 --> 00:31:53.359
it and then it's never going to convince you.

526
00:31:53.810 --> 00:31:56.989
But if, if every single friend you see in

527
00:31:57.000 --> 00:32:00.229
a week is also smoking banana peels. Now you're

528
00:32:00.239 --> 00:32:03.770
thinking, oh, maybe there's something to this. I mean,

529
00:32:03.780 --> 00:32:06.229
maybe you're thinking, I need new friends, but maybe

530
00:32:06.239 --> 00:32:10.829
you're thinking, you know, oh maybe there's a reason

531
00:32:10.839 --> 00:32:12.910
why all these people I know have done this.

532
00:32:12.920 --> 00:32:16.339
Maybe I should try it so we can put

533
00:32:16.349 --> 00:32:18.319
these kinds of assumptions into models. And it turns

534
00:32:18.329 --> 00:32:20.349
out that the dynamics end up being a little

535
00:32:20.359 --> 00:32:26.069
different. Like for example, um simple contagions that just

536
00:32:26.079 --> 00:32:30.079
require an individual to, to, to spread them, 11

537
00:32:30.089 --> 00:32:34.030
contact, um they just spread the more connected a

538
00:32:34.040 --> 00:32:38.660
network is. And uh if you make lots of,

539
00:32:38.670 --> 00:32:41.560
if you have connections between different parts of a

540
00:32:41.569 --> 00:32:43.699
network, like there's a big mix, you're likely to

541
00:32:43.709 --> 00:32:45.560
interact with lots of different people all the time.

542
00:32:46.290 --> 00:32:49.349
That's a great way to spread contagions like this

543
00:32:49.359 --> 00:32:53.849
really fast, really quickly. But if it's a complex

544
00:32:53.859 --> 00:32:57.510
contagion or you need this sort of extra influence,

545
00:32:57.520 --> 00:33:03.270
there's frequency dependence, consensus bias, sometimes it's called um

546
00:33:04.900 --> 00:33:06.949
then it's not, that's not gonna be the same,

547
00:33:06.959 --> 00:33:11.569
right? Then actually more tight knit clustered networks where

548
00:33:11.930 --> 00:33:15.670
groups of people that all know each other are,

549
00:33:15.680 --> 00:33:17.709
are around, right? That's going to be much more

550
00:33:17.719 --> 00:33:19.739
effective than a network where lots of people know

551
00:33:19.750 --> 00:33:24.099
lots of other people, but your friends don't necessarily

552
00:33:24.109 --> 00:33:26.390
happen to be friends with each other. That's a

553
00:33:26.400 --> 00:33:30.199
great situation for spreading a simple contagion but not

554
00:33:30.209 --> 00:33:34.920
a complex contagion. So we can build models. Um

555
00:33:35.010 --> 00:33:37.500
Like this, the last thing I'll mention is that

556
00:33:37.790 --> 00:33:39.430
I, you know, I talked about disease models. And

557
00:33:39.439 --> 00:33:42.469
I talked about behavioral models and as if they're

558
00:33:42.479 --> 00:33:46.260
separate things, but actually a big trend in public

559
00:33:46.270 --> 00:33:49.130
health recently in the last couple of decades has

560
00:33:49.140 --> 00:33:53.449
been combining these things because of course, um the

561
00:33:53.459 --> 00:33:57.760
transmission of diseases is influenced by actual social behavior

562
00:33:57.770 --> 00:34:02.260
and beliefs. Right? So if you, for example, believe

563
00:34:02.270 --> 00:34:05.339
that it's really important to get vaccinated or it's

564
00:34:05.349 --> 00:34:07.770
really important to wear a mask or it's really

565
00:34:07.780 --> 00:34:12.310
important to stay inside. Um WHEN you know, someone's

566
00:34:12.320 --> 00:34:16.739
sick, that's going to affect uh whether or not

567
00:34:16.750 --> 00:34:19.719
how, how a disease spreads. But of course, those

568
00:34:19.728 --> 00:34:23.219
beliefs can also be transmitted socially from person to

569
00:34:23.228 --> 00:34:26.830
person. And so we see in exploring these ideas,

570
00:34:26.840 --> 00:34:29.120
we've, we've seen the rise of what's sometimes called

571
00:34:29.129 --> 00:34:32.179
coupled contagion models. So there's two contagions, there's the

572
00:34:32.188 --> 00:34:36.300
disease contagion and a behavioral contagion and looking at

573
00:34:36.310 --> 00:34:37.659
how these things interact

574
00:34:38.610 --> 00:34:42.250
when it comes to innovation specifically, do we have

575
00:34:42.260 --> 00:34:46.330
a good understanding of what are perhaps the factors

576
00:34:46.340 --> 00:34:50.679
or the circumstances that facilitate the spread or the

577
00:34:50.688 --> 00:34:52.600
diffusion of innovations?

578
00:34:54.428 --> 00:34:57.259
That's a good question. And it really depends on

579
00:34:57.269 --> 00:35:01.428
the kinds of innovations and um how easy they

580
00:35:01.438 --> 00:35:05.198
are to be adopted by an individual. So, for

581
00:35:05.208 --> 00:35:09.659
example, you know, an innovation that's, you don't need

582
00:35:09.669 --> 00:35:12.179
any extra things to get to, to get the

583
00:35:12.189 --> 00:35:16.879
benefit from those things spread really easily. Um There

584
00:35:16.889 --> 00:35:19.649
are certain things that might involve like it doesn't

585
00:35:19.659 --> 00:35:21.729
benefit me unless lots of people also have the

586
00:35:21.739 --> 00:35:25.600
innovation. So like a classic in the olden days,

587
00:35:25.610 --> 00:35:28.659
right, when like fax machines were new, like the

588
00:35:28.669 --> 00:35:30.760
first people who had fax machines, which is like,

589
00:35:30.770 --> 00:35:33.129
you could send like an image of a, on

590
00:35:33.139 --> 00:35:34.969
a piece of paper through the telephone. Like in

591
00:35:34.979 --> 00:35:37.870
the 19 seventies, it took a while for that

592
00:35:37.879 --> 00:35:41.209
to spread because having a fax machine does you

593
00:35:41.219 --> 00:35:45.169
no good unless you know, people with fax machines.

594
00:35:45.350 --> 00:35:47.929
And so it, it takes a kind of critical

595
00:35:47.939 --> 00:35:51.629
mass um for these kinds of things to spread.

596
00:35:51.909 --> 00:35:59.600
Um And you know, it, it also depends on

597
00:35:59.610 --> 00:36:03.459
like how, how easily can certain individuals implement an

598
00:36:03.469 --> 00:36:06.860
innovation? Uh THAT'S going to affect how much it

599
00:36:06.870 --> 00:36:10.879
spreads. How hard is it to replicate an innovation?

600
00:36:10.889 --> 00:36:12.699
Like there are certain things where like, oh, great,

601
00:36:12.850 --> 00:36:14.750
I can believe that too or I can just

602
00:36:14.760 --> 00:36:16.620
go to the store and buy that too. But

603
00:36:16.629 --> 00:36:19.300
there are other things that might be more difficult

604
00:36:19.310 --> 00:36:24.590
to implement. Um And so those things will be

605
00:36:24.600 --> 00:36:27.979
harder to spread. They will tend to require more

606
00:36:27.989 --> 00:36:31.959
constant reinforcement and, and, and clustered networks where lots

607
00:36:31.969 --> 00:36:33.399
of people are talking to each other so they

608
00:36:33.409 --> 00:36:35.790
can build up a community. Um

609
00:36:36.649 --> 00:36:40.860
So, uh let me ask you now about opinions.

610
00:36:40.870 --> 00:36:44.270
So, uh in the book, you talk about how

611
00:36:44.570 --> 00:36:48.860
uh people basically form opinions and how they might

612
00:36:48.870 --> 00:36:51.649
change over time. But uh before we get into

613
00:36:51.699 --> 00:36:55.959
how we can study how opinions change over time.

614
00:36:56.229 --> 00:36:57.709
What is an opinion?

615
00:36:59.020 --> 00:37:02.870
I mean, that's the million dollar question. Um So

616
00:37:02.879 --> 00:37:06.959
there's this whole class of modeling that uh I

617
00:37:06.969 --> 00:37:10.739
think was originally developed by physicists. Uh WELL, it

618
00:37:10.750 --> 00:37:14.580
was developed independently by physicists, sociologists and social ecologists

619
00:37:14.590 --> 00:37:16.870
kind of over a long period. But I think

620
00:37:16.879 --> 00:37:21.360
most, most enthusiastically taken up in terms of modeling

621
00:37:21.370 --> 00:37:25.370
by physicists for a while. Um WHICH is this

622
00:37:25.379 --> 00:37:30.459
domain of modeling called opinion dynamics, sometimes called belief

623
00:37:30.469 --> 00:37:36.060
dynamics. And a lot of these, I love these

624
00:37:36.070 --> 00:37:38.580
models because they're really fun to play with. They

625
00:37:38.689 --> 00:37:42.090
can give you some broad insights into the ways

626
00:37:42.100 --> 00:37:47.699
that population structure interacts with individual Proclivities. Um But

627
00:37:47.709 --> 00:37:51.620
it's, I think they're really hard to apply to

628
00:37:51.629 --> 00:37:54.570
make any concrete predictions with. And one of the

629
00:37:54.580 --> 00:37:59.629
reasons is that um it's very hard to model

630
00:37:59.639 --> 00:38:04.739
an opinion. So what is an opinion? And there's

631
00:38:04.750 --> 00:38:07.239
a lot of debate over the answer to that

632
00:38:07.250 --> 00:38:11.070
question. Um One thing that people still argue about

633
00:38:11.080 --> 00:38:13.489
is even whether or not people really have beliefs

634
00:38:13.500 --> 00:38:17.139
or opinions, you might say, well, obviously I have

635
00:38:17.149 --> 00:38:20.790
beliefs, but I might say, OK, um you know,

636
00:38:20.949 --> 00:38:26.280
what's your belief about um the color of triceratops?

637
00:38:27.120 --> 00:38:30.780
And I don't know, but I might even say

638
00:38:30.790 --> 00:38:34.679
something more obvious like what's your belief about um

639
00:38:34.750 --> 00:38:39.500
or opinion about um how delicious a cupcake is?

640
00:38:40.429 --> 00:38:44.679
Well, it could be the same cupcake and on

641
00:38:44.689 --> 00:38:47.219
some days you might be really excited about a

642
00:38:47.229 --> 00:38:50.639
cupcake and say, I think my opinion is that

643
00:38:50.649 --> 00:38:53.040
this cupcake is delicious and some days maybe you've

644
00:38:53.050 --> 00:38:55.120
had a lot of sugar recently, you're not feeling

645
00:38:55.129 --> 00:38:59.070
well. You don't think that's delicious at all. Um

646
00:38:59.489 --> 00:39:03.389
Our opinions are, are probably constantly reconstructed in the

647
00:39:03.399 --> 00:39:07.699
moment about, you know, context and salient, et cetera.

648
00:39:07.709 --> 00:39:11.360
So these things make opinions harder to pin down.

649
00:39:11.620 --> 00:39:17.409
Um FOR the sake of modeling opinions are usually

650
00:39:17.419 --> 00:39:20.909
modeled in a, in a very simplistic way. But

651
00:39:20.919 --> 00:39:22.959
one that still gives us a lot of insight

652
00:39:22.969 --> 00:39:26.500
in the way that the ways that in which

653
00:39:26.510 --> 00:39:29.179
people interact might influence them more generally. And I,

654
00:39:29.189 --> 00:39:31.060
and I think that there's actually a lot of

655
00:39:31.070 --> 00:39:33.439
this is a fairly new research area, even though

656
00:39:33.449 --> 00:39:35.139
it's been around for a couple decades. I think

657
00:39:35.149 --> 00:39:39.260
there's a lot of low hanging fruit in bringing

658
00:39:39.270 --> 00:39:43.909
in psychologists and social science scientists to some of

659
00:39:43.919 --> 00:39:46.439
the work in opinion dynamics. It's been developed so

660
00:39:46.449 --> 00:39:50.330
much by physicists and computer scientists in trying to

661
00:39:50.550 --> 00:39:55.600
bring in more psychologically realistic and sociologically realistic models

662
00:39:55.610 --> 00:40:00.719
of opinion formation and opinion dynamics. So opinion dynamics

663
00:40:00.729 --> 00:40:04.169
models usually say, all right, well, let's imagine individual

664
00:40:04.179 --> 00:40:08.350
has an opinion and that opinion is represented by

665
00:40:08.550 --> 00:40:11.909
a number that's between, let's say negative one and

666
00:40:11.919 --> 00:40:14.750
positive one and you could with any fraction in

667
00:40:14.760 --> 00:40:18.649
between. So negative one means you're super against the

668
00:40:18.659 --> 00:40:21.739
thing and positive one means you're super for the

669
00:40:21.750 --> 00:40:26.050
thing and uh zero means you're totally ambivalent you

670
00:40:26.060 --> 00:40:31.879
don't care. And we can look then at how

671
00:40:33.360 --> 00:40:36.469
people, the way people, you know, express their opinion

672
00:40:36.479 --> 00:40:39.370
and interact with others who have different opinions, might

673
00:40:39.379 --> 00:40:42.149
lead them to change their opinions based on social

674
00:40:42.159 --> 00:40:47.100
influence. Maybe you become positively influenced, you become more

675
00:40:47.110 --> 00:40:49.860
similar to someone you interact with. Maybe you become

676
00:40:49.959 --> 00:40:52.550
negatively influence. If someone's like, really different from you

677
00:40:52.560 --> 00:40:55.580
or you hate them or they're from another, an

678
00:40:55.590 --> 00:40:59.139
opposing identity group or social group, you might actually

679
00:40:59.149 --> 00:41:01.250
say, well, I don't want anything to do with

680
00:41:01.260 --> 00:41:04.060
them. So anything they believe I'm gonna move farther

681
00:41:04.070 --> 00:41:08.219
away from them to better distinguish myself, you could

682
00:41:08.229 --> 00:41:11.179
have multiple opinions that they could interact in multiple

683
00:41:11.189 --> 00:41:16.000
ways. Um But again, you know, if you need

684
00:41:16.010 --> 00:41:18.120
several things to build a real model of these

685
00:41:18.129 --> 00:41:20.639
kinds of things, one, you need a model of

686
00:41:20.649 --> 00:41:24.899
how individuals represent their own opinions in their own

687
00:41:24.909 --> 00:41:27.340
minds. Then you need a model of how they

688
00:41:27.350 --> 00:41:30.229
express those opinions and whether or not they're accurate

689
00:41:30.239 --> 00:41:32.570
in how they express their opinions. Then you need

690
00:41:32.580 --> 00:41:37.000
a model of how those expressions of opinions influence

691
00:41:37.010 --> 00:41:40.770
other people and how, um by hearing someone else's

692
00:41:40.780 --> 00:41:46.209
opinions, you might change your opinion or not and

693
00:41:46.590 --> 00:41:48.219
you need all these things and then you need

694
00:41:48.229 --> 00:41:50.610
a model for the ways in which people interact

695
00:41:50.620 --> 00:41:53.010
and have interactions. Do you interact with everyone? Do

696
00:41:53.020 --> 00:41:55.540
you interact preferentially with people who are already similar

697
00:41:55.550 --> 00:41:59.760
to you? Um And so there's been a lot

698
00:41:59.770 --> 00:42:04.800
of work already exploring variations, different answers to these

699
00:42:04.810 --> 00:42:08.280
kinds of questions. Uh And so it's an ongoing

700
00:42:08.290 --> 00:42:10.929
field and it's chapter five in the book goes

701
00:42:11.080 --> 00:42:13.340
through a lot of these models.

702
00:42:13.909 --> 00:42:16.120
OK. But let me just ask you then one

703
00:42:16.129 --> 00:42:20.169
more specific question about opinions and how they change.

704
00:42:20.179 --> 00:42:22.610
So when it comes to the role of social

705
00:42:22.620 --> 00:42:28.570
influence, does uh sort of frequency dependent uh V

706
00:42:28.699 --> 00:42:32.500
bias play a role here. I mean, if uh

707
00:42:32.510 --> 00:42:35.989
we have a particular kind of opinion and suddenly

708
00:42:36.080 --> 00:42:41.629
many people around us uh are expressing a different

709
00:42:41.639 --> 00:42:46.320
opinion, an opposite opinion, for example, are we more

710
00:42:46.330 --> 00:42:50.989
prone than to adopt that opinion or not?

711
00:42:53.030 --> 00:42:54.750
Yeah. So, I mean, clearly this is an empirical

712
00:42:54.760 --> 00:43:00.750
question and I think it depends um on a

713
00:43:00.760 --> 00:43:03.540
lot of, on a lot of circumstances, right? Do

714
00:43:03.550 --> 00:43:08.149
we have an incentive to coordinate or agree with

715
00:43:08.159 --> 00:43:12.060
those people who are expressing a different opinion? Um

716
00:43:12.330 --> 00:43:15.290
There's also evidence that people will express opinions to

717
00:43:15.300 --> 00:43:17.540
go along with the group but not actually change

718
00:43:17.550 --> 00:43:20.510
their internal beliefs nearly as much. You, you say

719
00:43:20.520 --> 00:43:22.760
things that aren't totally in line with what you

720
00:43:22.770 --> 00:43:28.560
really believe because it's just useful to uh express

721
00:43:28.570 --> 00:43:31.860
agreement. Um This is some, yeah, I mean, there's

722
00:43:31.870 --> 00:43:34.620
a difference, sometimes people call these like internal beliefs

723
00:43:34.629 --> 00:43:39.159
versus external beliefs or um variations on that. Um

724
00:43:40.979 --> 00:43:43.870
I think there is a lot of evidence from

725
00:43:43.879 --> 00:43:48.000
psychology that people do do this in terms of,

726
00:43:48.010 --> 00:43:50.520
you know, there's like the famous Solomon Ash conformity

727
00:43:50.530 --> 00:43:55.120
experiments. But uh people have replicated these kinds of

728
00:43:55.129 --> 00:43:58.580
phenomena in a number of circumstances. It is likely

729
00:43:58.590 --> 00:44:01.679
that people, yeah, do do this where they express

730
00:44:01.689 --> 00:44:04.610
a belief to go along with the majority. It's

731
00:44:04.620 --> 00:44:06.959
a little bit less clear and more ambiguous to

732
00:44:06.969 --> 00:44:09.340
the extent to which they really change their belief

733
00:44:09.350 --> 00:44:14.429
versus um just change how they express their beliefs.

734
00:44:15.389 --> 00:44:18.320
Um One of the things that we can do

735
00:44:18.330 --> 00:44:22.330
with models is think about is think about these

736
00:44:22.340 --> 00:44:25.649
alternative hypotheses and say, well, ok, what are the

737
00:44:25.659 --> 00:44:28.689
possibilities for how people are expressing their beliefs or

738
00:44:28.699 --> 00:44:32.179
changing their beliefs? And then what would it look

739
00:44:32.189 --> 00:44:34.360
like? What would the dynamics or what would the

740
00:44:34.370 --> 00:44:37.469
population look like in a world where people did

741
00:44:37.479 --> 00:44:41.159
these different things? And then use that to compare,

742
00:44:41.169 --> 00:44:43.389
compare it to the real data that we have

743
00:44:43.399 --> 00:44:45.669
and say, well, ok, well, if we assume that

744
00:44:45.679 --> 00:44:48.120
everyone just goes along with everyone else, it would

745
00:44:48.129 --> 00:44:49.919
look like this. If we assume that everyone is

746
00:44:49.929 --> 00:44:51.969
really stubborn and doesn't change their belief, it would

747
00:44:51.979 --> 00:44:55.669
look like this if we assume that people care

748
00:44:55.679 --> 00:44:58.159
about what the majority think, but only if they're,

749
00:44:58.169 --> 00:45:00.120
let's say in an identity group, it would look

750
00:45:00.129 --> 00:45:04.800
like this and that will help us pin down

751
00:45:04.810 --> 00:45:07.760
not only you know, what's true or, or what's

752
00:45:07.770 --> 00:45:09.540
more likely to be true or less likely to

753
00:45:09.550 --> 00:45:12.120
be true. But another aspect of theory that I

754
00:45:12.129 --> 00:45:15.330
think is underappreciated is the importance of what the

755
00:45:15.340 --> 00:45:19.239
sociologists call scope, which is, it's, it's not enough

756
00:45:19.250 --> 00:45:21.370
to just say, uh I have a theory that,

757
00:45:21.679 --> 00:45:25.449
um you know, here's a great example, like all

758
00:45:25.459 --> 00:45:28.840
this stuff, all this stuff on heuristics and biases

759
00:45:28.850 --> 00:45:32.209
that like Conman and Tversky uh put out like

760
00:45:32.219 --> 00:45:37.310
availability bias or uh confirmation bias. You know, they

761
00:45:37.320 --> 00:45:39.239
have lots of biases and some of them are

762
00:45:39.250 --> 00:45:44.050
mutually exclusive or would lead to different predictions. And

763
00:45:44.060 --> 00:45:46.659
this is a problem and it's not an, it

764
00:45:46.669 --> 00:45:49.409
doesn't mean these biases aren't real. But what it

765
00:45:49.419 --> 00:45:51.850
means is that the theory is under specified. It's

766
00:45:51.860 --> 00:45:54.330
not enough to say these biases exist. You have

767
00:45:54.350 --> 00:45:58.639
to also say when will one bias be prevalent

768
00:45:58.649 --> 00:46:00.669
and when should we expect one bias to be

769
00:46:00.679 --> 00:46:03.889
really important? And when should we expect another bias

770
00:46:03.899 --> 00:46:05.989
to be important? And that's scope, it's sort of

771
00:46:06.000 --> 00:46:08.840
the conditions under which a theory is expected to

772
00:46:08.850 --> 00:46:13.040
hold and when it's expected not to hold. So

773
00:46:13.050 --> 00:46:15.560
doing these models can also help us with, with

774
00:46:15.570 --> 00:46:18.000
uh the development of scope for our theories.

775
00:46:18.270 --> 00:46:22.750
Mhm. And how do we go from opinion dynamics

776
00:46:22.760 --> 00:46:28.649
to understanding phenomena like consensus and polarization, for example.

777
00:46:30.169 --> 00:46:35.399
Yeah. So we can define consensus as a situation

778
00:46:35.409 --> 00:46:37.870
where people tend to share opinions and we can

779
00:46:37.879 --> 00:46:42.040
define polarization as a situation in which people tend

780
00:46:42.050 --> 00:46:45.790
to have very different opinions and there may be

781
00:46:45.800 --> 00:46:49.370
separated into different groups where there are clusters of

782
00:46:49.379 --> 00:46:51.550
people who believe one thing and clusters of people

783
00:46:51.560 --> 00:46:54.389
who believe something that's sort of diametrically opposed. And

784
00:46:54.399 --> 00:46:56.070
the more opposed they are, we could say that

785
00:46:56.080 --> 00:47:01.199
the more polarized they are. Um And another, there

786
00:47:01.209 --> 00:47:03.479
are actually turns out there are lots of ways

787
00:47:03.489 --> 00:47:06.120
people have measured polarization. And this is also a

788
00:47:06.129 --> 00:47:12.739
challenge. Um So um one thing that might be

789
00:47:12.750 --> 00:47:14.889
the case is like if everyone in one group

790
00:47:14.899 --> 00:47:17.610
believes one set of things and then everyone in

791
00:47:17.620 --> 00:47:20.520
another group believes another set of things. That's a

792
00:47:20.530 --> 00:47:24.149
very polarized uh situation. What if you have a

793
00:47:24.159 --> 00:47:27.310
situation where lots, there are lots of things to

794
00:47:27.320 --> 00:47:32.695
believe and some people believe some constellation of things

795
00:47:32.705 --> 00:47:35.205
and other people believe other constellations of things, but

796
00:47:35.215 --> 00:47:37.524
it's less clear that everyone is in one big

797
00:47:37.534 --> 00:47:39.455
group. And actually there are sort of many little

798
00:47:39.465 --> 00:47:43.425
clusters, some people, each of which has some things,

799
00:47:43.435 --> 00:47:46.850
but not other things in common with other groups.

800
00:47:47.090 --> 00:47:49.780
That's a situation where there's still a lot of

801
00:47:49.790 --> 00:47:53.830
sort of individual opinion polarization, but the whole population

802
00:47:53.840 --> 00:47:58.939
is much less polarized because there's less sort of

803
00:47:58.949 --> 00:48:03.030
clustering of beliefs into one sort of block that

804
00:48:03.040 --> 00:48:04.959
is true for everyone in a group. So that

805
00:48:04.969 --> 00:48:07.580
would be a way of characterizing a, a less

806
00:48:07.590 --> 00:48:11.439
polarized uh system. And so these models are often

807
00:48:11.449 --> 00:48:18.399
used to, to think about how um individual, individual

808
00:48:18.409 --> 00:48:23.270
opinions, individual psychology or rules for communicating and updating

809
00:48:23.280 --> 00:48:26.850
one's opinions. And population structure will tend to lead

810
00:48:26.860 --> 00:48:31.500
toward either consensus or polarization. Um So one finding

811
00:48:31.510 --> 00:48:33.919
that's been in a lot of models, if you

812
00:48:33.929 --> 00:48:36.860
assume that people have lots of opinions and they

813
00:48:36.870 --> 00:48:40.120
tend to interact positively with people who are more

814
00:48:40.129 --> 00:48:42.100
similar than different to becoming a little bit more

815
00:48:42.110 --> 00:48:44.790
similar. But if you're more different than similar, you

816
00:48:44.800 --> 00:48:48.290
tend to kind of not like that person and,

817
00:48:48.300 --> 00:48:50.449
and move your opinions. But away from that person,

818
00:48:51.770 --> 00:48:54.189
if you have a population structure, that is fairly

819
00:48:54.199 --> 00:48:57.189
clustered into discrete communities that only interact a little

820
00:48:57.199 --> 00:48:59.790
bit with each other, what you end up getting

821
00:48:59.800 --> 00:49:04.250
is fairly hetero or sort of fairly homogeneous communities

822
00:49:04.260 --> 00:49:06.840
where everyone kind of ends up agreeing within a

823
00:49:06.850 --> 00:49:09.469
community, but they differ a little bit from their

824
00:49:09.479 --> 00:49:13.280
neighboring communities. And that's, that's OK. But then what

825
00:49:13.290 --> 00:49:16.659
if you start adding long range connections so that

826
00:49:16.669 --> 00:49:20.909
each community is then has occasional connections with communities

827
00:49:20.919 --> 00:49:24.870
that are really different from them. What can happen

828
00:49:24.879 --> 00:49:28.830
in, in some of these models is that communication

829
00:49:28.840 --> 00:49:31.560
with individuals with very different opinions, pushes your own

830
00:49:31.570 --> 00:49:35.070
opinion further away and it ends up polarizing the

831
00:49:35.080 --> 00:49:40.389
population really extremely. Um I remember reading this was

832
00:49:40.399 --> 00:49:44.000
uh II I do AAA slight variation on this

833
00:49:44.010 --> 00:49:47.860
model in the book. Um The original model is

834
00:49:47.870 --> 00:49:51.739
by two Sociologists, Andreas Flocka and Michael Macy. And

835
00:49:51.750 --> 00:49:54.060
I remember in this paper published in 2011 and

836
00:49:54.070 --> 00:49:56.070
I remember reading it at the time and just

837
00:49:56.080 --> 00:49:59.729
being like, oh, this is uh basically an argument

838
00:49:59.739 --> 00:50:04.979
uh about uh like the internet, polarizing people making

839
00:50:04.989 --> 00:50:08.479
opinions more extreme and polarizing. No, I mean, it's

840
00:50:08.489 --> 00:50:11.979
a simplification that doesn't capture lots of things about

841
00:50:11.989 --> 00:50:13.290
the real world. But that's the whole point of

842
00:50:13.300 --> 00:50:16.260
models is to say, OK, if this is the

843
00:50:16.270 --> 00:50:18.659
only thing that mattered, this is what would happen

844
00:50:18.860 --> 00:50:21.280
if that's not what happens, then what we put

845
00:50:21.290 --> 00:50:22.919
in the model can't be the only things that

846
00:50:22.929 --> 00:50:24.570
matter. So what else matters?

847
00:50:26.500 --> 00:50:29.439
Changing topics. Now, I would like to ask you

848
00:50:29.449 --> 00:50:33.320
a bit about uh how we study Cooper operation.

849
00:50:33.370 --> 00:50:37.280
So how do we model it? And are there

850
00:50:37.290 --> 00:50:43.879
specific theoretical assumptions that people bring to the table

851
00:50:43.889 --> 00:50:45.679
when studying Cooper operation?

852
00:50:46.729 --> 00:50:48.760
I think, you know that the answer is yes.

853
00:50:48.959 --> 00:50:54.540
So that second question. Um Yeah, I, so the

854
00:50:54.550 --> 00:50:57.179
Cooper Operation chapter six in the book um I

855
00:50:57.189 --> 00:51:00.209
love Cooper Operation models and the topic of cooper

856
00:51:00.219 --> 00:51:03.100
operation more generally, it was one of my, it's,

857
00:51:03.110 --> 00:51:06.590
it's a real strong interest of mine, passion of

858
00:51:06.600 --> 00:51:10.159
mine. Um I did a lot of my early

859
00:51:10.169 --> 00:51:13.600
research was on co-operation models and I still work

860
00:51:13.610 --> 00:51:17.760
on them sometimes. So Cooper operation can mean a

861
00:51:17.770 --> 00:51:20.459
lot of things. Uh It really just means working

862
00:51:20.469 --> 00:51:23.179
together, right? Um And there are lots of ways

863
00:51:23.189 --> 00:51:25.199
that can happen and there are ways to work

864
00:51:25.209 --> 00:51:29.879
together, for example, that are mutually beneficial that um

865
00:51:31.830 --> 00:51:36.159
don't require any sort of costly behavior on the

866
00:51:36.169 --> 00:51:38.270
part of anyone else. It's just, I'm gonna do

867
00:51:38.280 --> 00:51:40.959
this thing. You're gonna do that thing. Hey, if

868
00:51:40.969 --> 00:51:43.129
we do it together it's better for everyone. We

869
00:51:43.139 --> 00:51:47.139
have no incentives to. Not awesome. Um You can

870
00:51:47.149 --> 00:51:50.989
model situations like that. Uh The most common way

871
00:51:51.000 --> 00:51:53.209
to model it is, uh I think it's sort

872
00:51:53.219 --> 00:51:57.129
of the most interesting to theorists because it's, it's,

873
00:51:57.139 --> 00:52:02.139
this, this challenge is a modeling altruism which is

874
00:52:02.149 --> 00:52:05.649
Altruistic Cooper Operation, which is costly cooperations. The idea

875
00:52:05.659 --> 00:52:09.929
that helping others uh is costly, you take time,

876
00:52:09.939 --> 00:52:13.479
you give them resources, you um give up your

877
00:52:13.489 --> 00:52:18.050
own opportunities in a lot of animal species. You

878
00:52:18.060 --> 00:52:20.840
know, you give up your opportunities for reproduction or

879
00:52:20.850 --> 00:52:23.090
maybe you put yourself at risk for predation to

880
00:52:23.100 --> 00:52:29.500
help others. So, um you know, older sisters in,

881
00:52:29.510 --> 00:52:32.310
in a lot of species like certain birds will,

882
00:52:33.080 --> 00:52:36.159
they're, they're biologically, they're mature, they're ready to breathe

883
00:52:36.169 --> 00:52:38.580
and have their own eggs, but they'll stay for

884
00:52:38.590 --> 00:52:41.030
a couple of seasons and help their mother raise

885
00:52:41.040 --> 00:52:44.879
their younger sisters or, you know, certain social animals

886
00:52:44.889 --> 00:52:47.810
like alarm, uh like meerkats or, or ground squirrels

887
00:52:47.820 --> 00:52:50.010
will do these alarm calls while they, when they

888
00:52:50.020 --> 00:52:52.020
see a predator, they'll get up on a high

889
00:52:52.030 --> 00:52:53.959
space and, and make a big call to let

890
00:52:53.969 --> 00:52:56.040
everyone know that there's a predator, but of course,

891
00:52:56.050 --> 00:53:00.280
that puts them at risk. So these things are

892
00:53:01.080 --> 00:53:05.540
a challenge for explaining uh in an evolutionary framework,

893
00:53:05.729 --> 00:53:09.939
right? Because yeah, it's clearly good to cooper, everyone

894
00:53:09.949 --> 00:53:14.149
benefits from having cooperators around. And when you cooper

895
00:53:14.159 --> 00:53:16.199
with each other, we both benefit and we both

896
00:53:16.209 --> 00:53:18.340
do better than if we didn't help each other.

897
00:53:18.449 --> 00:53:23.179
So clearly cooperations good. However, the dilemma is that

898
00:53:23.189 --> 00:53:26.750
if you cooperated with me and I don't cooperated

899
00:53:26.760 --> 00:53:30.870
with you, I do better than you. So and

900
00:53:30.879 --> 00:53:32.989
I actually do better than if I did cooperated

901
00:53:33.000 --> 00:53:36.389
with you back. So I can exploit, it's bit

902
00:53:36.399 --> 00:53:39.449
easier to exploit. I, I I'm better off exploiting

903
00:53:39.459 --> 00:53:42.510
you than cooperating with you. But of course, you

904
00:53:42.520 --> 00:53:44.929
know that too. And so you don't want to

905
00:53:44.939 --> 00:53:47.969
be exploited, so you don't cooper either and then

906
00:53:47.979 --> 00:53:51.199
we're stuck. So the most common way to model

907
00:53:51.209 --> 00:53:53.709
this is using a prisoner's dilemma game, which is

908
00:53:53.719 --> 00:53:57.449
a very famous two player to move game where

909
00:53:57.459 --> 00:54:02.409
individuals can either cooper or defect. Um IN the

910
00:54:02.419 --> 00:54:04.889
format of the game that most biologists use that

911
00:54:04.899 --> 00:54:07.010
I use in the book. Um We can have

912
00:54:07.020 --> 00:54:10.379
just two variables here. There's the, the benefit that

913
00:54:10.389 --> 00:54:15.110
you give to another person by cooperating with them.

914
00:54:15.120 --> 00:54:17.030
And then it's the cost that you incur on

915
00:54:17.040 --> 00:54:21.750
yourself um by cooperating. And so if we both

916
00:54:21.760 --> 00:54:24.620
cooper, we each get the benefit from the other

917
00:54:24.629 --> 00:54:29.060
person's aid minus the cost that we give. And

918
00:54:29.070 --> 00:54:30.570
we assume that the benefit is more than the

919
00:54:30.580 --> 00:54:34.550
cost. Um IF the benefit is not more than

920
00:54:34.560 --> 00:54:37.070
the cost and no one will ever cooper. But

921
00:54:37.080 --> 00:54:39.120
if the benefit is less than the cost. Sorry,

922
00:54:39.129 --> 00:54:40.629
if the benefit is less than the cost, no

923
00:54:40.639 --> 00:54:42.250
one will ever cooper. The benefit is more than

924
00:54:42.260 --> 00:54:44.949
the cost, then we will, it's worth it and

925
00:54:44.959 --> 00:54:47.229
it's better than just getting nothing by not doing

926
00:54:47.239 --> 00:54:50.699
anything. But of course, if I pay the cost

927
00:54:50.709 --> 00:54:53.719
in order to get the benefit that sucks. So

928
00:54:54.540 --> 00:54:59.290
um turns out there's a number of mechanisms that

929
00:54:59.300 --> 00:55:05.310
people have identified uh that enable cooper operation to

930
00:55:06.040 --> 00:55:10.139
take off to um succeed and even invade sort

931
00:55:10.149 --> 00:55:13.050
of uh increase, even if it's rare in a

932
00:55:13.060 --> 00:55:18.739
population. And the most basic is just uh assortment.

933
00:55:19.929 --> 00:55:24.449
So, if the benefits of co-operation are preferentially directed

934
00:55:24.739 --> 00:55:31.040
toward other cooperating individuals, that means that those benefits

935
00:55:31.209 --> 00:55:33.959
are going toward individuals who are more likely who

936
00:55:33.969 --> 00:55:36.530
are also going to give other people cooper operation,

937
00:55:36.540 --> 00:55:40.889
the aid of cooper operation and so co-operative uh

938
00:55:40.899 --> 00:55:45.229
strategies, individuals using cooper strategies will always do better

939
00:55:45.239 --> 00:55:48.540
than individuals, not using co-operative strategies. And there are

940
00:55:48.550 --> 00:55:53.120
a lot of mechanisms um that enable this kind

941
00:55:53.129 --> 00:55:57.040
of assortment. Um So the, the most common we

942
00:55:57.050 --> 00:55:59.840
discuss one in biology is inclusive fitness or kin

943
00:55:59.850 --> 00:56:03.580
selection, which is this idea of giving a preferentially

944
00:56:03.590 --> 00:56:06.310
to close kin because they will share your genes.

945
00:56:06.320 --> 00:56:08.919
So cooper individuals will tend to be related to

946
00:56:08.929 --> 00:56:12.330
other co-operative individuals. Um BUT you don't need genes

947
00:56:12.340 --> 00:56:15.520
at all. Actually, all you need is some mechanism

948
00:56:15.540 --> 00:56:20.580
that keeps individuals interacting with others who are like

949
00:56:20.590 --> 00:56:25.790
them So uh population structure, a sort of rigid

950
00:56:25.800 --> 00:56:28.350
population structure where individuals tend to interact with the

951
00:56:28.360 --> 00:56:31.649
same people over and over again will produce this

952
00:56:32.179 --> 00:56:35.129
because just by chance you'll end up with clusters

953
00:56:35.139 --> 00:56:37.439
of cooperators and they're just going to do better

954
00:56:37.449 --> 00:56:42.300
than everyone else. Um Other mechanisms. So we model,

955
00:56:42.310 --> 00:56:47.020
we can model this uh using mathematics by having

956
00:56:47.040 --> 00:56:51.169
uh parameters for the probability of assorting with similar

957
00:56:51.179 --> 00:56:54.149
types versus random types. We can do this with

958
00:56:54.159 --> 00:56:56.500
agent based models by having, let's say a rigid

959
00:56:56.510 --> 00:56:59.330
network structure or putting agents in a space and

960
00:56:59.340 --> 00:57:02.169
having them interact just with local agents. And that

961
00:57:02.179 --> 00:57:04.919
works too. Uh And I do both those examples

962
00:57:04.929 --> 00:57:08.500
in the book. And we can also have strategies

963
00:57:08.510 --> 00:57:12.929
like repeated interactions. So what happens if we're not

964
00:57:12.939 --> 00:57:17.110
related to each other if we're not always interacting

965
00:57:17.120 --> 00:57:20.629
with the same people for our whole lives? But

966
00:57:20.989 --> 00:57:23.989
when we do interact with someone, there's the potential

967
00:57:24.000 --> 00:57:26.389
for that interaction to go on for a long

968
00:57:26.399 --> 00:57:32.459
time to have many, many interactions. And this has

969
00:57:32.469 --> 00:57:36.600
been well known since the 19 seventies that repeated

970
00:57:36.610 --> 00:57:41.010
interactions provides the opportunity for reciprocity for if you

971
00:57:41.020 --> 00:57:44.540
help me, I'll help you. And if you don't

972
00:57:44.550 --> 00:57:48.889
help me, I won't help you. And reciprocal reciprocating

973
00:57:48.899 --> 00:57:52.949
strategies that are able to provide co-operation to other,

974
00:57:52.959 --> 00:57:56.340
you know, two individuals who cooperated with them but

975
00:57:56.540 --> 00:58:00.189
not reward defection with cooperations. So stop cooperating with

976
00:58:00.199 --> 00:58:03.149
people who don't co-operate with them. This does really

977
00:58:03.159 --> 00:58:07.699
well because co-operative individuals get the benefit of co-operation.

978
00:58:07.709 --> 00:58:10.719
So again, the benefits of cooper operation are directed

979
00:58:10.729 --> 00:58:14.020
toward cooper individuals. And it's also a form of

980
00:58:14.030 --> 00:58:17.159
assortment in this way, it's just assortment in terms

981
00:58:17.169 --> 00:58:21.750
of benefit provision rather than um let's say uh

982
00:58:21.760 --> 00:58:27.800
physical proximity. Um And I focus in the, in

983
00:58:27.810 --> 00:58:30.120
the book on these two examples and use both

984
00:58:30.129 --> 00:58:33.760
spatial agent based models and also mathematical models um

985
00:58:34.120 --> 00:58:37.540
from evolutionary game theory to derive general principles for

986
00:58:37.550 --> 00:58:39.709
this. But there are lots of others. Um I

987
00:58:39.719 --> 00:58:41.919
talk more a bit about, you know, people have

988
00:58:41.929 --> 00:58:46.050
identified things like the ability to use reputation. Like

989
00:58:46.060 --> 00:58:48.719
I actually don't need to interact with you specifically

990
00:58:48.729 --> 00:58:51.959
many times to know if you're a cooperator. If

991
00:58:52.260 --> 00:58:54.820
I trust my friend who's interacted with you and

992
00:58:54.830 --> 00:58:57.989
says you're a cooperator. So I'll cooperated with you

993
00:58:58.000 --> 00:59:01.090
because my friend vouches for you or, you know,

994
00:59:01.100 --> 00:59:04.459
if we have other kinds of, of spatial structure

995
00:59:04.469 --> 00:59:06.449
where we're both members of a, of a group,

996
00:59:06.459 --> 00:59:08.879
then we have collective uh we have shared interests

997
00:59:09.659 --> 00:59:14.919
um in larger groups, things like reciprocity don't work

998
00:59:14.929 --> 00:59:17.560
so well. So there's a related game called the

999
00:59:17.570 --> 00:59:21.870
Public Goods Game, which is, is, is designed to

1000
00:59:21.879 --> 00:59:25.070
model cooper operation in larger groups. And the idea

1001
00:59:25.080 --> 00:59:27.469
there is, it's very similar, you can give aid

1002
00:59:27.479 --> 00:59:30.179
to the group or not. Anything that you give

1003
00:59:30.189 --> 00:59:33.060
to the group gets sort of multiplied and divided

1004
00:59:33.070 --> 00:59:36.790
among everyone, whether or not they cooperated. And so

1005
00:59:36.800 --> 00:59:39.929
it's, it's providing a public good. But of course,

1006
00:59:39.959 --> 00:59:41.600
the best thing to do there is to let

1007
00:59:41.610 --> 00:59:43.810
everyone else provide the benefit and you free ride

1008
00:59:43.820 --> 00:59:46.250
off their, their work, you get all the benefit

1009
00:59:46.260 --> 00:59:50.179
you pay. No, the cost. Reciprocity doesn't really work

1010
00:59:50.189 --> 00:59:52.409
because what happens if you're in a group of

1011
00:59:52.419 --> 00:59:55.790
10 and one person shirks, do you stop contributing

1012
00:59:55.800 --> 00:59:58.639
to the group? Just because one person stopped contributing,

1013
00:59:58.739 --> 01:00:02.050
that hurts everyone, including those who, who also contributed.

1014
01:00:03.020 --> 01:00:07.520
And so this kind of reciprocity fails and you

1015
01:00:07.530 --> 01:00:11.790
need other mechanisms like ostracism, like kicking people out

1016
01:00:11.800 --> 01:00:16.439
of groups that don't co-operate um or institutional mechanisms

1017
01:00:16.449 --> 01:00:21.280
like punishment where you literally pay an extra cost

1018
01:00:22.370 --> 01:00:25.229
to uh in incur a punishment upon people who

1019
01:00:25.239 --> 01:00:28.030
don't co-operate. And there are lots of models showing

1020
01:00:28.040 --> 01:00:30.979
how this, there are lots of conditions under which

1021
01:00:30.989 --> 01:00:37.310
this will work. Um And so it goes on,

1022
01:00:37.320 --> 01:00:40.030
there's, there's a very well developed, you know, decades

1023
01:00:40.040 --> 01:00:43.790
old theoretical literature on different types of co-operation and

1024
01:00:43.800 --> 01:00:47.429
different conditions uh for it to emerge uh or

1025
01:00:47.439 --> 01:00:48.239
evolve.

1026
01:00:49.739 --> 01:00:53.110
So another very interesting thing that you also talk

1027
01:00:53.120 --> 01:00:57.750
about in your book is uh norms. So what

1028
01:00:57.760 --> 01:01:02.360
are norms and how do you model how they

1029
01:01:02.370 --> 01:01:03.120
emerge

1030
01:01:05.939 --> 01:01:08.840
again? A good question also. Yeah, chapter seven in

1031
01:01:08.850 --> 01:01:13.360
the book uh on norms. Uh SO, I mean,

1032
01:01:14.439 --> 01:01:15.889
you know, I think to some extent, we all

1033
01:01:15.899 --> 01:01:18.080
know what norms are, they're the, the things that

1034
01:01:18.090 --> 01:01:20.679
are normative in a, in a collective. They are

1035
01:01:20.689 --> 01:01:22.899
the things that most people do that we expect

1036
01:01:22.909 --> 01:01:25.080
most people to do how we expect people to

1037
01:01:25.090 --> 01:01:28.850
behave in certain situations. Now, there, there are some

1038
01:01:28.860 --> 01:01:32.899
norms that are just kind of um they're normative

1039
01:01:32.909 --> 01:01:40.020
because they uh let's say, have benefits for individuals

1040
01:01:40.370 --> 01:01:43.560
and we want everyone to do something because it,

1041
01:01:43.570 --> 01:01:45.760
it, it would hurt, let's say a group to

1042
01:01:45.770 --> 01:01:47.929
not do those things like, like a norm of

1043
01:01:47.939 --> 01:01:51.540
brushing your teeth. Like that's normative. You should brush

1044
01:01:51.550 --> 01:01:54.629
your teeth, you're gonna get um sort of shamed

1045
01:01:54.639 --> 01:01:56.300
a bit if you just refuse to brush your

1046
01:01:56.310 --> 01:01:59.959
teeth every day. Um There are other kinds of

1047
01:01:59.969 --> 01:02:05.939
norms that involve interactions and uh those are the

1048
01:02:05.949 --> 01:02:09.080
kinds of norms that I'm focusing more on in

1049
01:02:09.090 --> 01:02:12.969
the chapter. And these norms involve the ability to

1050
01:02:12.979 --> 01:02:16.979
do basically the same thing or do things in

1051
01:02:16.989 --> 01:02:21.080
the same way as other people. Now doing things

1052
01:02:21.090 --> 01:02:22.739
in the same way, could mean doing the same

1053
01:02:22.750 --> 01:02:25.800
thing or it could mean doing the complimentary thing.

1054
01:02:25.810 --> 01:02:27.459
And we explore both ideas in the, in the

1055
01:02:27.469 --> 01:02:33.090
chapter. Um And this allows us to model norms

1056
01:02:33.100 --> 01:02:37.399
as solutions to a coordinating game. So a coordinating

1057
01:02:37.409 --> 01:02:40.729
game is another kind of game um that you

1058
01:02:40.739 --> 01:02:44.129
can study using game theory. Uh And that's, that's

1059
01:02:44.139 --> 01:02:47.209
how it's done in the book. Uh WHERE I

1060
01:02:47.219 --> 01:02:48.949
want to do the same thing as a simple

1061
01:02:48.959 --> 01:02:50.820
coordinating game is just like each of us are

1062
01:02:50.830 --> 01:02:53.370
trying to do the same thing. Um You can

1063
01:02:53.379 --> 01:02:56.250
imagine like if we're greeting each other, we could

1064
01:02:56.260 --> 01:02:58.719
give each other a handshake or we could high

1065
01:02:58.729 --> 01:03:01.830
five. Now, if I go to shake your hand

1066
01:03:01.840 --> 01:03:03.399
and you go to give me a high five,

1067
01:03:03.409 --> 01:03:06.469
that's a Miscoordination. If we do the same thing,

1068
01:03:06.479 --> 01:03:08.100
it doesn't really matter if we high five our

1069
01:03:08.110 --> 01:03:09.989
handshake. As long as we're doing the same thing.

1070
01:03:11.219 --> 01:03:13.719
In this example, the cost to mis coordinating is

1071
01:03:13.729 --> 01:03:16.439
pretty low. But you can imagine cases in which

1072
01:03:16.449 --> 01:03:19.719
it's not so low. Like for example, if you

1073
01:03:19.729 --> 01:03:21.300
drive on the right side of the road and

1074
01:03:21.310 --> 01:03:22.639
I drive on the left side of the road

1075
01:03:22.649 --> 01:03:24.739
and we're driving toward each other, that would be

1076
01:03:24.750 --> 01:03:30.219
a really catastrophic Miscoordination. Um And you know, coordinations,

1077
01:03:30.229 --> 01:03:32.790
Miscoordination can be enforced through all kinds of mechanisms

1078
01:03:32.800 --> 01:03:35.010
like punishment. Like if you just don't do things

1079
01:03:35.020 --> 01:03:38.729
the way other people do them, there are communities

1080
01:03:38.739 --> 01:03:42.469
where failure to conform to the norm can result

1081
01:03:42.479 --> 01:03:46.820
in, in, in pretty strong punishment um including things

1082
01:03:46.830 --> 01:03:50.719
like imprisonment or, or beatings or even death. So

1083
01:03:53.669 --> 01:03:56.500
we model, you can model a simple co ordination

1084
01:03:56.510 --> 01:04:01.110
game to think about. Well, how, how do people

1085
01:04:01.120 --> 01:04:04.699
converge on doing the same thing? Well, if you

1086
01:04:04.709 --> 01:04:07.110
interact with a bunch of people in a population,

1087
01:04:07.870 --> 01:04:09.610
the thing to do, if, if it doesn't matter

1088
01:04:09.620 --> 01:04:12.939
which norm, you're gonna, you're gonna have as long

1089
01:04:12.949 --> 01:04:14.600
as you do the same thing, then the best

1090
01:04:14.610 --> 01:04:17.050
strategy is always just do the popular thing, do

1091
01:04:17.060 --> 01:04:20.550
the common thing. And we can show really easily

1092
01:04:20.560 --> 01:04:23.050
like that in a population where all that matters

1093
01:04:23.060 --> 01:04:26.090
is doing the same thing as everyone else. Any

1094
01:04:26.219 --> 01:04:28.969
behavior that is more popular than others will just

1095
01:04:28.979 --> 01:04:30.899
always go to fixation, everyone will end up doing

1096
01:04:30.909 --> 01:04:36.800
that. But that, that implies that uh norms are

1097
01:04:36.810 --> 01:04:39.139
really hard to change and then norms will always,

1098
01:04:39.149 --> 01:04:41.560
you know, go to fixation and norms, whatever norm

1099
01:04:41.570 --> 01:04:46.330
is, is popular will always dominate. Um What if

1100
01:04:46.340 --> 01:04:49.169
a norm? What if norms are some norms are

1101
01:04:49.179 --> 01:04:54.679
better than others, right? Like there's two all norms

1102
01:04:54.689 --> 01:04:58.310
if their coordinations are equilibria because everyone is doing

1103
01:04:58.320 --> 01:05:00.719
the same thing and deviations from the norm can

1104
01:05:00.729 --> 01:05:04.540
be punished. But what if there are two possibilities

1105
01:05:04.550 --> 01:05:07.479
to equilibria or more equilibria? And one is better

1106
01:05:07.489 --> 01:05:12.270
than others? So, you know, what if one population

1107
01:05:12.300 --> 01:05:17.300
has a, you know, a norm where um everyone

1108
01:05:17.330 --> 01:05:21.580
uh who anyone who steals is immediately put to

1109
01:05:21.590 --> 01:05:24.949
death. And another group where people who steals are

1110
01:05:24.959 --> 01:05:29.620
steal are, you know, first chastised and talked to

1111
01:05:29.719 --> 01:05:32.530
and then have a, a slowly increasing slope of

1112
01:05:32.540 --> 01:05:37.070
punishment. Tom that second group is probably gonna do

1113
01:05:37.080 --> 01:05:41.479
a bit better under some circumstances than the first

1114
01:05:41.489 --> 01:05:44.850
group. Uh I'm a big, I like unions, not

1115
01:05:44.860 --> 01:05:47.330
everyone does, but I think unions are good in

1116
01:05:47.340 --> 01:05:51.570
general. Uh YOU know, not, not universally good in

1117
01:05:51.580 --> 01:05:53.780
every single domain, but I think overall they provide

1118
01:05:53.790 --> 01:05:56.879
a real benefit. You can imagine, like if you're

1119
01:05:56.889 --> 01:05:59.110
in a population that doesn't have a union in,

1120
01:05:59.120 --> 01:06:03.189
in, in, in, in whatever your job is and

1121
01:06:03.199 --> 01:06:06.219
you start organizing or for, to try to get

1122
01:06:06.229 --> 01:06:08.360
everyone to be in a union that's gonna come

1123
01:06:08.370 --> 01:06:10.810
at a cost that's not gonna go well for

1124
01:06:10.820 --> 01:06:14.739
you maybe. But if you can convince everyone and

1125
01:06:14.750 --> 01:06:17.179
you are in a union, that union group might

1126
01:06:17.189 --> 01:06:19.110
do better, like you have better benefits and job

1127
01:06:19.120 --> 01:06:25.719
security and et cetera. Um And so in models

1128
01:06:25.729 --> 01:06:29.139
like this, you, you, it's not always better to

1129
01:06:29.149 --> 01:06:32.590
do the more popular thing because even a something

1130
01:06:32.600 --> 01:06:35.379
that is not in the majority can be better

1131
01:06:35.389 --> 01:06:39.040
overall because when you can coordinate on that norm,

1132
01:06:39.100 --> 01:06:43.320
you do better, but it's not, that's usually not

1133
01:06:43.330 --> 01:06:47.689
enough to get the invasion of or the spread

1134
01:06:47.699 --> 01:06:49.709
of a rare norm. Even if it's better for

1135
01:06:49.719 --> 01:06:52.979
everyone to do a better thing. If you're trying

1136
01:06:52.989 --> 01:06:55.479
to agitate to get people to join a union,

1137
01:06:55.530 --> 01:06:57.760
you might think everyone might agree. It would be

1138
01:06:57.770 --> 01:06:59.879
better if we could all join the union. But

1139
01:06:59.889 --> 01:07:03.500
everyone's worried about doing it because if you try

1140
01:07:03.510 --> 01:07:05.000
and it's just a few of you doing it,

1141
01:07:05.010 --> 01:07:09.989
you all get fired. That's no good. So how

1142
01:07:10.000 --> 01:07:15.139
do you get uh rare norms that are ultimately

1143
01:07:15.149 --> 01:07:18.030
group, beneficial to spread and one solution that's been

1144
01:07:18.040 --> 01:07:20.239
proposed that we talk about? I talk about in

1145
01:07:20.250 --> 01:07:24.560
the book is group structure. So having community structure

1146
01:07:24.570 --> 01:07:27.340
where there are multiple communities that have that different

1147
01:07:27.350 --> 01:07:32.080
norms um can allow uh for the spread of

1148
01:07:32.550 --> 01:07:36.189
norms that are beneficial to flow from the better

1149
01:07:36.199 --> 01:07:38.510
performing group to the other performing group through several

1150
01:07:38.520 --> 01:07:42.010
mechanisms. Um THROUGH direct copying where people are more

1151
01:07:42.020 --> 01:07:45.409
like if you people occasionally copy norms associated with

1152
01:07:45.419 --> 01:07:50.469
success, even if occasionally they're from um individuals in

1153
01:07:50.479 --> 01:07:54.040
another community, it's much easier to spread rare norms.

1154
01:07:54.050 --> 01:07:57.409
If people move from a high performing group to

1155
01:07:57.419 --> 01:07:58.959
uh from a low performing group to a high

1156
01:07:58.969 --> 01:08:01.500
performing group, it allows norms to spread there. There

1157
01:08:01.510 --> 01:08:06.310
are several mechanisms. Um And this allows us to,

1158
01:08:06.320 --> 01:08:09.510
to think about populations, not just as like one

1159
01:08:09.520 --> 01:08:11.340
big group, which is a really common way to

1160
01:08:11.350 --> 01:08:13.820
model populations in a lot of fields, but think

1161
01:08:13.830 --> 01:08:16.810
about the importance of group structure and how that

1162
01:08:16.819 --> 01:08:20.950
matters when groups compete or even just interact.

1163
01:08:21.490 --> 01:08:25.229
So we've been talking here about modeling in the

1164
01:08:25.240 --> 01:08:28.169
social sciences, but I guess that we can also

1165
01:08:28.180 --> 01:08:33.450
talk about science itself as an institution as a

1166
01:08:33.459 --> 01:08:38.549
social phenomenon, right? So how do you approach that

1167
01:08:38.560 --> 01:08:41.589
through modeling? I mean, what are perhaps or what

1168
01:08:41.600 --> 01:08:43.359
would you say are perhaps some of the most

1169
01:08:43.370 --> 01:08:48.470
interesting aspects of how social dynamics operate in science

1170
01:08:48.819 --> 01:08:51.270
that will allow for us to have a better

1171
01:08:51.279 --> 01:08:55.839
understanding of how knowledge production occurs there.

1172
01:08:57.798 --> 01:09:00.627
Yeah. So this is an area of research that

1173
01:09:01.559 --> 01:09:05.309
I've sort of, I've been involved in since about

1174
01:09:05.318 --> 01:09:10.099
2014. And uh it started out as this kind

1175
01:09:10.108 --> 01:09:14.258
of side project and, and spiraled out into uh

1176
01:09:14.269 --> 01:09:17.868
more major projects because, because people ended up being

1177
01:09:17.877 --> 01:09:25.038
interested. Um And I think it's, I think science

1178
01:09:25.048 --> 01:09:28.738
in general is science is a, is a social

1179
01:09:28.749 --> 01:09:31.818
phenomenon like a lot of others, it has norms,

1180
01:09:32.078 --> 01:09:37.008
it involves beliefs and it involves uh people doing

1181
01:09:37.019 --> 01:09:42.139
things responding to incentives and we can model how

1182
01:09:42.148 --> 01:09:47.879
the sort of best practices uh for learning truth

1183
01:09:48.060 --> 01:09:55.120
interact with social factors like norms and incentives. So,

1184
01:09:55.339 --> 01:09:59.089
um I also think like it's really bad to

1185
01:09:59.100 --> 01:10:03.240
do science uncritically. I mean, thinking about what, what

1186
01:10:03.250 --> 01:10:05.629
are we doing when we're doing science? What, what

1187
01:10:05.640 --> 01:10:09.189
is science? How does, how does knowledge progress is

1188
01:10:09.200 --> 01:10:12.709
a nontrivial thing? And you know, it's been said

1189
01:10:12.720 --> 01:10:18.029
uh there, there is no science um that doesn't

1190
01:10:19.229 --> 01:10:22.310
involve philosophy of science, there's just science that is

1191
01:10:22.319 --> 01:10:27.660
uncritical about its philosophical assumptions. Um And I think,

1192
01:10:27.669 --> 01:10:30.479
I think that's right. Um So there are lots

1193
01:10:30.490 --> 01:10:32.649
of ways to model science and, and of course,

1194
01:10:32.660 --> 01:10:36.580
when we model anything we have to simplify. So

1195
01:10:37.129 --> 01:10:39.459
uh what I like to start out with is

1196
01:10:39.470 --> 01:10:42.540
to say, well, let's imagine a kind of simplified

1197
01:10:42.549 --> 01:10:49.479
pure science of a hypothesis tester testing hypotheses using

1198
01:10:49.490 --> 01:10:55.470
some qua high quality but occasionally error prone methods

1199
01:10:55.479 --> 01:10:59.060
like all methods are and then trying to discover

1200
01:10:59.069 --> 01:11:03.620
whether or not uh the results of the experiment

1201
01:11:04.339 --> 01:11:10.189
either support or fail to support the hypothesis. So

1202
01:11:11.709 --> 01:11:14.810
already this is an oversimplification but you know, it's,

1203
01:11:14.819 --> 01:11:16.680
it's, it's a model and, and it gets us

1204
01:11:16.689 --> 01:11:19.180
started and so we can model things this way

1205
01:11:19.189 --> 01:11:25.020
like OK, I've got, um I've got a result.

1206
01:11:25.689 --> 01:11:27.450
I have a method. I tested it using the

1207
01:11:27.459 --> 01:11:30.620
result. I know that if my result is correct,

1208
01:11:30.669 --> 01:11:33.359
that my method will give me a positive result.

1209
01:11:33.370 --> 01:11:36.290
If my hypothesis is correct, my, my method will

1210
01:11:36.299 --> 01:11:39.259
give me positive results. Let's say 80% of the

1211
01:11:39.270 --> 01:11:42.689
time, it's pretty good. And if my uh method

1212
01:11:42.700 --> 01:11:47.140
is wrong, my, if my hypothesis is wrong, my

1213
01:11:48.029 --> 01:11:50.509
method will give me a positive result. Still false

1214
01:11:50.520 --> 01:11:53.879
positive. Let's say 5% of the time most people

1215
01:11:53.890 --> 01:11:56.250
are comfortable with this kind of error, 5% error.

1216
01:11:57.069 --> 01:11:59.009
And so let's say I get a positive result.

1217
01:12:01.350 --> 01:12:05.209
What, what's the likelihood that I'm actually right? Well,

1218
01:12:05.419 --> 01:12:08.700
I got a positive result. So I know that

1219
01:12:08.709 --> 01:12:11.180
if, if the thing was true, it would be

1220
01:12:11.189 --> 01:12:14.859
positive 80% of the time. If it was false,

1221
01:12:14.870 --> 01:12:18.700
it would be positive 5% of the time. Which

1222
01:12:18.709 --> 01:12:21.790
means I don't know, a lot of people would

1223
01:12:21.799 --> 01:12:26.009
say, well, 5% chance that it's a false positive.

1224
01:12:26.020 --> 01:12:30.560
So there's a 95% chance that it's right. And

1225
01:12:31.609 --> 01:12:33.930
problem is that that might be right and it

1226
01:12:33.939 --> 01:12:36.069
might not be because actually I haven't given you

1227
01:12:36.080 --> 01:12:40.229
enough information to answer the question because an important

1228
01:12:40.240 --> 01:12:42.560
piece of information that you don't know in the

1229
01:12:42.569 --> 01:12:45.850
way I framed it is what was the probability

1230
01:12:45.859 --> 01:12:49.520
that any hypothesis I generate was correct? What's the

1231
01:12:49.529 --> 01:12:52.509
probability before I did the experiment that I was?

1232
01:12:52.520 --> 01:12:57.270
Right. And that matters a lot because it turns

1233
01:12:57.279 --> 01:13:01.689
out if most hypotheses I generate are correct and

1234
01:13:01.700 --> 01:13:03.540
I get a positive result with a, you know,

1235
01:13:03.549 --> 01:13:06.120
low error rate, then I, I can have pretty

1236
01:13:06.129 --> 01:13:08.640
good confidence. But if most hypotheses I generate are

1237
01:13:08.649 --> 01:13:10.560
wrong. If I'm testing lots of things that are

1238
01:13:10.569 --> 01:13:14.200
wrong, then there's a good chance that when I

1239
01:13:14.209 --> 01:13:15.919
do get a positive result, it's going to be

1240
01:13:15.930 --> 01:13:20.009
a false positive just because most hypotheses are wrong.

1241
01:13:20.939 --> 01:13:23.959
And so you need to consider that. And so

1242
01:13:23.970 --> 01:13:27.359
we can model is using, there's a, a very

1243
01:13:27.370 --> 01:13:31.299
famous equation in probability theory called base theorem uh

1244
01:13:31.310 --> 01:13:37.419
which just relates conditional probabilities. And so using things

1245
01:13:37.430 --> 01:13:40.970
like posit the probability of positive and uh sort

1246
01:13:40.979 --> 01:13:44.009
of type one or type two errors and the

1247
01:13:44.020 --> 01:13:46.770
prior probability of being right, which is also called

1248
01:13:46.779 --> 01:13:51.620
the base rate of hypotheses. We can explore how

1249
01:13:51.629 --> 01:13:54.950
error and hypothesis selection interact to produce more or

1250
01:13:54.959 --> 01:14:00.000
less accurate results from that. That's the sort of

1251
01:14:00.009 --> 01:14:02.689
building block of the rest of the models of

1252
01:14:02.700 --> 01:14:05.439
science that I explore in the chapter we can

1253
01:14:05.450 --> 01:14:09.919
explore things like what about publication bias. So what

1254
01:14:09.930 --> 01:14:14.089
happens? Uh IF, if I get a result and

1255
01:14:14.100 --> 01:14:18.180
it's positive or negative, I might have some probability,

1256
01:14:18.189 --> 01:14:19.970
I might be able to say, I think it's

1257
01:14:19.979 --> 01:14:23.120
right with some confidence. But if I know that

1258
01:14:23.129 --> 01:14:26.729
my error, my methods are error prone. And I

1259
01:14:26.740 --> 01:14:29.669
wasn't super confident in my answer in the beginning,

1260
01:14:30.000 --> 01:14:34.330
getting one positive result will probably increase my confidence.

1261
01:14:34.339 --> 01:14:37.270
But shouldn't make me certain, getting one negative result

1262
01:14:37.279 --> 01:14:39.689
will decrease my confidence. But shouldn't make me certain,

1263
01:14:40.129 --> 01:14:43.009
I need more evidence. And we can use these

1264
01:14:43.020 --> 01:14:47.259
sort of Bayesian approaches sometimes called Bayesian updating to

1265
01:14:47.270 --> 01:14:49.430
imagine. Well, ok. Well, what if we have replication?

1266
01:14:49.439 --> 01:14:53.520
What if we have lots of evidence, evidence for

1267
01:14:53.529 --> 01:14:56.870
or against a hypothesis? Well, if that evidence is

1268
01:14:56.879 --> 01:15:03.850
accurate and unbiased, even if it's all noisy overall,

1269
01:15:03.859 --> 01:15:05.990
as long as it's more likely to be right

1270
01:15:06.000 --> 01:15:09.200
than wrong bay and updating will eventually let me

1271
01:15:09.209 --> 01:15:12.779
converge on the correct res response, the correct inference

1272
01:15:12.790 --> 01:15:15.439
and say I'm highly confident after a number of

1273
01:15:15.450 --> 01:15:19.770
trials that it's correct or it's incorrect. The problem

1274
01:15:19.779 --> 01:15:22.060
is what if there's publication bias, right? What if

1275
01:15:22.069 --> 01:15:25.959
negative results are much less likely to be published

1276
01:15:26.049 --> 01:15:31.000
or communicated than positive results? And so confirmatory results

1277
01:15:31.009 --> 01:15:33.959
are more like confirmatory evidence is more likely to

1278
01:15:33.970 --> 01:15:37.270
get factored into my ba updating than disc confirmatory

1279
01:15:37.279 --> 01:15:41.779
evidence. And we can show that this has a

1280
01:15:41.790 --> 01:15:48.669
terrible effect, right? With uh when you have publication

1281
01:15:48.680 --> 01:15:52.490
bias, you can end up to having very, very

1282
01:15:52.500 --> 01:15:55.270
high confidence that some result is correct even when

1283
01:15:55.279 --> 01:15:58.939
it's not correct because your source of information is

1284
01:15:58.950 --> 01:16:02.209
is messed up. Um Finally, we can also look

1285
01:16:02.220 --> 01:16:05.959
at, you know, individual scientists and models in which

1286
01:16:06.169 --> 01:16:09.839
individuals vary in the quality of their methods. And

1287
01:16:09.850 --> 01:16:14.310
what happens when they look uh they're responding to

1288
01:16:14.319 --> 01:16:16.600
different kinds of incentives, like let's say incentives to

1289
01:16:16.609 --> 01:16:21.359
publish. So maybe every everyone is no one's being

1290
01:16:21.370 --> 01:16:23.830
strategic in, in some of these models, we can

1291
01:16:23.839 --> 01:16:25.459
say, well, let's say everyone is doing the best.

1292
01:16:25.470 --> 01:16:27.950
They can, they just have different methods, some methods

1293
01:16:27.959 --> 01:16:32.410
are better than others. But the lazy methods make

1294
01:16:32.419 --> 01:16:34.939
it easier to publish positive results than the more

1295
01:16:34.950 --> 01:16:38.729
rigorous methods which take longer or um are more

1296
01:16:38.740 --> 01:16:41.930
likely to correctly identify wrong results that don't get

1297
01:16:41.939 --> 01:16:48.080
you the prestigious publications. Well, if individuals who publish

1298
01:16:48.089 --> 01:16:51.470
lots of positive results are the ones that get

1299
01:16:51.479 --> 01:16:54.259
good jobs that get big grants that attract graduate

1300
01:16:54.270 --> 01:16:56.779
students. And therefore they're the ones who passed on

1301
01:16:56.790 --> 01:17:01.450
their methods. We get what, what Richard mcelreath and

1302
01:17:01.459 --> 01:17:03.990
I called the natural selection of bad science, which

1303
01:17:04.000 --> 01:17:07.930
is sort of this evolutionary dynamic in which the

1304
01:17:07.939 --> 01:17:11.129
quality of methods can degrade. Now, that doesn't have

1305
01:17:11.140 --> 01:17:14.779
to happen. Right. Again, the model, the conclusions of

1306
01:17:14.790 --> 01:17:18.200
the model only follow if the assumptions of the

1307
01:17:18.209 --> 01:17:21.020
model hold and there are cases in which those

1308
01:17:21.029 --> 01:17:23.189
assumptions don't hold. If, if it's not so easy

1309
01:17:23.200 --> 01:17:26.729
to publish low quality results. If peer review is

1310
01:17:26.740 --> 01:17:30.490
high quality, if there are other selection forces um

1311
01:17:30.770 --> 01:17:35.500
that, that maintain uh more rigorous results. If uh

1312
01:17:36.080 --> 01:17:39.310
the number of publications is less important than, you

1313
01:17:39.319 --> 01:17:41.919
know, the rigor of your work, then those dynamics

1314
01:17:41.930 --> 01:17:44.740
won't hold. Um But we can use models to

1315
01:17:44.750 --> 01:17:47.939
try to explore the nature of the science that

1316
01:17:47.950 --> 01:17:50.180
we are doing and, and the trustworthiness of the

1317
01:17:50.189 --> 01:17:53.770
science that we're doing. So that's, that's chapter eight.

1318
01:17:55.430 --> 01:17:58.209
Uh By the way, do you think that by

1319
01:17:58.220 --> 01:18:01.859
modeling science as a social phenomenon and learning more

1320
01:18:01.870 --> 01:18:06.689
about the social dynamics behind science as an institution

1321
01:18:06.890 --> 01:18:10.580
that perhaps we could, we could potentially use some

1322
01:18:10.589 --> 01:18:15.870
of these knowledge to uh try to reduce or

1323
01:18:15.879 --> 01:18:20.859
ideally eliminate some of the bad incentives that scientists

1324
01:18:20.870 --> 01:18:21.930
are exposed to?

1325
01:18:23.220 --> 01:18:25.410
I really hope so. Yeah, I mean, that's one

1326
01:18:25.419 --> 01:18:28.080
of the reasons I do it. Um YOU know,

1327
01:18:28.089 --> 01:18:30.890
to, to begin with the that early work, some

1328
01:18:30.899 --> 01:18:32.750
of that early work was, was really just to

1329
01:18:32.759 --> 01:18:36.770
call attention to the fact that, you know, this

1330
01:18:36.779 --> 01:18:39.490
kind of selection pressure for publication exists in a

1331
01:18:39.500 --> 01:18:42.169
lot of fields and if it goes unchecked, it

1332
01:18:42.180 --> 01:18:44.759
will lead to really bad outcomes. And, and I

1333
01:18:44.770 --> 01:18:47.830
think that that paper in particular, um it ends

1334
01:18:47.839 --> 01:18:50.089
up being my most cited paper. So obviously, it

1335
01:18:50.100 --> 01:18:52.779
had some influence. But um you know, I I

1336
01:18:52.790 --> 01:18:58.459
think it called attention to uh to this phenomenon

1337
01:18:58.470 --> 01:19:00.740
and, and it did encourage people along with lots

1338
01:19:00.750 --> 01:19:03.060
of other work by lots of people. I encourage

1339
01:19:03.069 --> 01:19:06.580
people to, to try to um downgrade these kinds

1340
01:19:06.589 --> 01:19:09.529
of incentives. Um I think these kinds of models

1341
01:19:09.540 --> 01:19:13.669
also can allow us to consider different kinds of

1342
01:19:13.680 --> 01:19:17.379
interventions, like what would happen if there were different

1343
01:19:17.390 --> 01:19:23.319
kinds of incentive structures. Um uh I with the

1344
01:19:23.330 --> 01:19:25.240
uh with Klin o'connor, who I know you, you

1345
01:19:25.250 --> 01:19:28.629
interviewed previously, we uh did a model of science

1346
01:19:28.899 --> 01:19:31.740
um that kind of integrated ideas about norms spreading

1347
01:19:31.750 --> 01:19:34.919
between communities and looked at how, you know, perhaps

1348
01:19:35.419 --> 01:19:38.899
interdisciplinary could get around some of the problems where

1349
01:19:38.910 --> 01:19:43.259
norms get entrenched in communities. And uh it becomes

1350
01:19:43.270 --> 01:19:46.799
harder for new and better methods to spread when

1351
01:19:46.810 --> 01:19:49.450
people have a bias toward doing things the way

1352
01:19:49.459 --> 01:19:52.319
they always did things? Um But if you promote

1353
01:19:52.850 --> 01:19:56.200
um the importance of getting ideas from other communities

1354
01:19:56.209 --> 01:20:00.270
or um getting the approval of individuals in other

1355
01:20:00.279 --> 01:20:03.450
communities, then this can allow um more high quality

1356
01:20:03.459 --> 01:20:05.470
methods to spread. And then we talk about that

1357
01:20:05.479 --> 01:20:07.029
in a, in a paper of mine that that's

1358
01:20:07.040 --> 01:20:08.830
not covered in the book but that there is

1359
01:20:09.779 --> 01:20:12.040
uh some things we can use these kinds of

1360
01:20:12.049 --> 01:20:13.160
models to do.

1361
01:20:13.720 --> 01:20:18.319
Mhm So I have a couple more questions. These

1362
01:20:18.330 --> 01:20:21.509
were not more general that I would like to

1363
01:20:21.520 --> 01:20:26.350
ask you. So um are there or is there

1364
01:20:26.359 --> 01:20:32.459
a set of guidelines that uh scientists, students and

1365
01:20:32.470 --> 01:20:36.740
other people should follow when they want to turn

1366
01:20:36.879 --> 01:20:40.580
an idea, an hypothesis that they might want to

1367
01:20:40.589 --> 01:20:43.180
test into a model?

1368
01:20:45.379 --> 01:20:47.770
It's a good question. I've tried to answer that.

1369
01:20:47.779 --> 01:20:51.600
Um Yeah, I wrote a 2020 paper called uh

1370
01:20:51.609 --> 01:20:54.080
How to turn a verbal verbal theory into a

1371
01:20:54.089 --> 01:20:57.200
formal model which uh I, I put a version

1372
01:20:57.209 --> 01:20:58.859
of that in the, in the book um in

1373
01:20:58.870 --> 01:21:03.149
chapter 10. Um And so I've tried to come

1374
01:21:03.160 --> 01:21:06.560
up with some guidelines uh and, and things to

1375
01:21:06.569 --> 01:21:10.069
keep in mind. Um And I'll, I'll go through

1376
01:21:10.080 --> 01:21:11.879
a little bit of that. But I, I want

1377
01:21:11.890 --> 01:21:14.720
to also say like, like so many things that

1378
01:21:14.729 --> 01:21:16.890
are ultimately that are creative in nature and I

1379
01:21:16.899 --> 01:21:20.240
think model building is creative in nature. Um There's,

1380
01:21:20.250 --> 01:21:23.970
there's no one right way to do it, but

1381
01:21:24.299 --> 01:21:26.740
there is, there are lots of the wrong ways

1382
01:21:26.750 --> 01:21:30.419
to do it too and um wrong ways to

1383
01:21:30.430 --> 01:21:35.540
do it are generally involve um just lack of

1384
01:21:35.549 --> 01:21:41.959
care and laziness or failure to consider one's assumptions

1385
01:21:42.339 --> 01:21:46.439
um or lack of preparation. So, failure to do

1386
01:21:46.450 --> 01:21:51.649
due diligence in um finding good techniques or uh

1387
01:21:51.669 --> 01:21:55.529
probating literature, et cetera. Um But I think that,

1388
01:21:55.870 --> 01:22:03.040
you know, generally being, being familiar with models helps

1389
01:22:03.049 --> 01:22:07.600
you build models. Um II, I want to talk

1390
01:22:07.609 --> 01:22:10.859
to somebody I think was a psychologist about developing

1391
01:22:10.870 --> 01:22:15.180
their model to, to sort of formalize a theory

1392
01:22:15.189 --> 01:22:16.890
they had. And, and I was like, I think

1393
01:22:16.899 --> 01:22:20.089
you should try to like go to the literature

1394
01:22:20.100 --> 01:22:22.370
and look at similar things that have been done

1395
01:22:22.750 --> 01:22:25.129
and maybe try to replicate some of those models

1396
01:22:25.140 --> 01:22:27.410
and really understand them before you try to build

1397
01:22:27.419 --> 01:22:30.250
your own model. And they were kind of surprised

1398
01:22:30.259 --> 01:22:32.169
about this result. And I just was sort of

1399
01:22:32.180 --> 01:22:36.299
shocked and I'm like, it's sort of like, you

1400
01:22:36.310 --> 01:22:39.600
know, it, if you're an architect, you don't just

1401
01:22:39.609 --> 01:22:42.740
start building up your own building, right? You, you,

1402
01:22:42.750 --> 01:22:46.509
you have to understand how previous architecture works. If

1403
01:22:46.520 --> 01:22:49.990
you're a painter, like you under, you learn basic,

1404
01:22:50.000 --> 01:22:54.240
you know, uh techniques about color and mixing and,

1405
01:22:54.250 --> 01:22:57.939
and brush techniques and you learn how different classic

1406
01:22:57.950 --> 01:23:01.180
painters made their paintings before you go and, and

1407
01:23:01.189 --> 01:23:04.000
you, you paint your own things. That's how good

1408
01:23:04.009 --> 01:23:06.000
painting works. That's how good architecture works and it's

1409
01:23:06.009 --> 01:23:12.870
how good model building works. Um They, there are

1410
01:23:12.879 --> 01:23:16.009
lots of, you know, when I, whenever I build

1411
01:23:16.020 --> 01:23:19.410
new models, there's often processes where I'm just like

1412
01:23:20.029 --> 01:23:23.629
taking things from other models I've seen and thinking

1413
01:23:23.640 --> 01:23:27.200
about how to, you know how certain pieces that

1414
01:23:27.209 --> 01:23:29.970
I've seen before might be able to fit together

1415
01:23:29.979 --> 01:23:32.790
to solve the problem that I'm working with now.

1416
01:23:33.540 --> 01:23:38.459
And ultimately, we wanna try to articulate our problem

1417
01:23:38.470 --> 01:23:42.189
or our idea as simply and clearly as possible.

1418
01:23:42.689 --> 01:23:44.890
There have been other cases in which I mean,

1419
01:23:44.899 --> 01:23:46.740
I can think of at least one instance where,

1420
01:23:47.850 --> 01:23:49.609
you know, we, we thought of all the things

1421
01:23:49.620 --> 01:23:53.189
involved in some scenario that would be maybe important

1422
01:23:53.200 --> 01:23:56.970
to some dynamic, some process and thought, OK, we'll

1423
01:23:56.979 --> 01:23:58.229
put this in the model and this in the

1424
01:23:58.240 --> 01:23:59.529
model and this in the model and this in

1425
01:23:59.540 --> 01:24:02.470
the model and you know, I can code a

1426
01:24:02.479 --> 01:24:06.020
model like that, I can run simulations with it,

1427
01:24:06.290 --> 01:24:08.830
but it comes in very difficult to analyze when

1428
01:24:08.839 --> 01:24:11.640
there's so many things going on. And part of

1429
01:24:11.649 --> 01:24:15.160
the, the value of a model is the simplification.

1430
01:24:15.790 --> 01:24:18.979
So what ended up having to happen was thinking

1431
01:24:18.990 --> 01:24:21.680
about, well, what are these, what of these factors,

1432
01:24:21.689 --> 01:24:24.540
which of them are essential to the idea that

1433
01:24:24.549 --> 01:24:26.919
I'm trying to develop and which of them are

1434
01:24:26.930 --> 01:24:31.790
maybe things to add later. But they, they're not

1435
01:24:31.799 --> 01:24:35.330
the core idea or the core, the core point

1436
01:24:35.339 --> 01:24:37.709
of the theory. And so if they're not, you

1437
01:24:37.720 --> 01:24:39.990
chuck them for now, it doesn't mean they're not

1438
01:24:40.000 --> 01:24:43.560
important, but it means they're not important to establish

1439
01:24:43.569 --> 01:24:48.220
a baseline. It's also like in any given domain,

1440
01:24:48.229 --> 01:24:51.520
like if I'm modeling cooper operation, there's hundreds or

1441
01:24:51.529 --> 01:24:54.270
thousands of co-operation models. So I'm gonna build on

1442
01:24:54.279 --> 01:24:56.859
all those and use what has been learned there

1443
01:24:56.870 --> 01:24:59.229
and maybe I can start extending and build a

1444
01:24:59.240 --> 01:25:01.509
fairly complicated model because I know how all the

1445
01:25:01.520 --> 01:25:05.359
simpler versions will work. If I'm, you know, modeling

1446
01:25:05.370 --> 01:25:09.109
AAA phenomenon that's not extensively modeled, that hasn't been

1447
01:25:09.120 --> 01:25:11.540
modeled much. I'm gonna try to start as simply

1448
01:25:11.549 --> 01:25:17.180
as possible because I need to establish baselines and,

1449
01:25:17.189 --> 01:25:19.120
and often it's frustrating because you want to go

1450
01:25:19.129 --> 01:25:22.640
more complicated. Um I'll tell you some advice that

1451
01:25:22.649 --> 01:25:25.629
um I, I took from uh there's a biologist

1452
01:25:25.640 --> 01:25:28.589
named John Wilkins, um who, who, who wrote a

1453
01:25:28.600 --> 01:25:30.859
really nice blog post along along a while ago,

1454
01:25:30.870 --> 01:25:34.549
maybe 1015 years ago. Um THAT I I've been

1455
01:25:34.560 --> 01:25:38.379
really influenced by on the relationship between modelers and

1456
01:25:38.390 --> 01:25:43.069
empirical scientists. And um John points out that it's

1457
01:25:43.080 --> 01:25:45.950
really important to not think of a modeler if

1458
01:25:45.959 --> 01:25:48.810
you're an empirical scientist interested in working with modelers,

1459
01:25:48.819 --> 01:25:53.830
like the modeler is not a technician whose job

1460
01:25:53.839 --> 01:25:56.029
it is to prove your idea is true with

1461
01:25:56.040 --> 01:26:00.879
math, right? The modeler is a theorist and a

1462
01:26:00.890 --> 01:26:05.649
and a scientist whose work contributes to the refinement

1463
01:26:05.660 --> 01:26:09.569
and further development of a theory. And it's not

1464
01:26:09.580 --> 01:26:11.540
gonna keep the theory as it is, it's going

1465
01:26:11.549 --> 01:26:14.439
to change the theory by formalizing it because you

1466
01:26:14.450 --> 01:26:16.370
have to realize what is in there and what's

1467
01:26:16.379 --> 01:26:18.790
not. And so some advice I always have is

1468
01:26:18.799 --> 01:26:25.029
like if you're a modeler, you know, take what

1469
01:26:25.040 --> 01:26:28.549
the emir the empirical researchers have done very seriously.

1470
01:26:28.899 --> 01:26:30.770
Um It, it drives me a little bit crazy

1471
01:26:30.779 --> 01:26:34.879
when people um say, well, they'll be doing a

1472
01:26:34.890 --> 01:26:37.640
model of, you know, a biological or social phenomenon.

1473
01:26:37.649 --> 01:26:39.359
And they'll say, well, I'm not really a biologist

1474
01:26:39.370 --> 01:26:41.509
or I'm not really a social scientist. And my

1475
01:26:41.520 --> 01:26:44.729
response is always like, but you're doing a model

1476
01:26:44.740 --> 01:26:46.870
of social science of a social phenomenon or of

1477
01:26:46.879 --> 01:26:49.799
a biological phenomenon. So in this time, like for

1478
01:26:49.810 --> 01:26:52.509
this project, you are a biologist or you are

1479
01:26:52.520 --> 01:26:55.279
a social scientist and therefore you need to be

1480
01:26:55.290 --> 01:26:59.779
judged by the standards of that group. And it's

1481
01:26:59.790 --> 01:27:02.540
not enough, you can't be lazy in the way

1482
01:27:02.549 --> 01:27:04.839
you make assumptions, you need to be thoughtful about

1483
01:27:04.850 --> 01:27:10.180
them. Um And you know, even like biologists and

1484
01:27:10.189 --> 01:27:12.169
social scientists who don't do modeling, like they know

1485
01:27:12.180 --> 01:27:15.970
a lot of things and talking to them will

1486
01:27:15.979 --> 01:27:19.930
help make sure that a model is grounded in

1487
01:27:19.939 --> 01:27:22.810
what's already known is consistent with the best evidence

1488
01:27:22.819 --> 01:27:26.580
that we have that maybe engages with some potential

1489
01:27:26.589 --> 01:27:29.790
theories that this will help to formalize or disambiguate

1490
01:27:29.799 --> 01:27:33.609
between the the the reverse of that is if

1491
01:27:33.620 --> 01:27:36.029
you are an empirical researcher, right, take what the

1492
01:27:36.040 --> 01:27:39.399
modeler is doing seriously and understand that they are

1493
01:27:39.410 --> 01:27:42.109
not a technician who is there to, you know,

1494
01:27:42.120 --> 01:27:44.779
just show that you're right, but they are a

1495
01:27:44.790 --> 01:27:49.459
collaborator. And that means that you have to understand

1496
01:27:49.470 --> 01:27:52.709
how the model works, at least enough to be

1497
01:27:52.720 --> 01:27:58.750
able to assess um the quality of the assumptions

1498
01:27:58.759 --> 01:28:00.720
that are going into it. And that's actually a

1499
01:28:00.729 --> 01:28:04.540
really important contribution empirical researchers make to modeling. When

1500
01:28:04.549 --> 01:28:06.990
I collaborate with people who spend more of their

1501
01:28:07.000 --> 01:28:10.470
time doing empirical work, it's extremely valuable and and

1502
01:28:10.479 --> 01:28:12.759
a really rewarding because they are able to bring

1503
01:28:12.770 --> 01:28:15.970
all of their domain knowledge to the modeling, which

1504
01:28:15.979 --> 01:28:19.140
helps make the model much better. So I think

1505
01:28:19.149 --> 01:28:21.799
actually like there's a lot of room for collaboration

1506
01:28:21.930 --> 01:28:25.959
between modelers and and researchers who focus more on

1507
01:28:25.970 --> 01:28:26.839
empirical work.

1508
01:28:28.109 --> 01:28:31.089
So I I guess that you've already ended up

1509
01:28:31.100 --> 01:28:35.140
talking about many of the considerations that we have

1510
01:28:35.149 --> 01:28:38.490
to take into account when it comes to my

1511
01:28:38.500 --> 01:28:41.770
last question here. But what would you say are

1512
01:28:41.779 --> 01:28:47.419
probably some of the main limitations of models that

1513
01:28:47.430 --> 01:28:50.970
it is important for people to keep in mind

1514
01:28:50.979 --> 01:28:55.470
when modeling and uh after modeling and after they

1515
01:28:55.479 --> 01:28:59.399
get the results, the kinds of conclusions that they

1516
01:28:59.410 --> 01:29:01.390
can derive from them.

1517
01:29:02.069 --> 01:29:08.729
Yeah. So I, I feel like as a just

1518
01:29:08.740 --> 01:29:10.169
in general and certainly in the book, I'm a

1519
01:29:10.180 --> 01:29:12.859
bit of a modeling evangelist where I, you know,

1520
01:29:12.870 --> 01:29:15.229
modeling is, is great and I, I love it

1521
01:29:15.240 --> 01:29:18.729
and I want, I want social scientists, you know,

1522
01:29:18.740 --> 01:29:23.509
broadly defined to use models more. But um of

1523
01:29:23.520 --> 01:29:27.879
course, there are caveats, right? And the world is

1524
01:29:27.890 --> 01:29:33.040
really complicated and any one model is going to

1525
01:29:33.049 --> 01:29:36.399
omit a ton of important details about the world

1526
01:29:36.779 --> 01:29:40.390
and that's fine and that's actually like, right and

1527
01:29:40.399 --> 01:29:45.049
good. That's the point of models is their omission

1528
01:29:45.220 --> 01:29:49.169
because by omitting things, they draw our attention to

1529
01:29:49.180 --> 01:29:52.359
the simplified versions that he help us see patterns

1530
01:29:52.370 --> 01:29:54.259
that are harder to see when we look at

1531
01:29:54.270 --> 01:29:58.529
the real world. But um if we rely too

1532
01:29:58.540 --> 01:30:01.979
much on a model uh to base our con

1533
01:30:02.060 --> 01:30:05.419
you know, base our assumptions on, um we end

1534
01:30:05.430 --> 01:30:10.810
up forgetting all the things that are not realistic,

1535
01:30:10.819 --> 01:30:13.529
right, or limitations to the model or things, maybe

1536
01:30:13.540 --> 01:30:17.410
a model didn't take into consideration. Well, the model

1537
01:30:17.419 --> 01:30:20.509
made the assumption, let's say that people uh interact

1538
01:30:20.520 --> 01:30:24.100
randomly but people don't interact randomly. So how do

1539
01:30:24.109 --> 01:30:26.810
the conclusions of the model change when we population

1540
01:30:26.819 --> 01:30:29.240
structure? Or the model assumes that the only thing

1541
01:30:29.250 --> 01:30:32.379
people care about is um material wealth, but people

1542
01:30:32.390 --> 01:30:34.439
care about other things and material wealth. So how

1543
01:30:34.450 --> 01:30:39.009
does the behavior that we expect change when people's

1544
01:30:39.020 --> 01:30:44.359
um values are different um like economics, which is

1545
01:30:44.370 --> 01:30:49.529
the social science um that embraced modeling earliest and

1546
01:30:49.540 --> 01:30:54.859
most enthusiastically had a real problem where they, you

1547
01:30:54.870 --> 01:30:57.200
know, you know, for a while and, and especially

1548
01:30:57.209 --> 01:31:01.180
like coming into the 19 seventies ish, um we're

1549
01:31:01.189 --> 01:31:04.209
making a lot of assumptions about the way things

1550
01:31:04.220 --> 01:31:09.970
are um based on models that were not continuously

1551
01:31:09.979 --> 01:31:14.700
checked against empirical data that were not continuously refined

1552
01:31:14.709 --> 01:31:19.430
to better align with empirical data. And um the

1553
01:31:19.439 --> 01:31:22.229
observations of people who are actually out on the

1554
01:31:22.240 --> 01:31:27.080
streets. And that's basically why behavioral economics as a

1555
01:31:27.089 --> 01:31:29.520
field, it was able to get as big as

1556
01:31:29.529 --> 01:31:31.250
it did and become as important as it was.

1557
01:31:31.259 --> 01:31:32.870
I mean, that whole field is, is just a

1558
01:31:32.879 --> 01:31:37.270
cottage industry of demonstrating all the cases in which

1559
01:31:37.279 --> 01:31:39.990
the models of economists were wrong were making like

1560
01:31:40.000 --> 01:31:43.540
incorrect assumptions and it was really beneficial. And I

1561
01:31:43.549 --> 01:31:46.520
think for, for that field to, to, to see

1562
01:31:46.529 --> 01:31:50.359
all these mismatches between their model and the way

1563
01:31:50.370 --> 01:31:53.750
people actually behave because then it forces a revision

1564
01:31:53.759 --> 01:31:57.160
of the models it, it forces frameworks. Uh It

1565
01:31:57.169 --> 01:32:00.729
follows models um to be revised. And this is,

1566
01:32:00.740 --> 01:32:03.220
is one of the reasons why some of us

1567
01:32:03.229 --> 01:32:07.350
are really interested in not just individual theories where

1568
01:32:07.359 --> 01:32:11.129
each model corresponds to one theory in isolation, but

1569
01:32:11.140 --> 01:32:15.140
developing frameworks theoretical frameworks where we have many, many

1570
01:32:15.149 --> 01:32:19.660
linked sets of assumptions. Um And, and the idea

1571
01:32:19.669 --> 01:32:23.229
of building a theoretical framework means that the more

1572
01:32:23.240 --> 01:32:27.160
rich our framework is the, the, the more our

1573
01:32:27.339 --> 01:32:32.020
model assumptions are constrained because they have to um

1574
01:32:33.959 --> 01:32:38.640
they have to agree not just with uh aspects

1575
01:32:38.649 --> 01:32:42.359
of the system that we're modeling right there. And

1576
01:32:42.370 --> 01:32:46.049
then, but lots of aspects of human behavior and

1577
01:32:46.060 --> 01:32:50.569
psychology and social organization and evolution and culture that

1578
01:32:50.580 --> 01:32:52.490
we know from lots and lots of work and

1579
01:32:52.500 --> 01:32:55.240
lots of different models and lots of empirical science.

1580
01:32:55.419 --> 01:32:57.529
And so all of this ends up being in

1581
01:32:57.540 --> 01:33:03.790
service hopefully of developing more coherent and cogent theoretical

1582
01:33:03.799 --> 01:33:08.759
frameworks for uh the social sciences. Um I, I

1583
01:33:08.770 --> 01:33:11.319
like the, the analogy of maps and territories, right?

1584
01:33:11.330 --> 01:33:13.589
Like the map is not the territory, but you

1585
01:33:13.600 --> 01:33:16.950
would rather have a map to explore the territory

1586
01:33:17.189 --> 01:33:20.520
and the the solution ultimately to, you know, to

1587
01:33:20.529 --> 01:33:22.270
get a better sense of the territory is to

1588
01:33:22.279 --> 01:33:25.830
have many maps that serve different functions.

1589
01:33:27.950 --> 01:33:31.529
Great. So let's send on that note then and

1590
01:33:31.540 --> 01:33:35.759
the book is again modeling social behavior, mathematical and

1591
01:33:35.770 --> 01:33:39.569
agent based models of social dynamics and cultural evolution

1592
01:33:39.870 --> 01:33:41.939
and leaving a link to it in the description

1593
01:33:41.950 --> 01:33:45.919
box of the interview. And doctor Min, just before

1594
01:33:45.930 --> 01:33:48.669
we go apart from the book, would you like

1595
01:33:48.680 --> 01:33:51.020
to tell people where they can find you and

1596
01:33:51.029 --> 01:33:52.569
your work on the internet?

1597
01:33:53.890 --> 01:33:57.129
Sure. Yeah. Uh You can uh the easiest way

1598
01:33:57.140 --> 01:33:59.799
to get in touch or, or find out what

1599
01:33:59.810 --> 01:34:02.009
I'm up to is my website which is just

1600
01:34:02.020 --> 01:34:06.430
malvino.com. Um I'm not really on Twitter or X

1601
01:34:06.439 --> 01:34:09.180
anymore. Um But I, I am occasionally on blue

1602
01:34:09.189 --> 01:34:12.709
sky for those who are using it. Um But

1603
01:34:12.720 --> 01:34:14.509
the best way is probably just to check my

1604
01:34:14.520 --> 01:34:16.950
website. Um So yeah,

1605
01:34:17.919 --> 01:34:20.669
great. So thank you so much again for taking

1606
01:34:20.680 --> 01:34:22.330
the time to come on the show. It's been

1607
01:34:22.339 --> 01:34:23.669
a pleasure to talk with you.

1608
01:34:24.500 --> 01:34:25.600
It was great. Thank you.

1609
01:34:26.839 --> 01:34:29.569
Hi guys. Thank you for watching this interview. Until

1610
01:34:29.580 --> 01:34:31.740
the end. If you liked it, please share it.

1611
01:34:31.750 --> 01:34:34.549
Leave a like and hit the subscription button. The

1612
01:34:34.560 --> 01:34:36.660
show is brought to you by N Lights learning

1613
01:34:36.669 --> 01:34:39.680
and development. Then differently check the website at N

1614
01:34:39.689 --> 01:34:43.640
lights.com and also please consider supporting the show on

1615
01:34:43.649 --> 01:34:46.689
Patreon or paypal. I would also like to give

1616
01:34:46.700 --> 01:34:49.000
a huge thank you to my main patrons and

1617
01:34:49.009 --> 01:34:53.220
paypal supporters, Perera Larson, Jerry Muller and Frederick Suno,

1618
01:34:53.270 --> 01:34:56.339
Bernard Seche O of Alex Adam, Castle Matthew Whitting

1619
01:34:56.379 --> 01:34:59.620
B no wolf, Tim Ho Erica LJ Connors, Philip

1620
01:34:59.629 --> 01:35:02.540
Forrest Connelly. Then the Met Robert Wine in NAI

1621
01:35:02.899 --> 01:35:06.290
Z Mark Nevs calling in Holbrook Field governor Mikel

1622
01:35:06.299 --> 01:35:09.729
Stormer Samuel Andre Francis for the Agns Ferger, Ken

1623
01:35:10.100 --> 01:35:14.310
Herz and Lain Jung Y and the Samuel K

1624
01:35:14.500 --> 01:35:18.169
Hes Mark Smith Jungle, Tom Hummel Sran, David Sloan

1625
01:35:18.259 --> 01:35:23.640
Wilson Yasa, Dear Roman Roach Diego, Jan Punter, Romani

1626
01:35:23.859 --> 01:35:27.089
Charlotte Bli Nicole Barba Ad Hunt Pavlo Stassi, Nale

1627
01:35:27.759 --> 01:35:31.689
Me, Gary G Alman, Samos, Ari and YPJ Barboza

1628
01:35:31.979 --> 01:35:36.359
Julian Price Edward Hall, Eden Broner Douglas Fry Franca

1629
01:35:36.930 --> 01:35:43.870
Lati Gilon Cortez Solis Scott Zachary. Ftw Daniel Friedman,

1630
01:35:43.879 --> 01:35:48.290
William Buckner, Paul Giorgino, Luke Loki, Georgio Theophanous Chris

1631
01:35:48.299 --> 01:35:52.220
Williams and Peter Wo David Williams Di A Costa

1632
01:35:52.229 --> 01:35:56.430
Anton Erickson, Charles Murray, Alex Shaw, Marie Martinez, Coralie

1633
01:35:56.439 --> 01:36:02.180
Chevalier, Bangalore Fist, Larry Dey Junior, Old Einon Starry

1634
01:36:02.189 --> 01:36:05.620
Michael Bailey then Spur by Robert Grassy Zorn, Jeff

1635
01:36:05.629 --> 01:36:09.890
mcmahon, Jake Zul Barnabas Radis Mark Temple, Thomas Dvor

1636
01:36:10.200 --> 01:36:14.689
Luke Neeson Chris to Kimberley Johnson Benjamin Gilbert Jessica.

1637
01:36:14.700 --> 01:36:19.620
No, Linda Brendan Nicholas Carlson, Ismael Bensley Man George

1638
01:36:20.209 --> 01:36:25.109
Katis Valentine Steinman, Perlis Kate Van Goler, Alexander Abert

1639
01:36:25.180 --> 01:36:30.939
Liam Dan Biar Masoud Ali Mohammadi Perpendicular J Ner

1640
01:36:31.520 --> 01:36:35.009
Urla. Good enough Gregory Hastings David Pins of Sean

1641
01:36:35.470 --> 01:36:39.665
Nelson, Mike Levin and Jos Net. A special thanks

1642
01:36:39.674 --> 01:36:42.055
to my producers is our web, Jim Frank Luca

1643
01:36:42.645 --> 01:36:46.334
Stein. Tom Ween, Bernard N Cortes Dixon, Bendik Muller

1644
01:36:46.345 --> 01:36:50.084
Thomas Trumble, Catherine and Patrick Tobin, John Carl Negro,

1645
01:36:50.294 --> 01:36:52.984
Nick Ortiz and Nick Golden. And to my executive

1646
01:36:52.995 --> 01:36:57.044
producers, Matthew Lavender, Si Adrian Bogdan Knits and Rosie.

1647
01:36:57.055 --> 01:36:58.004
Thank you for all.

