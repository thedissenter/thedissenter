WEBVTT

1
00:00:00.239 --> 00:00:03.170
Hello everyone. Welcome to a new episode of The

2
00:00:03.170 --> 00:00:06.489
Dissenter. I'm your host, as always, Ricardo Lops, and

3
00:00:06.489 --> 00:00:09.930
today I'm joined by Doctor Michael Cholby. He's professor

4
00:00:09.930 --> 00:00:12.890
and personal chair in philosophy at the University of

5
00:00:12.890 --> 00:00:17.389
Edinburgh. He has published widely in ethical theory, practical

6
00:00:17.389 --> 00:00:20.930
ethics, and the philosophy of Deaf and Dying. And

7
00:00:20.930 --> 00:00:23.819
today we're going to talk about the philosophy of

8
00:00:23.819 --> 00:00:26.569
death and dying. So Doctor Cholby, welcome to the

9
00:00:26.569 --> 00:00:28.010
show. It's a pleasure to everyone.

10
00:00:28.909 --> 00:00:30.180
It's a pleasure for me and I appreciate the

11
00:00:30.180 --> 00:00:30.750
invitation.

12
00:00:32.029 --> 00:00:36.349
So let me start with perhaps uh an introductory

13
00:00:36.349 --> 00:00:39.229
question. So what is the philosophy of death and

14
00:00:39.229 --> 00:00:42.509
dying? What kinds of topics and questions do you

15
00:00:42.509 --> 00:00:43.509
explore there?

16
00:00:44.979 --> 00:00:48.700
Well, almost every, uh, philosophical tradition in the world,

17
00:00:48.819 --> 00:00:54.080
Western, uh, non-Western, Asian, etc. uh, EVENTUALLY comes to,

18
00:00:54.139 --> 00:00:57.220
uh, the question of, of what significance or importance

19
00:00:57.220 --> 00:00:59.380
we should assign to our deaths and to the

20
00:00:59.380 --> 00:01:01.860
fact that we are mortal. Um, YOU know, it's

21
00:01:01.860 --> 00:01:05.059
a question entertained by, you know, Socrates and Confucius

22
00:01:05.059 --> 00:01:07.900
and, and so many, um, illustrious figures in the

23
00:01:07.900 --> 00:01:11.080
history of philosophy. Uh, I suppose, um, some of

24
00:01:11.080 --> 00:01:13.050
the questions that we entertain in philosophy of death

25
00:01:13.050 --> 00:01:15.489
and dying could be thought of as metaphysical questions,

26
00:01:15.849 --> 00:01:18.250
just what is death? When does it occur? Uh,

27
00:01:18.330 --> 00:01:20.510
IS there any prospect that we might survive it?

28
00:01:20.809 --> 00:01:23.489
Um, MANY of the questions that we undertake, I

29
00:01:23.489 --> 00:01:27.529
suppose, are, um, ethical questions insofar as they're questions

30
00:01:27.529 --> 00:01:31.529
about the prudential significance of death, whether death could

31
00:01:31.529 --> 00:01:34.830
be a harm to us, uh, whether there's, uh,

32
00:01:34.930 --> 00:01:37.569
anything about death that might merit fear or some

33
00:01:37.569 --> 00:01:41.069
other emotion. Um, AND then, of course, there's many

34
00:01:41.069 --> 00:01:44.470
questions that, uh, are arguably, uh, moral questions, questions

35
00:01:44.470 --> 00:01:47.779
about, um, exactly what makes, um, you know, killing,

36
00:01:47.949 --> 00:01:51.449
uh, morally objectionable, and that, of course, encompasses both,

37
00:01:51.870 --> 00:01:54.610
uh, the killing of others, but also self-killing, suicide.

38
00:01:55.430 --> 00:01:57.510
Um, I think when you look into the literature

39
00:01:57.510 --> 00:01:59.830
that philosophers have produced on the, on, uh, death

40
00:01:59.830 --> 00:02:02.389
and dying, You know, it, it extends in quite

41
00:02:02.389 --> 00:02:04.599
a few directions. Um, YOU know, people have been

42
00:02:04.599 --> 00:02:07.389
exploring in recent years, you know, questions at the

43
00:02:07.389 --> 00:02:10.470
intersection of death and dying and technology, whether there's

44
00:02:10.470 --> 00:02:13.210
some sense in which, uh, we could, uh, uh,

45
00:02:13.220 --> 00:02:16.029
cheat death through certain kinds of technologies, avoid it,

46
00:02:16.309 --> 00:02:19.050
uh, or perhaps soften the blow by, um, creating,

47
00:02:19.110 --> 00:02:21.589
uh, you know, digital avatars of ourselves that our

48
00:02:21.589 --> 00:02:23.789
descendants and so forth could interact with and sort

49
00:02:23.789 --> 00:02:26.529
of keep us alive in their inner subjective world.

50
00:02:26.990 --> 00:02:29.889
Um, AND of course, you know, um, Uh, whenever

51
00:02:29.889 --> 00:02:31.690
you're talking about death and dying, questions of medical

52
00:02:31.690 --> 00:02:34.429
ethics are also, you know, um, uh, in the,

53
00:02:34.479 --> 00:02:37.169
uh, atmosphere as well. So, it's quite a broad

54
00:02:37.169 --> 00:02:38.729
field with a very long history, and it's, and

55
00:02:38.729 --> 00:02:40.250
it's very vibrant right now too.

56
00:02:41.360 --> 00:02:44.729
Great, so let's explore some of those questions then.

57
00:02:45.029 --> 00:02:47.949
What is death? How is it defined?

58
00:02:48.699 --> 00:02:50.460
Well, this is one of the contentious issues in

59
00:02:50.460 --> 00:02:53.059
our field. I suppose the straightforward answer is simply

60
00:02:53.059 --> 00:02:55.179
to say that uh death is the condition in

61
00:02:55.179 --> 00:02:58.149
which uh a person no longer exists. Uh, BUT

62
00:02:58.149 --> 00:03:01.300
of course, that's doesn't seem quite right since, um,

63
00:03:01.460 --> 00:03:02.619
you know, if you happen to believe that there's

64
00:03:02.619 --> 00:03:05.059
an afterlife, you still believe that we die, and

65
00:03:05.059 --> 00:03:07.339
yet, you know, we, we somehow managed to survive

66
00:03:07.339 --> 00:03:10.360
death. So I suppose the starting place, which, uh,

67
00:03:10.539 --> 00:03:13.539
that almost everyone agrees to is that death is

68
00:03:13.539 --> 00:03:17.300
that fact or event wherein, um, we come to

69
00:03:17.300 --> 00:03:21.100
no longer be alive in a biological sense, um,

70
00:03:21.339 --> 00:03:23.899
but there are, uh, uh, even beneath that, a

71
00:03:23.899 --> 00:03:26.580
good many, uh, disputed questions. There are those who

72
00:03:26.580 --> 00:03:28.740
think that when we think about death, we should

73
00:03:28.740 --> 00:03:31.660
be thinking about death primarily in terms of the

74
00:03:31.660 --> 00:03:34.619
cessation of our consciousness, that perhaps what it is

75
00:03:34.619 --> 00:03:36.660
to die is to no longer be a conscious

76
00:03:36.660 --> 00:03:40.850
being. Others who think, um, uh, that death should

77
00:03:40.850 --> 00:03:44.410
be thought of, uh, as, uh, the cessation of

78
00:03:44.410 --> 00:03:47.490
certain kinds of functions we have as organisms, so,

79
00:03:47.770 --> 00:03:50.399
uh, the functioning of our brains or perhaps the,

80
00:03:50.410 --> 00:03:54.410
uh, cardiopulmonary activity, the, the, the, uh, circulation of

81
00:03:54.410 --> 00:03:57.490
our blood, the, the aspiration of, of oxygen, and

82
00:03:57.490 --> 00:04:00.589
others who think that the relevant fact, uh, to,

83
00:04:00.649 --> 00:04:02.649
uh, point to when thinking about death is one

84
00:04:02.649 --> 00:04:04.369
might think of as a loss of personhood, sort

85
00:04:04.369 --> 00:04:07.570
of the loss of Um, perhaps not conscious awareness,

86
00:04:07.669 --> 00:04:10.229
but sort of a sense of oneself as existing

87
00:04:10.229 --> 00:04:13.259
across time. Um, YOU might think that in a

88
00:04:13.259 --> 00:04:16.640
certain way, um, there, uh, to, to not have,

89
00:04:16.670 --> 00:04:19.149
uh, memories of one's past selves, or not be

90
00:04:19.149 --> 00:04:21.309
able to, uh, think about one's future is in

91
00:04:21.309 --> 00:04:24.450
a sense to die, uh, uh, according to some.

92
00:04:24.829 --> 00:04:26.989
So, it's one of the contentious issues, uh, within

93
00:04:26.989 --> 00:04:29.890
the philosophy of death and dying, how to characterize

94
00:04:29.890 --> 00:04:31.880
the nature of death, and that in turn has

95
00:04:31.880 --> 00:04:37.589
ramifications for uh how death, uh, criteria, right, are

96
00:04:37.589 --> 00:04:40.390
understood within medicine. Whether we should think that someone

97
00:04:40.390 --> 00:04:43.470
dies, you know, when a certain, uh, threshold of

98
00:04:43.470 --> 00:04:45.910
brain activity is met, or whether we should certainly,

99
00:04:46.019 --> 00:04:48.109
uh, whether we should think that death is simply

100
00:04:48.109 --> 00:04:52.269
the cessation of your conscious awareness. So, um, there's

101
00:04:52.269 --> 00:04:54.549
sort of easy answer, right? That's sort of the

102
00:04:54.549 --> 00:04:57.070
cessation of our existence, but after that, there aren't

103
00:04:57.070 --> 00:04:58.250
too many easy answers.

104
00:04:58.910 --> 00:05:02.549
So, is that good or bad for us, and

105
00:05:02.549 --> 00:05:04.989
how do we approach such a question?

106
00:05:06.720 --> 00:05:08.869
Well, I think there's an array of views that

107
00:05:08.869 --> 00:05:12.000
philosophers have held here. A good starting point is

108
00:05:12.000 --> 00:05:14.959
the view that was advocated by the ancient Epicurean

109
00:05:14.959 --> 00:05:19.079
philosophers. They argued that um death is neither a

110
00:05:19.079 --> 00:05:21.720
harm nor a benefit to us, uh, because if

111
00:05:21.720 --> 00:05:23.989
in fact it is the cessation of our existence,

112
00:05:24.279 --> 00:05:26.880
then there won't be uh anything to it, right?

113
00:05:26.989 --> 00:05:29.119
To, to, to quote Epicurus himself, death is nothing

114
00:05:29.119 --> 00:05:30.869
to us. It's not a condition in which we

115
00:05:30.869 --> 00:05:34.799
exist. Um, AND so there's nothing harmful or beneficial

116
00:05:34.799 --> 00:05:39.170
about being in that condition. Um, MORE contemporary philosophers,

117
00:05:39.209 --> 00:05:41.529
I think, are, are drawn to the idea that

118
00:05:41.890 --> 00:05:45.170
the harms or perhaps benefits of death are not

119
00:05:45.170 --> 00:05:46.970
to be thought of in terms of what it's

120
00:05:46.970 --> 00:05:50.519
like to be dead, right? But rather what, uh,

121
00:05:50.529 --> 00:05:54.489
impact death has on the duration and quality of

122
00:05:54.489 --> 00:05:57.350
your life. So, a prominent view, which with I,

123
00:05:57.450 --> 00:06:00.209
with which I'm pretty sympathetic, is known as comparativism

124
00:06:00.209 --> 00:06:04.100
or deprivationism about death's value. And that view essentially

125
00:06:04.100 --> 00:06:06.700
says that to think about whether or not someone's

126
00:06:06.700 --> 00:06:08.820
death at a given time is good or bad

127
00:06:08.820 --> 00:06:12.059
for them, you compare the life that they, uh,

128
00:06:12.540 --> 00:06:15.859
realize, right, by dying at that time, to the

129
00:06:15.859 --> 00:06:18.290
longer life they would have had if they'd survived

130
00:06:18.290 --> 00:06:20.739
further, right? So we're comparing sort of an actual

131
00:06:20.739 --> 00:06:23.820
life to a counterfactual life that a person would

132
00:06:23.820 --> 00:06:26.679
have had if they'd survived longer. And the thought

133
00:06:26.679 --> 00:06:29.480
is that the, uh, death is a harm to

134
00:06:29.480 --> 00:06:31.839
you, just in case, had you lived longer, you

135
00:06:31.839 --> 00:06:34.640
had a better and longer life, or uh it's

136
00:06:34.640 --> 00:06:36.480
a benefit to you if, uh, had you lived

137
00:06:36.480 --> 00:06:38.880
longer, uh, you would have been worse off, your

138
00:06:38.880 --> 00:06:40.799
life would have been on balance, not as good

139
00:06:40.799 --> 00:06:43.720
for you. Um, I think that that's a pretty

140
00:06:43.720 --> 00:06:45.920
attractive position in part because it seems to me

141
00:06:45.920 --> 00:06:48.910
that we should Um, begin from the presumption that

142
00:06:49.170 --> 00:06:51.450
probably it isn't true that death is always bad

143
00:06:51.450 --> 00:06:53.200
for us, and probably it isn't true that death

144
00:06:53.200 --> 00:06:55.570
is always good for us. Probably, it's good or

145
00:06:55.570 --> 00:06:58.489
bad, depending upon the circumstances and the timing of

146
00:06:58.489 --> 00:07:01.489
it. And so, I think that comparativeist view gives

147
00:07:01.489 --> 00:07:03.250
us a pretty good handle on on how that

148
00:07:03.250 --> 00:07:05.329
could be, right? Uh, YOU know, your, your death

149
00:07:05.329 --> 00:07:07.250
is good or bad for you, depending on when

150
00:07:07.250 --> 00:07:08.959
it happens, and then sort of what would have

151
00:07:08.959 --> 00:07:10.929
happened to you, had you not died when you

152
00:07:10.929 --> 00:07:11.209
did.

153
00:07:12.739 --> 00:07:15.290
Would the immortality be good for us?

154
00:07:16.850 --> 00:07:18.619
Well, this is uh a question that's been much

155
00:07:18.619 --> 00:07:22.730
disputed in recent years. Um, I think those who

156
00:07:22.730 --> 00:07:25.649
think that immortality would be good for us, uh,

157
00:07:25.690 --> 00:07:28.230
are apt to say, well, it gives us more

158
00:07:28.230 --> 00:07:32.040
opportunity to enjoy the goods of life. Um, I

159
00:07:32.040 --> 00:07:34.040
point out in one of my articles that, that

160
00:07:34.040 --> 00:07:36.690
also means it's more opportunity to face the bads

161
00:07:36.690 --> 00:07:39.170
of life. And so one has to think about

162
00:07:39.450 --> 00:07:42.410
whether or not, uh, uh, the, the future one

163
00:07:42.410 --> 00:07:44.369
would have, which would be infinitely long if we

164
00:07:44.369 --> 00:07:47.309
were truly immortal, would be one that contains enough,

165
00:07:47.489 --> 00:07:50.010
right, of the goods, uh, uh, to outweigh the

166
00:07:50.010 --> 00:07:53.049
potential bads. The argument that I think has been

167
00:07:53.049 --> 00:07:56.380
um most influential amongst philosophers in terms of thinking

168
00:07:56.380 --> 00:07:59.380
about this, is an argument due to um the

169
00:07:59.380 --> 00:08:02.579
British philosopher Bernard Williams, who argued that the ultimate

170
00:08:02.579 --> 00:08:05.579
problem with immortality is that somehow we would ultimately

171
00:08:05.579 --> 00:08:07.820
find it tedious or boring, right? We would run

172
00:08:07.820 --> 00:08:12.410
out of endeavors or pursuits that are um, uh,

173
00:08:12.420 --> 00:08:14.859
sufficiently attractive to us to make us think that

174
00:08:14.859 --> 00:08:17.859
it'd be good to, to be alive forever. Um,

175
00:08:18.149 --> 00:08:20.750
NOT all philosophers have accepted, uh, Williams's argument. It's

176
00:08:20.750 --> 00:08:23.190
in fact, one of the most, uh, disputed arguments

177
00:08:23.190 --> 00:08:25.149
in our field. Uh, BUT it set a lot

178
00:08:25.149 --> 00:08:26.950
of the agenda, right? It certainly got a lot

179
00:08:26.950 --> 00:08:29.809
of people thinking about whether or not Um, it

180
00:08:29.809 --> 00:08:32.849
would be desirable, right, to face a future that

181
00:08:32.849 --> 00:08:35.890
was infinitely long. Uh, I've come to the conclusion

182
00:08:35.890 --> 00:08:38.010
actually that probably it'd be good for some and

183
00:08:38.010 --> 00:08:40.330
bad for others, right? Some people have trouble fill,

184
00:08:40.440 --> 00:08:42.609
filling up, you know, 70 or 80 years of

185
00:08:42.609 --> 00:08:45.729
their lives, uh, with worthwhile activities, and so, you

186
00:08:45.729 --> 00:08:48.450
know, living a billion years doesn't sound, uh, like

187
00:08:48.450 --> 00:08:51.840
a, a welcome prospect for them. But others, uh,

188
00:08:51.929 --> 00:08:53.969
you know, seem to find, uh, no end of,

189
00:08:54.049 --> 00:08:57.630
of exciting possibilities for their lives. And so, you

190
00:08:57.630 --> 00:08:59.679
know, for them, immortality could well be a boon.

191
00:09:00.619 --> 00:09:00.630
Mhm.

192
00:09:01.619 --> 00:09:04.380
So let me ask you about suicide now. Is

193
00:09:04.380 --> 00:09:08.700
it best approached as a moral question, a psychological

194
00:09:08.700 --> 00:09:11.059
question, or a medical question?

195
00:09:12.479 --> 00:09:14.739
Well, I think in contemporary, uh, you know, sort

196
00:09:14.739 --> 00:09:19.780
of advanced industrial societies, um, suicide is mostly treated,

197
00:09:20.080 --> 00:09:22.520
uh, or addressed through, through a medical lens, right?

198
00:09:22.559 --> 00:09:25.799
It's thought of in terms of, uh, what, uh,

199
00:09:25.869 --> 00:09:30.000
physicians and, and clinical personnel, uh, think about, you

200
00:09:30.000 --> 00:09:32.119
know, a person's mental state and, and whether they're

201
00:09:32.119 --> 00:09:35.030
going to, um, you know, uh, in their lives

202
00:09:35.030 --> 00:09:37.630
and what should be done about that. Um, I

203
00:09:37.630 --> 00:09:39.859
think that's in many ways a kind of mistake.

204
00:09:39.929 --> 00:09:41.849
This isn't to say that medicine doesn't have a

205
00:09:41.849 --> 00:09:44.599
role in thinking about suicide, but it's clear that

206
00:09:44.599 --> 00:09:48.090
it's a, a question that, um, has, uh, facets

207
00:09:48.090 --> 00:09:50.559
to it that aren't maybe best addressed, right, through,

208
00:09:50.690 --> 00:09:53.330
through medical means. It's not a medical question whether

209
00:09:53.330 --> 00:09:55.450
your life, uh, your future is going to be

210
00:09:55.450 --> 00:09:59.359
meaningful. It's not a medical question whether, um, You

211
00:09:59.359 --> 00:10:02.390
know, uh, you can withstand additional pain and suffering,

212
00:10:02.400 --> 00:10:04.039
and so forth. Those aren't, you know, sort of

213
00:10:04.039 --> 00:10:06.260
questions of medicine, those are questions, broadly speaking, of,

214
00:10:06.359 --> 00:10:10.229
of philosophy or ethics. Um, So, I mean, I

215
00:10:10.229 --> 00:10:12.349
think it's a, it's a multifaceted question to which

216
00:10:12.349 --> 00:10:15.190
um many different fields have a contribution to make.

217
00:10:15.510 --> 00:10:18.090
Um, THE moral question, you know, has been entertained,

218
00:10:18.109 --> 00:10:21.590
uh, in philosophy, you know, essentially forever. Um, YOU

219
00:10:21.590 --> 00:10:24.390
know, when you think about the famous, um, you

220
00:10:24.390 --> 00:10:26.419
know, scene of, of Socrates facing his own death,

221
00:10:27.270 --> 00:10:29.679
uh, you know, in, in the Phado, he's in

222
00:10:29.679 --> 00:10:31.669
effect, you know, deciding whether it's kind of morally

223
00:10:31.669 --> 00:10:35.390
defensible for him to, uh, acquiesce, right, in this

224
00:10:35.390 --> 00:10:38.150
suicide that it's been handed down as a judicial

225
00:10:38.150 --> 00:10:41.200
sentence, right? Um, SO, it's been on the minds

226
00:10:41.200 --> 00:10:43.429
of, of people for quite a long time. Uh,

227
00:10:43.479 --> 00:10:46.200
AND the moral question has, uh, been approached, you

228
00:10:46.200 --> 00:10:49.200
know, through different kinds of lenses, um, you know,

229
00:10:49.359 --> 00:10:51.960
theological, you know, in terms of our relationship to

230
00:10:51.960 --> 00:10:56.000
God, say, um, utilitarian in terms of sort of

231
00:10:56.000 --> 00:10:59.400
questions about well-being, but also, you know, um, considerations

232
00:10:59.400 --> 00:11:02.280
such as, uh, you know, uh, a person's autonomy,

233
00:11:02.359 --> 00:11:03.840
right? Whether they have a right to decide the

234
00:11:03.840 --> 00:11:06.849
duration of their lives. That's been an important, um,

235
00:11:06.869 --> 00:11:08.119
element of the conversation too.

236
00:11:09.770 --> 00:11:14.159
Is suicide rationally or morally defensible?

237
00:11:15.169 --> 00:11:18.460
Uh, IT can be. Um, AS always, I think

238
00:11:18.460 --> 00:11:22.140
we should, uh, suppose that, uh, universal claims should

239
00:11:22.140 --> 00:11:24.580
be treated with skepticism here, right? That, that it

240
00:11:24.580 --> 00:11:27.260
would always be rational or morally defensible, or never

241
00:11:27.260 --> 00:11:29.979
rationally or morally defensible. Those seem to me to

242
00:11:29.979 --> 00:11:33.900
be, um, pretty unlikely, uh, hypotheses just at the

243
00:11:33.900 --> 00:11:36.580
outset. But I think it can be rational for

244
00:11:36.580 --> 00:11:39.500
people to think that they would be better off,

245
00:11:39.700 --> 00:11:41.900
right, for having a shorter rather than a longer

246
00:11:41.900 --> 00:11:44.969
life. Um, This isn't going to be true for

247
00:11:44.969 --> 00:11:47.450
much of people's lives. Probably throughout most of our

248
00:11:47.450 --> 00:11:49.950
lives, we do stand to benefit from living further.

249
00:11:50.320 --> 00:11:52.969
Um, BUT there can come points in people's lives,

250
00:11:53.289 --> 00:11:55.690
where it seems, um, that it would be rational

251
00:11:55.690 --> 00:11:57.409
for them to prefer a shorter rather than a

252
00:11:57.409 --> 00:12:00.969
longer life. I think that's an important, um, consideration,

253
00:12:01.049 --> 00:12:04.000
right? Because, uh, you know, sometimes you hear people

254
00:12:04.000 --> 00:12:07.049
describe suicide or assisted dying as, as choosing death.

255
00:12:07.289 --> 00:12:08.690
And I like to point, point out to people

256
00:12:08.690 --> 00:12:11.549
that's, um, quite confused in a way, right? Because,

257
00:12:11.650 --> 00:12:13.369
you know, we are mortal creatures, we will die

258
00:12:13.369 --> 00:12:15.630
at some time. And so the person who's opting

259
00:12:15.630 --> 00:12:19.609
for suicide or for assisted dying, say, isn't deciding

260
00:12:19.609 --> 00:12:22.770
to die, they're deciding to die earlier, right? And

261
00:12:22.770 --> 00:12:25.640
that I think is, is a crucial observation. In

262
00:12:25.640 --> 00:12:29.010
terms of the moral defensibility of it, um, that's

263
00:12:29.010 --> 00:12:32.640
a huge conversation. Um, A useful way, I think,

264
00:12:32.650 --> 00:12:35.570
to, to divide things up, uh, to kind of

265
00:12:35.570 --> 00:12:38.010
get a handle on it is, is to use

266
00:12:38.010 --> 00:12:40.030
the approach that David Hume used in a famous

267
00:12:40.030 --> 00:12:43.479
essay of his on suicide. Uh, Scottish philosopher David

268
00:12:43.479 --> 00:12:45.539
Hume, where he said, well, you know, if there's

269
00:12:45.539 --> 00:12:48.739
something morally objectionable about suicide, that's going to be

270
00:12:48.739 --> 00:12:51.179
for one of three reasons. One is that it

271
00:12:51.179 --> 00:12:55.099
violates duties that we owe to God. Uh, SECOND,

272
00:12:55.179 --> 00:12:57.580
that it, uh, violates duties that we owe to

273
00:12:57.580 --> 00:13:00.979
other people or to society at large. Or third,

274
00:13:01.059 --> 00:13:04.080
that it violates duties that we owe to ourselves.

275
00:13:04.500 --> 00:13:06.859
Um, I find it difficult to see in any

276
00:13:06.859 --> 00:13:10.489
of those lines of argument. Um, ARGUMENTS that are

277
00:13:10.489 --> 00:13:12.849
so compelling to show that suicide could never be

278
00:13:12.849 --> 00:13:15.729
morally defensible, but I think the arguments sometimes put

279
00:13:15.729 --> 00:13:18.570
their finger on important reasons why we should certainly

280
00:13:18.570 --> 00:13:20.969
be hesitant about suicide from a moral perspective.

281
00:13:22.200 --> 00:13:27.080
Is the meaningfulness of life a relevant question as

282
00:13:27.080 --> 00:13:30.599
to whether it is rational to commit suicide?

283
00:13:32.169 --> 00:13:35.210
I think it's um more relevant in some ways

284
00:13:35.210 --> 00:13:37.309
than, than many people come to appreciate. I think

285
00:13:37.309 --> 00:13:40.729
when we look at the um literature about suicide,

286
00:13:40.809 --> 00:13:43.130
the research literature on suicide, the research literature on

287
00:13:43.130 --> 00:13:46.609
assisted dying, what you often uh uh see, I

288
00:13:46.609 --> 00:13:50.380
think, is an underappreciation of how it is that

289
00:13:50.380 --> 00:13:53.729
considerations of meaningfulness play a pretty prominent role, right,

290
00:13:53.809 --> 00:13:58.400
in, um, people's decisions regarding whether to, uh, continue

291
00:13:58.400 --> 00:14:01.919
their lives. So, for example, in the conversation around

292
00:14:01.919 --> 00:14:06.640
assisted dying, people tend to assume that um People's

293
00:14:06.640 --> 00:14:09.130
reasons for wanting assisted dying, uh, have to do

294
00:14:09.130 --> 00:14:13.250
primarily with, um, problems related to like physical pain,

295
00:14:13.409 --> 00:14:15.809
right? So, physical pain associated with, you know, let's

296
00:14:15.809 --> 00:14:20.059
say, a disease like cancer. But in reality, right,

297
00:14:20.229 --> 00:14:22.989
when you look at people's reasons for seeking this,

298
00:14:23.270 --> 00:14:24.710
a lot of what seems to be on their

299
00:14:24.710 --> 00:14:27.390
minds is the sense that they're not able to

300
00:14:27.390 --> 00:14:30.349
do the kinds of things that are sources of

301
00:14:30.349 --> 00:14:33.830
meaningfulness in their lives. They're not able to communicate

302
00:14:33.830 --> 00:14:36.549
fully with others, they're not able to appreciate certain

303
00:14:36.549 --> 00:14:39.989
kinds of experiences. They're not, uh, mobile, right? So

304
00:14:39.989 --> 00:14:42.710
they can't, you know, uh, enjoy different environments that

305
00:14:42.710 --> 00:14:46.320
might otherwise be afforded them. So, I think that

306
00:14:46.320 --> 00:14:49.840
meaningfulness and, and the desire for meaningfulness plays a

307
00:14:49.840 --> 00:14:53.719
more prominent role in people's thinking around um suicide

308
00:14:53.719 --> 00:14:57.159
than uh uh many researchers and many members of

309
00:14:57.159 --> 00:14:59.559
the public appreciate. I think people will put up

310
00:14:59.559 --> 00:15:03.359
with quite a lot of discomfort, pain, suffering, even

311
00:15:03.599 --> 00:15:05.390
if they feel that their lives hold out the

312
00:15:05.390 --> 00:15:08.239
prospects of meaningfulness. So, I think that it, it

313
00:15:08.239 --> 00:15:11.000
should, um, get more attention in the conversation around

314
00:15:11.000 --> 00:15:13.099
suicide and assisted dying than it tends to get.

315
00:15:14.309 --> 00:15:16.349
Yeah, I'm going to ask you about the assisted

316
00:15:16.349 --> 00:15:20.270
dying in a bit, but first, are people allowed

317
00:15:20.270 --> 00:15:23.369
or should they be allowed to kill themselves?

318
00:15:25.039 --> 00:15:27.239
Well, here, I think, uh, again, uh, I would

319
00:15:27.239 --> 00:15:30.239
be wary about a categorical answer. I do think

320
00:15:30.239 --> 00:15:32.000
there's a lot to be said for the simple

321
00:15:32.000 --> 00:15:34.719
thought that, um, you know, we're brought into the

322
00:15:34.719 --> 00:15:37.359
world without any, uh, without our consent, right? We

323
00:15:37.359 --> 00:15:39.400
sort of exist, not because, uh, uh, any of

324
00:15:39.400 --> 00:15:42.239
us decided upon it. Uh, WE'RE, we're here because

325
00:15:42.239 --> 00:15:44.770
somebody else decided upon it. And it does seem

326
00:15:44.770 --> 00:15:47.409
like it should be viewed as a fundamental right

327
00:15:47.650 --> 00:15:49.650
to decide how long we're going to be here,

328
00:15:49.929 --> 00:15:52.969
right? Uh, uh, AFTER all, you know, um, our

329
00:15:52.969 --> 00:15:55.450
lives are most centrally our own, right? My life

330
00:15:55.450 --> 00:15:57.369
matters the most to me, your life matters the

331
00:15:57.369 --> 00:15:59.770
most to you. Uh, IF anyone has a right

332
00:15:59.770 --> 00:16:02.890
to, uh, decide that their, uh, life should be

333
00:16:02.890 --> 00:16:05.609
shorter rather than longer, it should be the individual

334
00:16:05.609 --> 00:16:09.150
whose life it, it is. Um, THAT said, you

335
00:16:09.150 --> 00:16:11.989
know, I think that societies have some interest in

336
00:16:11.989 --> 00:16:15.789
making sure that, um, people, uh, make that decision.

337
00:16:16.080 --> 00:16:20.190
Um, WITH some care or consideration, right? Uh, THAT,

338
00:16:20.239 --> 00:16:22.599
the idea that we should at least make it

339
00:16:22.599 --> 00:16:25.559
available to people to, uh, you know, seek out

340
00:16:25.559 --> 00:16:28.799
the kind of, uh, medical care, pastoral care, maybe

341
00:16:28.799 --> 00:16:31.750
philosophical care, uh, that they might need in order

342
00:16:31.750 --> 00:16:34.400
to really think this decision through, seems to me

343
00:16:34.400 --> 00:16:36.080
to be wise. After all, this is a pretty

344
00:16:36.080 --> 00:16:38.659
momentous decision. If you, um, you know, end up,

345
00:16:38.679 --> 00:16:40.799
um, ending your life due to suicide, it, it's

346
00:16:40.799 --> 00:16:42.919
a one-time deal, right? You can't, you know, reverse

347
00:16:42.919 --> 00:16:46.969
it. So, it's a fairly momentous choice. And societies

348
00:16:46.969 --> 00:16:50.890
also, I think, face important choices about um how

349
00:16:50.890 --> 00:16:53.880
easy or difficult, right? They want to make, um,

350
00:16:54.150 --> 00:16:59.090
uh, self-determination, right? Um, SO clearly, uh, you know,

351
00:16:59.169 --> 00:17:02.130
one of the things that drives, um, the prevalence

352
00:17:02.130 --> 00:17:06.250
of suicide within societies is how readily available different

353
00:17:06.250 --> 00:17:10.170
kinds of, you know, technologies, uh, are, right? So,

354
00:17:10.598 --> 00:17:13.020
uh, clearly, for example, all the research seems to

355
00:17:13.020 --> 00:17:16.680
suggest that Uh, the easy availability of, of guns,

356
00:17:16.848 --> 00:17:18.979
right? Seems to, you know, increase the prevalence of,

357
00:17:19.050 --> 00:17:22.329
of suicide within a society. I think societies have

358
00:17:22.329 --> 00:17:25.290
important questions, uh, that they should ask themselves about

359
00:17:25.530 --> 00:17:28.030
how easy or difficult they want to make this.

360
00:17:28.368 --> 00:17:30.449
Uh, JUST to give another example, here in the

361
00:17:30.449 --> 00:17:33.489
United Kingdom in the 1950s, um, you know, the

362
00:17:33.489 --> 00:17:36.430
government began to phase phase out the production of,

363
00:17:36.729 --> 00:17:40.760
um, ovens, right? Uh, THAT produced coal gas, right,

364
00:17:40.810 --> 00:17:43.670
which is toxic, right? If you inhale it. And,

365
00:17:43.719 --> 00:17:45.939
you know, these were phased out, and the suicide

366
00:17:45.939 --> 00:17:48.000
rate, you know, went down. And I think that's

367
00:17:48.000 --> 00:17:51.239
probably a welcome development. Probably we shouldn't want there

368
00:17:51.239 --> 00:17:53.349
to be in most people's homes, you know, a

369
00:17:53.349 --> 00:17:57.750
relatively easy sort of suicide technology, uh, like that.

370
00:17:58.119 --> 00:17:59.760
Uh, SO those are the kinds of questions that

371
00:17:59.760 --> 00:18:01.910
I think are on the agenda, uh, you know,

372
00:18:01.959 --> 00:18:03.599
in terms of whether people should be allowed to

373
00:18:03.599 --> 00:18:05.680
do it. I, I, I tend to, you know,

374
00:18:05.839 --> 00:18:07.719
think that there is something quite powerful to the

375
00:18:07.719 --> 00:18:10.250
thought that You know, the right to die is

376
00:18:10.250 --> 00:18:12.569
a credible notion, right? If there's anybody who has

377
00:18:12.569 --> 00:18:14.410
the right to decide the duration of your life,

378
00:18:14.489 --> 00:18:15.270
it's, it's you.

379
00:18:16.630 --> 00:18:22.020
Right. How do questions surrounding autonomy connect to suicide?

380
00:18:23.739 --> 00:18:26.380
Well, um, I think there's a couple of aspects

381
00:18:26.380 --> 00:18:29.680
to, uh, the relationship between autonomy and suicide. So,

382
00:18:30.069 --> 00:18:34.020
um, there's one point of view, uh, germinating out

383
00:18:34.020 --> 00:18:37.099
of Kant that, uh, actually says that the fact

384
00:18:37.099 --> 00:18:40.260
that we are autonomous beings, uh, is an argument

385
00:18:40.260 --> 00:18:43.020
for the moral impermissibility of suicide, right? Kant seemed

386
00:18:43.020 --> 00:18:46.099
to have argued that the very fact that we

387
00:18:46.099 --> 00:18:49.079
are autonomous beings lends us a certain kind of

388
00:18:49.339 --> 00:18:53.020
value, dignity was his term for that, uh, value.

389
00:18:53.420 --> 00:18:56.020
Uh, WHICH in turn entails that we are not

390
00:18:56.020 --> 00:18:58.540
to, uh, treat ourselves as just a kind of

391
00:18:58.540 --> 00:19:00.699
instrument of our own interests, right? To sort of

392
00:19:00.699 --> 00:19:03.609
discard our, our lives when we think that, uh,

393
00:19:03.619 --> 00:19:05.819
our lives are no longer going well enough or

394
00:19:05.819 --> 00:19:09.479
won't go well enough. But another aspect of, of

395
00:19:09.479 --> 00:19:12.109
the relationship between autonomy and suicide is that, again,

396
00:19:12.160 --> 00:19:15.209
in terms of preventing suicide, uh, it does seem

397
00:19:15.209 --> 00:19:17.760
relevant whether or not someone's uh opting to end

398
00:19:17.760 --> 00:19:20.650
their life in a way that is autonomous. Whether

399
00:19:20.650 --> 00:19:24.400
they've really, you know, considered, um, you know, the,

400
00:19:24.500 --> 00:19:27.119
the, the evidence and, and reasons, right, that speak

401
00:19:27.119 --> 00:19:29.699
for and against their continuing their lives. I mean,

402
00:19:29.750 --> 00:19:30.910
I think we can all agree that there are

403
00:19:30.910 --> 00:19:33.979
cases where we would think that someone's ending their

404
00:19:33.979 --> 00:19:37.150
life would be hasty, uh, would not be autonomous,

405
00:19:37.219 --> 00:19:38.989
and that, you know, people would have a good

406
00:19:38.989 --> 00:19:42.390
reason to want to interfere or intercede. Uh, YOU

407
00:19:42.390 --> 00:19:46.219
know, the, the adolescent, right, who's, um, just heartbroken

408
00:19:46.219 --> 00:19:49.849
by their first romantic breakup, right? Uh, YOU know,

409
00:19:49.910 --> 00:19:53.469
um, sadly, sometimes people do in their lives subsequent

410
00:19:53.469 --> 00:19:55.189
to that sort of event, but you definitely want

411
00:19:55.189 --> 00:19:57.349
to say to such a person, You know, it

412
00:19:57.349 --> 00:19:59.829
will be OK. People do survive this. It, it

413
00:19:59.829 --> 00:20:01.989
may seem difficult in the moment, but a measure

414
00:20:01.989 --> 00:20:04.750
of patience and, and recognition from others will probably

415
00:20:04.750 --> 00:20:07.660
allow you to Uh, you know, uh, get over

416
00:20:07.660 --> 00:20:09.660
this and, and live, you know, a, a life

417
00:20:09.660 --> 00:20:12.510
that you can be happy with. So, uh, I

418
00:20:12.510 --> 00:20:14.859
think we do want to, you know, ensure that,

419
00:20:14.869 --> 00:20:18.310
um, people, uh, are, uh, opting to end their

420
00:20:18.310 --> 00:20:21.349
lives in ways that are rational and well-informed, which

421
00:20:21.349 --> 00:20:23.510
is sort of the heart of autonomy. Though of

422
00:20:23.510 --> 00:20:25.229
course, we can also go overboard there. I think

423
00:20:25.229 --> 00:20:28.510
it's possible for us to, um, overshoot the mark

424
00:20:28.510 --> 00:20:33.079
trying to determine whether, uh, someone's, um, Suicide is

425
00:20:33.079 --> 00:20:37.760
autonomous and, and subject them to um excessive burdens,

426
00:20:37.949 --> 00:20:40.439
right? In terms of trying to demonstrate to the

427
00:20:40.439 --> 00:20:43.209
world that they're choosing uh to die in a

428
00:20:43.209 --> 00:20:45.569
way that's autonomous. Uh, I would say, for example,

429
00:20:45.650 --> 00:20:48.530
that the, um, assisted dying bill that actually was

430
00:20:48.530 --> 00:20:50.290
just passed in the House of Commons here in

431
00:20:50.290 --> 00:20:52.810
the United Kingdom is an example of, of something

432
00:20:52.810 --> 00:20:56.050
that's too burdensome. Uh, IT involves a, a panel,

433
00:20:56.209 --> 00:21:00.030
right, of psychiatrists and social workers and judges approving,

434
00:21:00.489 --> 00:21:03.000
right, requests for assisted dying. And this looks to

435
00:21:03.000 --> 00:21:06.410
me to be, um, not only very cumbersome, but

436
00:21:06.410 --> 00:21:09.369
an example of, uh, overshooting the mark in terms

437
00:21:09.369 --> 00:21:11.890
of uh determining whether or not people are making

438
00:21:11.890 --> 00:21:14.630
their decisions autonomously. It looks gratuitous to me.

439
00:21:15.050 --> 00:21:20.250
Mhm. On what grounds is suicide intervention justified?

440
00:21:21.599 --> 00:21:23.869
I suppose the most common one, right, is paternalistic,

441
00:21:24.119 --> 00:21:26.160
right? That we think people are making some sort

442
00:21:26.160 --> 00:21:28.839
of mistake, right? Uh, AND they're not just making

443
00:21:28.839 --> 00:21:32.280
a mistake, um, innocently, right? They're making a mistake

444
00:21:32.760 --> 00:21:35.880
because they, uh, lack the evidence needed to, to

445
00:21:35.880 --> 00:21:39.640
make this determination rationally, or they're perhaps subject to

446
00:21:39.640 --> 00:21:42.640
some sort of, um, you know, mental condition, perhaps

447
00:21:42.640 --> 00:21:45.319
a mental illness of some sort, that's distorting their

448
00:21:45.319 --> 00:21:49.739
thinking. Um, So, I think that the most common

449
00:21:49.739 --> 00:21:53.449
rationale that people offer is the paternalistic one that,

450
00:21:53.619 --> 00:21:56.219
you know, people's deciding in some cases to end

451
00:21:56.219 --> 00:21:59.020
their lives. Well, they're making a genuine decision, but

452
00:21:59.020 --> 00:22:02.420
not the decision, sort of decision that um uh

453
00:22:02.660 --> 00:22:05.979
others must respect, right? Uh, AND when we think

454
00:22:05.979 --> 00:22:09.260
that someone is uh making not merely a, a

455
00:22:09.260 --> 00:22:12.060
modest mistake, but a very serious mistake about their

456
00:22:12.060 --> 00:22:15.420
well-being, Um, and they're doing so in ways that

457
00:22:15.420 --> 00:22:19.699
are perhaps ill-informed, or again, uh, insufficiently autonomous. Perhaps

458
00:22:19.699 --> 00:22:23.900
that can provide grounds for, um, intervening. Um, I

459
00:22:23.900 --> 00:22:26.780
should say that in general, right, my, my leanings

460
00:22:26.780 --> 00:22:29.459
on these matters is to, is that, uh, we

461
00:22:29.459 --> 00:22:32.260
overemphasize when it comes to suicide prevention, trying to

462
00:22:32.260 --> 00:22:36.099
identify who is likely to engage in suicide, um,

463
00:22:36.260 --> 00:22:39.719
predicting, right, uh, who amongst Uh, you know, a

464
00:22:39.719 --> 00:22:41.930
large group of people with, let's say, depression or

465
00:22:41.930 --> 00:22:44.569
bipolar disorder are likely to end their lives with,

466
00:22:44.650 --> 00:22:48.160
um, uh, due to suicide, is actually extremely difficult,

467
00:22:48.410 --> 00:22:51.209
right? Uh, YES, lots of people who, uh, in

468
00:22:51.209 --> 00:22:53.369
their lives due to suicide have those kinds of

469
00:22:53.369 --> 00:22:56.410
conditions, but the, the, uh, the correlation is rather

470
00:22:56.410 --> 00:22:58.359
weak in the other direction. Most of the people,

471
00:22:58.530 --> 00:23:01.369
uh, overall majority of people who, who are depressed,

472
00:23:01.439 --> 00:23:05.170
etc. DO not. Um, And I would be in

473
00:23:05.170 --> 00:23:08.530
favor actually of our beginning to rethink the ethics

474
00:23:08.530 --> 00:23:13.420
of suicide intervention to uh de-emphasize, right? Trying to

475
00:23:13.420 --> 00:23:17.130
identify who is suicidal, uh, and put more emphasis

476
00:23:17.130 --> 00:23:23.640
on trying to create environments, right? Um, THAT, uh,

477
00:23:26.089 --> 00:23:28.290
Discouraged, right? To the extent we think that people

478
00:23:28.290 --> 00:23:31.469
ought to be discouraged, um, from engaging in suicide.

479
00:23:31.689 --> 00:23:34.040
So, to give an analogy, Craig Bryan in a

480
00:23:34.040 --> 00:23:36.770
recent book of his, um, he's a psychologist working

481
00:23:36.770 --> 00:23:38.890
in the United States, uh, you know, makes a

482
00:23:38.890 --> 00:23:40.650
nice sort of comparison that when we think about,

483
00:23:40.729 --> 00:23:44.530
you know, uh, roadway safety, right? We don't think

484
00:23:44.530 --> 00:23:46.410
first and foremost about who are the good drivers,

485
00:23:46.489 --> 00:23:47.729
who are the ones who are likely to get

486
00:23:47.729 --> 00:23:50.410
in accidents and who's not. We think rather about,

487
00:23:50.489 --> 00:23:52.689
you know, is this curve dangerous? Is this, you

488
00:23:52.689 --> 00:23:55.439
know, intersection well lit, and so forth. And I

489
00:23:55.439 --> 00:23:57.560
think that idea or that kind of picture of

490
00:23:57.560 --> 00:23:59.839
how we should approach suicide intervention in a more

491
00:23:59.839 --> 00:24:03.060
systematic or social way, kind of public health approach,

492
00:24:03.560 --> 00:24:06.430
is, I think, likely to be no less effective,

493
00:24:06.760 --> 00:24:09.020
but also doesn't involve us trying to figure out,

494
00:24:09.199 --> 00:24:11.239
right? You know, just who amongst all those who

495
00:24:11.239 --> 00:24:14.359
might be depressed or might have suffered personal setbacks

496
00:24:14.359 --> 00:24:17.310
or might have, you know, uh, you know, uh,

497
00:24:17.329 --> 00:24:20.140
uh, uh, devastating personal illnesses, that sort of thing.

498
00:24:20.630 --> 00:24:23.170
Uh, We can move away from this approach of

499
00:24:23.170 --> 00:24:26.170
trying to identify who is suicidal and simply uh

500
00:24:26.170 --> 00:24:27.969
try to make it the case that it's perhaps

501
00:24:27.969 --> 00:24:29.890
a little more difficult for people to engage in

502
00:24:29.890 --> 00:24:30.510
suicide.

503
00:24:31.619 --> 00:24:34.050
Right. So let me ask you a little bit

504
00:24:34.050 --> 00:24:38.500
about assisted dying now. What questions does it raise?

505
00:24:40.239 --> 00:24:41.680
Well, it raises many of the questions that we've

506
00:24:41.680 --> 00:24:43.770
already touched upon. Can people, you know, decide to

507
00:24:43.770 --> 00:24:46.640
end their lives rationally, autonomously? What reasons might they

508
00:24:46.640 --> 00:24:49.800
have, reasons of, uh, you know, well-being, but also

509
00:24:49.800 --> 00:24:52.599
reasons around meaningfulness. I do think the meaningfulness point

510
00:24:52.599 --> 00:24:55.839
is, is again worth emphasizing there. I suppose traditionally,

511
00:24:55.920 --> 00:24:58.040
right, when people think about assisted dying, they're thinking

512
00:24:58.040 --> 00:25:03.550
about assisted dying, um, as facilitated by, uh, doctors,

513
00:25:03.719 --> 00:25:07.410
right, through medical means, um. And that, you know,

514
00:25:07.530 --> 00:25:10.390
I suppose puts on the agenda questions about whether

515
00:25:11.130 --> 00:25:14.650
assisted dying is compatible with the ethical norms that

516
00:25:14.650 --> 00:25:19.290
define medicine. Um, ONE, you know, long-standing argument against

517
00:25:19.290 --> 00:25:23.609
assisted dying, against euthanasia, uh, is that, uh, it

518
00:25:23.609 --> 00:25:26.209
should be a cornerstone principle of medical ethics that

519
00:25:26.209 --> 00:25:29.689
doctors do not kill or assist in the killing

520
00:25:29.689 --> 00:25:34.089
of their patients. Um, NOW, that argument, I think

521
00:25:34.089 --> 00:25:36.689
has been subjected, uh, to a good bit of

522
00:25:36.689 --> 00:25:39.550
criticism. Much of it strikes me as convincing criticism,

523
00:25:39.890 --> 00:25:42.770
uh, insofar as, uh, the point here seems to

524
00:25:42.770 --> 00:25:46.449
be that, that doctors should not, um, contribute to

525
00:25:46.449 --> 00:25:48.689
the killing of their patients when the killing is

526
00:25:48.689 --> 00:25:50.719
not consensual, or when the killing is not a

527
00:25:50.719 --> 00:25:53.209
benefit to the patient, but on the condition that

528
00:25:53.209 --> 00:25:56.290
the patient does consent and, uh, is benefited by

529
00:25:56.290 --> 00:25:58.729
it, perhaps that's a case where we should make

530
00:25:58.729 --> 00:26:02.640
an exception. Um, THERE are also, I suppose, questions,

531
00:26:02.699 --> 00:26:06.900
uh, within medicine around the, uh, sort of distribution

532
00:26:06.900 --> 00:26:10.020
of healthcare resources. Um, YOU know, many of those

533
00:26:10.020 --> 00:26:13.300
who might seek assisted dying, for example, uh, are

534
00:26:13.300 --> 00:26:15.219
seeking it toward the end of their lives, which

535
00:26:15.219 --> 00:26:18.319
is typically a period where, uh, people are, um,

536
00:26:18.979 --> 00:26:22.060
subjected to a great deal of, uh, medical attention

537
00:26:22.060 --> 00:26:26.060
and medical intervention. Much of that medical attention intervention

538
00:26:26.060 --> 00:26:28.800
is, is costly. Uh, IT could perhaps be used

539
00:26:28.800 --> 00:26:31.680
to, um, help people who, uh, you know, uh,

540
00:26:31.780 --> 00:26:34.290
have different medical conditions or who might stand to

541
00:26:34.290 --> 00:26:38.219
benefit more. So it raises questions around, um, the

542
00:26:38.219 --> 00:26:41.660
distributive justice, right, of, of healthcare resources.

543
00:26:43.199 --> 00:26:46.530
So many advocates for a moral right to assist

544
00:26:46.530 --> 00:26:51.140
the dying hold that individuals are entitled to death

545
00:26:51.310 --> 00:26:55.280
with dignity. What does dignity mean in this context?

546
00:26:56.839 --> 00:26:59.079
Well, dignity is one of those terms that everyone

547
00:26:59.079 --> 00:27:01.239
likes, but no one can seem to agree quite

548
00:27:01.239 --> 00:27:04.839
on what it is. Um, SO you hear that

549
00:27:04.839 --> 00:27:07.880
notion invoked both, as you just said, right, by

550
00:27:07.880 --> 00:27:10.520
advocates of assisted dying, but also by opponents of

551
00:27:10.520 --> 00:27:13.640
assisted dying. So, that then raises the puzzle, well,

552
00:27:13.760 --> 00:27:15.439
you know, how could it be that dignity speaks

553
00:27:15.439 --> 00:27:18.680
both for and against assisted dying. Um, I find

554
00:27:18.680 --> 00:27:21.750
that deeply puzzling, um, It seems to me that

555
00:27:21.750 --> 00:27:23.699
it seems very clear then that the, the, uh,

556
00:27:23.709 --> 00:27:26.270
different parties are using the term to designate different

557
00:27:26.270 --> 00:27:29.829
kinds of notions. Uh, AND I'm not, uh, terribly

558
00:27:29.829 --> 00:27:31.390
patient with the question of sort of which of

559
00:27:31.390 --> 00:27:33.510
these views is, is the right way to invoke

560
00:27:33.510 --> 00:27:35.550
the notion of dignity. I think, you know, it

561
00:27:35.550 --> 00:27:37.500
may just be one of those terms that has,

562
00:27:37.829 --> 00:27:39.500
you know, a lot of different meanings and is

563
00:27:39.500 --> 00:27:41.469
used by a lot of people to use, to,

564
00:27:41.540 --> 00:27:44.550
to mean different things. I think those who favor

565
00:27:44.550 --> 00:27:48.630
the legalization and availability of assisted dying, Typically have

566
00:27:48.630 --> 00:27:51.680
in mind, uh, by a death with dignity, a

567
00:27:51.680 --> 00:27:54.839
death that accords with a person's conception of, of

568
00:27:54.839 --> 00:27:57.619
who they are and perhaps what's best about themselves.

569
00:27:58.619 --> 00:28:03.599
OK. So, you know, dying nowadays is more often

570
00:28:03.599 --> 00:28:05.979
the case, most often the uh uh the case,

571
00:28:06.160 --> 00:28:09.310
a condition where, Uh, you know, you suffer from

572
00:28:09.310 --> 00:28:12.310
different kinds of debilities, you may have, uh, difficulty

573
00:28:12.310 --> 00:28:16.510
walking, sitting up, speaking, uh, hearing, seeing, you know,

574
00:28:16.670 --> 00:28:19.780
caring for yourself in different sorts of ways. Um,

575
00:28:19.869 --> 00:28:22.180
AND I think what people who, who want, uh,

576
00:28:22.270 --> 00:28:23.670
what they think of as the death of dignity

577
00:28:23.670 --> 00:28:25.589
have in mind, is that they want to be

578
00:28:25.589 --> 00:28:28.290
able to die in a way that accords with

579
00:28:28.589 --> 00:28:30.790
their own conception of what's valuable in their own

580
00:28:30.790 --> 00:28:32.829
lives. They don't want to, you know, be dying

581
00:28:32.829 --> 00:28:35.430
in a way where they sort of final moments

582
00:28:35.430 --> 00:28:38.680
are at odds with Uh, their best selves, right?

583
00:28:38.760 --> 00:28:41.839
They're capable selves, they're, they're selves that, uh, are

584
00:28:41.839 --> 00:28:45.140
able to communicate and think and are conscious. Um,

585
00:28:45.319 --> 00:28:48.300
SO they want to avoid a death, um, such

586
00:28:48.300 --> 00:28:50.270
that the end of their life sort of represents,

587
00:28:50.479 --> 00:28:51.760
you know, if you will, not the best of

588
00:28:51.760 --> 00:28:56.250
them, but the worst of them. Right, um, And

589
00:28:56.250 --> 00:28:58.310
I think that there's also a kind of interpersonal

590
00:28:58.310 --> 00:29:00.949
dimension of dignity, uh, for many advocates of assisted

591
00:29:00.949 --> 00:29:03.630
dying. They want to die in a condition that

592
00:29:03.630 --> 00:29:09.280
is relatively speaking, relatively speaking, uh, less dependent upon

593
00:29:09.280 --> 00:29:13.199
others, right? Uh, SO to die with dignity means

594
00:29:13.199 --> 00:29:15.719
in their minds something like, to die before one

595
00:29:15.719 --> 00:29:18.969
is completely, um, sort of at the mercy, right,

596
00:29:19.079 --> 00:29:22.119
of, of your caregivers and, uh, at the mercy

597
00:29:22.119 --> 00:29:25.079
of, of the medical interventions that are Uh, sort

598
00:29:25.079 --> 00:29:27.640
of treating your condition and, and sustaining your life.

599
00:29:28.119 --> 00:29:30.719
Um, BUT again, dignity is invoked kind of on

600
00:29:30.719 --> 00:29:32.589
all sides of the debate. So in some ways,

601
00:29:32.859 --> 00:29:35.599
you know, whenever I, I, you know, have conversations

602
00:29:35.599 --> 00:29:38.160
about, uh, about assisted dying with people, I, I

603
00:29:38.160 --> 00:29:40.119
kind of wanna sort of, you know, give a

604
00:29:40.119 --> 00:29:42.020
timeout and that's just, you know, let's not talk

605
00:29:42.020 --> 00:29:44.229
about dignity cause sort of everybody has their own

606
00:29:44.229 --> 00:29:47.729
conception. Um, BUT it's clearly had, uh, a lot

607
00:29:47.729 --> 00:29:51.810
of rhetorical, uh, significance in this debate. I mean,

608
00:29:51.959 --> 00:29:53.849
a lot of the groups that, you know, advocate

609
00:29:53.849 --> 00:29:56.689
for the legalization of assisted dying, that's how they

610
00:29:56.689 --> 00:29:59.079
describe themselves, you know, dignity and dying, death with

611
00:29:59.079 --> 00:30:02.020
dignity. So, clearly it's capturing something that people are,

612
00:30:02.130 --> 00:30:03.609
uh, that resonates with many people.

613
00:30:04.790 --> 00:30:06.790
Should people have the right

614
00:30:06.790 --> 00:30:09.290
to die? I think they do as a moral

615
00:30:09.290 --> 00:30:10.810
matter, have the right to die in the sense

616
00:30:10.810 --> 00:30:14.489
that I think that um there is a limited

617
00:30:14.489 --> 00:30:19.680
claim, right, um, against other people interfering, right? With,

618
00:30:19.839 --> 00:30:23.140
um, our choices about the duration of our lives.

619
00:30:23.219 --> 00:30:25.540
Again, you know, uh, ending one's life is, is

620
00:30:25.540 --> 00:30:28.329
not the decision to die, it's a decision to

621
00:30:28.329 --> 00:30:31.339
die earlier rather than later. And that's a question

622
00:30:31.339 --> 00:30:34.140
that I think, um, if anybody is well situated

623
00:30:34.140 --> 00:30:37.540
to make that determination, uh, it's, um, the person

624
00:30:37.540 --> 00:30:39.459
whose life it is. This doesn't mean that people

625
00:30:39.459 --> 00:30:42.780
always make this determination perfectly, or that we shouldn't

626
00:30:42.780 --> 00:30:46.180
perhaps, um, help them make that decision better. Um,

627
00:30:46.260 --> 00:30:49.459
BUT that ultimately, uh, they're the biggest stakeholder, right,

628
00:30:49.500 --> 00:30:52.099
in that decision, and they are the ones who

629
00:30:52.099 --> 00:30:55.020
have the most familiarity, the greatest degree of knowledge

630
00:30:55.020 --> 00:30:57.339
about whether their life is, is, you know, worth

631
00:30:57.339 --> 00:30:59.930
it for them. Uh, THIS doesn't mean that, you

632
00:30:59.930 --> 00:31:01.729
know, we should encourage people to do it frivolously.

633
00:31:01.849 --> 00:31:04.449
It also doesn't mean that we should, uh, neglect,

634
00:31:04.650 --> 00:31:07.469
right, moral considerations that are germane to the decision.

635
00:31:07.770 --> 00:31:10.760
Uh, SURELY, you know, if you are a parent,

636
00:31:11.089 --> 00:31:13.250
uh, it should weigh very heavily on your mind

637
00:31:13.250 --> 00:31:15.530
whether or not, you know, ending your life would

638
00:31:15.530 --> 00:31:17.969
mean that you would end up neglecting or harming

639
00:31:17.969 --> 00:31:19.810
your children. That seems to me to be a

640
00:31:19.810 --> 00:31:22.689
very weighty moral consideration. Uh, YOU know, the effects

641
00:31:22.689 --> 00:31:25.209
on others certainly is something that people should be,

642
00:31:25.329 --> 00:31:29.050
uh, mindful of, right? In thinking about. Um, uh,

643
00:31:29.150 --> 00:31:30.969
THE decision of whether or not to continue or,

644
00:31:31.020 --> 00:31:33.540
or to foreshorten their lives. But I think if

645
00:31:33.540 --> 00:31:35.540
the, if the question is taken just sort of

646
00:31:35.540 --> 00:31:37.089
at face value, is there a right to die?

647
00:31:37.109 --> 00:31:39.619
I'm inclined to say, yes, though, of course, as

648
00:31:39.619 --> 00:31:41.800
with almost any right, it's not an unlimited right,

649
00:31:42.060 --> 00:31:45.400
and, and it's one that should be exercised, um,

650
00:31:45.619 --> 00:31:47.520
uh, very carefully and very judiciously.

651
00:31:49.099 --> 00:31:52.260
So I have one last question then, are there

652
00:31:52.260 --> 00:31:56.099
any situations where people have a duty to die?

653
00:31:57.959 --> 00:32:01.239
Well, um, that phrase duty to die, you know,

654
00:32:01.359 --> 00:32:05.219
seems to, um, uh, Light a fire in a

655
00:32:05.219 --> 00:32:07.300
lot of people's minds, right? This thought that you

656
00:32:07.300 --> 00:32:09.780
could ever have a duty to die. Um, SO

657
00:32:09.780 --> 00:32:11.020
there are cases where I think a lot of

658
00:32:11.020 --> 00:32:13.489
people would agree that, uh, you have a duty

659
00:32:13.489 --> 00:32:15.609
to do something that will put your, uh, life

660
00:32:15.609 --> 00:32:18.489
at risk, right? So, you know, the firefighter who

661
00:32:18.489 --> 00:32:21.800
confronts a burning building, uh, and is, is asked

662
00:32:21.800 --> 00:32:25.050
to, you know, remove somebody from it. Uh, THEY'RE

663
00:32:25.050 --> 00:32:27.910
not intending to die, perhaps, but they're certainly, uh,

664
00:32:28.030 --> 00:32:30.530
undertaking what could be a lethal risk, and you

665
00:32:30.530 --> 00:32:32.400
might think in that sense, they have a duty,

666
00:32:32.770 --> 00:32:34.530
perhaps not to die, but at least a duty

667
00:32:34.530 --> 00:32:37.170
to, uh, put their lives at risk. Likewise, you

668
00:32:37.170 --> 00:32:39.689
know, the soldier who was, uh, uh, following a

669
00:32:39.689 --> 00:32:42.449
legitimate order, uh, you know, to go into battle,

670
00:32:42.530 --> 00:32:45.329
to enter into combat, um, is in some sense,

671
00:32:45.410 --> 00:32:47.770
you know, uh, acting on a, a punitive duty

672
00:32:47.770 --> 00:32:49.609
to, if not to die, at least to risk

673
00:32:49.609 --> 00:32:52.859
their lives. Um, Now, of course, you can come

674
00:32:52.859 --> 00:32:56.420
up with scenarios, right, where you might think that

675
00:32:56.420 --> 00:32:59.810
someone has a duty to die, um. You know,

676
00:32:59.890 --> 00:33:02.050
a famous example, uh, in, in the philosophy of

677
00:33:02.050 --> 00:33:04.479
death and dying literature, you know, um, again, a

678
00:33:04.479 --> 00:33:07.160
military example, uh, you know, a grenade is thrown

679
00:33:07.160 --> 00:33:10.479
into a foxhole, and, uh, a soldier sees that

680
00:33:10.479 --> 00:33:12.890
if the grenade goes off, uh, a number of

681
00:33:12.890 --> 00:33:15.849
his, uh, fellow soldiers will be killed. Uh, BUT

682
00:33:15.849 --> 00:33:18.489
if he, uh, leaps upon it and absorbs the

683
00:33:18.489 --> 00:33:21.079
blast into his own body, he will be killed,

684
00:33:21.160 --> 00:33:24.920
but the others, um, um, The others will be

685
00:33:24.920 --> 00:33:27.709
saved, right? Now, there are certain lines of moral

686
00:33:27.709 --> 00:33:30.800
argument that would say, actually, you know, that's his

687
00:33:30.800 --> 00:33:33.469
duty. It's not him being heroic or anything if

688
00:33:33.469 --> 00:33:34.959
he were to do this. This is actually what

689
00:33:34.959 --> 00:33:37.400
he's uh morally obligated to do. That's the sort

690
00:33:37.400 --> 00:33:40.439
of reasoning we might associate, say, with utilitarian thinking

691
00:33:40.439 --> 00:33:43.599
and morality, that we should, um, make sacrifices so

692
00:33:43.599 --> 00:33:46.839
long as those sacrifices have greater benefits, right, to

693
00:33:46.839 --> 00:33:50.709
other people. Um, I think the more contentious examples

694
00:33:50.709 --> 00:33:53.069
of duty, of a duty to die, and, and

695
00:33:53.069 --> 00:33:54.770
these are the sorts of examples that were behind,

696
00:33:54.910 --> 00:33:58.349
uh, John Hardwick's, uh, quite controversial article from the

697
00:33:58.349 --> 00:34:01.969
1990s is their duty to die, are situations where

698
00:34:02.189 --> 00:34:05.719
a person, um, Uh, by continuing to exist is

699
00:34:05.719 --> 00:34:09.129
imposing certain kinds of burdens on others, right? So,

700
00:34:09.370 --> 00:34:11.879
um, a person might have a very serious illness

701
00:34:11.879 --> 00:34:15.199
that, uh, requires very burdensome care on the part

702
00:34:15.199 --> 00:34:17.750
of others. Or they could have a very, uh,

703
00:34:17.760 --> 00:34:21.438
serious illness that imposes, uh, quite heavy financial burdens

704
00:34:21.438 --> 00:34:25.790
on others. Uh, Hardwick argues in his article that,

705
00:34:26.280 --> 00:34:30.139
um, Uh, there can be cases where, uh, by

706
00:34:30.139 --> 00:34:32.969
continuing to exist, you are imposing unjust burdens on

707
00:34:32.969 --> 00:34:36.300
other people. Um, MANY people have not been convinced

708
00:34:36.300 --> 00:34:38.418
by that line of argument. I have my hesitations

709
00:34:38.418 --> 00:34:40.379
over it, but I think one thing that is,

710
00:34:40.418 --> 00:34:44.260
um, worth noting there, is that we should perhaps

711
00:34:44.260 --> 00:34:47.500
take more seriously the idea that sometimes by dying,

712
00:34:47.580 --> 00:34:50.860
we can signal what we care about, right? Sometimes

713
00:34:50.860 --> 00:34:53.449
dying can itself be a, a, a morally, uh,

714
00:34:53.458 --> 00:34:55.379
relevant act, right? That it can have a kind

715
00:34:55.379 --> 00:34:57.739
of moral significance. And one of the ways that

716
00:34:57.739 --> 00:35:00.030
it can have moral significance is in terms of

717
00:35:00.030 --> 00:35:03.860
helping other people, right? You know, people, um, uh,

718
00:35:03.870 --> 00:35:06.070
when they're conscientious, you know, make out a will,

719
00:35:06.149 --> 00:35:08.350
you know, to make sure that other people, you

720
00:35:08.350 --> 00:35:10.149
know, what to do with their belongings, and they,

721
00:35:10.219 --> 00:35:12.030
and they tell people how they want their bodies

722
00:35:12.030 --> 00:35:15.780
to be handled, right, after death. Um, THOSE are

723
00:35:15.780 --> 00:35:17.939
altruistic acts in a certain way, and I think

724
00:35:17.939 --> 00:35:21.129
what Hardwig is inviting us to ask, right, is

725
00:35:21.540 --> 00:35:24.419
whether we should perhaps be more tolerant of people

726
00:35:24.419 --> 00:35:26.899
wanting to act altruistically at the end of their

727
00:35:26.899 --> 00:35:32.379
lives, um, in particular by, um, foregoing, right? Certain

728
00:35:32.379 --> 00:35:35.719
kinds of benefits. The provision of which is very

729
00:35:35.719 --> 00:35:38.760
burdensome to other people, right? I often find it

730
00:35:39.080 --> 00:35:41.239
quite odd that we think that somehow in death,

731
00:35:41.439 --> 00:35:44.199
uh, you know, altruism, uh, should be ruled out

732
00:35:44.199 --> 00:35:47.189
and selfishness should be the norm. Um, I think

733
00:35:47.189 --> 00:35:50.750
there's something laudable about, um, someone who says, well,

734
00:35:50.919 --> 00:35:53.590
by continuing to live, I'm imposing, you know, these

735
00:35:53.590 --> 00:35:56.709
very serious burdens on other people. Uh, AND so

736
00:35:56.709 --> 00:35:59.149
by, by shortening my life, I'm relieving them of

737
00:35:59.149 --> 00:36:01.209
those burdens. Whether or not they have a duty

738
00:36:01.209 --> 00:36:04.350
to do so is, is more controversial, but I

739
00:36:04.350 --> 00:36:07.010
think that, uh, that line of argument deserves credit

740
00:36:07.010 --> 00:36:09.389
for getting us to ask a question that we

741
00:36:09.389 --> 00:36:11.429
sometimes don't ask, right? Why shouldn't we think of

742
00:36:11.429 --> 00:36:14.189
death as an opportunity in some sense for uh

743
00:36:14.189 --> 00:36:15.629
beneficence or altruism?

744
00:36:16.419 --> 00:36:20.330
Mhm. Great. So Doctor Cholby, just before we go,

745
00:36:20.540 --> 00:36:24.100
where can people find your work on the internet?

746
00:36:24.969 --> 00:36:26.620
Uh, WELL, one place to start is just at

747
00:36:26.620 --> 00:36:30.739
my own personal website, which is at michael.holi.com. Um,

748
00:36:30.860 --> 00:36:33.760
IF you're looking for, um, a very comprehensive picture,

749
00:36:34.229 --> 00:36:37.250
um, the, the entry that I have at, uh,

750
00:36:37.260 --> 00:36:39.979
the database fill papers will give you access to,

751
00:36:40.129 --> 00:36:42.820
you know, most all of my published work. Um,

752
00:36:43.100 --> 00:36:44.500
BUT I'm certainly not a stranger on the internet.

753
00:36:44.580 --> 00:36:46.199
I shouldn't be too hard to find, so.

754
00:36:46.969 --> 00:36:49.540
OK, great. So, thank you so much for taking

755
00:36:49.540 --> 00:36:51.580
the time to come on the show. It's been

756
00:36:51.580 --> 00:36:53.520
a real pleasure to talk with you.

757
00:36:53.979 --> 00:36:56.260
It's been my pleasure. Always, uh, intrigued to talk

758
00:36:56.260 --> 00:36:58.159
about these topics. So, thanks very much.

759
00:36:59.409 --> 00:37:01.929
Hi guys, thank you for watching this interview until

760
00:37:01.929 --> 00:37:04.050
the end. If you liked it, please share it,

761
00:37:04.250 --> 00:37:07.020
leave a like and hit the subscription button. The

762
00:37:07.020 --> 00:37:09.280
show is brought to you by Enlights Learning and

763
00:37:09.280 --> 00:37:13.310
Development done differently. Check their website at enlights.com and

764
00:37:13.310 --> 00:37:17.040
also please consider supporting the show on Patreon or

765
00:37:17.040 --> 00:37:19.520
PayPal. I would also like to give a huge

766
00:37:19.520 --> 00:37:22.620
thank you to my main patrons and PayPal supporters,

767
00:37:23.040 --> 00:37:26.879
Perergo Larsson, Jerry Muller, Frederick Sundo, Bernard Seyaz Olaf,

768
00:37:26.919 --> 00:37:30.189
Alex, Adam Cassel, Matthew Whittingbird, Arnaud Wolff, Tim Hollis,

769
00:37:30.340 --> 00:37:33.909
Eric Elena, John Connors, Philip Forrest Connolly. Then Dmitri

770
00:37:33.909 --> 00:37:37.939
Robert Windegeru Inai Zu Mark Nevs, Colin Holbrookfield, Governor,

771
00:37:38.419 --> 00:37:42.209
Michel Stormir, Samuel Andrea, Francis Forti Agnun, Svergoo, and

772
00:37:42.209 --> 00:37:45.989
Hal Herzognon, Michel Jonathan Labran, John Yardston, and Samuel

773
00:37:45.989 --> 00:37:49.909
Curric Hines, Mark Smith, John Ware, Tom Hammel, Sardusran,

774
00:37:50.070 --> 00:37:53.750
David Sloan Wilson, Yasilla Dezara Romain Roach, Diego Londono

775
00:37:53.750 --> 00:37:58.139
Correa. Yannik Punteran Ruzmani, Charlotte Blis Nicole Barbaro, Adam

776
00:37:58.139 --> 00:38:02.090
Hunt, Pavlostazevski, Alekbaka, Madison, Gary G. Alman, Semov, Zal

777
00:38:02.090 --> 00:38:06.100
Adrian Yei Poltontin, John Barboza, Julian Price, Edward Hall,

778
00:38:06.179 --> 00:38:10.500
Eddin Bronner, Douglas Fry, Franco Bartolotti, Gabriel Pan Scortez

779
00:38:10.500 --> 00:38:14.179
or Suliliski, Scott Zachary Fish, Tim Duffy, Sony Smith,

780
00:38:14.260 --> 00:38:18.290
and Wiseman. Daniel Friedman, William Buckner, Paul Georg Jarno,

781
00:38:18.649 --> 00:38:23.389
Luke Lovai, Georgios Theophanous, Chris Williamson, Peter Wolozin, David

782
00:38:23.399 --> 00:38:27.250
Williams, Di Acosta, Anton Ericsson, Charles Murray, Alex Shaw,

783
00:38:27.560 --> 00:38:31.699
Marie Martinez, Coralli Chevalier, Bangalore atheists, Larry D. Lee

784
00:38:31.699 --> 00:38:36.649
Junior. Old Eringbon. Esterri, Michael Bailey, then Spurber, Robert

785
00:38:36.649 --> 00:38:41.290
Grassy, Zigoren, Jeff McMahon, Jake Zul, Barnabas Raddix, Mark

786
00:38:41.290 --> 00:38:45.280
Kempel, Thomas Dovner, Luke Neeson, Chris Story, Kimberly Johnson,

787
00:38:45.570 --> 00:38:50.090
Benjamin Galbert, Jessica Nowicki, Linda Brendan, Nicholas Carlson, Ismael

788
00:38:50.090 --> 00:38:55.050
Bensleyman. George Ekoriati, Valentine Steinmann, Per Crawley, Kate Van

789
00:38:55.050 --> 00:39:01.899
Goler, Alexander Obert, Liam Dunaway, BR, Massoud Ali Mohammadi,

790
00:39:02.169 --> 00:39:07.719
Perpendicular, Janusertner, Ursula Guinov, Gregory Hastings, David Pinsov, Sean

791
00:39:08.000 --> 00:39:12.060
Nelson, Mike Levin, and Jos Necht. A special thanks

792
00:39:12.060 --> 00:39:15.020
to my producers Iar Webb, Jim Frank Lucas Stink,

793
00:39:15.139 --> 00:39:19.550
Tom Vanneden, Bernardine Curtis Dixon, Benedict Mueller, Thomas Trumbull,

794
00:39:19.860 --> 00:39:22.800
Catherine and Patrick Tobin, John Carlomon Negro, Al Nick

795
00:39:22.800 --> 00:39:26.010
Cortiz and Nick Golden, and to my executive producers,

796
00:39:26.020 --> 00:39:29.739
Matthew Lavender, Sergio Quadrian, Bogdan Kanis, and Rosie. Thank

797
00:39:29.739 --> 00:39:30.399
you for all.

