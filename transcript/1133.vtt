WEBVTT

1
00:00:00.400 --> 00:00:03.359
Hello, everyone. Welcome to a new episode of the

2
00:00:03.359 --> 00:00:06.320
Center. I'm your host, as always, Ricardo Lopes and

3
00:00:06.320 --> 00:00:09.300
today I'm joined by Doctor Blake Rober. He is

4
00:00:09.300 --> 00:00:12.680
the Michael Pig. Grace, the 2nd Associate professor of

5
00:00:12.680 --> 00:00:16.840
philosophy at Notre Dame. And today we're talking about

6
00:00:16.840 --> 00:00:20.879
his book Political Humility, The Limits of Knowledge in

7
00:00:20.879 --> 00:00:25.280
Our Partisan Political Climate. So, Doctor Robert, welcome to

8
00:00:25.280 --> 00:00:26.879
the show. It's a pleasure to everyone.

9
00:00:27.610 --> 00:00:28.920
Thank you. Yeah, I appreciate that.

10
00:00:30.340 --> 00:00:33.509
So, uh, I mean, let's talk a little bit

11
00:00:33.509 --> 00:00:37.049
first about political knowledge, but uh I would like

12
00:00:37.049 --> 00:00:39.580
to ask you to start off with what is

13
00:00:39.580 --> 00:00:42.979
the main claim you make in your book in

14
00:00:42.979 --> 00:00:45.409
regards to political knowledge?

15
00:00:46.490 --> 00:00:50.310
Yeah, so it's um it's it's really a claim

16
00:00:50.310 --> 00:00:56.770
about Americans and their interaction with political information right

17
00:00:56.770 --> 00:01:02.619
now, um. So this may generalize to other populations.

18
00:01:02.700 --> 00:01:04.690
I, I would suspect it does, but it's gonna

19
00:01:04.690 --> 00:01:07.419
be kind of an empirical question whether it does

20
00:01:07.419 --> 00:01:11.339
and um and how it does. And uh I

21
00:01:11.339 --> 00:01:15.459
am um unfamiliar with uh or I, I'm not

22
00:01:15.459 --> 00:01:18.129
aware of enough literature to really answer that question.

23
00:01:18.500 --> 00:01:21.940
So the idea is, um, Whether or not a

24
00:01:21.940 --> 00:01:25.930
particular person knows something is going to depend on

25
00:01:25.930 --> 00:01:29.529
the details of the acquisition of the specific belief

26
00:01:29.529 --> 00:01:33.010
in question, and specifically it's going to depend on

27
00:01:33.010 --> 00:01:35.370
whether or not that belief is the product of

28
00:01:35.370 --> 00:01:38.809
a reliable cognitive process. And reliable here means reliable

29
00:01:38.809 --> 00:01:42.230
with respect to the truth. So any sort of

30
00:01:42.230 --> 00:01:46.290
cognitive process that produces mostly true beliefs is going

31
00:01:46.290 --> 00:01:49.069
to be reliable. A cognitive process that fails to

32
00:01:49.069 --> 00:01:51.220
produce mostly true beliefs is not going to be

33
00:01:51.220 --> 00:01:54.629
reliable. And in order for a belief to count

34
00:01:54.629 --> 00:01:56.550
as knowledge, it's gonna, at a minimum, it's gonna

35
00:01:56.550 --> 00:01:58.769
have to be the product of a reliable process.

36
00:01:59.349 --> 00:02:01.349
And so the kind of the core idea of

37
00:02:01.349 --> 00:02:05.529
the book is that um You might think of

38
00:02:05.529 --> 00:02:08.729
it this way. Americans have kind of taken themselves

39
00:02:08.729 --> 00:02:12.119
out of position to know very much about politics

40
00:02:12.119 --> 00:02:16.100
because our politics have gotten so combative, so acrimonious.

41
00:02:16.940 --> 00:02:20.460
Um, SO polarized or however you want to put

42
00:02:20.460 --> 00:02:24.380
it, that our interaction with political information is no

43
00:02:24.380 --> 00:02:28.149
longer truth-oriented, it's much more about kind of picking

44
00:02:28.149 --> 00:02:32.660
sides and defending your side come what may. Um,

45
00:02:32.740 --> 00:02:35.020
AS a consequence of this way of interacting with

46
00:02:35.020 --> 00:02:38.860
political information, it's no longer true that any significant

47
00:02:38.860 --> 00:02:41.899
percentage of American political beliefs are products of reliable

48
00:02:41.899 --> 00:02:45.789
processes. As a consequence of that, very few American

49
00:02:45.789 --> 00:02:49.300
political beliefs are going to amount to knowledge. Um,

50
00:02:49.630 --> 00:02:52.300
SO that's like, uh, that was sort of jargonny

51
00:02:52.300 --> 00:02:55.029
and long, but that's kind of the, the simplest

52
00:02:55.029 --> 00:02:56.679
way to put the idea of the book. It's

53
00:02:56.679 --> 00:02:59.779
sort of crucial to understand. That what I'm saying

54
00:02:59.779 --> 00:03:03.259
about um the absence of political knowledge is a

55
00:03:03.259 --> 00:03:07.220
contingent thing. It's, it's a byproduct of, of the

56
00:03:07.220 --> 00:03:10.580
way Americans are right now forming their political beliefs.

57
00:03:10.619 --> 00:03:13.339
Um, IT'S, it's in principle possible to have a

58
00:03:13.339 --> 00:03:15.130
lot of political knowledge. The idea in the book

59
00:03:15.130 --> 00:03:17.100
is that right now, given what we're actually forming

60
00:03:17.100 --> 00:03:18.889
our political beliefs, we have to not have very

61
00:03:18.889 --> 00:03:21.779
much political knowledge. Uh, SO yeah, that's the gist

62
00:03:21.779 --> 00:03:22.020
of it.

63
00:03:22.929 --> 00:03:25.800
OK, uh, yeah, we're going to break down some

64
00:03:25.800 --> 00:03:27.839
of what you said there for the course of

65
00:03:27.839 --> 00:03:33.039
our conversation, but political knowledge, what is it exactly?

66
00:03:33.160 --> 00:03:35.660
What counts as political knowledge?

67
00:03:36.080 --> 00:03:39.429
Yeah, good. So. So, so I mean something really

68
00:03:39.429 --> 00:03:42.149
specific by that. Um, SO there are lots of

69
00:03:42.149 --> 00:03:44.789
different kinds of knowledge. There's, you know, so knowing

70
00:03:44.789 --> 00:03:47.899
a person, there's, uh, knowing how to do something.

71
00:03:48.270 --> 00:03:51.750
Then there's also what episteepistemologists call propositional knowledge, and

72
00:03:51.750 --> 00:03:54.979
propositional knowledge always gets expressed uh with the sentence,

73
00:03:55.070 --> 00:03:57.589
you know, I know that it was raining, um,

74
00:03:57.710 --> 00:03:59.110
so that, you know, the proposition that it was

75
00:03:59.110 --> 00:04:01.059
raining, I know that it was raining as propositional

76
00:04:01.059 --> 00:04:03.190
knowledge. Um, YOU know, I know that Austin is

77
00:04:03.190 --> 00:04:05.509
the capital of Texas. There's the proposition that Austin

78
00:04:05.509 --> 00:04:08.339
is the capital of Texas, that's propositional knowledge. The

79
00:04:08.339 --> 00:04:10.970
propositional knowledge is always knowledge of that, and then

80
00:04:11.179 --> 00:04:13.820
there's a proposition, and then the proposition is the

81
00:04:13.820 --> 00:04:18.130
content of what's known. So, political knowledge is knowledge

82
00:04:18.130 --> 00:04:20.858
of political propositions, and I mean something really specific

83
00:04:20.858 --> 00:04:25.299
by political propositions. Political propositions are basically propositions that

84
00:04:25.299 --> 00:04:29.239
are sort of argumentatively useful in the contest between

85
00:04:29.239 --> 00:04:32.140
Democrats and Republicans. So the book focuses entirely on

86
00:04:32.140 --> 00:04:35.399
the contest between Democrats and Republicans. And so there

87
00:04:35.399 --> 00:04:38.239
are all these sort of standard examples of claims

88
00:04:38.239 --> 00:04:41.200
made by one political side that are disputed by

89
00:04:41.200 --> 00:04:43.760
the other side. The reason they're disputed by the

90
00:04:43.760 --> 00:04:46.630
other side is because if these claims are true,

91
00:04:47.320 --> 00:04:50.359
that gives support for the political positions of the

92
00:04:50.359 --> 00:04:54.040
one side, right? So, um, the, you know, for

93
00:04:54.040 --> 00:04:57.160
example, the claim that human beings are responsible for

94
00:04:57.160 --> 00:05:01.079
climate change supports a bunch of democratic policies. These

95
00:05:01.079 --> 00:05:04.640
are policies that Republicans oppose. Um, AND so if

96
00:05:04.640 --> 00:05:08.109
it can be known that, uh, human beings are

97
00:05:08.109 --> 00:05:11.579
responsible for, uh, climate change or something like that,

98
00:05:11.989 --> 00:05:15.880
this puts Democrats in a favorable position because now

99
00:05:15.880 --> 00:05:19.799
we know something that sort of, uh, um, can

100
00:05:19.799 --> 00:05:23.190
be used to leverage power for the Democratic Party

101
00:05:23.190 --> 00:05:26.239
or something like that. So political knowledge would basically

102
00:05:26.239 --> 00:05:29.239
be knowledge that's sort of like argumentatively useful in

103
00:05:29.239 --> 00:05:31.799
this way, it's knowledge of facts that are contested

104
00:05:31.799 --> 00:05:37.140
by Uh, the parties precisely because, um, these things

105
00:05:37.140 --> 00:05:41.779
would be argumentatively useful or useful for, uh, defending

106
00:05:41.779 --> 00:05:44.220
the positions or policies or whatever the candidates of

107
00:05:44.220 --> 00:05:46.540
one party over another. So that's the idea of

108
00:05:46.540 --> 00:05:47.260
political knowledge.

109
00:05:48.140 --> 00:05:50.859
So you said earlier that you focus on the

110
00:05:50.859 --> 00:05:54.670
US and the American political system. Is that the

111
00:05:54.670 --> 00:06:00.429
main reason why you focus on Republicans and Democrats

112
00:06:00.429 --> 00:06:01.029
specifically?

113
00:06:02.160 --> 00:06:04.929
I, I, I guess really the main reason is

114
00:06:04.929 --> 00:06:08.679
um because the, the central thesis of the book

115
00:06:08.679 --> 00:06:12.890
depends so heavily on the empirical literature, the actual

116
00:06:12.890 --> 00:06:16.750
like uh um kind of scientific studies. Uh, I

117
00:06:16.750 --> 00:06:19.350
just simply didn't have the time to really look

118
00:06:19.350 --> 00:06:22.429
into all of the relevant facts that would allow

119
00:06:22.429 --> 00:06:25.230
me to come to conclusions about other countries. I

120
00:06:25.230 --> 00:06:28.059
mean, it's just, just, just, uh, managing this stuff,

121
00:06:28.269 --> 00:06:30.670
you know, of course, like, you know, uh, Homebase

122
00:06:30.670 --> 00:06:32.350
for me is the United States. So it's what

123
00:06:32.350 --> 00:06:34.510
I'm already most familiar with. It's what I already

124
00:06:34.510 --> 00:06:36.910
knew the most about. And even giving that starting

125
00:06:36.910 --> 00:06:40.000
point, it was still a huge project, several years'

126
00:06:40.000 --> 00:06:42.660
worth of reading. We're required to, to get in

127
00:06:42.660 --> 00:06:44.950
position, even kind of start writing the book. Um.

128
00:06:45.679 --> 00:06:48.850
And uh sort of extending the arguments to the

129
00:06:48.850 --> 00:06:52.570
political situations of other countries was just um it,

130
00:06:52.649 --> 00:06:54.079
I just didn't have the time to do it,

131
00:06:54.089 --> 00:06:54.839
honestly.

132
00:06:56.440 --> 00:06:58.989
No, that's fair enough, but do you have any

133
00:06:58.989 --> 00:07:02.549
idea at all whether the conclusions you arrive at,

134
00:07:02.700 --> 00:07:05.850
we, you arrive at in your book would also

135
00:07:05.850 --> 00:07:11.600
apply to political contexts or party systems outside of

136
00:07:11.600 --> 00:07:13.640
and different from the American?

137
00:07:15.059 --> 00:07:16.500
Yeah, so I mean at least I can, I

138
00:07:16.500 --> 00:07:18.549
can sort of like give you a, a schema

139
00:07:18.549 --> 00:07:20.899
that would allow people who know about the actual

140
00:07:20.899 --> 00:07:22.339
facts on the ground to come to a very

141
00:07:22.339 --> 00:07:25.679
quick conclusion about this. So, the, the idea is

142
00:07:25.679 --> 00:07:28.119
basically something like, um, if you look at any

143
00:07:28.119 --> 00:07:31.869
country in which people are highly invested in politics,

144
00:07:32.000 --> 00:07:34.079
and to the extent that people have started to

145
00:07:34.079 --> 00:07:37.239
kind of identify with one political party or the

146
00:07:37.239 --> 00:07:40.920
other, you're going to see, um, at least you

147
00:07:40.920 --> 00:07:42.640
should see all the patterns that you see in

148
00:07:42.640 --> 00:07:45.440
the United States, and basically those patterns consist in

149
00:07:45.440 --> 00:07:48.750
forms of reasoning. That are no longer aimed at

150
00:07:48.750 --> 00:07:51.589
truth or forms of uh or forms of cognition

151
00:07:51.589 --> 00:07:53.230
that are no longer aimed at truth. What they're

152
00:07:53.230 --> 00:07:55.670
aimed at instead is something like in the kind

153
00:07:55.670 --> 00:07:58.470
of the paradigmatic case, what these forms of cognition

154
00:07:58.470 --> 00:08:01.989
are aimed at is preserving one's um membership inside

155
00:08:01.989 --> 00:08:05.790
of this identity defining community. So, you know, pick

156
00:08:05.790 --> 00:08:08.149
any country. If it's the case that a high

157
00:08:08.149 --> 00:08:11.410
percentage of partisans in that country strongly identify with

158
00:08:11.410 --> 00:08:14.799
their political parties. Then what you would expect is

159
00:08:14.799 --> 00:08:17.799
for the cognitive processes responsible for the political beliefs

160
00:08:17.799 --> 00:08:21.869
in that country to be products of, of um

161
00:08:22.260 --> 00:08:24.519
of cognitive processes that are aimed not at truth

162
00:08:24.519 --> 00:08:27.359
but at uh kind of retaining one's membership and

163
00:08:27.359 --> 00:08:30.399
that identity defining group. And then you would expect

164
00:08:30.399 --> 00:08:33.440
that the resulting beliefs to be unreliably formed, and

165
00:08:33.440 --> 00:08:36.159
you'd have the same conclusions about those political beliefs

166
00:08:36.159 --> 00:08:38.359
as I have in my book about American political

167
00:08:38.359 --> 00:08:40.808
beliefs. Um. And, and from what I know, I

168
00:08:40.808 --> 00:08:42.799
mean, you do see this sort of like, you

169
00:08:42.799 --> 00:08:47.010
know, strong identification with, uh, political parties in lots

170
00:08:47.010 --> 00:08:50.090
of countries outside of the United States. Um, I

171
00:08:50.090 --> 00:08:52.450
just am not nearly as familiar with that literature.

172
00:08:52.599 --> 00:08:55.609
So I, I kinda, I, I try to, try

173
00:08:55.609 --> 00:08:57.289
to keep my mouth shut about things I don't

174
00:08:57.289 --> 00:09:00.880
know that much about, so. So, yeah, so I'm,

175
00:09:01.130 --> 00:09:03.049
this is, I'm, I'm just speculating here.

176
00:09:03.770 --> 00:09:06.590
Yeah, no, that's totally fine. I was just wondering

177
00:09:06.590 --> 00:09:09.140
whether you would have any idea if it would

178
00:09:09.140 --> 00:09:13.549
also apply to contexts outside of the US, but

179
00:09:13.549 --> 00:09:18.419
that's perfectly fine. Uh, WHAT do words like liberal

180
00:09:18.419 --> 00:09:21.909
and conservative mean? And I would imagine that in

181
00:09:21.909 --> 00:09:25.429
this particular case, we would be talking about what

182
00:09:25.429 --> 00:09:29.309
they mean also in the context of uh the

183
00:09:29.309 --> 00:09:30.349
US. Correct.

184
00:09:30.640 --> 00:09:34.280
Yeah, good. Um, YEAH. So there's, uh, there's, you

185
00:09:34.280 --> 00:09:37.780
know, um, These words are adjectives, right, liberal and

186
00:09:37.780 --> 00:09:44.179
conservative, and as adjectives, they describe sort of sensibilities

187
00:09:44.179 --> 00:09:48.030
about things. Um. There are also the words liberals

188
00:09:48.030 --> 00:09:51.390
and conservatives, um, and these, I think in the

189
00:09:51.390 --> 00:09:53.909
American context at least at least kind of function

190
00:09:53.909 --> 00:09:57.270
as labels. And so I'll be honest with you,

191
00:09:57.309 --> 00:09:58.750
I'm kind of confused by the use of these

192
00:09:58.750 --> 00:10:00.590
words, but so far as I can tell in

193
00:10:00.590 --> 00:10:03.739
the American context, when these words function as labels,

194
00:10:04.070 --> 00:10:07.789
they're very nearly synonyms for Democrats and Republicans, right?

195
00:10:07.890 --> 00:10:10.309
So the liberals, the label is nearly a synonym

196
00:10:10.309 --> 00:10:13.880
for Democrats and conservatives, the label is nearly a

197
00:10:13.880 --> 00:10:17.969
synonym for Republicans. Or if it's not quite that,

198
00:10:17.979 --> 00:10:21.710
it's maybe like, you know, Democrat and Republican implies

199
00:10:21.710 --> 00:10:25.070
party membership, and maybe liberals and conservatives implies sort

200
00:10:25.070 --> 00:10:27.630
of just liberals are the people who sort of

201
00:10:27.630 --> 00:10:30.150
like the ideas endorsed by Democrats even if they

202
00:10:30.150 --> 00:10:32.150
maybe aren't, you know, members of that party and

203
00:10:32.150 --> 00:10:35.309
conservatives or something like people who like the ideas

204
00:10:35.309 --> 00:10:37.150
endorsed by Republicans, even if they're not members of

205
00:10:37.150 --> 00:10:40.429
that party, um. Uh, AS I say in the

206
00:10:40.429 --> 00:10:43.799
book, I dislike these words because I think it's,

207
00:10:43.989 --> 00:10:46.390
you know, if, if you, if you take liberals

208
00:10:46.390 --> 00:10:48.469
and the words liberals and conservatives and use them

209
00:10:48.469 --> 00:10:51.739
as labels, and then think about the words liberal

210
00:10:51.739 --> 00:10:55.349
and conservative as adjectives, I think it's liberal literally

211
00:10:55.349 --> 00:10:57.630
true that liberals are not liberal and conservatives are

212
00:10:57.630 --> 00:11:01.969
not conservative. Um, SO, which is, you know, of

213
00:11:01.969 --> 00:11:04.599
course, is rather confusing, right? If, if the label

214
00:11:04.599 --> 00:11:06.450
liberals picks out a lot of people who are

215
00:11:06.450 --> 00:11:08.729
not liberal and the label conservatives picks out a

216
00:11:08.729 --> 00:11:10.789
lot of people who are not conservative, seems like

217
00:11:10.789 --> 00:11:12.210
we could do a little better with our, our

218
00:11:12.210 --> 00:11:16.210
vocabulary. Um, SO I sort of just avoid that

219
00:11:16.210 --> 00:11:18.799
and mess entirely in the book by just talking

220
00:11:18.799 --> 00:11:22.250
about Republicans and conservatives, or excuse me, Republicans and

221
00:11:22.250 --> 00:11:26.989
Democrats. Because, uh, I think lots of Democratic positions

222
00:11:26.989 --> 00:11:29.590
are actually very conservative positions. I think lots of

223
00:11:29.590 --> 00:11:31.909
Republican positions are actually very liberal positions.

224
00:11:33.510 --> 00:11:38.500
What are, what are liberal and conservative synonyms, uh

225
00:11:38.510 --> 00:11:42.909
uh uh synonyms of left wing and right wing.

226
00:11:43.809 --> 00:11:48.950
Maybe, yeah, yeah, I mean, um. Again, it's gonna

227
00:11:48.950 --> 00:11:53.280
depend on like, You know, I'm not sure to

228
00:11:53.280 --> 00:11:56.000
what extent the, the labels left wing wing and

229
00:11:56.000 --> 00:11:59.919
right-wing pick out anything very principled either. Um, IT

230
00:11:59.919 --> 00:12:02.280
might be that those are just sort of, at

231
00:12:02.280 --> 00:12:04.679
least in the American context, just kind of synonyms

232
00:12:04.679 --> 00:12:09.609
for Democrats or Republicans too. Um, SO, yeah, and

233
00:12:09.609 --> 00:12:11.919
you know, these, these words get used an awful

234
00:12:11.919 --> 00:12:14.840
lot by people who have not given very much

235
00:12:14.840 --> 00:12:17.559
thought to what the words actually mean, and I

236
00:12:17.559 --> 00:12:20.799
think in popular usage, right wing, left wing, liberals,

237
00:12:20.919 --> 00:12:26.770
conservatives. These are just uh uh very loose attempts

238
00:12:26.770 --> 00:12:28.849
to try to pick out people who are, you

239
00:12:28.849 --> 00:12:32.719
know, roughly Republican and people who are roughly Democrats.

240
00:12:33.479 --> 00:12:33.489
Right,

241
00:12:34.859 --> 00:12:37.030
right, I mean, I asked you that question because

242
00:12:37.030 --> 00:12:41.500
many times we hear people using those terms interchangeably.

243
00:12:41.750 --> 00:12:44.390
Right. Mhm. Yeah, exactly. So

244
00:12:44.390 --> 00:12:47.690
it might, it might get a bit confusing sometimes.

245
00:12:48.710 --> 00:12:49.020
Uh,

246
00:12:49.789 --> 00:12:54.469
uh, WHERE do we get our political information from?

247
00:12:54.510 --> 00:12:57.390
I mean, what sources do we draw it from?

248
00:12:58.400 --> 00:13:00.450
Yeah, that's so great. That's a very good question.

249
00:13:00.580 --> 00:13:03.510
Um, So again, this is kind of an empirical

250
00:13:03.510 --> 00:13:05.830
question, right? Uh, IT'S a, it's a question that

251
00:13:05.830 --> 00:13:08.950
you can't answer um entirely from the armchair. You

252
00:13:08.950 --> 00:13:11.510
have to sort of actually look at, at real

253
00:13:11.510 --> 00:13:13.679
life, the real world and see how people actually

254
00:13:13.679 --> 00:13:16.880
form their political views. Um. And one of the

255
00:13:16.880 --> 00:13:19.429
things that I emphasize in the book is that,

256
00:13:19.479 --> 00:13:20.960
I mean, so they are all variety of ways

257
00:13:20.960 --> 00:13:24.369
you could come by your political opinions, um, but

258
00:13:24.369 --> 00:13:28.479
as a consequence of just, you know, straightforward human

259
00:13:28.479 --> 00:13:32.440
limitations, um, and as a consequence of basically our

260
00:13:32.440 --> 00:13:35.080
access to the world by way of our normal

261
00:13:35.080 --> 00:13:38.840
cognitive faculties, it turns out that in the best

262
00:13:38.840 --> 00:13:43.599
case scenario, virtually all of our political information comes

263
00:13:43.599 --> 00:13:46.380
to us by way of testimony. And by testimony,

264
00:13:46.440 --> 00:13:49.239
I just mean we're being told what the world

265
00:13:49.239 --> 00:13:52.020
is like politically by other people one way or

266
00:13:52.020 --> 00:13:53.659
another. So this can be just sort of verbal

267
00:13:53.659 --> 00:13:58.250
testimony where somebody just tells me, you know, um,

268
00:13:58.659 --> 00:14:01.140
whatever it happens to be. Trump said yada yada

269
00:14:01.140 --> 00:14:03.820
yada on TV yesterday. It might be testimony in

270
00:14:03.820 --> 00:14:05.690
the form of writing, right? So I might read

271
00:14:05.690 --> 00:14:07.219
this in the newspaper, I might read it on

272
00:14:07.219 --> 00:14:11.090
a Facebook post. Um, MORE broadly, it might be

273
00:14:11.090 --> 00:14:13.729
testimony in the form of, say, like statistics or

274
00:14:13.729 --> 00:14:15.969
chart or a graph or something like that. So

275
00:14:15.969 --> 00:14:17.849
the way I use the word testimony in the

276
00:14:17.849 --> 00:14:20.929
book, um, if I look at a chart that

277
00:14:20.929 --> 00:14:25.109
basically like, uh, you know, Plots, uh, open or

278
00:14:25.109 --> 00:14:28.580
ocean temperatures across time or something like that. Um,

279
00:14:28.659 --> 00:14:31.140
THAT would be a form of testimony too. So

280
00:14:31.140 --> 00:14:32.770
testimony is basically the way I use it in

281
00:14:32.770 --> 00:14:38.239
the book is Information presented by human beings um

282
00:14:38.239 --> 00:14:40.659
to other human beings one way or another. Um,

283
00:14:40.669 --> 00:14:43.700
AND so the idea is virtually, if things are

284
00:14:43.700 --> 00:14:45.739
going well, if things are going as well as

285
00:14:45.739 --> 00:14:48.020
they could be going, virtually all of our political

286
00:14:48.020 --> 00:14:49.380
beliefs are based on testimony.

287
00:14:50.619 --> 00:14:54.590
And how much can we trust other people when

288
00:14:54.590 --> 00:14:57.190
they give us political information?

289
00:14:58.830 --> 00:15:02.989
So, um, it's, it's going to depend on the

290
00:15:02.989 --> 00:15:05.830
time and place and person, um, but kind of

291
00:15:05.830 --> 00:15:08.590
one of the central ideas in the book is

292
00:15:08.590 --> 00:15:13.299
that right now, unfortunately, we can't be very trustworthy

293
00:15:13.299 --> 00:15:15.700
of other people when they're giving us political information.

294
00:15:15.950 --> 00:15:18.030
And this goes back to something I was saying

295
00:15:18.030 --> 00:15:21.830
a little while ago about The highly combative nature

296
00:15:21.830 --> 00:15:24.840
of American politics right now and the fact that

297
00:15:24.840 --> 00:15:27.950
a high percentage of Americans are identifying with their

298
00:15:27.950 --> 00:15:32.020
politics right now. Um, AND precisely because it's the

299
00:15:32.020 --> 00:15:35.299
case that high percentages of Americans are identifying with

300
00:15:35.299 --> 00:15:38.419
their politics, it is right now, I think, the

301
00:15:38.419 --> 00:15:41.580
case that high percentages of American political beliefs are

302
00:15:41.580 --> 00:15:45.659
byproducts of unreliable cognitive faculties, and as a consequence

303
00:15:45.659 --> 00:15:49.010
of that, anytime somebody is giving us political information,

304
00:15:49.299 --> 00:15:53.400
we should be seriously worried um about the reliability

305
00:15:53.400 --> 00:15:56.570
of the source of that information, um, and then,

306
00:15:56.609 --> 00:15:58.880
then the upshot of that should be. Uh, SORT

307
00:15:58.880 --> 00:16:02.270
of an attitude of skepticism or distrust towards political

308
00:16:02.270 --> 00:16:02.950
information right now.

309
00:16:04.270 --> 00:16:09.760
What are those unreliable cognitive uh processes I think

310
00:16:09.760 --> 00:16:12.109
that that you are referring to there?

311
00:16:13.119 --> 00:16:16.400
Kind of the, the, the, um, maybe the most

312
00:16:16.400 --> 00:16:19.359
interesting or most widely discussed one right now is

313
00:16:19.359 --> 00:16:24.109
what people call identity protective cognition. And identity cognitive

314
00:16:24.109 --> 00:16:29.210
identity protective cognition is basically cognition aimed at preserving

315
00:16:29.210 --> 00:16:32.450
one's membership in some identity defining group, um, rather

316
00:16:32.450 --> 00:16:36.159
than cognition aimed at the truth. And so the

317
00:16:36.159 --> 00:16:39.349
idea would be something like, um, you know, take

318
00:16:39.349 --> 00:16:42.440
any group that you're part of, um, that also

319
00:16:42.440 --> 00:16:45.159
sort of forms part of your, you know, self-conception

320
00:16:45.159 --> 00:16:48.940
of, you know, who you are. And uh there

321
00:16:48.940 --> 00:16:52.099
will be, um, at least in, you know, in

322
00:16:52.099 --> 00:16:54.409
the case of many of these groups, part of

323
00:16:54.409 --> 00:16:57.299
being a member of that group will consist in

324
00:16:57.299 --> 00:17:01.010
agreeing with certain things and disagreeing with other things.

325
00:17:01.460 --> 00:17:04.969
And the idea behind identity protective cognition is that

326
00:17:04.969 --> 00:17:06.939
if you are a member of one of these

327
00:17:06.939 --> 00:17:09.819
groups and identify uh with this group in that

328
00:17:09.819 --> 00:17:14.310
way, your mind is going to handle information. Um,

329
00:17:14.439 --> 00:17:18.270
IN a way that's aimed more at ensuring agreement

330
00:17:18.358 --> 00:17:21.719
with the members of the group than actually cottoning

331
00:17:21.719 --> 00:17:25.260
onto the truth. Um, AND this is going to

332
00:17:25.260 --> 00:17:28.579
be more than anything else, uh, um, for the

333
00:17:28.579 --> 00:17:32.540
purposes of basically preserving these relationships that matter to

334
00:17:32.540 --> 00:17:32.819
you.

335
00:17:34.479 --> 00:17:38.270
So should we assume that when people start talking

336
00:17:38.270 --> 00:17:42.030
about politics, they are mostly unreliable?

337
00:17:44.569 --> 00:17:49.719
Um, Yeah, and this is going to be not

338
00:17:49.719 --> 00:17:52.839
so much um because you think like the, you

339
00:17:52.839 --> 00:17:55.839
know, the individual person is mostly unreliable, it's gonna

340
00:17:55.839 --> 00:18:00.890
be a byproduct. Of just this, you know, extremely

341
00:18:00.890 --> 00:18:06.209
complex. Uh, I guess system in which information could

342
00:18:06.209 --> 00:18:08.959
flow from person to person to person to person.

343
00:18:09.050 --> 00:18:11.930
So, um, for the exact same reason, you know,

344
00:18:12.050 --> 00:18:14.020
so an example I use in the book is,

345
00:18:14.130 --> 00:18:15.810
I, I, I talk a lot about the border

346
00:18:15.810 --> 00:18:17.890
wall between the United States and Mexico and this

347
00:18:17.890 --> 00:18:20.010
question of whether or not, uh, this, the border

348
00:18:20.010 --> 00:18:23.619
wall should be finished and The question, you know,

349
00:18:23.900 --> 00:18:29.420
depends on all variety of questions about who's crossing

350
00:18:29.420 --> 00:18:32.219
the border and how many of these people are

351
00:18:32.219 --> 00:18:34.719
crossing the border and what these people are doing

352
00:18:35.060 --> 00:18:37.969
after they cross the border and whether or not

353
00:18:37.969 --> 00:18:40.660
the wall would really make any significant impact on

354
00:18:40.660 --> 00:18:42.459
the number of people or the kinds of people

355
00:18:42.459 --> 00:18:44.819
that cross the border, how much money the wall

356
00:18:44.819 --> 00:18:46.979
is going to cost and how much money it's

357
00:18:46.979 --> 00:18:50.180
going to be required to. To keep the wall

358
00:18:50.180 --> 00:18:52.619
in service and yada yada yada yada, right? And

359
00:18:52.619 --> 00:18:56.900
these are all fantastically complicated questions. Um, JUST as

360
00:18:56.900 --> 00:18:59.170
if, just as, you know, I'm not in position

361
00:18:59.170 --> 00:19:00.900
to sort of answer any one of these questions

362
00:19:00.900 --> 00:19:02.619
by myself, right? It's not as if I can

363
00:19:02.619 --> 00:19:04.160
go and, you know, so suppose I start wondering,

364
00:19:04.300 --> 00:19:07.040
ah, you know, who is crossing the border? It's

365
00:19:07.040 --> 00:19:08.939
not as if I can just, you know, get

366
00:19:08.939 --> 00:19:11.579
in the car and cruise down to the United

367
00:19:11.579 --> 00:19:13.459
States, Mexico border and sort of just like hang

368
00:19:13.459 --> 00:19:15.270
out and watch for a while and get a

369
00:19:15.270 --> 00:19:18.290
good idea of who's crossing the border, right? Um,

370
00:19:18.459 --> 00:19:20.699
WELL, I can't do it. So if I want

371
00:19:20.699 --> 00:19:22.819
to know that, I got to ask somebody. But

372
00:19:22.819 --> 00:19:24.819
whoever I ask, that person can't do it either,

373
00:19:25.060 --> 00:19:27.420
right? So that person's got to ask somebody. But

374
00:19:27.420 --> 00:19:29.459
whoever that person asks can't do it either. So

375
00:19:29.459 --> 00:19:31.780
that person's got to ask somebody, which raises this

376
00:19:31.780 --> 00:19:35.050
question, where could this information possibly come from? Um,

377
00:19:35.069 --> 00:19:36.900
AND to the extent that this information has come

378
00:19:36.900 --> 00:19:40.859
from anything even approximating a good source, the information

379
00:19:40.859 --> 00:19:43.260
is going to have to have come from many,

380
00:19:43.310 --> 00:19:45.859
many, many sources that sort of like kind of

381
00:19:45.859 --> 00:19:49.530
funnel together, right? And so maybe I get my

382
00:19:49.530 --> 00:19:52.010
information from one person, but that one person has

383
00:19:52.010 --> 00:19:56.089
gotten their information ultimately from this vast network of

384
00:19:56.089 --> 00:20:00.369
people who are providing little teeny parts of the

385
00:20:00.369 --> 00:20:03.790
overall picture. And so in the book when I

386
00:20:03.790 --> 00:20:07.250
say I shouldn't be especially confident in the reliability

387
00:20:07.250 --> 00:20:10.069
of the person who's talking to me, the skepticism

388
00:20:10.069 --> 00:20:13.349
isn't so much about that person individually as it

389
00:20:13.349 --> 00:20:19.030
is about the whole system. Behind this person's acquisition

390
00:20:19.030 --> 00:20:20.959
of that little bit of information, right? So I

391
00:20:20.959 --> 00:20:25.270
think that given the highly combative nature of American

392
00:20:25.270 --> 00:20:27.280
politics and the way we strongly identify with our

393
00:20:27.280 --> 00:20:30.599
political parties right now, basically the system is unreliable

394
00:20:30.599 --> 00:20:32.760
and as a consequence of the system being unreliable,

395
00:20:32.800 --> 00:20:34.760
none of the people in that system are reliable

396
00:20:34.760 --> 00:20:35.119
either.

397
00:20:36.780 --> 00:20:40.319
Is it possible for us to tell if other

398
00:20:40.319 --> 00:20:45.119
people are giving us false information? In other words,

399
00:20:45.239 --> 00:20:47.800
is it possible for us to avoid deception?

400
00:20:49.790 --> 00:20:52.180
Well, so the, the empirical literature suggests that we're

401
00:20:52.180 --> 00:20:56.599
uh really quite bad at this. Um, What we

402
00:20:56.599 --> 00:21:00.800
might be good at doing is uh being skeptical

403
00:21:00.800 --> 00:21:03.469
of information that conflicts with what we already believe,

404
00:21:04.000 --> 00:21:07.479
um, being skeptical of information that somehow undermines our

405
00:21:07.479 --> 00:21:10.430
goals or our projects and those sorts of things.

406
00:21:10.560 --> 00:21:14.199
So, um, I think we're highly gullible, but the

407
00:21:14.199 --> 00:21:16.640
form of gullibility is not just like, oh, I'm

408
00:21:16.640 --> 00:21:19.719
gonna believe absolutely anything anybody tells me. Um, THE

409
00:21:19.719 --> 00:21:24.219
form of gullibility is if somebody says something, That,

410
00:21:24.310 --> 00:21:27.750
uh, agrees with my politics and, and doesn't seem

411
00:21:27.750 --> 00:21:31.030
wildly implausible on the face of it, and doesn't

412
00:21:31.030 --> 00:21:33.150
sort of like undermine any of my projects or

413
00:21:33.150 --> 00:21:35.979
anything like that, then I'm gonna believe the person,

414
00:21:36.150 --> 00:21:38.829
um, whether or not what they're telling me is

415
00:21:38.829 --> 00:21:42.890
actually true. So the idea would be something like,

416
00:21:42.900 --> 00:21:50.599
um, We have uh No real ability to check

417
00:21:50.599 --> 00:21:52.910
the facts or tell when we're getting good or

418
00:21:52.910 --> 00:21:56.589
bad information beyond whether or not that information sort

419
00:21:56.589 --> 00:21:59.280
of is kind of what we want to hear,

420
00:21:59.439 --> 00:22:00.910
right? Or what we want to hear is a

421
00:22:00.910 --> 00:22:03.589
function of whether coherence is what we already believe

422
00:22:03.589 --> 00:22:07.000
and so on and so forth. Um, SO yeah,

423
00:22:07.069 --> 00:22:09.800
I think, I think that we're actually, uh, really

424
00:22:09.800 --> 00:22:13.479
pretty poorly positioned to look at all the political

425
00:22:13.479 --> 00:22:16.160
information coming our way and sort the good information

426
00:22:16.160 --> 00:22:18.680
from the bad information. So it's not just the

427
00:22:18.680 --> 00:22:22.150
testifiers are unreliable in this broken system, it's that

428
00:22:22.560 --> 00:22:26.160
the receivers of this testimony we're also unreliable ourselves,

429
00:22:26.359 --> 00:22:28.599
right? So we're getting all this bad information, you

430
00:22:28.599 --> 00:22:31.880
know, the, the, the, the sort of um Output

431
00:22:31.880 --> 00:22:35.359
is unreliable, um, and then we are also unreliable

432
00:22:35.359 --> 00:22:38.000
at sifting the good information from the bad information.

433
00:22:38.359 --> 00:22:39.589
That's at least that's my view.

434
00:22:40.900 --> 00:22:44.920
Yeah, uh, what is political skepticism then?

435
00:22:46.290 --> 00:22:47.819
Yeah, so, so this is like kind of a

436
00:22:47.819 --> 00:22:49.819
term of art that I just introduced in the

437
00:22:49.819 --> 00:22:53.780
book. Um, AND basically what I mean by it

438
00:22:53.780 --> 00:22:57.750
is it's just a, a label for the central

439
00:22:57.750 --> 00:23:01.540
idea of the book that in our, uh, kind

440
00:23:01.540 --> 00:23:05.900
of current political environment, um, nobody is in a

441
00:23:05.900 --> 00:23:08.540
position to know very much about politics or nobody

442
00:23:08.540 --> 00:23:12.260
has very, uh, any high amount of political uh

443
00:23:12.260 --> 00:23:17.099
knowledge. So most forms of skepticism. ARE just categorically

444
00:23:17.099 --> 00:23:19.859
deny knowledge. Um, AND so in a and, and,

445
00:23:20.020 --> 00:23:23.459
and the labels a little bit misleading in that

446
00:23:23.459 --> 00:23:26.140
sense. At least it's misleading to epistemologists. So when

447
00:23:26.140 --> 00:23:28.949
epistemologists hear the label skepticism, they usually, usually think,

448
00:23:29.040 --> 00:23:31.410
oh, it's a view that says knowledge is impossible

449
00:23:31.410 --> 00:23:33.650
in this particular domain. I'm not saying knowledge is

450
00:23:33.650 --> 00:23:36.219
impossible. I'm just saying we have very little of

451
00:23:36.219 --> 00:23:39.130
it. Um. And another way in which the term

452
00:23:39.130 --> 00:23:42.949
is kind of misleading is usually an epistemology, skepticism

453
00:23:42.949 --> 00:23:45.189
is kind of a, a, a sort of armchair

454
00:23:45.189 --> 00:23:48.310
view where just like, it's, it's not just that

455
00:23:48.310 --> 00:23:52.189
we, you know, can't know very much. Given our

456
00:23:52.189 --> 00:23:54.910
contingent circumstances right now, it's that we can't know

457
00:23:54.910 --> 00:23:56.439
very much or can't know anything at all in

458
00:23:56.439 --> 00:23:59.420
the domain, no matter what our circumstances might be,

459
00:23:59.660 --> 00:24:02.270
right? Um, AND if that's your conclusion, you might

460
00:24:02.270 --> 00:24:03.949
be able to defend that conclusion from the armchair.

461
00:24:04.390 --> 00:24:06.790
My view is important and different. My view is

462
00:24:06.790 --> 00:24:09.589
that we have some knowledge, just not very much

463
00:24:09.589 --> 00:24:12.130
of it, and my view is also that there's,

464
00:24:12.260 --> 00:24:14.030
the, the reason for this is that it's a

465
00:24:14.030 --> 00:24:16.310
byproduct of these contingent facts about the formation of

466
00:24:16.310 --> 00:24:19.800
our political lis right now. Uh, IF we were

467
00:24:20.099 --> 00:24:23.010
to stop identifying with our politics, for example, Then

468
00:24:23.010 --> 00:24:24.719
it may well be the case that we'd be

469
00:24:24.719 --> 00:24:27.520
able to engage with political information um in a

470
00:24:27.520 --> 00:24:31.680
totally different way, rely on uh uh reliable cognitive

471
00:24:31.680 --> 00:24:35.270
faculties rather than cognitive biases to process political information,

472
00:24:35.400 --> 00:24:36.599
and then we would be in a position where

473
00:24:36.599 --> 00:24:38.150
we actually do have a lot of political knowledge.

474
00:24:38.979 --> 00:24:40.890
Uh, I just say that we don't right now,

475
00:24:40.939 --> 00:24:43.579
and that's the skepticism behind political skepticism in the

476
00:24:43.579 --> 00:24:43.939
book.

477
00:24:45.030 --> 00:24:48.030
But what does it mean exactly for you to

478
00:24:48.030 --> 00:24:52.030
say that we don't have much political knowledge? I

479
00:24:52.030 --> 00:24:54.589
mean, we have some but not much. I mean,

480
00:24:54.750 --> 00:24:57.750
we don't have much political knowledge in what sense

481
00:24:57.750 --> 00:25:01.420
exactly, in the sense that we would need more

482
00:25:01.420 --> 00:25:07.219
political knowledge to make proper political decisions like voting,

483
00:25:07.310 --> 00:25:08.030
for example.

484
00:25:09.140 --> 00:25:13.189
Oh, good. Yeah. OK. So, um, So I think

485
00:25:13.189 --> 00:25:15.790
you can do all, I mean, you can make

486
00:25:15.790 --> 00:25:19.469
your way through life perfectly well with almost no

487
00:25:19.469 --> 00:25:22.680
knowledge um. So, I mean, just think of like

488
00:25:22.680 --> 00:25:25.040
any sort of example, uh, whatever you're trying to

489
00:25:25.040 --> 00:25:26.630
find your way to a restaurant and you get

490
00:25:26.630 --> 00:25:28.680
to an intersection that you didn't expect on the

491
00:25:28.680 --> 00:25:30.599
route, right? So you, you, your friend is giving

492
00:25:30.599 --> 00:25:34.199
you direct and back in the day before, you

493
00:25:34.199 --> 00:25:36.680
know, GPS and Google Maps and all that, you

494
00:25:36.680 --> 00:25:38.319
know, you ask your friend for directions to the

495
00:25:38.319 --> 00:25:39.839
restaurant and they ride it out for you and

496
00:25:39.839 --> 00:25:41.160
it's like you take a ride at this street

497
00:25:41.160 --> 00:25:42.319
and the left this street, and then all of

498
00:25:42.319 --> 00:25:43.959
a sudden you come to, uh, a T in

499
00:25:43.959 --> 00:25:46.739
the road. Um, AND that's not in the directions,

500
00:25:46.750 --> 00:25:48.270
and so you don't, you know, which way do

501
00:25:48.270 --> 00:25:50.989
I go? Uh, YOU don't know, but maybe you

502
00:25:50.989 --> 00:25:52.869
can look at the two streets and looking left,

503
00:25:52.910 --> 00:25:55.030
it seems like, you know, it turns from 4

504
00:25:55.030 --> 00:25:56.510
lanes down to 2 lanes, and all of a

505
00:25:56.510 --> 00:25:58.619
sudden it looks like it's a neighborhood, you know,

506
00:25:58.869 --> 00:26:01.109
and it just seems kind of improbable that you

507
00:26:01.109 --> 00:26:04.119
should go left. Um, AND so you turn right.

508
00:26:04.280 --> 00:26:05.569
Uh, YOU know, in this sort of case, you

509
00:26:05.569 --> 00:26:07.780
don't know that you should turn right. Maybe left

510
00:26:07.780 --> 00:26:10.099
is the right direction, but, you know, given your

511
00:26:10.099 --> 00:26:13.329
overall evidential situation, it looks more probable that the

512
00:26:13.329 --> 00:26:15.569
right turn is the correct turn and so you

513
00:26:15.569 --> 00:26:18.329
turn right. Um, THIS is a decision not made

514
00:26:18.329 --> 00:26:20.569
not on knowledge, but on the basis of an

515
00:26:20.569 --> 00:26:24.339
educated guess, um, and Start paying attention to like

516
00:26:24.339 --> 00:26:26.459
what we do in day to day life, absolutely

517
00:26:26.459 --> 00:26:30.079
vast amount of our decisions are made not on

518
00:26:30.079 --> 00:26:31.540
the basis of knowledge, but on the basis of

519
00:26:31.540 --> 00:26:34.099
educated guesses. And we can navigate the world really

520
00:26:34.099 --> 00:26:37.410
pretty well on the basis of educated guesses. By

521
00:26:37.410 --> 00:26:39.579
navigating by pretty well, I don't mean necessarily that

522
00:26:39.579 --> 00:26:41.819
we make the right decisions, it's just that we're

523
00:26:41.819 --> 00:26:44.459
perfectly capable of decision making in the absence of

524
00:26:44.459 --> 00:26:47.430
knowledge. Um, So when I, when I talk about

525
00:26:47.430 --> 00:26:50.729
political skepticism in the book, Uh, the idea is

526
00:26:50.729 --> 00:26:54.410
not that we're totally out of position to make

527
00:26:54.410 --> 00:26:56.959
decisions within politics, right? So the idea is not,

528
00:26:57.130 --> 00:26:59.099
well, we have no idea how we should vote,

529
00:26:59.680 --> 00:27:03.040
um, etc. ETC. It's that our voting and our

530
00:27:03.040 --> 00:27:07.369
other political activity is going to be based much

531
00:27:07.369 --> 00:27:10.209
more on educated guessing than it is going to

532
00:27:10.209 --> 00:27:14.000
be based on knowledge. Um, AND our speech, uh,

533
00:27:14.089 --> 00:27:16.790
in fact, when we talk about politics. In a

534
00:27:16.790 --> 00:27:18.459
lot of ways, this is kind of the primary

535
00:27:18.459 --> 00:27:21.229
concern of the book. Uh, I don't say this

536
00:27:21.229 --> 00:27:22.390
in the book, but a lot of what I

537
00:27:22.390 --> 00:27:25.829
was actually worried about in writing the book was

538
00:27:25.829 --> 00:27:28.550
the way people talk to each other, um, when

539
00:27:28.550 --> 00:27:32.750
they have political conversations and specifically. Uh, I, you

540
00:27:32.750 --> 00:27:39.430
know, it's like family, um, conversations, watching people assert

541
00:27:39.430 --> 00:27:44.930
with what struck me as, uh, massive overconfidence and

542
00:27:44.930 --> 00:27:49.109
the various things that they were saying, um. So,

543
00:27:49.160 --> 00:27:54.040
the political skepticism is meant to, uh, I guess

544
00:27:54.040 --> 00:27:57.479
kind of undermine that mode of political interaction, instead

545
00:27:57.479 --> 00:28:01.069
of just saying, you know, whatever about gun control,

546
00:28:01.199 --> 00:28:03.479
um, you know, if we all owned pistols, the

547
00:28:03.479 --> 00:28:05.119
world would be a safer place, right? I mean,

548
00:28:05.160 --> 00:28:06.989
I could talk like that, or instead I could

549
00:28:06.989 --> 00:28:11.369
say. Well, I don't know. I mean, you kind

550
00:28:11.369 --> 00:28:13.560
of try to give some consideration that maybe support

551
00:28:13.560 --> 00:28:15.239
my view and then say that, I don't know,

552
00:28:15.280 --> 00:28:17.239
that sounds kind of reasonable to me or something

553
00:28:17.239 --> 00:28:18.920
like that. But it's kind of like really hedgy

554
00:28:18.920 --> 00:28:21.050
kind of thing where what I'm doing is conveying

555
00:28:21.050 --> 00:28:24.280
to my interlocutor, that I'm really just trying to

556
00:28:24.280 --> 00:28:25.920
do my best in the world, and this is

557
00:28:25.920 --> 00:28:28.040
kind of just an educated guess. And I, I

558
00:28:28.040 --> 00:28:30.160
don't really know whether I'm right, but this is

559
00:28:30.160 --> 00:28:32.520
how things seem to me. Um, THAT mode of

560
00:28:32.520 --> 00:28:37.250
political engagement is what I want to follow from.

561
00:28:37.780 --> 00:28:41.140
Political skepticism rather than just like, oh, we can't

562
00:28:41.140 --> 00:28:43.099
act, we can't do anything, just we have to

563
00:28:43.099 --> 00:28:46.939
just, you know, ignore politics entirely. Um IT it

564
00:28:47.099 --> 00:28:48.530
that was kind of rambling. Did that make sense?

565
00:28:49.119 --> 00:28:51.300
Uh YES, I mean, but one of the reasons

566
00:28:51.300 --> 00:28:53.459
I asked you that is because it came to

567
00:28:53.459 --> 00:28:58.569
my mind that sometimes people argue on the basis

568
00:28:58.569 --> 00:29:03.819
of, as we know, most people not having much

569
00:29:03.819 --> 00:29:08.219
political knowledge and that possibly being a problem when

570
00:29:08.219 --> 00:29:12.579
it comes to their political participation in voting, for

571
00:29:12.579 --> 00:29:17.333
example. They, they pick up on that and try

572
00:29:17.333 --> 00:29:22.282
to make a case on, on those rounds of

573
00:29:22.692 --> 00:29:27.932
some sort of political epistocracy where they defend that,

574
00:29:28.172 --> 00:29:32.493
uh, only people with a certain level of political

575
00:29:32.493 --> 00:29:37.052
knowledge should be, should be allowed to vote or

576
00:29:37.052 --> 00:29:40.796
part. Participate politically in different ways and of course,

577
00:29:41.005 --> 00:29:45.885
many times it's also not very, very clear what

578
00:29:45.885 --> 00:29:49.985
they mean by having a certain level or enough

579
00:29:49.985 --> 00:29:54.836
political knowledge because, I mean, what kinds of things

580
00:29:54.836 --> 00:29:58.485
exactly should people know so, I mean, you know

581
00:29:58.485 --> 00:30:01.595
that, do, what do you think about that?

582
00:30:02.489 --> 00:30:06.209
Yes, that's a, that's a good question. Um, This

583
00:30:07.109 --> 00:30:09.829
one, on a, on a really kind of straightforward

584
00:30:09.829 --> 00:30:12.540
reading of the book, you might think, oh well,

585
00:30:12.750 --> 00:30:15.540
uh, this book is going to kind of count

586
00:30:15.540 --> 00:30:19.890
against um any kind of argument for epistocracy because

587
00:30:20.609 --> 00:30:23.540
Well, you know, epistocracy says, look, you should let

588
00:30:23.540 --> 00:30:25.810
the people with political knowledge sort of like run

589
00:30:25.810 --> 00:30:27.890
the country and then keep them who don't have

590
00:30:27.890 --> 00:30:30.599
any political knowledge uh out of politics. But then,

591
00:30:30.609 --> 00:30:32.250
of course, if none of us have any political

592
00:30:32.250 --> 00:30:34.750
knowledge, well then, you know, we can't do it

593
00:30:34.750 --> 00:30:38.569
that way. Um, IN fact, like I think the,

594
00:30:38.609 --> 00:30:40.290
the main lesson in the book is it would,

595
00:30:40.530 --> 00:30:43.329
at the, at most, if you're an uh if

596
00:30:43.329 --> 00:30:46.530
you like epistocracy, it's, it's, um, just going to

597
00:30:46.530 --> 00:30:48.290
recommend that you sort of change your description of

598
00:30:48.290 --> 00:30:51.819
what the view says. So instead of the distinction

599
00:30:51.819 --> 00:30:54.810
between, uh, instead of the distinction being between people

600
00:30:54.810 --> 00:30:57.579
who have uh political knowledge and don't have political

601
00:30:57.579 --> 00:31:00.170
knowledge, you'd want to draw a distinction between people

602
00:31:00.170 --> 00:31:03.770
who have, um, who can make sufficiently educated guesses

603
00:31:03.770 --> 00:31:07.050
and people who cannot make sufficiently educated guesses. So

604
00:31:07.050 --> 00:31:08.969
it's still going to be the case that the

605
00:31:08.969 --> 00:31:12.569
most politically informed people, so the people who've really

606
00:31:12.569 --> 00:31:14.989
worked hard to try to get a grip on,

607
00:31:15.109 --> 00:31:17.410
you know, whatever, whatever the issue is, you know,

608
00:31:17.449 --> 00:31:20.869
is raising the minimum wage. Going to increase poverty

609
00:31:20.869 --> 00:31:22.510
or something like that. Some people have worked really

610
00:31:22.510 --> 00:31:25.670
hard on trying to answer that question. Um, THOSE

611
00:31:25.670 --> 00:31:27.829
people are going to be able to make much

612
00:31:27.829 --> 00:31:31.479
more educated guesses than me, for example, because I

613
00:31:31.479 --> 00:31:33.449
personally don't know a whole lot about that, right?

614
00:31:33.459 --> 00:31:37.290
And so, uh, if you like epistocracy, You might

615
00:31:37.290 --> 00:31:40.050
just basically cash your view out in terms of

616
00:31:40.380 --> 00:31:42.459
giving control to the people who are in the

617
00:31:42.459 --> 00:31:45.619
best position to make educated guesses, um, rather than

618
00:31:45.619 --> 00:31:48.530
giving control to the people who, who know. Um,

619
00:31:48.699 --> 00:31:50.380
SO I think the, I think the book kind

620
00:31:50.380 --> 00:31:54.510
of cross-cuts the debate between Advocates of epistocracy and,

621
00:31:54.560 --> 00:31:58.079
and, and their opponents, um, if, if that's kind

622
00:31:58.079 --> 00:31:59.349
of what you're thinking about.

623
00:32:00.040 --> 00:32:02.270
Yeah, yeah, yeah, but, but I, I mean then

624
00:32:02.680 --> 00:32:05.839
you, you are not making a case or at

625
00:32:05.839 --> 00:32:10.319
least you don't use the information and arguments you

626
00:32:10.319 --> 00:32:13.609
present in the book to make a case for

627
00:32:13.920 --> 00:32:16.439
any sort of epistocracy or do you?

628
00:32:17.500 --> 00:32:19.439
Yeah, right. Exactly. Yeah. So I do not make

629
00:32:19.439 --> 00:32:21.910
a case for or against epistocracy in the book.

630
00:32:22.040 --> 00:32:24.439
Um, I think, I think the book is basically

631
00:32:24.439 --> 00:32:26.390
neutral with respect to that debate.

632
00:32:26.760 --> 00:32:31.770
OK. Uh, OK, so let's get into the Yeah,

633
00:32:32.989 --> 00:32:36.790
well, maybe different people have different interpretations. I don't

634
00:32:36.790 --> 00:32:37.300
know.

635
00:32:37.670 --> 00:32:38.930
Yeah, exactly.

636
00:32:39.869 --> 00:32:43.229
So, uh, let's get into the central topic of

637
00:32:43.229 --> 00:32:46.270
your book and even the what the title of

638
00:32:46.270 --> 00:32:50.790
the book refers to. What is political humility then?

639
00:32:51.930 --> 00:32:57.500
Yeah, good. So, um, So I, I don't give

640
00:32:57.500 --> 00:32:59.479
anything like a definition of political humility in the

641
00:32:59.479 --> 00:33:00.920
book, and I don't really even want to give

642
00:33:00.920 --> 00:33:03.040
a definition of political humility. I have to be

643
00:33:03.040 --> 00:33:06.800
perfectly honest with you, uh, I think like on

644
00:33:06.800 --> 00:33:08.880
a, on what I regard as kind of the

645
00:33:08.880 --> 00:33:12.839
best definition of humility. Uh, WHAT I describe in

646
00:33:12.839 --> 00:33:17.560
the book isn't really humility. Um, THE title basically

647
00:33:17.560 --> 00:33:20.800
was picked because, uh, I think it kind of

648
00:33:20.800 --> 00:33:25.959
resonates with, um, your average person outside of philosophy.

649
00:33:26.040 --> 00:33:28.000
So the book was intended to be read by

650
00:33:28.000 --> 00:33:30.680
a very general audience, people completely outside of academia,

651
00:33:30.719 --> 00:33:34.469
much less, you know, uh, outside of philosophy. Um,

652
00:33:34.599 --> 00:33:38.959
AND I think the title will resonate with that

653
00:33:38.959 --> 00:33:41.589
kind of crowd. So basically what I mean by

654
00:33:41.589 --> 00:33:45.510
political humility is, is just a mode of engaging

655
00:33:45.510 --> 00:33:49.750
with politics where you acknowledge that you don't know

656
00:33:50.089 --> 00:33:53.469
most of the answers and you, uh, you acknowledge

657
00:33:53.469 --> 00:33:55.500
this to yourself, you keep it in mind yourself,

658
00:33:55.790 --> 00:33:59.109
and you also make this manifest to people that

659
00:33:59.109 --> 00:34:02.349
you're interacting with uh about politics. So when you

660
00:34:02.349 --> 00:34:04.930
speak to people, You don't present yourself as if

661
00:34:04.930 --> 00:34:06.810
you know things, you know well, you don't know.

662
00:34:06.890 --> 00:34:08.969
Instead, you make it very clear that you don't

663
00:34:08.969 --> 00:34:12.129
know the answers, um, that sort of thing, right?

664
00:34:12.208 --> 00:34:14.090
So that's kind of, that's, that's what, what, uh,

665
00:34:14.168 --> 00:34:17.010
that's what I'm calling political humility. But as I

666
00:34:17.010 --> 00:34:19.688
said, I don't think that that's actually what humility

667
00:34:19.688 --> 00:34:22.899
consists in. I think it's just a, uh, Good

668
00:34:22.899 --> 00:34:25.409
enough label for marketing purposes, basically.

669
00:34:28.050 --> 00:34:33.300
So how does political humility translate into behaviors like

670
00:34:33.300 --> 00:34:34.050
voting?

671
00:34:36.079 --> 00:34:38.529
Yeah, so, um, the main way it would translate

672
00:34:38.529 --> 00:34:43.559
into behaviors like voting is you regard yourself as

673
00:34:43.559 --> 00:34:45.708
making an educated guess when you walk into the

674
00:34:45.708 --> 00:34:49.518
voting booth. Um, SO that's the humility piece, um.

675
00:34:50.830 --> 00:34:52.949
There's a part of the book that goes under

676
00:34:52.949 --> 00:34:55.350
the label political humility that I think really has

677
00:34:55.350 --> 00:34:59.219
nothing to do with political humility. Um, BUT so

678
00:34:59.219 --> 00:35:02.350
on my view, the, and this goes back to

679
00:35:02.350 --> 00:35:06.050
our earlier discussion of the labels, liberal and conservative.

680
00:35:06.500 --> 00:35:11.270
Um, I don't think the democratic platform, uh, you

681
00:35:11.270 --> 00:35:14.459
know, sort of like forms this kind of nice

682
00:35:14.459 --> 00:35:16.989
system of liberal views that just sort of like

683
00:35:16.989 --> 00:35:19.459
fit together in some sort of like systematic way.

684
00:35:19.739 --> 00:35:23.020
And I don't think the Republican platform has this

685
00:35:23.020 --> 00:35:25.209
kind of collection of views that fit together in

686
00:35:25.209 --> 00:35:28.290
any systematic way either. Instead, so far as I

687
00:35:28.290 --> 00:35:30.689
can tell, the views, the, you know, the kind

688
00:35:30.689 --> 00:35:34.840
of the planks, the different parties' platforms are just

689
00:35:35.610 --> 00:35:38.489
totally, I mean, I think honestly kind of bizarre

690
00:35:38.489 --> 00:35:43.709
collections of values and ideas um and policies. Maybe

691
00:35:43.709 --> 00:35:45.550
they're not bizarre from a historical point of view.

692
00:35:45.629 --> 00:35:47.229
I think you can kind of make sense of

693
00:35:47.229 --> 00:35:48.899
all these things from a historical point of view,

694
00:35:49.030 --> 00:35:50.909
but from a strictly sort of like intellectual point

695
00:35:50.909 --> 00:35:54.020
of view. Um, I don't think there's anything, you

696
00:35:54.020 --> 00:35:55.820
know, so like if you look like a system

697
00:35:55.820 --> 00:35:57.820
in philosophy or something like that, and you're like,

698
00:35:57.860 --> 00:35:59.179
OK, what I want to do is I want

699
00:35:59.179 --> 00:36:00.810
to kind of drill down to like the core

700
00:36:00.810 --> 00:36:02.179
idea of the system. And once you get the

701
00:36:02.179 --> 00:36:03.979
core idea of the system, then you can sort

702
00:36:03.979 --> 00:36:06.699
of just work out logically what the rest of

703
00:36:06.699 --> 00:36:08.300
the system is going to have to say. If

704
00:36:08.300 --> 00:36:10.219
it's the case that A, well, then it's gonna

705
00:36:10.219 --> 00:36:12.020
have to be the B, and then therefore the

706
00:36:12.020 --> 00:36:14.219
C, and then therefore D. And then there's a

707
00:36:14.219 --> 00:36:16.699
sort of like a system that kind of hangs

708
00:36:16.699 --> 00:36:19.459
together in this like, you know, coherent or logical

709
00:36:19.459 --> 00:36:22.449
way where You can't just like swap things in

710
00:36:22.449 --> 00:36:26.030
and out without messing up the system. Um, And,

711
00:36:26.149 --> 00:36:28.629
and, in contrast to that, uh, so the metaphor

712
00:36:28.629 --> 00:36:30.350
I use in the book is a salad, a

713
00:36:30.350 --> 00:36:32.189
salad, you know, it's got all these ingredients. It's

714
00:36:32.189 --> 00:36:34.719
like you can have, you know, iceberg lettuce in

715
00:36:34.719 --> 00:36:37.689
there. But you could have used, you know, spinach

716
00:36:37.689 --> 00:36:40.090
instead, or you could have iceberg lettuce and spinach.

717
00:36:40.189 --> 00:36:42.459
You could have carrots in there, or you could

718
00:36:42.459 --> 00:36:44.340
have used cucumber instead, or you could have carrots

719
00:36:44.340 --> 00:36:46.489
and cucumber, you know, like when you look at

720
00:36:46.489 --> 00:36:48.979
the ingredients of the salad, basically, any of those

721
00:36:48.979 --> 00:36:51.100
ingredients, or look at the ingredients of two different

722
00:36:51.100 --> 00:36:54.100
salads. Basically, any ingredient in one salad could be

723
00:36:54.100 --> 00:36:56.179
put in the other salad and vice versa and

724
00:36:56.179 --> 00:36:58.100
they could just be kind of interchanged almost at

725
00:36:58.100 --> 00:37:02.870
will. Um, I think the Republican and Democratic platforms.

726
00:37:03.469 --> 00:37:06.149
RESEMBLE salads much more than they resemble the sort

727
00:37:06.149 --> 00:37:09.949
of systems that philosophers kind of aim at. Um.

728
00:37:10.729 --> 00:37:14.919
Um, AND a consequence of this is, um, it's

729
00:37:14.919 --> 00:37:18.310
like if you're looking at a philosophical system. Anywhere

730
00:37:18.310 --> 00:37:20.149
you kind of enter that system because of the

731
00:37:20.149 --> 00:37:22.350
way the idea is hanging together, if you like,

732
00:37:22.389 --> 00:37:25.629
OK, this thing right here is definitely correct. Well,

733
00:37:25.719 --> 00:37:28.500
if this thing is definitely correct, that's going to,

734
00:37:28.590 --> 00:37:30.270
you know, if, if it doesn't entail, it's at

735
00:37:30.270 --> 00:37:32.350
least going to strongly suggest that the other parts

736
00:37:32.350 --> 00:37:34.120
of the system are correct too, because of their

737
00:37:34.120 --> 00:37:37.939
logical relationships with one another. Um, AND if it

738
00:37:37.939 --> 00:37:41.020
were the case that the Republican and Democratic platforms

739
00:37:41.020 --> 00:37:43.889
were systems like that, you could pick your pet

740
00:37:43.889 --> 00:37:46.469
issue, you know, whatever abortion or whatever it happens

741
00:37:46.469 --> 00:37:48.739
to be, and then just turn the crank on

742
00:37:48.739 --> 00:37:50.419
that issue and get the whole rest of the

743
00:37:50.419 --> 00:37:53.300
platform, right? So if you think abortion is morally

744
00:37:53.300 --> 00:37:55.580
permissible, you know, you could, you could say, well,

745
00:37:55.620 --> 00:37:57.899
abortion is morally permissible, and then you could just

746
00:37:57.899 --> 00:38:02.229
work out logically from that, you know. Whatever policy

747
00:38:02.229 --> 00:38:05.469
for environmental regulation and whatever policy for border control

748
00:38:05.469 --> 00:38:08.360
and whatever policy for gun control and whatever policy

749
00:38:08.360 --> 00:38:10.479
for the minimum wage, and this is all just

750
00:38:10.479 --> 00:38:13.040
be this kind of armchair exercise based entirely on

751
00:38:13.040 --> 00:38:16.699
your sensibilities about abortion. Um, BUT it seems perfectly

752
00:38:16.699 --> 00:38:19.290
obvious to me that the Democratic platform is not

753
00:38:19.290 --> 00:38:21.889
systematic in that way at all. And the same

754
00:38:21.889 --> 00:38:23.489
thing with the Republican platform, right? So if you

755
00:38:23.489 --> 00:38:25.530
think abortion is morally impermissible, it's not as if

756
00:38:25.530 --> 00:38:27.409
you can get from, from there to sort of

757
00:38:27.409 --> 00:38:30.989
like You know, your, your favorite Republican view about

758
00:38:30.989 --> 00:38:36.510
taxation or whatever it is. Um, SO this is

759
00:38:36.510 --> 00:38:41.179
under the chapter of political humility, but um, Um,

760
00:38:41.320 --> 00:38:46.139
the, the point of this is, um, people should

761
00:38:46.139 --> 00:38:50.959
recognize. That they, even if they do can make

762
00:38:50.959 --> 00:38:54.879
an educated guess about one question, you know, and

763
00:38:54.879 --> 00:38:56.570
kind of the political landscape or something like that

764
00:38:56.570 --> 00:38:59.489
or maybe a handful of questions. They should recognize

765
00:38:59.489 --> 00:39:03.239
that they're still terribly positioned to answer other questions

766
00:39:03.239 --> 00:39:05.739
in the political domain, um, unless they've, of course,

767
00:39:06.360 --> 00:39:09.229
You know, looked into those questions too, right? So

768
00:39:09.229 --> 00:39:11.229
my ability to make an educated guess about, you

769
00:39:11.229 --> 00:39:13.959
know, whatever the minimum wage, for example, there's no

770
00:39:13.959 --> 00:39:15.939
reason why that should give me any confidence at

771
00:39:15.939 --> 00:39:18.709
all that I have any idea what we should

772
00:39:18.709 --> 00:39:21.889
do at the border with respect to abortion. Or

773
00:39:21.889 --> 00:39:25.399
whatever else. Um, SO that's the idea there. Right.

774
00:39:25.850 --> 00:39:29.489
But if we have as little political knowledge as

775
00:39:29.489 --> 00:39:32.770
you claim, then why do we tend to be

776
00:39:32.770 --> 00:39:37.010
so confident in our political views and decisions?

777
00:39:38.409 --> 00:39:41.280
Yeah, that's a, so that's a a big and

778
00:39:41.280 --> 00:39:46.500
complicated question. Um. There's a, there's a, a pretty

779
00:39:46.500 --> 00:39:50.219
big empirical literature, um, on this. Part of the

780
00:39:50.219 --> 00:39:53.330
empirical literature suggests that a lot of our political

781
00:39:53.330 --> 00:39:59.169
engagement, uh, resembles fanfare more than it resembles, um,

782
00:39:59.699 --> 00:40:03.449
sort of like dispassionate, uh, concern for the truth.

783
00:40:03.979 --> 00:40:06.530
Um, SO, you know, you, you, whatever you go

784
00:40:06.530 --> 00:40:11.040
to. In American football game or, you know, uh,

785
00:40:11.139 --> 00:40:13.260
European football, what we call, you know, a soccer

786
00:40:13.260 --> 00:40:16.379
match or whatever it happens to be, and you

787
00:40:16.379 --> 00:40:18.979
watch just the kind of passion of the fans

788
00:40:18.979 --> 00:40:22.179
and the crowd and what animates them. A lot

789
00:40:22.179 --> 00:40:26.429
of this empirical literature suggests that our interaction with

790
00:40:26.429 --> 00:40:31.050
politics is passionate and dogmatic and belligerent, etc. ETC.

791
00:40:31.179 --> 00:40:33.699
FOR the exact same reasons as you see these

792
00:40:33.699 --> 00:40:36.620
sort of behaviors, you know, in a sports stadium.

793
00:40:37.020 --> 00:40:39.800
Um, I think that's definitely, uh, part of the

794
00:40:39.800 --> 00:40:42.679
explanation. Um, ONE thing I talk about in the

795
00:40:42.679 --> 00:40:44.560
book, which I think is also part of the

796
00:40:44.560 --> 00:40:47.629
explanation, not by any means like the full explanation,

797
00:40:47.659 --> 00:40:49.409
and I wouldn't even suggest that it's the most

798
00:40:49.409 --> 00:40:52.199
important part of the explanation. It's something that I

799
00:40:52.199 --> 00:40:57.560
call, um, testimonial feedback loops. And essentially the idea

800
00:40:57.560 --> 00:41:01.409
here is, um, Well, I should back up a

801
00:41:01.409 --> 00:41:02.729
little bit before I even introduce the idea of

802
00:41:02.729 --> 00:41:04.770
a testimonial feedback loop. So one of the things

803
00:41:04.770 --> 00:41:08.370
the empirical literature is suggesting is that people are

804
00:41:08.370 --> 00:41:12.810
kind of sorting themselves into politically homogeneous communities. They're

805
00:41:12.810 --> 00:41:14.500
doing this in all variety of ways at the

806
00:41:14.500 --> 00:41:19.449
extreme, and people are moving to places where they

807
00:41:19.449 --> 00:41:21.570
don't have to have neighbors that don't share their

808
00:41:21.570 --> 00:41:24.290
politics. So people will move across the country. I

809
00:41:24.290 --> 00:41:26.729
mean, you know, you find stories in the news

810
00:41:26.729 --> 00:41:29.969
about people moving from California to Tennessee because they

811
00:41:29.969 --> 00:41:33.330
want Republican neighbors, um, and so on and so

812
00:41:33.330 --> 00:41:35.090
forth. So there's that sort of like extreme sort

813
00:41:35.090 --> 00:41:37.889
of thing. But then there's the kind of less

814
00:41:37.889 --> 00:41:40.770
extreme thing where it just turns out that like

815
00:41:40.770 --> 00:41:45.370
you don't really have any substantive relationships with people

816
00:41:45.370 --> 00:41:47.459
who don't share your politics. So maybe your neighbors

817
00:41:47.459 --> 00:41:50.580
don't share your politics and maybe you even like

818
00:41:50.899 --> 00:41:52.689
You know, go to church with somebody who doesn't

819
00:41:52.689 --> 00:41:54.800
share your politics, but when it boils down to

820
00:41:54.800 --> 00:41:59.280
like who you actually really share life with, everybody

821
00:41:59.280 --> 00:42:03.280
in your close circles shares your politics. Um, AND

822
00:42:03.280 --> 00:42:06.510
when that sort of thing happens, you, you tend

823
00:42:06.510 --> 00:42:10.429
to wind up in a situation where you hear

824
00:42:10.870 --> 00:42:13.080
information from people who don't share your politics. It's

825
00:42:13.080 --> 00:42:15.560
just that you never hear information from people that

826
00:42:15.560 --> 00:42:19.489
you trust. Who don't share your politics. And so

827
00:42:19.489 --> 00:42:23.129
then you wind up in these little homogeneous communities

828
00:42:23.129 --> 00:42:26.239
they get called echo chambers or filter bubbles or

829
00:42:26.239 --> 00:42:28.969
epistemic bubbles or different things like that and and

830
00:42:28.969 --> 00:42:32.830
in these communities. Basically, all the information you're getting

831
00:42:32.830 --> 00:42:35.679
from people you trust is information from people already

832
00:42:35.679 --> 00:42:39.120
share your political views. Um, AND then one part

833
00:42:39.120 --> 00:42:41.280
of the book basically talks about something that I

834
00:42:41.280 --> 00:42:44.320
call testimonial feedback loop, which is something that's likely

835
00:42:44.320 --> 00:42:49.330
to happen inside these communities. Where essentially, You tell

836
00:42:49.330 --> 00:42:52.010
somebody something and that information kind of gets passed

837
00:42:52.010 --> 00:42:54.570
around, and then that information gets passed back to

838
00:42:54.570 --> 00:42:57.969
you and you don't realize that essentially you're just

839
00:42:57.969 --> 00:43:01.639
hearing your own report echoed back to you. Um,

840
00:43:01.649 --> 00:43:06.439
AND so, Our sense of how well supported um

841
00:43:06.439 --> 00:43:09.989
a particular claim is, we often are sensitive to

842
00:43:09.989 --> 00:43:11.790
how often we've heard the claim, the bed be

843
00:43:11.790 --> 00:43:13.469
something like, well, I've heard that a million times,

844
00:43:13.600 --> 00:43:15.399
surely I wouldn't have heard that so many times

845
00:43:15.399 --> 00:43:19.649
if it weren't true. Um, AND in a case

846
00:43:19.649 --> 00:43:22.610
where each telling kind of gets vetted and where

847
00:43:22.610 --> 00:43:25.129
there, the all the tellings are sort of like

848
00:43:25.129 --> 00:43:28.959
supported by multiple inputs. That really is a good

849
00:43:28.959 --> 00:43:30.889
reason. The hearing it many times really is a

850
00:43:30.889 --> 00:43:32.949
good reason to be highly confident that it's true.

851
00:43:33.479 --> 00:43:35.879
Um, BUT if hearing it, like, you know, I

852
00:43:35.879 --> 00:43:39.949
mean, imagine like, um, You know, I get an

853
00:43:39.949 --> 00:43:42.449
idea and I write it down in my journal,

854
00:43:42.899 --> 00:43:45.060
and then I read the idea to myself every

855
00:43:45.060 --> 00:43:48.250
morning. And so, oh, you know, after a year,

856
00:43:48.340 --> 00:43:52.340
I've just heard that idea 365 times. Why? Cause

857
00:43:52.340 --> 00:43:53.939
I just read the very thing that I wrote

858
00:43:53.939 --> 00:43:58.649
down to myself, 365 times. If my confidence gets

859
00:43:58.649 --> 00:44:00.929
really, really high as a consequence of hearing that

860
00:44:00.929 --> 00:44:03.889
idea that many times in that way, something's obviously

861
00:44:03.889 --> 00:44:06.689
gone badly wrong. I shouldn't be any more confident

862
00:44:06.689 --> 00:44:09.820
after hearing myself repeat my own thing 365 times

863
00:44:09.820 --> 00:44:11.330
than I was after the very first time I

864
00:44:11.330 --> 00:44:15.149
wrote it down. Um, BUT if I'm in a

865
00:44:15.149 --> 00:44:17.149
community where I keep hearing the same thing repeated

866
00:44:17.149 --> 00:44:18.750
over and over again, and it turns out that

867
00:44:18.750 --> 00:44:20.229
the reason I keep hearing it over and over

868
00:44:20.229 --> 00:44:22.629
again is just cause, you know, I said it

869
00:44:22.629 --> 00:44:24.350
and then it eventually got repeated back to me,

870
00:44:24.389 --> 00:44:25.909
and then it eventually got repeated back to me

871
00:44:25.909 --> 00:44:28.350
again and again and again and again. And there's

872
00:44:28.350 --> 00:44:30.389
sort of like no outside or independent check on

873
00:44:30.389 --> 00:44:36.139
this information, just Constantly getting cycled around, then I'm

874
00:44:36.139 --> 00:44:40.320
going to probably misunderstand my evidential situation and I'm

875
00:44:40.320 --> 00:44:43.419
probably going to become much more confident than the

876
00:44:43.419 --> 00:44:47.219
actual evidence, uh, warrants in that sort of case.

877
00:44:47.340 --> 00:44:49.860
Um, AND then, epistemologists would, would quibble with the

878
00:44:49.860 --> 00:44:52.639
way I just described that, right? Um, BUT, but

879
00:44:52.639 --> 00:44:56.100
that's the basic idea. I, these, these, um, because

880
00:44:56.100 --> 00:44:59.699
of testimonial feedback loops, loops where information just gets

881
00:44:59.699 --> 00:45:02.379
repeated around and around and around like that. We

882
00:45:02.379 --> 00:45:04.790
could have a very distorted view of how of

883
00:45:04.790 --> 00:45:09.320
how well confirmed something is. And that's gonna contribute

884
00:45:09.320 --> 00:45:12.889
to our upper confidence. Yeah. Sorry to interrupt.

885
00:45:13.409 --> 00:45:15.409
Uh, NO, no, no, no worries. Uh, I was

886
00:45:15.409 --> 00:45:18.209
just going to ask you, is there anything that

887
00:45:18.209 --> 00:45:21.760
people can do to avoid that kind of situation?

888
00:45:21.860 --> 00:45:26.199
Of course, I just not moving to a place

889
00:45:26.649 --> 00:45:29.850
with the purpose of being, uh, of living in

890
00:45:29.850 --> 00:45:34.199
a context where all people or almost everyone around

891
00:45:34.199 --> 00:45:39.165
you is of the same political ideology. Let's say,

892
00:45:39.225 --> 00:45:43.854
is already better, not doing that is better than

893
00:45:43.854 --> 00:45:49.254
doing it. But, of course, people, uh, usually cannot

894
00:45:49.254 --> 00:45:54.205
really choose wherever they want to live. I mean,

895
00:45:54.294 --> 00:45:58.854
they are at least somewhat limited in terms of

896
00:45:58.854 --> 00:46:02.794
where they can live. So, I mean, what can

897
00:46:02.794 --> 00:46:07.489
they do about that? Because it might just Accidentally

898
00:46:07.489 --> 00:46:10.370
be the case that they live in a community

899
00:46:10.370 --> 00:46:15.030
where most people are of a particular political ideology

900
00:46:15.030 --> 00:46:16.570
and not another, so.

901
00:46:17.320 --> 00:46:20.209
Yeah, exactly. Yeah. For the vast majority of people

902
00:46:20.689 --> 00:46:22.840
moving to correct the situation is not going to

903
00:46:22.840 --> 00:46:25.719
be realistic. And even, you know, even kind of

904
00:46:25.719 --> 00:46:28.000
simpler things you might do are at the end

905
00:46:28.000 --> 00:46:30.000
of the day, not going to be especially realistic.

906
00:46:30.080 --> 00:46:33.159
I mean, so, so imagine like, you know, um,

907
00:46:33.169 --> 00:46:35.760
I think like, well, jeez, all of my friends

908
00:46:35.760 --> 00:46:38.520
are Republicans, except for this one person I know

909
00:46:38.520 --> 00:46:40.919
who's a Democrat, and I know this person's a

910
00:46:40.919 --> 00:46:42.969
member of a book club. And most of the

911
00:46:42.969 --> 00:46:44.570
people in the book club are Democrats, and I

912
00:46:44.570 --> 00:46:46.169
know what they read, and I bet I'll get

913
00:46:46.169 --> 00:46:48.010
this kind of nice exposure to that way of

914
00:46:48.010 --> 00:46:49.649
thinking if I joined this book club. So maybe

915
00:46:49.649 --> 00:46:51.770
I'll just join this book club. And then, you

916
00:46:51.770 --> 00:46:55.250
know, whatever, that's like. Uh, 2 hours a week

917
00:46:55.250 --> 00:46:57.520
actually driving to the book club and hanging out

918
00:46:57.520 --> 00:46:59.489
and coming home and then maybe another 2 or

919
00:46:59.489 --> 00:47:01.320
3 hours a week, like doing the readings like,

920
00:47:01.330 --> 00:47:04.330
oh, that's not a big deal. But it kind

921
00:47:04.330 --> 00:47:06.209
of is a big deal. I mean, for most

922
00:47:06.209 --> 00:47:08.810
adults, like we don't have 5 or 6 hours

923
00:47:08.810 --> 00:47:12.239
a week. To give to something like that in

924
00:47:12.239 --> 00:47:14.000
addition to what we're already doing, right? So we

925
00:47:14.000 --> 00:47:16.360
could only do it if we probably took something

926
00:47:16.360 --> 00:47:18.120
of value out of our lives in order to

927
00:47:18.120 --> 00:47:20.570
kind of make space for that. Um, AND you

928
00:47:20.570 --> 00:47:21.850
think of like sort of like other kind of

929
00:47:21.850 --> 00:47:23.729
analogous things you might do. I think when the

930
00:47:23.729 --> 00:47:26.489
rubber meets the road, uh, for an awful lot

931
00:47:26.489 --> 00:47:29.810
of people given just the normal responsibilities of their

932
00:47:29.810 --> 00:47:33.169
adult lives, um, changing their lives in this way

933
00:47:33.169 --> 00:47:36.709
is really not going to be very realistic. Um,

934
00:47:36.919 --> 00:47:38.760
I think it'd be, you know, it'd be awesome

935
00:47:38.760 --> 00:47:40.399
if we could do it and the people who,

936
00:47:40.560 --> 00:47:42.479
and I know some people who do stuff like

937
00:47:42.479 --> 00:47:44.360
this, I think it's really cool and valuable that

938
00:47:44.360 --> 00:47:46.439
they do it. Um, BUT for a lot of

939
00:47:46.439 --> 00:47:48.199
people who just simply can't afford to do that

940
00:47:48.199 --> 00:47:51.229
thing, I think their best bet is going to

941
00:47:51.229 --> 00:47:56.209
be basically Trying to be really mindful of, so,

942
00:47:56.250 --> 00:47:57.370
you know, for example, if you live in a

943
00:47:57.370 --> 00:47:59.570
homogeneous community, you know you live in a homogeneous

944
00:47:59.570 --> 00:48:02.340
community. I think it's probably a good idea to

945
00:48:02.340 --> 00:48:05.580
just consistently remind yourself like, OK, given that this

946
00:48:05.580 --> 00:48:07.500
is the community in which I live, and given

947
00:48:07.500 --> 00:48:10.979
that basically none of us have any inputs from

948
00:48:10.979 --> 00:48:13.379
another point of view, uh, at least from people

949
00:48:13.379 --> 00:48:14.820
we trust, right? We all hear what, you know,

950
00:48:14.899 --> 00:48:18.379
whatever, we all knew what Rush Limbaugh was saying

951
00:48:18.379 --> 00:48:19.860
back in the day, right? But it's not like

952
00:48:19.860 --> 00:48:21.860
we had any trust in him at all. And

953
00:48:21.860 --> 00:48:23.580
so that sort of like didn't affect our way

954
00:48:23.580 --> 00:48:27.330
of thinking about things. Um. So you know, if

955
00:48:27.330 --> 00:48:28.729
it's the case that we know that we live

956
00:48:28.729 --> 00:48:31.370
in these homogeneous communities where we're not hearing any

957
00:48:31.370 --> 00:48:34.489
information from opposing points of view from people that

958
00:48:34.489 --> 00:48:38.080
we trust. Then just sort of like being mindful

959
00:48:38.080 --> 00:48:41.800
of the potential for us being pretty misled about

960
00:48:41.800 --> 00:48:45.229
what the kind of broader evidential situation looks like

961
00:48:45.560 --> 00:48:46.699
would be a way for us to, I think,

962
00:48:46.719 --> 00:48:49.320
kind of, just kind of keep ourselves in check

963
00:48:49.320 --> 00:48:51.260
a little bit. It's like, OK, you know, I'm,

964
00:48:51.399 --> 00:48:53.159
I, I feel pretty confident and I mean it's

965
00:48:53.159 --> 00:48:54.879
an example I use in the book is, is

966
00:48:54.879 --> 00:48:58.979
like the communities in which I run. Um, I

967
00:48:58.979 --> 00:49:03.530
probably have never once heard anybody say anything positive

968
00:49:03.530 --> 00:49:07.699
about the border wall. Um. And, you know, as

969
00:49:07.699 --> 00:49:11.179
a consequence of that, I, I, as just a

970
00:49:11.179 --> 00:49:13.580
matter of my own psychology, it's very easy for

971
00:49:13.580 --> 00:49:15.899
me to just start interacting with the question whether

972
00:49:15.899 --> 00:49:18.860
or not we should complete this wall. It's just

973
00:49:18.860 --> 00:49:21.409
like the most obvious question in the whole world.

974
00:49:21.629 --> 00:49:23.899
As if it's just like, well, duh, like, obviously

975
00:49:23.899 --> 00:49:25.939
we shouldn't, as if there are no considerations at

976
00:49:25.939 --> 00:49:29.219
all that maybe would suggest, oh, maybe, maybe we

977
00:49:29.219 --> 00:49:31.310
should think about doing this. Um, AND then it

978
00:49:31.310 --> 00:49:34.149
kind of remind myself, oh, OK, right. So this

979
00:49:34.149 --> 00:49:37.550
is my, my psychological response is probably at least

980
00:49:37.550 --> 00:49:40.870
partially a byproduct of my being a member of

981
00:49:40.870 --> 00:49:45.179
this homogeneous community and just hearing the same things

982
00:49:45.629 --> 00:49:47.669
over and over and over again with no outside

983
00:49:47.669 --> 00:49:50.949
input. Um, SO yeah, so there's, there's a sort

984
00:49:50.949 --> 00:49:53.149
of like kind of self-monitoring thing that I think

985
00:49:53.149 --> 00:49:56.590
is probably more realistic for people. Then, you know,

986
00:49:56.879 --> 00:50:01.790
moving to Tennessee or or moving downtown or, or,

987
00:50:01.879 --> 00:50:03.750
you know, joining the book club or whatever.

988
00:50:04.760 --> 00:50:09.040
Yeah. So, uh, I think that this is something

989
00:50:09.040 --> 00:50:11.760
we've already touched on a little bit earlier when

990
00:50:11.760 --> 00:50:16.070
I asked you, um, how we can know or

991
00:50:16.159 --> 00:50:18.729
on what grounds do we know that we have,

992
00:50:18.840 --> 00:50:21.919
we do not have much political knowledge, but I

993
00:50:21.919 --> 00:50:23.719
think that what I'm going to ask you now

994
00:50:23.719 --> 00:50:28.489
is, uh, A different kind of question. So, uh,

995
00:50:28.879 --> 00:50:34.449
what are you arguing exactly from an epistemological perspective

996
00:50:34.879 --> 00:50:38.959
when you say that we have almost no political

997
00:50:38.959 --> 00:50:41.800
knowledge? I mean, on what grounds do you make

998
00:50:41.800 --> 00:50:42.879
that kind of claim?

999
00:50:44.080 --> 00:50:47.379
The, I mean, the kind of crucial idea is

1000
00:50:47.379 --> 00:50:50.949
that knowledge requires that the belief in question is

1001
00:50:50.949 --> 00:50:54.659
a product of a reliable process. Um, AND if

1002
00:50:54.659 --> 00:50:56.979
it's true that the vast majority of our political

1003
00:50:56.979 --> 00:50:59.699
beliefs are not products of reliable processes, then it's

1004
00:50:59.699 --> 00:51:01.500
just going to follow straight away that the vast

1005
00:51:01.500 --> 00:51:03.979
majority of our political beliefs are fall short of

1006
00:51:03.979 --> 00:51:07.689
knowledge. Um, SO that's, that's really the core idea,

1007
00:51:07.820 --> 00:51:12.199
um. Now there's um There's a lot of debate

1008
00:51:12.199 --> 00:51:16.639
about the place of reliability and epistemology, um, but

1009
00:51:16.639 --> 00:51:19.560
almost all of this debate is about the relationship

1010
00:51:19.560 --> 00:51:24.399
between reliability and something called justification. There's very little

1011
00:51:24.399 --> 00:51:26.959
debate about the relationship between reliability and knowledge. Virtually

1012
00:51:26.959 --> 00:51:31.040
all epistemologists agree that knowledge requires that the belief

1013
00:51:31.040 --> 00:51:33.040
in question is the product of a reliable process.

1014
00:51:33.129 --> 00:51:35.919
And so the, so the, that's, that kind of

1015
00:51:35.919 --> 00:51:38.770
core piece of the epistemology is It, I mean,

1016
00:51:38.780 --> 00:51:41.649
it's about as uncontroversial as things get within philosophy.

1017
00:51:41.709 --> 00:51:44.050
In, in philosophy, everything's kind of controversial, right? I

1018
00:51:44.050 --> 00:51:46.689
mean, there's basically no claim at all philosophy that

1019
00:51:46.689 --> 00:51:49.850
literally everybody agrees with. Um, AND so if you

1020
00:51:49.850 --> 00:51:52.370
can find a claim that the vast majority of

1021
00:51:52.370 --> 00:51:54.770
people agree with, that's like by philosophy standards, like

1022
00:51:54.770 --> 00:51:58.090
that's really uncontroversial. So, so the, the kind of

1023
00:51:58.090 --> 00:52:00.889
core epistemological claim in the book is, is about

1024
00:52:00.889 --> 00:52:03.590
as uncontroversial as anything gets in philosophy.

1025
00:52:05.270 --> 00:52:09.510
But are you applying any specific theory of knowledge

1026
00:52:09.510 --> 00:52:10.429
in your book?

1027
00:52:11.469 --> 00:52:13.689
Yeah, so I'm really, I'm really careful about not

1028
00:52:13.689 --> 00:52:17.330
doing that. Um, SO, You know, there are, there

1029
00:52:17.330 --> 00:52:19.709
are all a variety of different theories of knowledge,

1030
00:52:19.889 --> 00:52:22.969
um, but again, the one thing that virtually all

1031
00:52:22.969 --> 00:52:25.449
of the different theories of knowledge agree on is

1032
00:52:25.449 --> 00:52:30.090
that reliability is required for knowledge. And so, almost

1033
00:52:30.090 --> 00:52:33.320
any theory of knowledge that a particular epistemologist likes,

1034
00:52:33.810 --> 00:52:35.790
they could just plug that theory of knowledge into

1035
00:52:35.790 --> 00:52:37.169
the argument of the book and it would work

1036
00:52:37.169 --> 00:52:39.929
fine. Um. So yeah, I tried, I, I, I

1037
00:52:39.929 --> 00:52:41.649
really, well, and I'll be honest with you, part

1038
00:52:41.649 --> 00:52:45.040
of the reason I, I was convinced myself by

1039
00:52:45.040 --> 00:52:47.840
this argument is that the argument didn't depend on

1040
00:52:48.409 --> 00:52:50.290
my own particular theory of knowledge. It's like, OK,

1041
00:52:50.409 --> 00:52:52.209
so maybe I'm wrong about the particular theory of

1042
00:52:52.209 --> 00:52:54.600
knowledge that I happen to like, but Get the

1043
00:52:54.600 --> 00:52:57.820
exact same conclusion from basically all the, the competing

1044
00:52:57.820 --> 00:53:00.580
theories of knowledge too. Um, SO I think, you

1045
00:53:00.580 --> 00:53:01.840
know, it's like, well, OK, so even if I

1046
00:53:01.840 --> 00:53:03.239
happen to be wrong about my own particular theory

1047
00:53:03.239 --> 00:53:05.600
of knowledge, given that basically all of the different

1048
00:53:05.600 --> 00:53:07.469
theories of knowledge are gonna say the same thing,

1049
00:53:08.000 --> 00:53:09.600
the conclusion seems pretty solid.

1050
00:53:11.449 --> 00:53:15.870
OK, so do you have any final remarks about

1051
00:53:15.870 --> 00:53:19.350
the book and what you explored or anything that

1052
00:53:19.350 --> 00:53:22.030
I might have missed with my questions or?

1053
00:53:23.489 --> 00:53:25.830
I don't know, your questions are awesome. Um, LET'S

1054
00:53:25.830 --> 00:53:28.270
see. No, I mean, that's, that's, I guess that's

1055
00:53:28.270 --> 00:53:31.739
basically it. The, um, I, I, so I guess

1056
00:53:31.739 --> 00:53:33.540
I should say 11 thing, so this is the

1057
00:53:33.540 --> 00:53:36.060
very last thing that I say in the book

1058
00:53:36.060 --> 00:53:42.290
that I think is um pretty important. There's this,

1059
00:53:42.489 --> 00:53:44.750
you know, academics can get kind of hung up

1060
00:53:44.750 --> 00:53:48.129
on um how we want to categorize beliefs. So

1061
00:53:48.129 --> 00:53:50.129
we categorize, you know, so I take a, a

1062
00:53:50.129 --> 00:53:54.770
particular political belief. Um, CAN we categorize it as

1063
00:53:54.770 --> 00:53:56.889
an item of knowledge or not? Can we categorize

1064
00:53:56.889 --> 00:53:58.689
it as a justified belief or not? Can we

1065
00:53:58.689 --> 00:54:01.370
categorize it as a rational belief or not? Can

1066
00:54:01.370 --> 00:54:03.290
we, you know, this is, it's super jargon in

1067
00:54:03.290 --> 00:54:05.409
terms. Can we categorize it as a sensitive belief

1068
00:54:05.409 --> 00:54:08.409
or a safe belief or a warranted belief or

1069
00:54:08.409 --> 00:54:11.949
not? Um, I think those are all really interesting

1070
00:54:11.949 --> 00:54:13.489
questions. I mean, it's kind of my job to

1071
00:54:13.489 --> 00:54:15.870
answer those sorts of questions, but the, the book

1072
00:54:15.870 --> 00:54:19.389
is, is really not concerned about the proper way

1073
00:54:19.389 --> 00:54:21.659
of categorizing or, at the end of the day,

1074
00:54:21.669 --> 00:54:23.110
the book's not really concerned about the proper way

1075
00:54:23.110 --> 00:54:26.469
of categorizing our political beliefs. Um, SO like I

1076
00:54:26.469 --> 00:54:28.639
said at the very end of the book, there

1077
00:54:28.639 --> 00:54:32.010
are a handful of people in philosophy who defend

1078
00:54:32.010 --> 00:54:35.020
the view according to which knowledge is true belief.

1079
00:54:35.300 --> 00:54:37.899
And on this view, if you're right about something,

1080
00:54:38.189 --> 00:54:41.479
then you know. And so, if I, you know,

1081
00:54:41.600 --> 00:54:43.520
I, so somebody says, hey, should we finish the

1082
00:54:43.520 --> 00:54:45.239
border wall or not? And I get out a

1083
00:54:45.239 --> 00:54:47.760
coin and I say, OK, if it's heads, I'll

1084
00:54:47.760 --> 00:54:49.399
believe we should, and if it's tails, I'll believe

1085
00:54:49.399 --> 00:54:51.250
we shouldn't. And so I flip it and it

1086
00:54:51.250 --> 00:54:52.919
lands tails and on the base of my coin

1087
00:54:52.919 --> 00:54:55.840
toss, I believe we shouldn't finish it. According to

1088
00:54:55.840 --> 00:54:59.479
this view, if I'm right, as on the base

1089
00:54:59.479 --> 00:55:01.959
of my coin toss, then I know that we

1090
00:55:01.959 --> 00:55:05.320
shouldn't finish the wall. Very, very, very few epistemologists

1091
00:55:05.320 --> 00:55:06.760
accept that view, but it is a view that's

1092
00:55:06.760 --> 00:55:10.360
defended in the literature. Um. If that view is

1093
00:55:10.360 --> 00:55:14.159
correct, then everybody who's right about politics knows and

1094
00:55:14.159 --> 00:55:15.719
it's only the people who have the misfortune of

1095
00:55:15.719 --> 00:55:18.120
being wrong who don't know. And so probably there

1096
00:55:18.120 --> 00:55:19.439
are lots of people on that view who have

1097
00:55:19.439 --> 00:55:22.600
a lot of political knowledge. Um, BUT the thing

1098
00:55:22.600 --> 00:55:26.479
is, on that sort of view, you could know

1099
00:55:26.479 --> 00:55:28.840
something while being in a position where you shouldn't

1100
00:55:28.840 --> 00:55:32.260
even be remotely confident that you're right. And it's

1101
00:55:32.260 --> 00:55:34.580
really this this question of how confident we should

1102
00:55:34.580 --> 00:55:37.340
be that I'm primarily concerned about in the book.

1103
00:55:37.500 --> 00:55:40.739
So I think in just like a regular old

1104
00:55:40.739 --> 00:55:43.379
English, you can nicely express that you shouldn't be

1105
00:55:43.379 --> 00:55:46.689
confident in something by saying you don't know. Um,

1106
00:55:46.820 --> 00:55:48.820
BUT it's the confidence piece that I really care

1107
00:55:48.820 --> 00:55:50.820
about more than whether or not our beliefs can

1108
00:55:50.820 --> 00:55:53.699
be categorized as knowledge. What, what I really want

1109
00:55:53.699 --> 00:55:56.699
to get across in the book. Is that when

1110
00:55:56.699 --> 00:55:59.659
you look at all of the different things that

1111
00:55:59.659 --> 00:56:03.139
Republicans and Democrats argue about, um, I think at

1112
00:56:03.139 --> 00:56:06.899
least right now, we shouldn't be very confident in

1113
00:56:06.899 --> 00:56:09.820
our answers to those respective questions. Whether or not

1114
00:56:09.820 --> 00:56:11.530
our answers count as knowledge or not, the main

1115
00:56:11.530 --> 00:56:13.669
thing I care about is that we're not in

1116
00:56:13.669 --> 00:56:16.580
a position to be very confident in our answers.

1117
00:56:16.780 --> 00:56:19.300
So I guess, yeah, that's 11 sort of like

1118
00:56:19.300 --> 00:56:21.610
final reflection, I guess I'd like to add.

1119
00:56:22.899 --> 00:56:26.260
Right, so let's end on that reflection then and

1120
00:56:26.260 --> 00:56:29.780
the book is again Political Humility, The Limits of

1121
00:56:29.780 --> 00:56:33.500
Knowledge in our Partisan Political Climate. I'm leaving a

1122
00:56:33.500 --> 00:56:36.010
link to it in the description of the interview.

1123
00:56:36.300 --> 00:56:40.090
And Doctor Rober, just before we go apart from

1124
00:56:40.090 --> 00:56:42.379
the book, would you like to tell people where

1125
00:56:42.379 --> 00:56:44.780
they can find your work on the internet?

1126
00:56:45.899 --> 00:56:48.770
Oh, sure, yeah. Um, SO I guess the easiest

1127
00:56:48.770 --> 00:56:53.679
place would just be my personal website, um. Which

1128
00:56:53.679 --> 00:56:57.770
is. Oh gosh, what is the URL? It might

1129
00:56:57.770 --> 00:57:02.090
be Blake Raber.com. Uh, SO just my first name

1130
00:57:02.090 --> 00:57:04.919
and last name.com. It might not, it might not

1131
00:57:04.919 --> 00:57:07.330
be that. You can, so if you Google Notre

1132
00:57:07.330 --> 00:57:10.370
Dame's philosophy department, there'll be a link to my

1133
00:57:10.370 --> 00:57:13.719
website there. Um, IF you just Google my name,

1134
00:57:14.449 --> 00:57:16.889
you'll quickly get to either my website or the

1135
00:57:16.889 --> 00:57:20.409
link on the department. Um, AND I have So

1136
00:57:20.409 --> 00:57:23.050
all of my, I, I think 100% of my

1137
00:57:23.050 --> 00:57:27.530
publications are either available directly on my website or

1138
00:57:27.530 --> 00:57:30.090
links to those things like to the book on

1139
00:57:30.090 --> 00:57:33.000
Amazon or through Rutledge or something like that. Um,

1140
00:57:33.120 --> 00:57:35.270
SO that'd probably be the, yeah, the easiest way

1141
00:57:35.719 --> 00:57:37.830
to find it. I'm sort of embarrassed. I don't

1142
00:57:37.830 --> 00:57:38.310
know.

1143
00:57:39.350 --> 00:57:42.360
No, no, I, I will, I will find the

1144
00:57:42.360 --> 00:57:45.350
website and link, link it in the description as

1145
00:57:45.350 --> 00:57:45.629
well.

1146
00:57:45.750 --> 00:57:45.820
So thank you, no problem.

1147
00:57:48.000 --> 00:57:50.959
OK, so thank you so much for taking the

1148
00:57:50.959 --> 00:57:53.669
time to come on the show. It's been really

1149
00:57:53.790 --> 00:57:54.909
nice to talk with you.

1150
00:57:55.479 --> 00:57:57.000
Yeah, thank you so much for the interview. It

1151
00:57:57.000 --> 00:57:58.800
was really fun. Appreciate it.

1152
00:58:00.239 --> 00:58:02.719
Hi guys, thank you for watching this interview until

1153
00:58:02.719 --> 00:58:04.909
the end. If you liked it, please share it,

1154
00:58:05.080 --> 00:58:07.860
leave a like and hit the subscription button. The

1155
00:58:07.860 --> 00:58:10.060
show is brought to you by Nights Learning and

1156
00:58:10.060 --> 00:58:14.139
Development done differently, check their website at Nights.com and

1157
00:58:14.139 --> 00:58:17.860
also please consider supporting the show on Patreon or

1158
00:58:17.860 --> 00:58:20.340
PayPal. I would also like to give a huge

1159
00:58:20.340 --> 00:58:23.770
thank you to my main patrons and PayPal supporters

1160
00:58:23.770 --> 00:58:27.699
Pergo Larsson, Jerry Mullerns, Frederick Sundo, Bernard Seyche Olaf,

1161
00:58:27.780 --> 00:58:31.020
Alex Adam Castle, Matthew Whitting Barno, Wolf, Tim Hollis,

1162
00:58:31.120 --> 00:58:34.449
Erika Lenny, John Connors, Philip Fors Connolly. Then the

1163
00:58:34.449 --> 00:58:39.189
Mari Robert Windegaruyasi Zu Mark Nes called Holbrookfield governor

1164
00:58:39.189 --> 00:58:43.250
Michael Stormir, Samuel Andrea, Francis Forti Agnseroro and Hal

1165
00:58:43.250 --> 00:58:47.810
Herzognun Macha Joan Lays and the Samuel Corriere, Heinz,

1166
00:58:47.850 --> 00:58:51.489
Mark Smith, Jore, Tom Hummel, Sardus Fran David Sloan

1167
00:58:51.489 --> 00:58:55.800
Wilson, Asila dearraujoro and Roach Diego Londono Correa. Yannick

1168
00:58:55.800 --> 00:59:01.800
Punteran Rosmani Charlotte blinikol Barbara Adamhn Pavlostaevskynaleb medicine, Gary

1169
00:59:01.800 --> 00:59:06.760
Galman Samov Zaledrianei Poltonin John Barboza, Julian Price, Edward

1170
00:59:06.760 --> 00:59:11.629
Hall Edin Bronner, Douglas Fry, Franca Bartolotti Gabrielon Scorteseus

1171
00:59:11.629 --> 00:59:15.770
Slelisky, Scott Zachary Fish Tim Duffyani Smith John Wieman.

1172
00:59:16.120 --> 00:59:20.669
Daniel Friedman, William Buckner, Paul Georgianneau, Luke Lovai Giorgio

1173
00:59:20.669 --> 00:59:25.370
Theophanous, Chris Williamson, Peter Vozin, David Williams, the Augusta,

1174
00:59:25.479 --> 00:59:29.709
Anton Eriksson, Charles Murray, Alex Shaw, Marie Martinez, Coralli

1175
00:59:29.709 --> 00:59:34.080
Chevalier, bungalow atheists, Larry D. Lee Junior, Old Eringbo.

1176
00:59:34.830 --> 00:59:38.879
Sterry Michael Bailey, then Sperber, Robert Grassy Zigoren, Jeff

1177
00:59:38.879 --> 00:59:43.429
McMahon, Jake Zu, Barnabas radix, Mark Campbell, Thomas Dovner,

1178
00:59:43.550 --> 00:59:47.949
Luke Neeson, Chris Stor, Kimberly Johnson, Benjamin Galbert, Jessica

1179
00:59:47.949 --> 00:59:53.629
Nowicki, Linda Brandon, Nicholas Carlsson, Ismael Bensleyman. George Eoriatis,

1180
00:59:53.669 --> 00:59:59.459
Valentin Steinman, Perkrolis, Kate van Goller, Alexander Aubert, Liam

1181
00:59:59.729 --> 01:00:05.310
Dunaway, BR Masoud Ali Mohammadi, Perpendicular John Nertner, Ursula

1182
01:00:05.310 --> 01:00:10.070
Gudinov, Gregory Hastings, David Pinsoff Sean Nelson, Mike Levin,

1183
01:00:10.469 --> 01:00:13.649
and Jos Net. A special thanks to my producers.

1184
01:00:13.770 --> 01:00:16.629
These are Webb, Jim, Frank Lucas Steffinik, Tom Venneden,

1185
01:00:16.729 --> 01:00:21.250
Bernard Curtis Dixon, Benedict Muller, Thomas Trumbull, Catherine and

1186
01:00:21.250 --> 01:00:24.489
Patrick Tobin, Gian Carlo Montenegroal Ni Cortiz and Nick

1187
01:00:24.489 --> 01:00:28.010
Golden, and to my executive producers Matthew Lavender, Sergio

1188
01:00:28.010 --> 01:00:31.239
Quadrian, Bogdan Kanivets, and Rosie. Thank you for all.

